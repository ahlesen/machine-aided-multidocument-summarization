{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "f_zips = []\n",
    "for (dirpath, dirnames, filenames) in walk('../../gorc/'):\n",
    "    f_zips.extend(filenames)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002,\n",
       " ['0.jsonl.gz',\n",
       "  '1.jsonl.gz',\n",
       "  '10.jsonl.gz',\n",
       "  '100.jsonl.gz',\n",
       "  '1000.jsonl.gz',\n",
       "  '1001.jsonl.gz',\n",
       "  '1002.jsonl.gz',\n",
       "  '1003.jsonl.gz',\n",
       "  '1004.jsonl.gz',\n",
       "  '1005.jsonl.gz',\n",
       "  '1006.jsonl.gz',\n",
       "  '1007.jsonl.gz',\n",
       "  '1008.jsonl.gz',\n",
       "  '1009.jsonl.gz',\n",
       "  '101.jsonl.gz',\n",
       "  '1010.jsonl.gz',\n",
       "  '1011.jsonl.gz',\n",
       "  '1012.jsonl.gz',\n",
       "  '1013.jsonl.gz',\n",
       "  '1014.jsonl.gz',\n",
       "  '1015.jsonl.gz',\n",
       "  '1016.jsonl.gz',\n",
       "  '1017.jsonl.gz',\n",
       "  '1018.jsonl.gz',\n",
       "  '1019.jsonl.gz',\n",
       "  '102.jsonl.gz',\n",
       "  '1020.jsonl.gz',\n",
       "  '1021.jsonl.gz',\n",
       "  '1022.jsonl.gz',\n",
       "  '1023.jsonl.gz',\n",
       "  '1024.jsonl.gz',\n",
       "  '1025.jsonl.gz',\n",
       "  '1026.jsonl.gz',\n",
       "  '1027.jsonl.gz',\n",
       "  '1028.jsonl.gz',\n",
       "  '1029.jsonl.gz',\n",
       "  '103.jsonl.gz',\n",
       "  '1030.jsonl.gz',\n",
       "  '1031.jsonl.gz',\n",
       "  '1032.jsonl.gz',\n",
       "  '1033.jsonl.gz',\n",
       "  '1034.jsonl.gz',\n",
       "  '1035.jsonl.gz',\n",
       "  '1036.jsonl.gz',\n",
       "  '1037.jsonl.gz',\n",
       "  '1038.jsonl.gz',\n",
       "  '1039.jsonl.gz',\n",
       "  '104.jsonl.gz',\n",
       "  '1040.jsonl.gz',\n",
       "  '1041.jsonl.gz',\n",
       "  '1042.jsonl.gz',\n",
       "  '1043.jsonl.gz',\n",
       "  '1044.jsonl.gz',\n",
       "  '1045.jsonl.gz',\n",
       "  '1046.jsonl.gz',\n",
       "  '1047.jsonl.gz',\n",
       "  '1048.jsonl.gz',\n",
       "  '1049.jsonl.gz',\n",
       "  '105.jsonl.gz',\n",
       "  '1050.jsonl.gz',\n",
       "  '1051.jsonl.gz',\n",
       "  '1052.jsonl.gz',\n",
       "  '1053.jsonl.gz',\n",
       "  '1054.jsonl.gz',\n",
       "  '1055.jsonl.gz',\n",
       "  '1056.jsonl.gz',\n",
       "  '1057.jsonl.gz',\n",
       "  '1058.jsonl.gz',\n",
       "  '1059.jsonl.gz',\n",
       "  '106.jsonl.gz',\n",
       "  '1060.jsonl.gz',\n",
       "  '1061.jsonl.gz',\n",
       "  '1062.jsonl.gz',\n",
       "  '1063.jsonl.gz',\n",
       "  '1064.jsonl.gz',\n",
       "  '1065.jsonl.gz',\n",
       "  '1066.jsonl.gz',\n",
       "  '1067.jsonl.gz',\n",
       "  '1068.jsonl.gz',\n",
       "  '1069.jsonl.gz',\n",
       "  '107.jsonl.gz',\n",
       "  '1070.jsonl.gz',\n",
       "  '1071.jsonl.gz',\n",
       "  '1072.jsonl.gz',\n",
       "  '1073.jsonl.gz',\n",
       "  '1074.jsonl.gz',\n",
       "  '1075.jsonl.gz',\n",
       "  '1076.jsonl.gz',\n",
       "  '1077.jsonl.gz',\n",
       "  '1078.jsonl.gz',\n",
       "  '1079.jsonl.gz',\n",
       "  '108.jsonl.gz',\n",
       "  '1080.jsonl.gz',\n",
       "  '1081.jsonl.gz',\n",
       "  '1082.jsonl.gz',\n",
       "  '1083.jsonl.gz',\n",
       "  '1084.jsonl.gz',\n",
       "  '1085.jsonl.gz',\n",
       "  '1086.jsonl.gz',\n",
       "  '1087.jsonl.gz',\n",
       "  '1088.jsonl.gz',\n",
       "  '1089.jsonl.gz',\n",
       "  '109.jsonl.gz',\n",
       "  '1090.jsonl.gz',\n",
       "  '1091.jsonl.gz',\n",
       "  '1092.jsonl.gz',\n",
       "  '1093.jsonl.gz',\n",
       "  '1094.jsonl.gz',\n",
       "  '1095.jsonl.gz',\n",
       "  '1096.jsonl.gz',\n",
       "  '1097.jsonl.gz',\n",
       "  '1098.jsonl.gz',\n",
       "  '1099.jsonl.gz',\n",
       "  '11.jsonl.gz',\n",
       "  '110.jsonl.gz',\n",
       "  '1100.jsonl.gz',\n",
       "  '1101.jsonl.gz',\n",
       "  '1102.jsonl.gz',\n",
       "  '1103.jsonl.gz',\n",
       "  '1104.jsonl.gz',\n",
       "  '1105.jsonl.gz',\n",
       "  '1106.jsonl.gz',\n",
       "  '1107.jsonl.gz',\n",
       "  '1108.jsonl.gz',\n",
       "  '1109.jsonl.gz',\n",
       "  '111.jsonl.gz',\n",
       "  '1110.jsonl.gz',\n",
       "  '1111.jsonl.gz',\n",
       "  '1112.jsonl.gz',\n",
       "  '1113.jsonl.gz',\n",
       "  '1114.jsonl.gz',\n",
       "  '1115.jsonl.gz',\n",
       "  '1116.jsonl.gz',\n",
       "  '1117.jsonl.gz',\n",
       "  '1118.jsonl.gz',\n",
       "  '1119.jsonl.gz',\n",
       "  '112.jsonl.gz',\n",
       "  '1120.jsonl.gz',\n",
       "  '1121.jsonl.gz',\n",
       "  '1122.jsonl.gz',\n",
       "  '1123.jsonl.gz',\n",
       "  '1124.jsonl.gz',\n",
       "  '1125.jsonl.gz',\n",
       "  '1126.jsonl.gz',\n",
       "  '1127.jsonl.gz',\n",
       "  '1128.jsonl.gz',\n",
       "  '1129.jsonl.gz',\n",
       "  '113.jsonl.gz',\n",
       "  '1130.jsonl.gz',\n",
       "  '1131.jsonl.gz',\n",
       "  '1132.jsonl.gz',\n",
       "  '1133.jsonl.gz',\n",
       "  '1134.jsonl.gz',\n",
       "  '1135.jsonl.gz',\n",
       "  '1136.jsonl.gz',\n",
       "  '1137.jsonl.gz',\n",
       "  '1138.jsonl.gz',\n",
       "  '1139.jsonl.gz',\n",
       "  '114.jsonl.gz',\n",
       "  '1140.jsonl.gz',\n",
       "  '1141.jsonl.gz',\n",
       "  '1142.jsonl.gz',\n",
       "  '1143.jsonl.gz',\n",
       "  '1144.jsonl.gz',\n",
       "  '1145.jsonl.gz',\n",
       "  '1146.jsonl.gz',\n",
       "  '1147.jsonl.gz',\n",
       "  '1148.jsonl.gz',\n",
       "  '1149.jsonl.gz',\n",
       "  '115.jsonl.gz',\n",
       "  '1150.jsonl.gz',\n",
       "  '1151.jsonl.gz',\n",
       "  '1152.jsonl.gz',\n",
       "  '1153.jsonl.gz',\n",
       "  '1154.jsonl.gz',\n",
       "  '1155.jsonl.gz',\n",
       "  '1156.jsonl.gz',\n",
       "  '1157.jsonl.gz',\n",
       "  '1158.jsonl.gz',\n",
       "  '1159.jsonl.gz',\n",
       "  '116.jsonl.gz',\n",
       "  '1160.jsonl.gz',\n",
       "  '1161.jsonl.gz',\n",
       "  '1162.jsonl.gz',\n",
       "  '1163.jsonl.gz',\n",
       "  '1164.jsonl.gz',\n",
       "  '1165.jsonl.gz',\n",
       "  '1166.jsonl.gz',\n",
       "  '1167.jsonl.gz',\n",
       "  '1168.jsonl.gz',\n",
       "  '1169.jsonl.gz',\n",
       "  '117.jsonl.gz',\n",
       "  '1170.jsonl.gz',\n",
       "  '1171.jsonl.gz',\n",
       "  '1172.jsonl.gz',\n",
       "  '1173.jsonl.gz',\n",
       "  '1174.jsonl.gz',\n",
       "  '1175.jsonl.gz',\n",
       "  '1176.jsonl.gz',\n",
       "  '1177.jsonl.gz',\n",
       "  '1178.jsonl.gz',\n",
       "  '1179.jsonl.gz',\n",
       "  '118.jsonl.gz',\n",
       "  '1180.jsonl.gz',\n",
       "  '1181.jsonl.gz',\n",
       "  '1182.jsonl.gz',\n",
       "  '1183.jsonl.gz',\n",
       "  '1184.jsonl.gz',\n",
       "  '1185.jsonl.gz',\n",
       "  '1186.jsonl.gz',\n",
       "  '1187.jsonl.gz',\n",
       "  '1188.jsonl.gz',\n",
       "  '1189.jsonl.gz',\n",
       "  '119.jsonl.gz',\n",
       "  '1190.jsonl.gz',\n",
       "  '1191.jsonl.gz',\n",
       "  '1192.jsonl.gz',\n",
       "  '1193.jsonl.gz',\n",
       "  '1194.jsonl.gz',\n",
       "  '1195.jsonl.gz',\n",
       "  '1196.jsonl.gz',\n",
       "  '1197.jsonl.gz',\n",
       "  '1198.jsonl.gz',\n",
       "  '1199.jsonl.gz',\n",
       "  '12.jsonl.gz',\n",
       "  '120.jsonl.gz',\n",
       "  '1200.jsonl.gz',\n",
       "  '1201.jsonl.gz',\n",
       "  '1202.jsonl.gz',\n",
       "  '1203.jsonl.gz',\n",
       "  '1204.jsonl.gz',\n",
       "  '1205.jsonl.gz',\n",
       "  '1206.jsonl.gz',\n",
       "  '1207.jsonl.gz',\n",
       "  '1208.jsonl.gz',\n",
       "  '1209.jsonl.gz',\n",
       "  '121.jsonl.gz',\n",
       "  '1210.jsonl.gz',\n",
       "  '1211.jsonl.gz',\n",
       "  '1212.jsonl.gz',\n",
       "  '1213.jsonl.gz',\n",
       "  '1214.jsonl.gz',\n",
       "  '1215.jsonl.gz',\n",
       "  '1216.jsonl.gz',\n",
       "  '1217.jsonl.gz',\n",
       "  '1218.jsonl.gz',\n",
       "  '1219.jsonl.gz',\n",
       "  '122.jsonl.gz',\n",
       "  '1220.jsonl.gz',\n",
       "  '1221.jsonl.gz',\n",
       "  '1222.jsonl.gz',\n",
       "  '1223.jsonl.gz',\n",
       "  '1224.jsonl.gz',\n",
       "  '1225.jsonl.gz',\n",
       "  '1226.jsonl.gz',\n",
       "  '1227.jsonl.gz',\n",
       "  '1228.jsonl.gz',\n",
       "  '1229.jsonl.gz',\n",
       "  '123.jsonl.gz',\n",
       "  '1230.jsonl.gz',\n",
       "  '1231.jsonl.gz',\n",
       "  '1232.jsonl.gz',\n",
       "  '1233.jsonl.gz',\n",
       "  '1234.jsonl.gz',\n",
       "  '1235.jsonl.gz',\n",
       "  '1236.jsonl.gz',\n",
       "  '1237.jsonl.gz',\n",
       "  '1238.jsonl.gz',\n",
       "  '1239.jsonl.gz',\n",
       "  '124.jsonl.gz',\n",
       "  '1240.jsonl.gz',\n",
       "  '1241.jsonl.gz',\n",
       "  '1242.jsonl.gz',\n",
       "  '1243.jsonl.gz',\n",
       "  '1244.jsonl.gz',\n",
       "  '1245.jsonl.gz',\n",
       "  '1246.jsonl.gz',\n",
       "  '1247.jsonl.gz',\n",
       "  '1248.jsonl.gz',\n",
       "  '1249.jsonl.gz',\n",
       "  '125.jsonl.gz',\n",
       "  '1250.jsonl.gz',\n",
       "  '1251.jsonl.gz',\n",
       "  '1252.jsonl.gz',\n",
       "  '1253.jsonl.gz',\n",
       "  '1254.jsonl.gz',\n",
       "  '1255.jsonl.gz',\n",
       "  '1256.jsonl.gz',\n",
       "  '1257.jsonl.gz',\n",
       "  '1258.jsonl.gz',\n",
       "  '1259.jsonl.gz',\n",
       "  '126.jsonl.gz',\n",
       "  '1260.jsonl.gz',\n",
       "  '1261.jsonl.gz',\n",
       "  '1262.jsonl.gz',\n",
       "  '1263.jsonl.gz',\n",
       "  '1264.jsonl.gz',\n",
       "  '1265.jsonl.gz',\n",
       "  '1266.jsonl.gz',\n",
       "  '1267.jsonl.gz',\n",
       "  '1268.jsonl.gz',\n",
       "  '1269.jsonl.gz',\n",
       "  '127.jsonl.gz',\n",
       "  '1270.jsonl.gz',\n",
       "  '1271.jsonl.gz',\n",
       "  '1272.jsonl.gz',\n",
       "  '1273.jsonl.gz',\n",
       "  '1274.jsonl.gz',\n",
       "  '1275.jsonl.gz',\n",
       "  '1276.jsonl.gz',\n",
       "  '1277.jsonl.gz',\n",
       "  '1278.jsonl.gz',\n",
       "  '1279.jsonl.gz',\n",
       "  '128.jsonl.gz',\n",
       "  '1280.jsonl.gz',\n",
       "  '1281.jsonl.gz',\n",
       "  '1282.jsonl.gz',\n",
       "  '1283.jsonl.gz',\n",
       "  '1284.jsonl.gz',\n",
       "  '1285.jsonl.gz',\n",
       "  '1286.jsonl.gz',\n",
       "  '1287.jsonl.gz',\n",
       "  '1288.jsonl.gz',\n",
       "  '1289.jsonl.gz',\n",
       "  '129.jsonl.gz',\n",
       "  '1290.jsonl.gz',\n",
       "  '1291.jsonl.gz',\n",
       "  '1292.jsonl.gz',\n",
       "  '1293.jsonl.gz',\n",
       "  '1294.jsonl.gz',\n",
       "  '1295.jsonl.gz',\n",
       "  '1296.jsonl.gz',\n",
       "  '1297.jsonl.gz',\n",
       "  '1298.jsonl.gz',\n",
       "  '1299.jsonl.gz',\n",
       "  '13.jsonl.gz',\n",
       "  '130.jsonl.gz',\n",
       "  '1300.jsonl.gz',\n",
       "  '1301.jsonl.gz',\n",
       "  '1302.jsonl.gz',\n",
       "  '1303.jsonl.gz',\n",
       "  '1304.jsonl.gz',\n",
       "  '1305.jsonl.gz',\n",
       "  '1306.jsonl.gz',\n",
       "  '1307.jsonl.gz',\n",
       "  '1308.jsonl.gz',\n",
       "  '1309.jsonl.gz',\n",
       "  '131.jsonl.gz',\n",
       "  '1310.jsonl.gz',\n",
       "  '1311.jsonl.gz',\n",
       "  '1312.jsonl.gz',\n",
       "  '1313.jsonl.gz',\n",
       "  '1314.jsonl.gz',\n",
       "  '1315.jsonl.gz',\n",
       "  '1316.jsonl.gz',\n",
       "  '1317.jsonl.gz',\n",
       "  '1318.jsonl.gz',\n",
       "  '1319.jsonl.gz',\n",
       "  '132.jsonl.gz',\n",
       "  '1320.jsonl.gz',\n",
       "  '1321.jsonl.gz',\n",
       "  '1322.jsonl.gz',\n",
       "  '1323.jsonl.gz',\n",
       "  '1324.jsonl.gz',\n",
       "  '1325.jsonl.gz',\n",
       "  '1326.jsonl.gz',\n",
       "  '1327.jsonl.gz',\n",
       "  '1328.jsonl.gz',\n",
       "  '1329.jsonl.gz',\n",
       "  '133.jsonl.gz',\n",
       "  '1330.jsonl.gz',\n",
       "  '1331.jsonl.gz',\n",
       "  '1332.jsonl.gz',\n",
       "  '1333.jsonl.gz',\n",
       "  '1334.jsonl.gz',\n",
       "  '1335.jsonl.gz',\n",
       "  '1336.jsonl.gz',\n",
       "  '1337.jsonl.gz',\n",
       "  '1338.jsonl.gz',\n",
       "  '1339.jsonl.gz',\n",
       "  '134.jsonl.gz',\n",
       "  '1340.jsonl.gz',\n",
       "  '1341.jsonl.gz',\n",
       "  '1342.jsonl.gz',\n",
       "  '1343.jsonl.gz',\n",
       "  '1344.jsonl.gz',\n",
       "  '1345.jsonl.gz',\n",
       "  '1346.jsonl.gz',\n",
       "  '1347.jsonl.gz',\n",
       "  '1348.jsonl.gz',\n",
       "  '1349.jsonl.gz',\n",
       "  '135.jsonl.gz',\n",
       "  '1350.jsonl.gz',\n",
       "  '1351.jsonl.gz',\n",
       "  '1352.jsonl.gz',\n",
       "  '1353.jsonl.gz',\n",
       "  '1354.jsonl.gz',\n",
       "  '1355.jsonl.gz',\n",
       "  '1356.jsonl.gz',\n",
       "  '1357.jsonl.gz',\n",
       "  '1358.jsonl.gz',\n",
       "  '1359.jsonl.gz',\n",
       "  '136.jsonl.gz',\n",
       "  '1360.jsonl.gz',\n",
       "  '1361.jsonl.gz',\n",
       "  '1362.jsonl.gz',\n",
       "  '1363.jsonl.gz',\n",
       "  '1364.jsonl.gz',\n",
       "  '1365.jsonl.gz',\n",
       "  '1366.jsonl.gz',\n",
       "  '1367.jsonl.gz',\n",
       "  '1368.jsonl.gz',\n",
       "  '1369.jsonl.gz',\n",
       "  '137.jsonl.gz',\n",
       "  '1370.jsonl.gz',\n",
       "  '1371.jsonl.gz',\n",
       "  '1372.jsonl.gz',\n",
       "  '1373.jsonl.gz',\n",
       "  '1374.jsonl.gz',\n",
       "  '1375.jsonl.gz',\n",
       "  '1376.jsonl.gz',\n",
       "  '1377.jsonl.gz',\n",
       "  '1378.jsonl.gz',\n",
       "  '1379.jsonl.gz',\n",
       "  '138.jsonl.gz',\n",
       "  '1380.jsonl.gz',\n",
       "  '1381.jsonl.gz',\n",
       "  '1382.jsonl.gz',\n",
       "  '1383.jsonl.gz',\n",
       "  '1384.jsonl.gz',\n",
       "  '1385.jsonl.gz',\n",
       "  '1386.jsonl.gz',\n",
       "  '1387.jsonl.gz',\n",
       "  '1388.jsonl.gz',\n",
       "  '1389.jsonl.gz',\n",
       "  '139.jsonl.gz',\n",
       "  '1390.jsonl.gz',\n",
       "  '1391.jsonl.gz',\n",
       "  '1392.jsonl.gz',\n",
       "  '1393.jsonl.gz',\n",
       "  '1394.jsonl.gz',\n",
       "  '1395.jsonl.gz',\n",
       "  '1396.jsonl.gz',\n",
       "  '1397.jsonl.gz',\n",
       "  '1398.jsonl.gz',\n",
       "  '1399.jsonl.gz',\n",
       "  '14.jsonl.gz',\n",
       "  '140.jsonl.gz',\n",
       "  '1400.jsonl.gz',\n",
       "  '1401.jsonl.gz',\n",
       "  '1402.jsonl.gz',\n",
       "  '1403.jsonl.gz',\n",
       "  '1404.jsonl.gz',\n",
       "  '1405.jsonl.gz',\n",
       "  '1406.jsonl.gz',\n",
       "  '1407.jsonl.gz',\n",
       "  '1408.jsonl.gz',\n",
       "  '1409.jsonl.gz',\n",
       "  '141.jsonl.gz',\n",
       "  '1410.jsonl.gz',\n",
       "  '1411.jsonl.gz',\n",
       "  '1412.jsonl.gz',\n",
       "  '1413.jsonl.gz',\n",
       "  '1414.jsonl.gz',\n",
       "  '1415.jsonl.gz',\n",
       "  '1416.jsonl.gz',\n",
       "  '1417.jsonl.gz',\n",
       "  '1418.jsonl.gz',\n",
       "  '1419.jsonl.gz',\n",
       "  '142.jsonl.gz',\n",
       "  '1420.jsonl.gz',\n",
       "  '1421.jsonl.gz',\n",
       "  '1422.jsonl.gz',\n",
       "  '1423.jsonl.gz',\n",
       "  '1424.jsonl.gz',\n",
       "  '1425.jsonl.gz',\n",
       "  '1426.jsonl.gz',\n",
       "  '1427.jsonl.gz',\n",
       "  '1428.jsonl.gz',\n",
       "  '1429.jsonl.gz',\n",
       "  '143.jsonl.gz',\n",
       "  '1430.jsonl.gz',\n",
       "  '1431.jsonl.gz',\n",
       "  '1432.jsonl.gz',\n",
       "  '1433.jsonl.gz',\n",
       "  '1434.jsonl.gz',\n",
       "  '1435.jsonl.gz',\n",
       "  '1436.jsonl.gz',\n",
       "  '1437.jsonl.gz',\n",
       "  '1438.jsonl.gz',\n",
       "  '1439.jsonl.gz',\n",
       "  '144.jsonl.gz',\n",
       "  '1440.jsonl.gz',\n",
       "  '1441.jsonl.gz',\n",
       "  '1442.jsonl.gz',\n",
       "  '1443.jsonl.gz',\n",
       "  '1444.jsonl.gz',\n",
       "  '1445.jsonl.gz',\n",
       "  '1446.jsonl.gz',\n",
       "  '1447.jsonl.gz',\n",
       "  '1448.jsonl.gz',\n",
       "  '1449.jsonl.gz',\n",
       "  '145.jsonl.gz',\n",
       "  '1450.jsonl.gz',\n",
       "  '1451.jsonl.gz',\n",
       "  '1452.jsonl.gz',\n",
       "  '1453.jsonl.gz',\n",
       "  '1454.jsonl.gz',\n",
       "  '1455.jsonl.gz',\n",
       "  '1456.jsonl.gz',\n",
       "  '1457.jsonl.gz',\n",
       "  '1458.jsonl.gz',\n",
       "  '1459.jsonl.gz',\n",
       "  '146.jsonl.gz',\n",
       "  '1460.jsonl.gz',\n",
       "  '1461.jsonl.gz',\n",
       "  '1462.jsonl.gz',\n",
       "  '1463.jsonl.gz',\n",
       "  '1464.jsonl.gz',\n",
       "  '1465.jsonl.gz',\n",
       "  '1466.jsonl.gz',\n",
       "  '1467.jsonl.gz',\n",
       "  '1468.jsonl.gz',\n",
       "  '1469.jsonl.gz',\n",
       "  '147.jsonl.gz',\n",
       "  '1470.jsonl.gz',\n",
       "  '1471.jsonl.gz',\n",
       "  '1472.jsonl.gz',\n",
       "  '1473.jsonl.gz',\n",
       "  '1474.jsonl.gz',\n",
       "  '1475.jsonl.gz',\n",
       "  '1476.jsonl.gz',\n",
       "  '1477.jsonl.gz',\n",
       "  '1478.jsonl.gz',\n",
       "  '1479.jsonl.gz',\n",
       "  '148.jsonl.gz',\n",
       "  '1480.jsonl.gz',\n",
       "  '1481.jsonl.gz',\n",
       "  '1482.jsonl.gz',\n",
       "  '1483.jsonl.gz',\n",
       "  '1484.jsonl.gz',\n",
       "  '1485.jsonl.gz',\n",
       "  '1486.jsonl.gz',\n",
       "  '1487.jsonl.gz',\n",
       "  '1488.jsonl.gz',\n",
       "  '1489.jsonl.gz',\n",
       "  '149.jsonl.gz',\n",
       "  '1490.jsonl.gz',\n",
       "  '1491.jsonl.gz',\n",
       "  '1492.jsonl.gz',\n",
       "  '1493.jsonl.gz',\n",
       "  '1494.jsonl.gz',\n",
       "  '1495.jsonl.gz',\n",
       "  '1496.jsonl.gz',\n",
       "  '1497.jsonl.gz',\n",
       "  '1498.jsonl.gz',\n",
       "  '1499.jsonl.gz',\n",
       "  '15.jsonl.gz',\n",
       "  '150.jsonl.gz',\n",
       "  '1500.jsonl.gz',\n",
       "  '1501.jsonl.gz',\n",
       "  '1502.jsonl.gz',\n",
       "  '1503.jsonl.gz',\n",
       "  '1504.jsonl.gz',\n",
       "  '1505.jsonl.gz',\n",
       "  '1506.jsonl.gz',\n",
       "  '1507.jsonl.gz',\n",
       "  '1508.jsonl.gz',\n",
       "  '1509.jsonl.gz',\n",
       "  '151.jsonl.gz',\n",
       "  '1510.jsonl.gz',\n",
       "  '1511.jsonl.gz',\n",
       "  '1512.jsonl.gz',\n",
       "  '1513.jsonl.gz',\n",
       "  '1514.jsonl.gz',\n",
       "  '1515.jsonl.gz',\n",
       "  '1516.jsonl.gz',\n",
       "  '1517.jsonl.gz',\n",
       "  '1518.jsonl.gz',\n",
       "  '1519.jsonl.gz',\n",
       "  '152.jsonl.gz',\n",
       "  '1520.jsonl.gz',\n",
       "  '1521.jsonl.gz',\n",
       "  '1522.jsonl.gz',\n",
       "  '1523.jsonl.gz',\n",
       "  '1524.jsonl.gz',\n",
       "  '1525.jsonl.gz',\n",
       "  '1526.jsonl.gz',\n",
       "  '1527.jsonl.gz',\n",
       "  '1528.jsonl.gz',\n",
       "  '1529.jsonl.gz',\n",
       "  '153.jsonl.gz',\n",
       "  '1530.jsonl.gz',\n",
       "  '1531.jsonl.gz',\n",
       "  '1532.jsonl.gz',\n",
       "  '1533.jsonl.gz',\n",
       "  '1534.jsonl.gz',\n",
       "  '1535.jsonl.gz',\n",
       "  '1536.jsonl.gz',\n",
       "  '1537.jsonl.gz',\n",
       "  '1538.jsonl.gz',\n",
       "  '1539.jsonl.gz',\n",
       "  '154.jsonl.gz',\n",
       "  '1540.jsonl.gz',\n",
       "  '1541.jsonl.gz',\n",
       "  '1542.jsonl.gz',\n",
       "  '1543.jsonl.gz',\n",
       "  '1544.jsonl.gz',\n",
       "  '1545.jsonl.gz',\n",
       "  '1546.jsonl.gz',\n",
       "  '1547.jsonl.gz',\n",
       "  '1548.jsonl.gz',\n",
       "  '1549.jsonl.gz',\n",
       "  '155.jsonl.gz',\n",
       "  '1550.jsonl.gz',\n",
       "  '1551.jsonl.gz',\n",
       "  '1552.jsonl.gz',\n",
       "  '1553.jsonl.gz',\n",
       "  '1554.jsonl.gz',\n",
       "  '1555.jsonl.gz',\n",
       "  '1556.jsonl.gz',\n",
       "  '1557.jsonl.gz',\n",
       "  '1558.jsonl.gz',\n",
       "  '1559.jsonl.gz',\n",
       "  '156.jsonl.gz',\n",
       "  '1560.jsonl.gz',\n",
       "  '1561.jsonl.gz',\n",
       "  '1562.jsonl.gz',\n",
       "  '1563.jsonl.gz',\n",
       "  '1564.jsonl.gz',\n",
       "  '1565.jsonl.gz',\n",
       "  '1566.jsonl.gz',\n",
       "  '1567.jsonl.gz',\n",
       "  '1568.jsonl.gz',\n",
       "  '1569.jsonl.gz',\n",
       "  '157.jsonl.gz',\n",
       "  '1570.jsonl.gz',\n",
       "  '1571.jsonl.gz',\n",
       "  '1572.jsonl.gz',\n",
       "  '1573.jsonl.gz',\n",
       "  '1574.jsonl.gz',\n",
       "  '1575.jsonl.gz',\n",
       "  '1576.jsonl.gz',\n",
       "  '1577.jsonl.gz',\n",
       "  '1578.jsonl.gz',\n",
       "  '1579.jsonl.gz',\n",
       "  '158.jsonl.gz',\n",
       "  '1580.jsonl.gz',\n",
       "  '1581.jsonl.gz',\n",
       "  '1582.jsonl.gz',\n",
       "  '1583.jsonl.gz',\n",
       "  '1584.jsonl.gz',\n",
       "  '1585.jsonl.gz',\n",
       "  '1586.jsonl.gz',\n",
       "  '1587.jsonl.gz',\n",
       "  '1588.jsonl.gz',\n",
       "  '1589.jsonl.gz',\n",
       "  '159.jsonl.gz',\n",
       "  '1590.jsonl.gz',\n",
       "  '1591.jsonl.gz',\n",
       "  '1592.jsonl.gz',\n",
       "  '1593.jsonl.gz',\n",
       "  '1594.jsonl.gz',\n",
       "  '1595.jsonl.gz',\n",
       "  '1596.jsonl.gz',\n",
       "  '1597.jsonl.gz',\n",
       "  '1598.jsonl.gz',\n",
       "  '1599.jsonl.gz',\n",
       "  '16.jsonl.gz',\n",
       "  '160.jsonl.gz',\n",
       "  '1600.jsonl.gz',\n",
       "  '1601.jsonl.gz',\n",
       "  '1602.jsonl.gz',\n",
       "  '1603.jsonl.gz',\n",
       "  '1604.jsonl.gz',\n",
       "  '1605.jsonl.gz',\n",
       "  '1606.jsonl.gz',\n",
       "  '1607.jsonl.gz',\n",
       "  '1608.jsonl.gz',\n",
       "  '1609.jsonl.gz',\n",
       "  '161.jsonl.gz',\n",
       "  '1610.jsonl.gz',\n",
       "  '1611.jsonl.gz',\n",
       "  '1612.jsonl.gz',\n",
       "  '1613.jsonl.gz',\n",
       "  '1614.jsonl.gz',\n",
       "  '1615.jsonl.gz',\n",
       "  '1616.jsonl.gz',\n",
       "  '1617.jsonl.gz',\n",
       "  '1618.jsonl.gz',\n",
       "  '1619.jsonl.gz',\n",
       "  '162.jsonl.gz',\n",
       "  '1620.jsonl.gz',\n",
       "  '1621.jsonl.gz',\n",
       "  '1622.jsonl.gz',\n",
       "  '1623.jsonl.gz',\n",
       "  '1624.jsonl.gz',\n",
       "  '1625.jsonl.gz',\n",
       "  '1626.jsonl.gz',\n",
       "  '1627.jsonl.gz',\n",
       "  '1628.jsonl.gz',\n",
       "  '1629.jsonl.gz',\n",
       "  '163.jsonl.gz',\n",
       "  '1630.jsonl.gz',\n",
       "  '1631.jsonl.gz',\n",
       "  '1632.jsonl.gz',\n",
       "  '1633.jsonl.gz',\n",
       "  '1634.jsonl.gz',\n",
       "  '1635.jsonl.gz',\n",
       "  '1636.jsonl.gz',\n",
       "  '1637.jsonl.gz',\n",
       "  '1638.jsonl.gz',\n",
       "  '1639.jsonl.gz',\n",
       "  '164.jsonl.gz',\n",
       "  '1640.jsonl.gz',\n",
       "  '1641.jsonl.gz',\n",
       "  '1642.jsonl.gz',\n",
       "  '1643.jsonl.gz',\n",
       "  '1644.jsonl.gz',\n",
       "  '1645.jsonl.gz',\n",
       "  '1646.jsonl.gz',\n",
       "  '1647.jsonl.gz',\n",
       "  '1648.jsonl.gz',\n",
       "  '1649.jsonl.gz',\n",
       "  '165.jsonl.gz',\n",
       "  '1650.jsonl.gz',\n",
       "  '1651.jsonl.gz',\n",
       "  '1652.jsonl.gz',\n",
       "  '1653.jsonl.gz',\n",
       "  '1654.jsonl.gz',\n",
       "  '1655.jsonl.gz',\n",
       "  '1656.jsonl.gz',\n",
       "  '1657.jsonl.gz',\n",
       "  '1658.jsonl.gz',\n",
       "  '1659.jsonl.gz',\n",
       "  '166.jsonl.gz',\n",
       "  '1660.jsonl.gz',\n",
       "  '1661.jsonl.gz',\n",
       "  '1662.jsonl.gz',\n",
       "  '1663.jsonl.gz',\n",
       "  '1664.jsonl.gz',\n",
       "  '1665.jsonl.gz',\n",
       "  '1666.jsonl.gz',\n",
       "  '1667.jsonl.gz',\n",
       "  '1668.jsonl.gz',\n",
       "  '1669.jsonl.gz',\n",
       "  '167.jsonl.gz',\n",
       "  '1670.jsonl.gz',\n",
       "  '1671.jsonl.gz',\n",
       "  '1672.jsonl.gz',\n",
       "  '1673.jsonl.gz',\n",
       "  '1674.jsonl.gz',\n",
       "  '1675.jsonl.gz',\n",
       "  '1676.jsonl.gz',\n",
       "  '1677.jsonl.gz',\n",
       "  '1678.jsonl.gz',\n",
       "  '1679.jsonl.gz',\n",
       "  '168.jsonl.gz',\n",
       "  '1680.jsonl.gz',\n",
       "  '1681.jsonl.gz',\n",
       "  '1682.jsonl.gz',\n",
       "  '1683.jsonl.gz',\n",
       "  '1684.jsonl.gz',\n",
       "  '1685.jsonl.gz',\n",
       "  '1686.jsonl.gz',\n",
       "  '1687.jsonl.gz',\n",
       "  '1688.jsonl.gz',\n",
       "  '1689.jsonl.gz',\n",
       "  '169.jsonl.gz',\n",
       "  '1690.jsonl.gz',\n",
       "  '1691.jsonl.gz',\n",
       "  '1692.jsonl.gz',\n",
       "  '1693.jsonl.gz',\n",
       "  '1694.jsonl.gz',\n",
       "  '1695.jsonl.gz',\n",
       "  '1696.jsonl.gz',\n",
       "  '1697.jsonl.gz',\n",
       "  '1698.jsonl.gz',\n",
       "  '1699.jsonl.gz',\n",
       "  '17.jsonl.gz',\n",
       "  '170.jsonl.gz',\n",
       "  '1700.jsonl.gz',\n",
       "  '1701.jsonl.gz',\n",
       "  '1702.jsonl.gz',\n",
       "  '1703.jsonl.gz',\n",
       "  '1704.jsonl.gz',\n",
       "  '1705.jsonl.gz',\n",
       "  '1706.jsonl.gz',\n",
       "  '1707.jsonl.gz',\n",
       "  '1708.jsonl.gz',\n",
       "  '1709.jsonl.gz',\n",
       "  '171.jsonl.gz',\n",
       "  '1710.jsonl.gz',\n",
       "  '1711.jsonl.gz',\n",
       "  '1712.jsonl.gz',\n",
       "  '1713.jsonl.gz',\n",
       "  '1714.jsonl.gz',\n",
       "  '1715.jsonl.gz',\n",
       "  '1716.jsonl.gz',\n",
       "  '1717.jsonl.gz',\n",
       "  '1718.jsonl.gz',\n",
       "  '1719.jsonl.gz',\n",
       "  '172.jsonl.gz',\n",
       "  '1720.jsonl.gz',\n",
       "  '1721.jsonl.gz',\n",
       "  '1722.jsonl.gz',\n",
       "  '1723.jsonl.gz',\n",
       "  '1724.jsonl.gz',\n",
       "  '1725.jsonl.gz',\n",
       "  '1726.jsonl.gz',\n",
       "  '1727.jsonl.gz',\n",
       "  '1728.jsonl.gz',\n",
       "  '1729.jsonl.gz',\n",
       "  '173.jsonl.gz',\n",
       "  '1730.jsonl.gz',\n",
       "  '1731.jsonl.gz',\n",
       "  '1732.jsonl.gz',\n",
       "  '1733.jsonl.gz',\n",
       "  '1734.jsonl.gz',\n",
       "  '1735.jsonl.gz',\n",
       "  '1736.jsonl.gz',\n",
       "  '1737.jsonl.gz',\n",
       "  '1738.jsonl.gz',\n",
       "  '1739.jsonl.gz',\n",
       "  '174.jsonl.gz',\n",
       "  '1740.jsonl.gz',\n",
       "  '1741.jsonl.gz',\n",
       "  '1742.jsonl.gz',\n",
       "  '1743.jsonl.gz',\n",
       "  '1744.jsonl.gz',\n",
       "  '1745.jsonl.gz',\n",
       "  '1746.jsonl.gz',\n",
       "  '1747.jsonl.gz',\n",
       "  '1748.jsonl.gz',\n",
       "  '1749.jsonl.gz',\n",
       "  '175.jsonl.gz',\n",
       "  '1750.jsonl.gz',\n",
       "  '1751.jsonl.gz',\n",
       "  '1752.jsonl.gz',\n",
       "  '1753.jsonl.gz',\n",
       "  '1754.jsonl.gz',\n",
       "  '1755.jsonl.gz',\n",
       "  '1756.jsonl.gz',\n",
       "  '1757.jsonl.gz',\n",
       "  '1758.jsonl.gz',\n",
       "  '1759.jsonl.gz',\n",
       "  '176.jsonl.gz',\n",
       "  '1760.jsonl.gz',\n",
       "  '1761.jsonl.gz',\n",
       "  '1762.jsonl.gz',\n",
       "  '1763.jsonl.gz',\n",
       "  '1764.jsonl.gz',\n",
       "  '1765.jsonl.gz',\n",
       "  '1766.jsonl.gz',\n",
       "  '1767.jsonl.gz',\n",
       "  '1768.jsonl.gz',\n",
       "  '1769.jsonl.gz',\n",
       "  '177.jsonl.gz',\n",
       "  '1770.jsonl.gz',\n",
       "  '1771.jsonl.gz',\n",
       "  '1772.jsonl.gz',\n",
       "  '1773.jsonl.gz',\n",
       "  '1774.jsonl.gz',\n",
       "  '1775.jsonl.gz',\n",
       "  '1776.jsonl.gz',\n",
       "  '1777.jsonl.gz',\n",
       "  '1778.jsonl.gz',\n",
       "  '1779.jsonl.gz',\n",
       "  '178.jsonl.gz',\n",
       "  '1780.jsonl.gz',\n",
       "  '1781.jsonl.gz',\n",
       "  '1782.jsonl.gz',\n",
       "  '1783.jsonl.gz',\n",
       "  '1784.jsonl.gz',\n",
       "  '1785.jsonl.gz',\n",
       "  '1786.jsonl.gz',\n",
       "  '1787.jsonl.gz',\n",
       "  '1788.jsonl.gz',\n",
       "  '1789.jsonl.gz',\n",
       "  '179.jsonl.gz',\n",
       "  '1790.jsonl.gz',\n",
       "  '1791.jsonl.gz',\n",
       "  '1792.jsonl.gz',\n",
       "  '1793.jsonl.gz',\n",
       "  '1794.jsonl.gz',\n",
       "  '1795.jsonl.gz',\n",
       "  '1796.jsonl.gz',\n",
       "  '1797.jsonl.gz',\n",
       "  '1798.jsonl.gz',\n",
       "  '1799.jsonl.gz',\n",
       "  '18.jsonl.gz',\n",
       "  '180.jsonl.gz',\n",
       "  '1800.jsonl.gz',\n",
       "  '1801.jsonl.gz',\n",
       "  '1802.jsonl.gz',\n",
       "  '1803.jsonl.gz',\n",
       "  '1804.jsonl.gz',\n",
       "  '1805.jsonl.gz',\n",
       "  '1806.jsonl.gz',\n",
       "  '1807.jsonl.gz',\n",
       "  '1808.jsonl.gz',\n",
       "  '1809.jsonl.gz',\n",
       "  '181.jsonl.gz',\n",
       "  '1810.jsonl.gz',\n",
       "  '1811.jsonl.gz',\n",
       "  '1812.jsonl.gz',\n",
       "  '1813.jsonl.gz',\n",
       "  '1814.jsonl.gz',\n",
       "  '1815.jsonl.gz',\n",
       "  '1816.jsonl.gz',\n",
       "  '1817.jsonl.gz',\n",
       "  '1818.jsonl.gz',\n",
       "  '1819.jsonl.gz',\n",
       "  '182.jsonl.gz',\n",
       "  '1820.jsonl.gz',\n",
       "  '1821.jsonl.gz',\n",
       "  '1822.jsonl.gz',\n",
       "  '1823.jsonl.gz',\n",
       "  '1824.jsonl.gz',\n",
       "  '1825.jsonl.gz',\n",
       "  '1826.jsonl.gz',\n",
       "  '1827.jsonl.gz',\n",
       "  '1828.jsonl.gz',\n",
       "  '1829.jsonl.gz',\n",
       "  '183.jsonl.gz',\n",
       "  '1830.jsonl.gz',\n",
       "  '1831.jsonl.gz',\n",
       "  '1832.jsonl.gz',\n",
       "  '1833.jsonl.gz',\n",
       "  '1834.jsonl.gz',\n",
       "  '1835.jsonl.gz',\n",
       "  '1836.jsonl.gz',\n",
       "  '1837.jsonl.gz',\n",
       "  '1838.jsonl.gz',\n",
       "  '1839.jsonl.gz',\n",
       "  '184.jsonl.gz',\n",
       "  '1840.jsonl.gz',\n",
       "  '1841.jsonl.gz',\n",
       "  '1842.jsonl.gz',\n",
       "  '1843.jsonl.gz',\n",
       "  '1844.jsonl.gz',\n",
       "  '1845.jsonl.gz',\n",
       "  '1846.jsonl.gz',\n",
       "  '1847.jsonl.gz',\n",
       "  '1848.jsonl.gz',\n",
       "  '1849.jsonl.gz',\n",
       "  '185.jsonl.gz',\n",
       "  '1850.jsonl.gz',\n",
       "  '1851.jsonl.gz',\n",
       "  '1852.jsonl.gz',\n",
       "  '1853.jsonl.gz',\n",
       "  '1854.jsonl.gz',\n",
       "  '1855.jsonl.gz',\n",
       "  '1856.jsonl.gz',\n",
       "  '1857.jsonl.gz',\n",
       "  '1858.jsonl.gz',\n",
       "  '1859.jsonl.gz',\n",
       "  '186.jsonl.gz',\n",
       "  '1860.jsonl.gz',\n",
       "  '1861.jsonl.gz',\n",
       "  '1862.jsonl.gz',\n",
       "  '1863.jsonl.gz',\n",
       "  '1864.jsonl.gz',\n",
       "  '1865.jsonl.gz',\n",
       "  '1866.jsonl.gz',\n",
       "  '1867.jsonl.gz',\n",
       "  '1868.jsonl.gz',\n",
       "  '1869.jsonl.gz',\n",
       "  '187.jsonl.gz',\n",
       "  '1870.jsonl.gz',\n",
       "  '1871.jsonl.gz',\n",
       "  '1872.jsonl.gz',\n",
       "  '1873.jsonl.gz',\n",
       "  '1874.jsonl.gz',\n",
       "  '1875.jsonl.gz',\n",
       "  '1876.jsonl.gz',\n",
       "  '1877.jsonl.gz',\n",
       "  '1878.jsonl.gz',\n",
       "  '1879.jsonl.gz',\n",
       "  '188.jsonl.gz',\n",
       "  '1880.jsonl.gz',\n",
       "  '1881.jsonl.gz',\n",
       "  '1882.jsonl.gz',\n",
       "  '1883.jsonl.gz',\n",
       "  '1884.jsonl.gz',\n",
       "  '1885.jsonl.gz',\n",
       "  '1886.jsonl.gz',\n",
       "  '1887.jsonl.gz',\n",
       "  '1888.jsonl.gz',\n",
       "  '1889.jsonl.gz',\n",
       "  '189.jsonl.gz',\n",
       "  '1890.jsonl.gz',\n",
       "  '1891.jsonl.gz',\n",
       "  '1892.jsonl.gz',\n",
       "  '1893.jsonl.gz',\n",
       "  '1894.jsonl.gz',\n",
       "  '1895.jsonl.gz',\n",
       "  '1896.jsonl.gz',\n",
       "  '1897.jsonl.gz',\n",
       "  '1898.jsonl.gz',\n",
       "  ...])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_zips),f_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s2orc-master.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in f_zips if '.gz' not in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделение ACL статей,у которых есть  их статьи из reference c метаданными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Берем ACL статьи,у которых не менее 90% статьи из reference есть в датасете S2ORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'acl_scenario/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_name+'acl_only_articles_0_9.json') as data_file:\n",
    "    acl_only_articles = json.load(data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_only_articles_ids = list(map(lambda x: x['paper_id'],acl_only_articles))\n",
    "len(acl_only_articles_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Берем данные статьи с overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dir_name+'overview_papers_0_9.json') as data_file:\n",
    "    overview_papers = json.load(data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overview_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers_ids = list(map(lambda x: x['paper_id'],overview_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7259, 7259, 7259)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(overview_papers_ids)),len(set(acl_only_articles_ids)),len(set(overview_papers_ids+acl_only_articles_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**все верно**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Открываем все ACL OUT papers с метаданными и аннотациями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=os.listdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160a5b99055d443a9d75c673272bcc17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acl_scenario/acl_out_new_0.json\n",
      "acl_scenario/acl_out_new_10000.json\n",
      "acl_scenario/acl_out_new_2000.json\n",
      "acl_scenario/acl_out_new_4000.json\n",
      "acl_scenario/acl_out_new_6000.json\n",
      "acl_scenario/acl_out_new_8000.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in tqdm_notebook(files):\n",
    "    if ('acl_out_new_' in file) and ('json' in file) and not ('_0_8_'  in file):\n",
    "        file = dir_name+file\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b101c6e4cd04f34a3608eb1482c32dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=38), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acl_scenario/acl_out_new_0.json\n",
      "5\n",
      "acl_scenario/acl_out_new_10000.json\n",
      "5419\n",
      "acl_scenario/acl_out_new_2000.json\n",
      "5327\n",
      "acl_scenario/acl_out_new_4000.json\n",
      "5290\n",
      "acl_scenario/acl_out_new_6000.json\n",
      "5441\n",
      "acl_scenario/acl_out_new_8000.json\n",
      "5405\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_out_articles = []\n",
    "\n",
    "for file in tqdm_notebook(files):\n",
    "    if ('acl_out_new_' in file) and ('json' in file) and not ('_0_8_'  in file):\n",
    "        file = dir_name+file\n",
    "        print(file)\n",
    "        with open(file) as data_file:\n",
    "            context_dict_batch = json.load(data_file)\n",
    "        data_file.close()\n",
    "        print(len(context_dict_batch))\n",
    "        all_out_articles+=context_dict_batch\n",
    "        del context_dict_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26887"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_out_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26887, 26886)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_out_articles_ids = list(map(lambda x: x['paper_id'],all_out_articles))\n",
    "len(all_out_articles_ids),len(set(all_out_articles_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out_articles_ids_dict = {k:1 for k in all_out_articles_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('acl_scenario/acl_out_ids.txt', encoding=\"utf8\") as fin:\n",
    "    content = fin.readlines()\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_out_ids = [x.strip() for x in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26887, 26887)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_out_ids),len(set(acl_out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ids for ids in set(acl_out_ids) if ids not in all_out_articles_ids_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**как видим все супер** \n",
    "\n",
    "в нашей подборке out статей, нет только 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ подборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_paper_ids = [article['metadata']['acl_id'] for article in acl_only_articles if article['metadata']['acl_id']]\n",
    "len(acl_paper_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Удаление дублирования по acl_id статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubled_acl_id_papers = []\n",
    "for ind,cnt_of_acl_id in zip(pd.Series(acl_paper_ids).value_counts().index,pd.Series(acl_paper_ids).value_counts()):\n",
    "    if cnt_of_acl_id >=2:\n",
    "        doubled_acl_id_papers.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_items = []\n",
    "del_num_items = []\n",
    "for num_artic, article in enumerate(acl_only_articles):\n",
    "    if article['metadata']['acl_id'] in doubled_acl_id_papers:\n",
    "        del_items.append(article['paper_id'])\n",
    "        del_num_items.append(num_artic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(del_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_paper_ids = [article['metadata']['acl_id'] for article in acl_only_articles if article['metadata']['acl_id']]\n",
    "len(acl_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_items_from_papers(acl_ids_not_body_text,acl_only_articles):\n",
    "    del_items = []\n",
    "    del_num_items = []\n",
    "    for num_artic, article in enumerate(acl_only_articles):\n",
    "        if article['paper_id'] in acl_ids_not_body_text:\n",
    "            del_items.append(article['paper_id'])\n",
    "            del_num_items.append(num_artic)\n",
    "            \n",
    "    del_num_items = np.array(del_num_items)\n",
    "    acl_only_articles = np.array(acl_only_articles)\n",
    "    acl_only_articles = np.delete(acl_only_articles,del_num_items)\n",
    "    return acl_only_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### количество  всех статьей c body_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "для acl_only_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in acl_only_articles if article['grobid_parse']['body_text'] or (article['latex_parse'] and article['latex_parse']['body_text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse_abstract', 'latex_parse_abstract', 'grobid_parse_bib_entries', 'latex_parse_bib_entries'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_out_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '13756507',\n",
       " 'metadata': {'title': 'Collaborative Memory Network for Recommendation Systems',\n",
       "  'authors': [{'first': 'Travis', 'middle': [], 'last': 'Ebesu', 'suffix': ''},\n",
       "   {'first': 'Bin', 'middle': [], 'last': 'Shen', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Fang', 'suffix': ''}],\n",
       "  'abstract': \"Recommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.\",\n",
       "  'year': '2018',\n",
       "  'arxiv_id': '1804.10862',\n",
       "  'acl_id': None,\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.1145/3209978.3209991',\n",
       "  'venue': 'ArXiv',\n",
       "  'journal': 'ArXiv'},\n",
       " 's2_pdf_hash': '3944c1740bb643415583363a0a35f12f8bd19a16',\n",
       " 'grobid_parse_abstract': [{'text': \"ABSTRACTRecommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Abstract'}],\n",
       " 'latex_parse_abstract': None,\n",
       " 'grobid_parse_bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "   'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "   'authors': [{'first': 'Dzmitry',\n",
       "     'middle': [],\n",
       "     'last': 'Bahdanau',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "    {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '11212020'},\n",
       "  'BIBREF1': {'ref_id': 'b1',\n",
       "   'title': 'Attentive collaborative filtering: Multimedia recommendation with feature-and item-level attention',\n",
       "   'authors': [{'first': 'Jingyuan',\n",
       "     'middle': [],\n",
       "     'last': 'Chen',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Xiangnan', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Liqiang', 'middle': [], 'last': 'Nie', 'suffix': ''},\n",
       "    {'first': 'Wei', 'middle': [], 'last': 'Liu', 'suffix': ''},\n",
       "    {'first': 'Tatseng', 'middle': [], 'last': 'Chua', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF2': {'ref_id': 'b2',\n",
       "   'title': 'Wide & Deep Learning for Recommender Systems',\n",
       "   'authors': [{'first': '', 'middle': [], 'last': 'Heng-Tze', 'suffix': ''},\n",
       "    {'first': 'Levent', 'middle': [], 'last': 'Cheng', 'suffix': ''},\n",
       "    {'first': 'Jeremiah', 'middle': [], 'last': 'Koc', 'suffix': ''},\n",
       "    {'first': 'Tal', 'middle': [], 'last': 'Harmsen', 'suffix': ''},\n",
       "    {'first': 'Tushar', 'middle': [], 'last': 'Shaked', 'suffix': ''},\n",
       "    {'first': 'Hrishi', 'middle': [], 'last': 'Chandra', 'suffix': ''},\n",
       "    {'first': 'Glen', 'middle': [], 'last': 'Aradhye', 'suffix': ''},\n",
       "    {'first': 'Greg', 'middle': [], 'last': 'Anderson', 'suffix': ''},\n",
       "    {'first': 'Wei', 'middle': [], 'last': 'Corrado', 'suffix': ''},\n",
       "    {'first': 'Mustafa', 'middle': [], 'last': 'Chai', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Ispir', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3352400'},\n",
       "  'BIBREF3': {'ref_id': 'b3',\n",
       "   'title': 'Neural Semantic Personalized Ranking for item cold-start recommendation',\n",
       "   'authors': [{'first': 'Travis',\n",
       "     'middle': [],\n",
       "     'last': 'Ebesu',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Fang', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'Information Retrieval Journal',\n",
       "   'volume': '20',\n",
       "   'issn': '',\n",
       "   'pages': '109--131',\n",
       "   'other_ids': {},\n",
       "   'links': '19013891'},\n",
       "  'BIBREF4': {'ref_id': 'b4',\n",
       "   'title': 'Learning image and user features for recommendation in social networks',\n",
       "   'authors': [{'first': 'Xue', 'middle': [], 'last': 'Geng', 'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Jingwen', 'middle': [], 'last': 'Bian', 'suffix': ''},\n",
       "    {'first': 'Tat-Seng', 'middle': [], 'last': 'Chua', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'ICCV',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '1975342'},\n",
       "  'BIBREF5': {'ref_id': 'b5',\n",
       "   'title': 'Hashtag Recommendation Using AttentionBased Convolutional Neural Network',\n",
       "   'authors': [{'first': 'Yuyun', 'middle': [], 'last': 'Gong', 'suffix': ''},\n",
       "    {'first': 'Qi', 'middle': [], 'last': 'Zhang', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'IJCAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '17200126'},\n",
       "  'BIBREF6': {'ref_id': 'b6',\n",
       "   'title': 'Deep Learning',\n",
       "   'authors': [{'first': 'Ian',\n",
       "     'middle': [],\n",
       "     'last': 'Goodfellow',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''},\n",
       "    {'first': 'Aaron', 'middle': [], 'last': 'Courville', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF7': {'ref_id': 'b7',\n",
       "   'title': 'Hybrid computing using a neural network with dynamic external memory',\n",
       "   'authors': [{'first': 'Alex', 'middle': [], 'last': 'Graves', 'suffix': ''},\n",
       "    {'first': 'Greg', 'middle': [], 'last': 'Wayne', 'suffix': ''},\n",
       "    {'first': 'Malcolm', 'middle': [], 'last': 'Reynolds', 'suffix': ''},\n",
       "    {'first': 'Tim', 'middle': [], 'last': 'Harley', 'suffix': ''},\n",
       "    {'first': 'Ivo', 'middle': [], 'last': 'Danihelka', 'suffix': ''},\n",
       "    {'first': 'Agnieszka',\n",
       "     'middle': [],\n",
       "     'last': 'Grabska-Barwińska',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Sergio',\n",
       "     'middle': ['Gómez'],\n",
       "     'last': 'Colmenarejo',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Edward', 'middle': [], 'last': 'Grefenstette', 'suffix': ''},\n",
       "    {'first': 'Tiago', 'middle': [], 'last': 'Ramalho', 'suffix': ''},\n",
       "    {'first': 'John', 'middle': [], 'last': 'Agapiou', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'Nature',\n",
       "   'volume': '538',\n",
       "   'issn': '',\n",
       "   'pages': '471--476',\n",
       "   'other_ids': {},\n",
       "   'links': '4463466'},\n",
       "  'BIBREF8': {'ref_id': 'b8',\n",
       "   'title': 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classification',\n",
       "   'authors': [{'first': 'Kaiming', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Xiangyu', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Shaoqing', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "    {'first': 'Jian', 'middle': [], 'last': 'Sun', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'CVPR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '13740328'},\n",
       "  'BIBREF9': {'ref_id': 'b9',\n",
       "   'title': 'Deep residual learning for image recognition',\n",
       "   'authors': [{'first': 'Kaiming', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Xiangyu', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Shaoqing', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "    {'first': 'Jian', 'middle': [], 'last': 'Sun', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'CVPR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '2767606'},\n",
       "  'BIBREF10': {'ref_id': 'b10',\n",
       "   'title': 'Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering',\n",
       "   'authors': [{'first': 'Xiangnan', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Lizi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'Zhang', 'suffix': ''}],\n",
       "   'year': None,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF11': {'ref_id': 'b11',\n",
       "   'title': 'Session-based Recommendations with Recurrent Neural Networks',\n",
       "   'authors': [{'first': 'Balázs',\n",
       "     'middle': [],\n",
       "     'last': 'Hidasi',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Alexandros',\n",
       "     'middle': [],\n",
       "     'last': 'Karatzoglou',\n",
       "     'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '11810482'},\n",
       "  'BIBREF12': {'ref_id': 'b12',\n",
       "   'title': 'Hashtag Recommendation Using End-To-End Memory Networks with Hierarchical Attention',\n",
       "   'authors': [{'first': 'Haoran',\n",
       "     'middle': [],\n",
       "     'last': 'Huang',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Qi', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Yeyun', 'middle': [], 'last': 'Gong', 'suffix': ''},\n",
       "    {'first': 'Xuanjing', 'middle': [], 'last': 'Huang', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '1987772'},\n",
       "  'BIBREF14': {'ref_id': 'b14',\n",
       "   'title': 'When Recurrent Neural Networks Meet the Neighborhood for Session-Based Recommendation',\n",
       "   'authors': [{'first': 'Dietmar',\n",
       "     'middle': [],\n",
       "     'last': 'Jannach',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Malte', 'middle': [], 'last': 'Ludewig', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '31923308'},\n",
       "  'BIBREF15': {'ref_id': 'b15',\n",
       "   'title': 'Fism: factored item similarity models for top-n recommender systems',\n",
       "   'authors': [{'first': 'Santosh',\n",
       "     'middle': [],\n",
       "     'last': 'Kabbur',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Xia', 'middle': [], 'last': 'Ning', 'suffix': ''},\n",
       "    {'first': 'George', 'middle': [], 'last': 'Karypis', 'suffix': ''}],\n",
       "   'year': 2013,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '5166381'},\n",
       "  'BIBREF16': {'ref_id': 'b16',\n",
       "   'title': 'Convolutional Matrix Factorization for Document Context-Aware Recommendation',\n",
       "   'authors': [{'first': 'Donghyun',\n",
       "     'middle': [],\n",
       "     'last': 'Kim',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Chanyoung', 'middle': [], 'last': 'Park', 'suffix': ''},\n",
       "    {'first': 'Jinoh', 'middle': [], 'last': 'Oh', 'suffix': ''},\n",
       "    {'first': 'Sungyoung', 'middle': [], 'last': 'Lee', 'suffix': ''},\n",
       "    {'first': 'Hwanjo', 'middle': [], 'last': 'Yu', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '15635373'},\n",
       "  'BIBREF17': {'ref_id': 'b17',\n",
       "   'title': 'Factorization meets the neighborhood: a multifaceted collaborative filtering model',\n",
       "   'authors': [{'first': 'Yehuda',\n",
       "     'middle': [],\n",
       "     'last': 'Koren',\n",
       "     'suffix': ''}],\n",
       "   'year': 2008,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3351037'},\n",
       "  'BIBREF18': {'ref_id': 'b18',\n",
       "   'title': 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing',\n",
       "   'authors': [{'first': 'Ankit', 'middle': [], 'last': 'Kumar', 'suffix': ''},\n",
       "    {'first': 'Ozan', 'middle': [], 'last': 'Irsoy', 'suffix': ''},\n",
       "    {'first': 'Peter', 'middle': [], 'last': 'Ondruska', 'suffix': ''},\n",
       "    {'first': 'Mohit', 'middle': [], 'last': 'Iyyer', 'suffix': ''},\n",
       "    {'first': 'James', 'middle': [], 'last': 'Bradbury', 'suffix': ''},\n",
       "    {'first': 'Ishaan', 'middle': [], 'last': 'Gulrajani', 'suffix': ''},\n",
       "    {'first': 'Victor', 'middle': [], 'last': 'Zhong', 'suffix': ''},\n",
       "    {'first': 'Romain', 'middle': [], 'last': 'Paulus', 'suffix': ''},\n",
       "    {'first': 'Richard', 'middle': [], 'last': 'Socher', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '2319779'},\n",
       "  'BIBREF19': {'ref_id': 'b19',\n",
       "   'title': 'Deep Memory Networks for Attitude Identification',\n",
       "   'authors': [{'first': 'Cheng', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Xiaoxiao', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "    {'first': 'Qiaozhu', 'middle': [], 'last': 'Mei', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'WSDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '7973523'},\n",
       "  'BIBREF20': {'ref_id': 'b20',\n",
       "   'title': 'Deep Collaborative Filtering via Marginalized Denoising Auto-encoder',\n",
       "   'authors': [{'first': 'Sheng', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Jaya', 'middle': [], 'last': 'Kawale', 'suffix': ''},\n",
       "    {'first': 'Yun', 'middle': [], 'last': 'Fu', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'CIKM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3332579'},\n",
       "  'BIBREF21': {'ref_id': 'b21',\n",
       "   'title': 'Amazon.Com Recommendations: Item-to-Item Collaborative Filtering',\n",
       "   'authors': [{'first': 'Greg', 'middle': [], 'last': 'Linden', 'suffix': ''},\n",
       "    {'first': 'Brent', 'middle': [], 'last': 'Smith', 'suffix': ''},\n",
       "    {'first': 'Jeremy', 'middle': [], 'last': 'York', 'suffix': ''}],\n",
       "   'year': 2003,\n",
       "   'venue': 'IEEE Internet Computing',\n",
       "   'volume': '7',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF22': {'ref_id': 'b22',\n",
       "   'title': 'Trust-aware Recommender Systems',\n",
       "   'authors': [{'first': 'Paolo', 'middle': [], 'last': 'Massa', 'suffix': ''},\n",
       "    {'first': 'Paolo', 'middle': [], 'last': 'Avesani', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': 'RecSys',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '2212251'},\n",
       "  'BIBREF23': {'ref_id': 'b23',\n",
       "   'title': 'Slim: Sparse linear methods for top-n recommender systems',\n",
       "   'authors': [{'first': 'Xia', 'middle': [], 'last': 'Ning', 'suffix': ''},\n",
       "    {'first': 'George', 'middle': [], 'last': 'Karypis', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'ICDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '16782702'},\n",
       "  'BIBREF24': {'ref_id': 'b24',\n",
       "   'title': 'Factorization machines',\n",
       "   'authors': [{'first': '',\n",
       "     'middle': [],\n",
       "     'last': 'Steffen Rendle',\n",
       "     'suffix': ''}],\n",
       "   'year': 2010,\n",
       "   'venue': 'ICDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF25': {'ref_id': 'b25',\n",
       "   'title': 'BPR : Bayesian Personalized Ranking from Implicit Feedback. UAI',\n",
       "   'authors': [{'first': 'Steffen',\n",
       "     'middle': [],\n",
       "     'last': 'Rendle',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Christoph',\n",
       "     'middle': [],\n",
       "     'last': 'Freudenthaler',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Zeno', 'middle': [], 'last': 'Gantner', 'suffix': ''},\n",
       "    {'first': 'Lars', 'middle': [], 'last': 'Schmidt-Thieme', 'suffix': ''}],\n",
       "   'year': 2009,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '10795036'},\n",
       "  'BIBREF26': {'ref_id': 'b26',\n",
       "   'title': 'Introduction to recommender systems handbook',\n",
       "   'authors': [{'first': 'Francesco',\n",
       "     'middle': [],\n",
       "     'last': 'Ricci',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Lior', 'middle': [], 'last': 'Rokach', 'suffix': ''},\n",
       "    {'first': 'Bracha', 'middle': [], 'last': 'Shapira', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF27': {'ref_id': 'b27',\n",
       "   'title': 'Restricted Boltzmann machines for collaborative filtering',\n",
       "   'authors': [{'first': 'Ruslan',\n",
       "     'middle': [],\n",
       "     'last': 'Salakhutdinov',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Andriy', 'middle': [], 'last': 'Mnih', 'suffix': ''},\n",
       "    {'first': 'Geoffrey', 'middle': [], 'last': 'Hinton', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '7285098'},\n",
       "  'BIBREF28': {'ref_id': 'b28',\n",
       "   'title': 'AutoRec : Autoencoders Meet Collaborative Filtering',\n",
       "   'authors': [{'first': 'Suvash',\n",
       "     'middle': [],\n",
       "     'last': 'Sedhain',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Aditya', 'middle': ['Krishna'], 'last': 'Menon', 'suffix': ''},\n",
       "    {'first': 'Scott', 'middle': [], 'last': 'Sanner', 'suffix': ''},\n",
       "    {'first': 'Lexing', 'middle': [], 'last': 'Xie', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '16274986'},\n",
       "  'BIBREF29': {'ref_id': 'b29',\n",
       "   'title': 'Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction',\n",
       "   'authors': [{'first': 'Sungyong',\n",
       "     'middle': [],\n",
       "     'last': 'Seo',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Jing', 'middle': [], 'last': 'Huang', 'suffix': ''},\n",
       "    {'first': 'Hao', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "    {'first': 'Yan', 'middle': [], 'last': 'Liu', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'RecSys',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '22201036'},\n",
       "  'BIBREF30': {'ref_id': 'b30',\n",
       "   'title': 'EndTo-End Memory Networks',\n",
       "   'authors': [{'first': 'Sainbayar',\n",
       "     'middle': [],\n",
       "     'last': 'Sukhbaatar',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Arthur', 'middle': [], 'last': 'Szlam', 'suffix': ''},\n",
       "    {'first': 'Jason', 'middle': [], 'last': 'Weston', 'suffix': ''},\n",
       "    {'first': 'Rob', 'middle': [], 'last': 'Fergus', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'NIPS',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF31': {'ref_id': 'b31',\n",
       "   'title': 'Deep content-based music recommendation',\n",
       "   'authors': [{'first': 'Aäron',\n",
       "     'middle': [],\n",
       "     'last': 'Van Den Oord',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Sander', 'middle': [], 'last': 'Dieleman', 'suffix': ''},\n",
       "    {'first': 'Benjamin', 'middle': [], 'last': 'Schrauwen', 'suffix': ''}],\n",
       "   'year': 2013,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '7118498'},\n",
       "  'BIBREF32': {'ref_id': 'b32',\n",
       "   'title': 'Collaborative topic modeling for recommending scientific articles',\n",
       "   'authors': [{'first': 'Chong', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'M', 'middle': [], 'last': 'David', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Blei', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '96163'},\n",
       "  'BIBREF33': {'ref_id': 'b33',\n",
       "   'title': 'Collaborative deep learning for recommender systems',\n",
       "   'authors': [{'first': 'Hao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Naiyan', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Dit-Yan', 'middle': [], 'last': 'Yeung', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '4833213'},\n",
       "  'BIBREF34': {'ref_id': 'b34',\n",
       "   'title': 'IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models',\n",
       "   'authors': [{'first': 'Jun', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Lantao', 'middle': [], 'last': 'Yu', 'suffix': ''},\n",
       "    {'first': 'Weinan', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Yu', 'middle': [], 'last': 'Gong', 'suffix': ''},\n",
       "    {'first': 'Yinghui', 'middle': [], 'last': 'Xu', 'suffix': ''},\n",
       "    {'first': 'Benyou', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Peng', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Dell', 'middle': [], 'last': 'Zhang', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3331356'},\n",
       "  'BIBREF35': {'ref_id': 'b35',\n",
       "   'title': 'Memory Networks. In ICLR',\n",
       "   'authors': [{'first': 'Jason',\n",
       "     'middle': [],\n",
       "     'last': 'Weston',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Sumit', 'middle': [], 'last': 'Chopra', 'suffix': ''},\n",
       "    {'first': 'Antoine', 'middle': [], 'last': 'Bordes', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF37': {'ref_id': 'b37',\n",
       "   'title': 'Collaborative Denoising Auto-Encoders for Top-N Recommender Systems',\n",
       "   'authors': [{'first': 'Yao', 'middle': [], 'last': 'Wu', 'suffix': ''},\n",
       "    {'first': 'Christopher', 'middle': [], 'last': 'Dubois', 'suffix': ''},\n",
       "    {'first': 'Alice', 'middle': ['X'], 'last': 'Zheng', 'suffix': ''},\n",
       "    {'first': 'Martin', 'middle': [], 'last': 'Ester', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'WSDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '6392154'},\n",
       "  'BIBREF38': {'ref_id': 'b38',\n",
       "   'title': 'Attentional factorization machines: Learning the weight of feature interactions via attention networks',\n",
       "   'authors': [{'first': 'Jun', 'middle': [], 'last': 'Xiao', 'suffix': ''},\n",
       "    {'first': 'Xiangnan', 'middle': [], 'last': 'Hao Ye', 'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Fei', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Tat-Seng', 'middle': [], 'last': 'Wu', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Chua', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3836251'},\n",
       "  'BIBREF39': {'ref_id': 'b39',\n",
       "   'title': 'Dynamic memory networks for visual and textual question answering',\n",
       "   'authors': [{'first': 'Caiming',\n",
       "     'middle': [],\n",
       "     'last': 'Xiong',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Stephen', 'middle': [], 'last': 'Merity', 'suffix': ''},\n",
       "    {'first': 'Richard', 'middle': [], 'last': 'Socher', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '14294589'},\n",
       "  'BIBREF40': {'ref_id': 'b40',\n",
       "   'title': 'Deep Learning based Recommender System: A Survey and New Perspectives',\n",
       "   'authors': [{'first': 'Shuai', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Lina', 'middle': [], 'last': 'Yao', 'suffix': ''},\n",
       "    {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {'arXiv': ['arXiv:1707.07435']},\n",
       "   'links': '22475926'},\n",
       "  'BIBREF41': {'ref_id': 'b41',\n",
       "   'title': 'AutoSVD++: An Efficient Hybrid Collaborative Filtering Model via Contractive Auto-encoders',\n",
       "   'authors': [{'first': 'Shuai', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Lina', 'middle': [], 'last': 'Yao', 'suffix': ''},\n",
       "    {'first': 'Xiwei', 'middle': [], 'last': 'Xu', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '17793427'}},\n",
       " 'latex_parse_bib_entries': {'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "   'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "   'authors': [{'first': 'Dzmitry',\n",
       "     'middle': [],\n",
       "     'last': 'Bahdanau',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "    {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '11212020'},\n",
       "  'BIBREF31': {'ref_id': 'BIBREF31',\n",
       "   'title': 'Attentive collaborative filtering: Multimedia recommendation with feature-and item-level attention',\n",
       "   'authors': [{'first': 'Jingyuan',\n",
       "     'middle': [],\n",
       "     'last': 'Chen',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Xiangnan', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Liqiang', 'middle': [], 'last': 'Nie', 'suffix': ''},\n",
       "    {'first': 'Wei', 'middle': [], 'last': 'Liu', 'suffix': ''},\n",
       "    {'first': 'Tat-Seng', 'middle': [], 'last': 'Chua', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "   'title': 'Wide & Deep Learning for Recommender Systems',\n",
       "   'authors': [{'first': '', 'middle': [], 'last': 'Heng-Tze', 'suffix': ''},\n",
       "    {'first': 'Levent', 'middle': [], 'last': 'Cheng', 'suffix': ''},\n",
       "    {'first': 'Jeremiah', 'middle': [], 'last': 'Koc', 'suffix': ''},\n",
       "    {'first': 'Tal', 'middle': [], 'last': 'Harmsen', 'suffix': ''},\n",
       "    {'first': 'Tushar', 'middle': [], 'last': 'Shaked', 'suffix': ''},\n",
       "    {'first': 'Hrishi', 'middle': [], 'last': 'Chandra', 'suffix': ''},\n",
       "    {'first': 'Glen', 'middle': [], 'last': 'Aradhye', 'suffix': ''},\n",
       "    {'first': 'Greg', 'middle': [], 'last': 'Anderson', 'suffix': ''},\n",
       "    {'first': 'Wei', 'middle': [], 'last': 'Corrado', 'suffix': ''},\n",
       "    {'first': 'Mustafa', 'middle': [], 'last': 'Chai', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Ispir', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3352400'},\n",
       "  'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "   'title': 'Neural Semantic Personalized Ranking for item cold-start recommendation',\n",
       "   'authors': [{'first': 'Travis',\n",
       "     'middle': [],\n",
       "     'last': 'Ebesu',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Fang', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'Information Retrieval Journal',\n",
       "   'volume': '20',\n",
       "   'issn': '',\n",
       "   'pages': '109--131',\n",
       "   'other_ids': {},\n",
       "   'links': '19013891'},\n",
       "  'BIBREF40': {'ref_id': 'BIBREF40',\n",
       "   'title': 'Learning image and user features for recommendation in social networks',\n",
       "   'authors': [{'first': 'Xue', 'middle': [], 'last': 'Geng', 'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Jingwen', 'middle': [], 'last': 'Bian', 'suffix': ''},\n",
       "    {'first': 'Tat-Seng', 'middle': [], 'last': 'Chua', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'ICCV',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '1975342'},\n",
       "  'BIBREF29': {'ref_id': 'BIBREF29',\n",
       "   'title': 'Hashtag Recommendation Using Attention-Based Convolutional Neural Network',\n",
       "   'authors': [{'first': 'Yuyun', 'middle': [], 'last': 'Gong', 'suffix': ''},\n",
       "    {'first': 'Qi', 'middle': [], 'last': 'Zhang', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'IJCAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '17200126'},\n",
       "  'BIBREF28': {'ref_id': 'BIBREF28',\n",
       "   'title': 'Deep Learning',\n",
       "   'authors': [{'first': 'Ian',\n",
       "     'middle': [],\n",
       "     'last': 'Goodfellow',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''},\n",
       "    {'first': 'Aaron', 'middle': [], 'last': 'Courville', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "   'title': 'Hybrid computing using a neural network with dynamic external memory',\n",
       "   'authors': [{'first': 'Alex', 'middle': [], 'last': 'Graves', 'suffix': ''},\n",
       "    {'first': 'Greg', 'middle': [], 'last': 'Wayne', 'suffix': ''},\n",
       "    {'first': 'Malcolm', 'middle': [], 'last': 'Reynolds', 'suffix': ''},\n",
       "    {'first': 'Tim', 'middle': [], 'last': 'Harley', 'suffix': ''},\n",
       "    {'first': 'Ivo', 'middle': [], 'last': 'Danihelka', 'suffix': ''},\n",
       "    {'first': 'Agnieszka',\n",
       "     'middle': [],\n",
       "     'last': 'Grabska-Barwińska',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Sergio',\n",
       "     'middle': ['Gómez'],\n",
       "     'last': 'Colmenarejo',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Edward', 'middle': [], 'last': 'Grefenstette', 'suffix': ''},\n",
       "    {'first': 'Tiago', 'middle': [], 'last': 'Ramalho', 'suffix': ''},\n",
       "    {'first': 'John', 'middle': [], 'last': 'Agapiou', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'Nature',\n",
       "   'volume': '538',\n",
       "   'issn': '',\n",
       "   'pages': '471--476',\n",
       "   'other_ids': {},\n",
       "   'links': '4463466'},\n",
       "  'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "   'title': 'Delving deep into rectifiers: Surpassing human-level performance on imagenet classification',\n",
       "   'authors': [{'first': 'Kaiming', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Xiangyu', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Shaoqing', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "    {'first': 'Jian', 'middle': [], 'last': 'Sun', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'CVPR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '13740328'},\n",
       "  'BIBREF35': {'ref_id': 'BIBREF35',\n",
       "   'title': 'Deep residual learning for image recognition',\n",
       "   'authors': [{'first': 'Kaiming', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Xiangyu', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Shaoqing', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "    {'first': 'Jian', 'middle': [], 'last': 'Sun', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'CVPR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '2767606'},\n",
       "  'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "   'title': 'Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering',\n",
       "   'authors': [{'first': 'Xiangnan', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Lizi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'Zhang', 'suffix': ''}],\n",
       "   'year': None,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF25': {'ref_id': 'BIBREF25',\n",
       "   'title': 'Session-based Recommendations with Recurrent Neural Networks',\n",
       "   'authors': [{'first': 'Balázs',\n",
       "     'middle': [],\n",
       "     'last': 'Hidasi',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Alexandros',\n",
       "     'middle': [],\n",
       "     'last': 'Karatzoglou',\n",
       "     'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '11810482'},\n",
       "  'BIBREF30': {'ref_id': 'BIBREF30',\n",
       "   'title': 'Hashtag Recommendation Using End-To-End Memory Networks with Hierarchical Attention',\n",
       "   'authors': [{'first': 'Haoran',\n",
       "     'middle': [],\n",
       "     'last': 'Huang',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Qi', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Yeyun', 'middle': [], 'last': 'Gong', 'suffix': ''},\n",
       "    {'first': 'Xuanjing', 'middle': [], 'last': 'Huang', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'COLING',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '1987772'},\n",
       "  'BIBREF26': {'ref_id': 'BIBREF26',\n",
       "   'title': 'When Recurrent Neural Networks Meet the Neighborhood for Session-Based Recommendation',\n",
       "   'authors': [{'first': 'Dietmar',\n",
       "     'middle': [],\n",
       "     'last': 'Jannach',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Malte', 'middle': [], 'last': 'Ludewig', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '31923308'},\n",
       "  'BIBREF34': {'ref_id': 'BIBREF34',\n",
       "   'title': 'Fism: factored item similarity models for top-n recommender systems',\n",
       "   'authors': [{'first': 'Santosh',\n",
       "     'middle': [],\n",
       "     'last': 'Kabbur',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Xia', 'middle': [], 'last': 'Ning', 'suffix': ''},\n",
       "    {'first': 'George', 'middle': [], 'last': 'Karypis', 'suffix': ''}],\n",
       "   'year': 2013,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '5166381'},\n",
       "  'BIBREF23': {'ref_id': 'BIBREF23',\n",
       "   'title': 'Convolutional Matrix Factorization for Document Context-Aware Recommendation',\n",
       "   'authors': [{'first': 'Donghyun',\n",
       "     'middle': [],\n",
       "     'last': 'Kim',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Chanyoung', 'middle': [], 'last': 'Park', 'suffix': ''},\n",
       "    {'first': 'Jinoh', 'middle': [], 'last': 'Oh', 'suffix': ''},\n",
       "    {'first': 'Sungyoung', 'middle': [], 'last': 'Lee', 'suffix': ''},\n",
       "    {'first': 'Hwanjo', 'middle': [], 'last': 'Yu', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'RecSys',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '15635373'},\n",
       "  'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "   'title': 'Factorization meets the neighborhood: a multifaceted collaborative filtering model',\n",
       "   'authors': [{'first': 'Yehuda',\n",
       "     'middle': [],\n",
       "     'last': 'Koren',\n",
       "     'suffix': ''}],\n",
       "   'year': 2008,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3351037'},\n",
       "  'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "   'title': 'Ask Me Anything: Dynamic Memory Networks for Natural Language Processing',\n",
       "   'authors': [{'first': 'Ankit', 'middle': [], 'last': 'Kumar', 'suffix': ''},\n",
       "    {'first': 'Ozan', 'middle': [], 'last': 'Irsoy', 'suffix': ''},\n",
       "    {'first': 'Peter', 'middle': [], 'last': 'Ondruska', 'suffix': ''},\n",
       "    {'first': 'Mohit', 'middle': [], 'last': 'Iyyer', 'suffix': ''},\n",
       "    {'first': 'James', 'middle': [], 'last': 'Bradbury', 'suffix': ''},\n",
       "    {'first': 'Ishaan', 'middle': [], 'last': 'Gulrajani', 'suffix': ''},\n",
       "    {'first': 'Victor', 'middle': [], 'last': 'Zhong', 'suffix': ''},\n",
       "    {'first': 'Romain', 'middle': [], 'last': 'Paulus', 'suffix': ''},\n",
       "    {'first': 'Richard', 'middle': [], 'last': 'Socher', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '2319779'},\n",
       "  'BIBREF33': {'ref_id': 'BIBREF33',\n",
       "   'title': 'Deep Memory Networks for Attitude Identification',\n",
       "   'authors': [{'first': 'Cheng', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Xiaoxiao', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "    {'first': 'Qiaozhu', 'middle': [], 'last': 'Mei', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'WSDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '7973523'},\n",
       "  'BIBREF16': {'ref_id': 'BIBREF16',\n",
       "   'title': 'Deep Collaborative Filtering via Marginalized Denoising Auto-encoder',\n",
       "   'authors': [{'first': 'Sheng', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Jaya', 'middle': [], 'last': 'Kawale', 'suffix': ''},\n",
       "    {'first': 'Yun', 'middle': [], 'last': 'Fu', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'CIKM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3332579'},\n",
       "  'BIBREF2': {'ref_id': 'BIBREF2',\n",
       "   'title': 'Amazon.Com Recommendations: Item-to-Item Collaborative Filtering',\n",
       "   'authors': [{'first': 'Greg', 'middle': [], 'last': 'Linden', 'suffix': ''},\n",
       "    {'first': 'Brent', 'middle': [], 'last': 'Smith', 'suffix': ''},\n",
       "    {'first': 'Jeremy', 'middle': [], 'last': 'York', 'suffix': ''}],\n",
       "   'year': 2003,\n",
       "   'venue': 'IEEE Internet Computing',\n",
       "   'volume': '7',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF38': {'ref_id': 'BIBREF38',\n",
       "   'title': 'Trust-aware Recommender Systems',\n",
       "   'authors': [{'first': 'Paolo', 'middle': [], 'last': 'Massa', 'suffix': ''},\n",
       "    {'first': 'Paolo', 'middle': [], 'last': 'Avesani', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '2212251'},\n",
       "  'BIBREF37': {'ref_id': 'BIBREF37',\n",
       "   'title': 'Slim: Sparse linear methods for top-n recommender systems',\n",
       "   'authors': [{'first': 'Xia', 'middle': [], 'last': 'Ning', 'suffix': ''},\n",
       "    {'first': 'George', 'middle': [], 'last': 'Karypis', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'ICDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '16782702'},\n",
       "  'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "   'title': 'Factorization machines',\n",
       "   'authors': [{'first': '',\n",
       "     'middle': [],\n",
       "     'last': 'Steffen Rendle',\n",
       "     'suffix': ''}],\n",
       "   'year': 2010,\n",
       "   'venue': 'ICDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF36': {'ref_id': 'BIBREF36',\n",
       "   'title': 'BPR : Bayesian Personalized Ranking from Implicit Feedback. UAI',\n",
       "   'authors': [{'first': 'Steffen',\n",
       "     'middle': [],\n",
       "     'last': 'Rendle',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Christoph',\n",
       "     'middle': [],\n",
       "     'last': 'Freudenthaler',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Zeno', 'middle': [], 'last': 'Gantner', 'suffix': ''},\n",
       "    {'first': 'Lars', 'middle': [], 'last': 'Schmidt-Thieme', 'suffix': ''}],\n",
       "   'year': 2009,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '10795036'},\n",
       "  'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "   'title': 'Introduction to recommender systems handbook',\n",
       "   'authors': [{'first': 'Francesco',\n",
       "     'middle': [],\n",
       "     'last': 'Ricci',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Lior', 'middle': [], 'last': 'Rokach', 'suffix': ''},\n",
       "    {'first': 'Bracha', 'middle': [], 'last': 'Shapira', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF12': {'ref_id': 'BIBREF12',\n",
       "   'title': 'Restricted Boltzmann machines for collaborative filtering',\n",
       "   'authors': [{'first': 'Ruslan',\n",
       "     'middle': [],\n",
       "     'last': 'Salakhutdinov',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Andriy', 'middle': [], 'last': 'Mnih', 'suffix': ''},\n",
       "    {'first': 'Geoffrey', 'middle': [], 'last': 'Hinton', 'suffix': ''}],\n",
       "   'year': 2007,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '7285098'},\n",
       "  'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "   'title': 'AutoRec : Autoencoders Meet Collaborative Filtering',\n",
       "   'authors': [{'first': 'Suvash',\n",
       "     'middle': [],\n",
       "     'last': 'Sedhain',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Aditya', 'middle': ['Krishna'], 'last': 'Menon', 'suffix': ''},\n",
       "    {'first': 'Scott', 'middle': [], 'last': 'Sanner', 'suffix': ''},\n",
       "    {'first': 'Lexing', 'middle': [], 'last': 'Xie', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '16274986'},\n",
       "  'BIBREF22': {'ref_id': 'BIBREF22',\n",
       "   'title': 'Interpretable Convolutional Neural Networks with Dual Local and Global Attention for Review Rating Prediction',\n",
       "   'authors': [{'first': 'Sungyong',\n",
       "     'middle': [],\n",
       "     'last': 'Seo',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Jing', 'middle': [], 'last': 'Huang', 'suffix': ''},\n",
       "    {'first': 'Hao', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "    {'first': 'Yan', 'middle': [], 'last': 'Liu', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'RecSys',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '22201036'},\n",
       "  'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "   'title': 'End-To-End Memory Networks',\n",
       "   'authors': [{'first': 'Sainbayar',\n",
       "     'middle': [],\n",
       "     'last': 'Sukhbaatar',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Arthur', 'middle': [], 'last': 'Szlam', 'suffix': ''},\n",
       "    {'first': 'Jason', 'middle': [], 'last': 'Weston', 'suffix': ''},\n",
       "    {'first': 'Rob', 'middle': [], 'last': 'Fergus', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'NIPS',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '1399322'},\n",
       "  'BIBREF21': {'ref_id': 'BIBREF21',\n",
       "   'title': 'Deep content-based music recommendation',\n",
       "   'authors': [{'first': 'Aäron',\n",
       "     'middle': [],\n",
       "     'last': 'Van Den Oord',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Sander', 'middle': [], 'last': 'Dieleman', 'suffix': ''},\n",
       "    {'first': 'Benjamin', 'middle': [], 'last': 'Schrauwen', 'suffix': ''}],\n",
       "   'year': 2013,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '7118498'},\n",
       "  'BIBREF39': {'ref_id': 'BIBREF39',\n",
       "   'title': 'Collaborative topic modeling for recommending scientific articles',\n",
       "   'authors': [{'first': 'Chong', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'M', 'middle': [], 'last': 'David', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Blei', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '96163'},\n",
       "  'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "   'title': 'Collaborative deep learning for recommender systems',\n",
       "   'authors': [{'first': 'Hao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Naiyan', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Dit-Yan', 'middle': [], 'last': 'Yeung', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'SIGKDD',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '4833213'},\n",
       "  'BIBREF27': {'ref_id': 'BIBREF27',\n",
       "   'title': 'IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models',\n",
       "   'authors': [{'first': 'Jun', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Lantao', 'middle': [], 'last': 'Yu', 'suffix': ''},\n",
       "    {'first': 'Weinan', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Yu', 'middle': [], 'last': 'Gong', 'suffix': ''},\n",
       "    {'first': 'Yinghui', 'middle': [], 'last': 'Xu', 'suffix': ''},\n",
       "    {'first': 'Benyou', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Peng', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Dell', 'middle': [], 'last': 'Zhang', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3331356'},\n",
       "  'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "   'title': 'Memory Networks. In ICLR',\n",
       "   'authors': [{'first': 'Jason',\n",
       "     'middle': [],\n",
       "     'last': 'Weston',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Sumit', 'middle': [], 'last': 'Chopra', 'suffix': ''},\n",
       "    {'first': 'Antoine', 'middle': [], 'last': 'Bordes', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF24': {'ref_id': 'BIBREF24',\n",
       "   'title': '',\n",
       "   'authors': [{'first': '', 'middle': [], 'last': 'Chao-Yuan', 'suffix': ''},\n",
       "    {'first': 'Amr', 'middle': [], 'last': 'Wu', 'suffix': ''},\n",
       "    {'first': 'Alex', 'middle': [], 'last': 'Ahmed', 'suffix': ''},\n",
       "    {'first': 'Alexander', 'middle': ['J'], 'last': 'Beutel', 'suffix': ''},\n",
       "    {'first': 'How', 'middle': [], 'last': 'Smola', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Jing', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'Recurrent Recommender Networks. In WSDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "   'title': 'Collaborative Denoising Auto-Encoders for Top-N Recommender Systems',\n",
       "   'authors': [{'first': 'Yao', 'middle': [], 'last': 'Wu', 'suffix': ''},\n",
       "    {'first': 'Christopher', 'middle': [], 'last': 'Dubois', 'suffix': ''},\n",
       "    {'first': 'Alice', 'middle': ['X'], 'last': 'Zheng', 'suffix': ''},\n",
       "    {'first': 'Martin', 'middle': [], 'last': 'Ester', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'WSDM',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '6392154'},\n",
       "  'BIBREF32': {'ref_id': 'BIBREF32',\n",
       "   'title': 'Attentional factorization machines: Learning the weight of feature interactions via attention networks',\n",
       "   'authors': [{'first': 'Jun', 'middle': [], 'last': 'Xiao', 'suffix': ''},\n",
       "    {'first': 'Xiangnan', 'middle': [], 'last': 'Hao Ye', 'suffix': ''},\n",
       "    {'first': 'Hanwang', 'middle': [], 'last': 'He', 'suffix': ''},\n",
       "    {'first': 'Fei', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Tat-Seng', 'middle': [], 'last': 'Wu', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Chua', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '3836251'},\n",
       "  'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "   'title': 'Dynamic memory networks for visual and textual question answering',\n",
       "   'authors': [{'first': 'Caiming',\n",
       "     'middle': [],\n",
       "     'last': 'Xiong',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Stephen', 'middle': [], 'last': 'Merity', 'suffix': ''},\n",
       "    {'first': 'Richard', 'middle': [], 'last': 'Socher', 'suffix': ''}],\n",
       "   'year': 2016,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '14294589'},\n",
       "  'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "   'title': 'Deep Learning based Recommender System: A Survey and New Perspectives',\n",
       "   'authors': [{'first': 'Shuai', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Lina', 'middle': [], 'last': 'Yao', 'suffix': ''},\n",
       "    {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {'arXiv': ['arXiv:1707.07435']},\n",
       "   'links': '22475926'},\n",
       "  'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "   'title': 'AutoSVD++: An Efficient Hybrid Collaborative Filtering Model via Contractive Auto-encoders',\n",
       "   'authors': [{'first': 'Shuai', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Lina', 'middle': [], 'last': 'Yao', 'suffix': ''},\n",
       "    {'first': 'Xiwei', 'middle': [], 'last': 'Xu', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '17793427'}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_out_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15568"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_out_articles if article['grobid_parse_abstract'] or article['latex_parse_abstract']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка ситуации, когда есть **latex_parse**, но  нет **grobid_parse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_art ,article in enumerate(acl_only_articles):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        if len(article['grobid_parse']['body_text'])==0:\n",
    "            print(num_art,article['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1 for num_art ,article in enumerate(all_out_articles) if article['latex_parse_abstract'] and not article['grobid_parse_abstract']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Формирование признакового пространства по каждой статье"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "признаки по каждой статье:\n",
    "- цитируемость автора и статьи\n",
    "- дата выхода статьи\n",
    "- тематические признаки:\n",
    "   - тема ref статьи общая с темой\n",
    "     исходной статьи\n",
    "   - тема ref статьи общая с темой\n",
    "     с темой подбор\n",
    "- мб журнал где была ссылка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse_abstract', 'latex_parse_abstract', 'grobid_parse_bib_entries', 'latex_parse_bib_entries'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_out_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ABSTRACTRecommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_out_articles[0]['grobid_parse_abstract'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497200\n",
      "5e3cd43b9c0288a08c782b8df1311dd3e4f06c9b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': \"ABSTRACTRecommendation systems play a vital role to keep users engaged with personalized content in modern online platforms. Deep learning has revolutionized many research fields and there is a recent surge of interest in applying it to collaborative filtering (CF). However, existing methods compose deep learning architectures with the latent factor model ignoring a major class of CF models, neighborhood or memory-based approaches. We propose Collaborative Memory Networks (CMN), a deep architecture to unify the two classes of CF models capitalizing on the strengths of the global structure of latent factor model and local neighborhood-based structure in a nonlinear fashion. Motivated by the success of Memory Networks, we fuse a memory component and neural attention mechanism as the neighborhood component. The associative addressing scheme with the user and item memories in the memory module encodes complex user-item relations coupled with the neural attention mechanism to learn a user-item specific neighborhood. Finally, the output module jointly exploits the neighborhood with the user and item memories to produce the ranking score. Stacking multiple memory modules together yield deeper architectures capturing increasingly complex user-item relations. Furthermore, we show strong connections between CMN components, memory networks and the three classes of CF models. Comprehensive experimental results demonstrate the effectiveness of CMN on three public datasets outperforming competitive baselines. Qualitative visualization of the attention weights provide insight into the model's recommendation process and suggest the presence of higher order interactions.\",\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': 'Abstract'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_out_articles[3]['paper_id'])\n",
    "print(all_out_articles[3]['s2_pdf_hash'])\n",
    "all_out_articles[0]['grobid_parse_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ids = []\n",
    "p_s2pdf = []\n",
    "p_year = []\n",
    "p_arxiv = []\n",
    "p_acl = []\n",
    "p_pmc = []\n",
    "p_pubmed = []\n",
    "p_doi = []\n",
    "p_venue = []\n",
    "p_journal = []\n",
    "p_abstract = []\n",
    "p_abstract_grobid = []\n",
    "p_abstract_latex = []\n",
    "p_title = []\n",
    "p_authors = []\n",
    "p_cnt_out_grobid = []\n",
    "p_cnt_out_latex = []\n",
    "for article in all_out_articles:\n",
    "    p_ids.append(article['paper_id'])\n",
    "    p_s2pdf.append(article['s2_pdf_hash'])\n",
    "    \n",
    "    if article['metadata']['year']:\n",
    "        p_year.append(article['metadata']['year'])\n",
    "    else:\n",
    "        p_year.append(0)\n",
    "        \n",
    "    if article['metadata']['arxiv_id']:\n",
    "        p_arxiv.append(1)\n",
    "    else:\n",
    "        p_arxiv.append(0)\n",
    "    if article['metadata']['acl_id']:\n",
    "        p_acl.append(1)\n",
    "    else:\n",
    "        p_acl.append(0)\n",
    "    if article['metadata']['pmc_id']:\n",
    "        p_pmc.append(1)\n",
    "    else:\n",
    "        p_pmc.append(0)\n",
    "    if article['metadata']['pubmed_id']:\n",
    "        p_pubmed.append(1)\n",
    "    else:\n",
    "        p_pubmed.append(0)\n",
    "    if article['metadata']['doi']:\n",
    "        p_doi.append(1)\n",
    "    else:\n",
    "        p_doi.append(0)\n",
    "        \n",
    "    if article['metadata']['venue']:\n",
    "        p_venue.append(article['metadata']['venue'])\n",
    "    else:\n",
    "        p_venue.append(0)\n",
    "    if article['metadata']['journal']:\n",
    "        p_journal.append(article['metadata']['journal'])\n",
    "    else:\n",
    "        p_journal.append(0)\n",
    "        \n",
    "    if article['metadata']['abstract']:\n",
    "        p_abstract.append(article['metadata']['abstract'])\n",
    "    else:\n",
    "        p_abstract.append(0)\n",
    "    if article['metadata']['title']:\n",
    "        p_title.append(article['metadata']['title'])\n",
    "    else:\n",
    "        p_title.append(0)\n",
    "    if article['grobid_parse_abstract']:\n",
    "        p_abstract_grobid.append(article['grobid_parse_abstract'][0]['text'])\n",
    "    else:\n",
    "        p_abstract_grobid.append(0)\n",
    "    if article['latex_parse_abstract']:\n",
    "        p_abstract_latex.append(article['latex_parse_abstract'][0]['text'])\n",
    "    else:\n",
    "        p_abstract_latex.append(0)\n",
    "    \n",
    "        \n",
    "    if article['grobid_parse_bib_entries']:\n",
    "        p_cnt_out_grobid.append(len(article['grobid_parse_bib_entries']))\n",
    "    else:\n",
    "        p_cnt_out_grobid.append(0)\n",
    "    if  article['latex_parse_bib_entries']:\n",
    "        p_cnt_out_latex.append(len(article['latex_parse_bib_entries']))\n",
    "    else:\n",
    "        p_cnt_out_latex.append(0)\n",
    "    \n",
    "    authors = []\n",
    "    if article['metadata']['authors']:\n",
    "        for author in article['metadata']['authors']:\n",
    "            if author['first']:\n",
    "                authors.append(author['first']+' '+author['last'])\n",
    "            elif author['middle'] and author['suffix']:\n",
    "                print('can be ERROR',article['paper_id'])\n",
    "            elif author['last']:\n",
    "                authors.append(author['last'])\n",
    "            else:\n",
    "                print('can be ERROR',article['paper_id'])\n",
    "    p_authors.append(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_papers_features = {'paper_id': p_ids,'s2_pdf_hash':p_s2pdf,'year':p_year,\n",
    "                       'arxiv_id': p_arxiv,'acl_id':p_acl,'pmc_id':p_pmc,'pubmed_id':p_pubmed,'doi':p_doi,\n",
    "                       'venue': p_venue,'journal': p_journal,\n",
    "                       'abstract':p_abstract,'title':p_title,'authors':p_authors,\n",
    "                       'abstract_grobid':p_abstract_grobid,'abstract_latex':p_abstract_latex,\n",
    "                       'cnt_out_grobid':p_cnt_out_grobid,'cnt_out_latex':p_cnt_out_latex}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acl_features  = pd.DataFrame(acl_papers_features)\n",
    "df_acl_features.index = p_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>s2_pdf_hash</th>\n",
       "      <th>year</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>acl_id</th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>cnt_out_grobid</th>\n",
       "      <th>cnt_out_latex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13756507</th>\n",
       "      <td>13756507</td>\n",
       "      <td>3944c1740bb643415583363a0a35f12f8bd19a16</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ArXiv</td>\n",
       "      <td>ArXiv</td>\n",
       "      <td>Recommendation systems play a vital role to ke...</td>\n",
       "      <td>Collaborative Memory Network for Recommendatio...</td>\n",
       "      <td>[Travis Ebesu, Bin Shen, Yi Fang]</td>\n",
       "      <td>ABSTRACTRecommendation systems play a vital ro...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>f3de408be7d2e2720a61451bd196ac7e1ed9363a</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ACL</td>\n",
       "      <td>Proceedings of the 53rd Annual Meeting of the ...</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21369026</th>\n",
       "      <td>21369026</td>\n",
       "      <td>abd6baaf7e9d830feb3889e1019a989b6959ae89</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TPDL</td>\n",
       "      <td>Lecture Notes in Computer Science</td>\n",
       "      <td>0</td>\n",
       "      <td>On the Uses of Word Sense Change for Research ...</td>\n",
       "      <td>[Nina Tahmasebi, Thomas Risse]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497200</th>\n",
       "      <td>2497200</td>\n",
       "      <td>5e3cd43b9c0288a08c782b8df1311dd3e4f06c9b</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TOIS</td>\n",
       "      <td>ACM Trans. Inf. Syst.</td>\n",
       "      <td>We describe an approach to text classification...</td>\n",
       "      <td>Information extraction as a basis for high-pre...</td>\n",
       "      <td>[Ellen Riloff, Wendy Lehnert]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406684</th>\n",
       "      <td>406684</td>\n",
       "      <td>a68dc3c376272c5c08df211c6974f02380295ebd</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IEEE Transactions on Knowledge and Data Engine...</td>\n",
       "      <td>IEEE Transactions on Knowledge and Data Engine...</td>\n",
       "      <td>In many practical data mining applications, su...</td>\n",
       "      <td>Tri-training: exploiting unlabeled data using ...</td>\n",
       "      <td>[Zhi-Hua Zhou, Ming Li]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4964135</th>\n",
       "      <td>4964135</td>\n",
       "      <td>e540e8c852ad8e7e80f2dda4c3deae536bd544aa</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ICML '09</td>\n",
       "      <td>Proceedings of the 26th Annual International C...</td>\n",
       "      <td>We propose a multiple source domain adaptation...</td>\n",
       "      <td>Domain adaptation from multiple sources via au...</td>\n",
       "      <td>[Lixin Duan, Ivor Tsang, Dong Xu, Tat-Seng Chua]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6222370</th>\n",
       "      <td>6222370</td>\n",
       "      <td>08823db4f7acd0a72ffdbac4f1adfd3e52e22516</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IEEE Transactions on Audio, Speech, and Langua...</td>\n",
       "      <td>IEEE Transactions on Audio, Speech, and Langua...</td>\n",
       "      <td>This paper describes a method for obtaining a ...</td>\n",
       "      <td>A chorus section detection method for musical ...</td>\n",
       "      <td>[M. Goto]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34547425</th>\n",
       "      <td>34547425</td>\n",
       "      <td>1e21d0480f2c03859461dc7abd1c11b137bd5dcf</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ArXiv</td>\n",
       "      <td>ArXiv</td>\n",
       "      <td>This paper introduces THUMT, an open-source to...</td>\n",
       "      <td>THUMT: An Open Source Toolkit for Neural Machi...</td>\n",
       "      <td>[Jiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yon...</td>\n",
       "      <td>AbstractThis paper introduces THUMT, an openso...</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53079238</th>\n",
       "      <td>53079238</td>\n",
       "      <td>792f5bf93d5e43b51a3f9ad4c1bb5af48bd69b2e</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>Proceedings of the 2018 Conference on Empirica...</td>\n",
       "      <td>0</td>\n",
       "      <td>Improving Large-Scale Fact-Checking using Deco...</td>\n",
       "      <td>[Nayeon Lee, Chien-Sheng Wu, Pascale Fung]</td>\n",
       "      <td>AbstractFact-checking of textual sources needs...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7431927</th>\n",
       "      <td>7431927</td>\n",
       "      <td>044490c14a54471055564043c19d23434b29da2d</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ACL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Crosslingual Induction of Semantic Roles</td>\n",
       "      <td>[Ivan Titov, Alexandre Klementiev]</td>\n",
       "      <td>AbstractWe argue that multilingual parallel da...</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17981782</th>\n",
       "      <td>17981782</td>\n",
       "      <td>78f8901f907e1b2a6ceff139aabeca5a438a12b4</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ACL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Role of Syntax in Vector Space Models of C...</td>\n",
       "      <td>[Karl Hermann, Phil Blunsom]</td>\n",
       "      <td>AbstractModelling the compositional process by...</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18147382</th>\n",
       "      <td>18147382</td>\n",
       "      <td>fdf29d5dec6f929409e0bb340ae973a91680ad17</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>IJCNLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Cross-domain Feature Selection for Language Id...</td>\n",
       "      <td>[Marco Lui, Timothy Baldwin]</td>\n",
       "      <td>AbstractWe show that transductive (cross-domai...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7068371</th>\n",
       "      <td>7068371</td>\n",
       "      <td>afd48d6b8b4455b76aa56e0f9b472b9c92200e46</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Streaming Analysis of Discourse Participants</td>\n",
       "      <td>[Benjamin Van Durme]</td>\n",
       "      <td>AbstractInferring attributes of discourse part...</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13748556</th>\n",
       "      <td>13748556</td>\n",
       "      <td>e4dd0962666d0d98783da0c492269458fc551935</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ACL2018</td>\n",
       "      <td>ArXiv</td>\n",
       "      <td>Unsupervised neural machine translation (NMT) ...</td>\n",
       "      <td>Unsupervised Neural Machine Translation with W...</td>\n",
       "      <td>[Zhen Yang, Wei Chen, Feng Wang, Bo Xu]</td>\n",
       "      <td>AbstractUnsupervised neural machine translatio...</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538820</th>\n",
       "      <td>538820</td>\n",
       "      <td>413c1142de9d91804d6d11c67ff3fed59c9fc279</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Journal of Machine Learning Research</td>\n",
       "      <td>J. Mach. Learn. Res.</td>\n",
       "      <td>0</td>\n",
       "      <td>Adaptive subgradient methods for online learni...</td>\n",
       "      <td>[John Duchi, Elad Hazan, Yoram Singer]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140282220</th>\n",
       "      <td>140282220</td>\n",
       "      <td>None</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>'Affective computing' is a branch of computing...</td>\n",
       "      <td>A Blueprint for Affective Computing: A sourceb...</td>\n",
       "      <td>[Klaus Scherer, Tanja Bänziger, Etienne Roesch]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417913</th>\n",
       "      <td>417913</td>\n",
       "      <td>b52fe0b796e4c899624ed3e9d9ea566453156844</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Proceedings of the 49th Annual Meeting of the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Recognizing Named Entities in Tweets</td>\n",
       "      <td>[Xiaohua Liu, Shaodian Zhang, Furu Wei, Ming Z...</td>\n",
       "      <td>AbstractThe challenges of Named Entities Recog...</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6743006</th>\n",
       "      <td>6743006</td>\n",
       "      <td>98ae53714a7927cafe7062db3d08dd2303502eb6</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>Proceedings of the 2017 Conference on Empirica...</td>\n",
       "      <td>0</td>\n",
       "      <td>Does syntax help discourse segmentation? Not s...</td>\n",
       "      <td>[Chloé Braud, Ophélie Lacroix, Anders Søgaard]</td>\n",
       "      <td>AbstractDiscourse segmentation is the first st...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17058294</th>\n",
       "      <td>17058294</td>\n",
       "      <td>18488096e2e7226af1599f5e785ef4f490f2ece3</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>EACL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A Support Platform for Event Detection using S...</td>\n",
       "      <td>[Timothy Baldwin, Paul Cook, Bo Han, Aaron Har...</td>\n",
       "      <td>AbstractThis paper describes a system designed...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267094</th>\n",
       "      <td>267094</td>\n",
       "      <td>7cf14744f00ba64f30ac4b4b1519b111f2f89d10</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Proceedings of the Third Workshop on Innovativ...</td>\n",
       "      <td>Proceedings of the Third Workshop on Innovativ...</td>\n",
       "      <td>The automatic analysis and categorization of w...</td>\n",
       "      <td>Real Time Web Text Classification and Analysis...</td>\n",
       "      <td>[Eleni Miltsakaki, Audrey Troutt]</td>\n",
       "      <td>AbstractThe automatic analysis and categorizat...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            paper_id                               s2_pdf_hash  year  \\\n",
       "13756507    13756507  3944c1740bb643415583363a0a35f12f8bd19a16  2018   \n",
       "14472576    14472576  f3de408be7d2e2720a61451bd196ac7e1ed9363a  2015   \n",
       "21369026    21369026  abd6baaf7e9d830feb3889e1019a989b6959ae89  2017   \n",
       "2497200      2497200  5e3cd43b9c0288a08c782b8df1311dd3e4f06c9b  1994   \n",
       "406684        406684  a68dc3c376272c5c08df211c6974f02380295ebd  2005   \n",
       "4964135      4964135  e540e8c852ad8e7e80f2dda4c3deae536bd544aa  2009   \n",
       "6222370      6222370  08823db4f7acd0a72ffdbac4f1adfd3e52e22516  2006   \n",
       "34547425    34547425  1e21d0480f2c03859461dc7abd1c11b137bd5dcf  2017   \n",
       "53079238    53079238  792f5bf93d5e43b51a3f9ad4c1bb5af48bd69b2e  2018   \n",
       "7431927      7431927  044490c14a54471055564043c19d23434b29da2d  2012   \n",
       "17981782    17981782  78f8901f907e1b2a6ceff139aabeca5a438a12b4  2013   \n",
       "18147382    18147382  fdf29d5dec6f929409e0bb340ae973a91680ad17  2011   \n",
       "7068371      7068371  afd48d6b8b4455b76aa56e0f9b472b9c92200e46  2012   \n",
       "13748556    13748556  e4dd0962666d0d98783da0c492269458fc551935  2018   \n",
       "538820        538820  413c1142de9d91804d6d11c67ff3fed59c9fc279  2010   \n",
       "140282220  140282220                                      None  2010   \n",
       "417913        417913  b52fe0b796e4c899624ed3e9d9ea566453156844  2011   \n",
       "6743006      6743006  98ae53714a7927cafe7062db3d08dd2303502eb6  2017   \n",
       "17058294    17058294  18488096e2e7226af1599f5e785ef4f490f2ece3  2012   \n",
       "267094        267094  7cf14744f00ba64f30ac4b4b1519b111f2f89d10  2008   \n",
       "\n",
       "           arxiv_id  acl_id  pmc_id  pubmed_id  doi  \\\n",
       "13756507          1       0       0          0    1   \n",
       "14472576          0       1       0          0    1   \n",
       "21369026          0       0       0          0    1   \n",
       "2497200           0       0       0          0    1   \n",
       "406684            0       0       0          0    1   \n",
       "4964135           0       0       0          0    1   \n",
       "6222370           0       0       0          0    1   \n",
       "34547425          1       0       0          0    0   \n",
       "53079238          0       1       0          0    1   \n",
       "7431927           0       1       0          0    0   \n",
       "17981782          0       1       0          0    0   \n",
       "18147382          0       1       0          0    0   \n",
       "7068371           0       1       0          0    0   \n",
       "13748556          1       1       0          0    1   \n",
       "538820            0       0       0          0    0   \n",
       "140282220         0       0       0          0    0   \n",
       "417913            0       1       0          0    0   \n",
       "6743006           0       1       0          0    1   \n",
       "17058294          0       1       0          0    0   \n",
       "267094            0       1       0          0    1   \n",
       "\n",
       "                                                       venue  \\\n",
       "13756507                                               ArXiv   \n",
       "14472576                                                 ACL   \n",
       "21369026                                                TPDL   \n",
       "2497200                                                 TOIS   \n",
       "406684     IEEE Transactions on Knowledge and Data Engine...   \n",
       "4964135                                             ICML '09   \n",
       "6222370    IEEE Transactions on Audio, Speech, and Langua...   \n",
       "34547425                                               ArXiv   \n",
       "53079238                                               EMNLP   \n",
       "7431927                                                  ACL   \n",
       "17981782                                                 ACL   \n",
       "18147382                                              IJCNLP   \n",
       "7068371                                                EMNLP   \n",
       "13748556                                             ACL2018   \n",
       "538820                  Journal of Machine Learning Research   \n",
       "140282220                                                  0   \n",
       "417913     Proceedings of the 49th Annual Meeting of the ...   \n",
       "6743006                                                EMNLP   \n",
       "17058294                                                EACL   \n",
       "267094     Proceedings of the Third Workshop on Innovativ...   \n",
       "\n",
       "                                                     journal  \\\n",
       "13756507                                               ArXiv   \n",
       "14472576   Proceedings of the 53rd Annual Meeting of the ...   \n",
       "21369026                   Lecture Notes in Computer Science   \n",
       "2497200                                ACM Trans. Inf. Syst.   \n",
       "406684     IEEE Transactions on Knowledge and Data Engine...   \n",
       "4964135    Proceedings of the 26th Annual International C...   \n",
       "6222370    IEEE Transactions on Audio, Speech, and Langua...   \n",
       "34547425                                               ArXiv   \n",
       "53079238   Proceedings of the 2018 Conference on Empirica...   \n",
       "7431927                                                    0   \n",
       "17981782                                                   0   \n",
       "18147382                                                   0   \n",
       "7068371                                                    0   \n",
       "13748556                                               ArXiv   \n",
       "538820                                  J. Mach. Learn. Res.   \n",
       "140282220                                                  0   \n",
       "417913                                                     0   \n",
       "6743006    Proceedings of the 2017 Conference on Empirica...   \n",
       "17058294                                                   0   \n",
       "267094     Proceedings of the Third Workshop on Innovativ...   \n",
       "\n",
       "                                                    abstract  \\\n",
       "13756507   Recommendation systems play a vital role to ke...   \n",
       "14472576   How do we build a semantic parser in a new dom...   \n",
       "21369026                                                   0   \n",
       "2497200    We describe an approach to text classification...   \n",
       "406684     In many practical data mining applications, su...   \n",
       "4964135    We propose a multiple source domain adaptation...   \n",
       "6222370    This paper describes a method for obtaining a ...   \n",
       "34547425   This paper introduces THUMT, an open-source to...   \n",
       "53079238                                                   0   \n",
       "7431927                                                    0   \n",
       "17981782                                                   0   \n",
       "18147382                                                   0   \n",
       "7068371                                                    0   \n",
       "13748556   Unsupervised neural machine translation (NMT) ...   \n",
       "538820                                                     0   \n",
       "140282220  'Affective computing' is a branch of computing...   \n",
       "417913                                                     0   \n",
       "6743006                                                    0   \n",
       "17058294                                                   0   \n",
       "267094     The automatic analysis and categorization of w...   \n",
       "\n",
       "                                                       title  \\\n",
       "13756507   Collaborative Memory Network for Recommendatio...   \n",
       "14472576                Building a Semantic Parser Overnight   \n",
       "21369026   On the Uses of Word Sense Change for Research ...   \n",
       "2497200    Information extraction as a basis for high-pre...   \n",
       "406684     Tri-training: exploiting unlabeled data using ...   \n",
       "4964135    Domain adaptation from multiple sources via au...   \n",
       "6222370    A chorus section detection method for musical ...   \n",
       "34547425   THUMT: An Open Source Toolkit for Neural Machi...   \n",
       "53079238   Improving Large-Scale Fact-Checking using Deco...   \n",
       "7431927             Crosslingual Induction of Semantic Roles   \n",
       "17981782   The Role of Syntax in Vector Space Models of C...   \n",
       "18147382   Cross-domain Feature Selection for Language Id...   \n",
       "7068371         Streaming Analysis of Discourse Participants   \n",
       "13748556   Unsupervised Neural Machine Translation with W...   \n",
       "538820     Adaptive subgradient methods for online learni...   \n",
       "140282220  A Blueprint for Affective Computing: A sourceb...   \n",
       "417913                  Recognizing Named Entities in Tweets   \n",
       "6743006    Does syntax help discourse segmentation? Not s...   \n",
       "17058294   A Support Platform for Event Detection using S...   \n",
       "267094     Real Time Web Text Classification and Analysis...   \n",
       "\n",
       "                                                     authors  \\\n",
       "13756507                   [Travis Ebesu, Bin Shen, Yi Fang]   \n",
       "14472576          [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "21369026                      [Nina Tahmasebi, Thomas Risse]   \n",
       "2497200                        [Ellen Riloff, Wendy Lehnert]   \n",
       "406684                               [Zhi-Hua Zhou, Ming Li]   \n",
       "4964135     [Lixin Duan, Ivor Tsang, Dong Xu, Tat-Seng Chua]   \n",
       "6222370                                            [M. Goto]   \n",
       "34547425   [Jiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yon...   \n",
       "53079238          [Nayeon Lee, Chien-Sheng Wu, Pascale Fung]   \n",
       "7431927                   [Ivan Titov, Alexandre Klementiev]   \n",
       "17981782                        [Karl Hermann, Phil Blunsom]   \n",
       "18147382                        [Marco Lui, Timothy Baldwin]   \n",
       "7068371                                 [Benjamin Van Durme]   \n",
       "13748556             [Zhen Yang, Wei Chen, Feng Wang, Bo Xu]   \n",
       "538820                [John Duchi, Elad Hazan, Yoram Singer]   \n",
       "140282220    [Klaus Scherer, Tanja Bänziger, Etienne Roesch]   \n",
       "417913     [Xiaohua Liu, Shaodian Zhang, Furu Wei, Ming Z...   \n",
       "6743006       [Chloé Braud, Ophélie Lacroix, Anders Søgaard]   \n",
       "17058294   [Timothy Baldwin, Paul Cook, Bo Han, Aaron Har...   \n",
       "267094                     [Eleni Miltsakaki, Audrey Troutt]   \n",
       "\n",
       "                                             abstract_grobid abstract_latex  \\\n",
       "13756507   ABSTRACTRecommendation systems play a vital ro...              0   \n",
       "14472576   AbstractHow do we build a semantic parser in a...              0   \n",
       "21369026                                                   0              0   \n",
       "2497200                                                    0              0   \n",
       "406684                                                     0              0   \n",
       "4964135                                                    0              0   \n",
       "6222370                                                    0              0   \n",
       "34547425   AbstractThis paper introduces THUMT, an openso...              0   \n",
       "53079238   AbstractFact-checking of textual sources needs...              0   \n",
       "7431927    AbstractWe argue that multilingual parallel da...              0   \n",
       "17981782   AbstractModelling the compositional process by...              0   \n",
       "18147382   AbstractWe show that transductive (cross-domai...              0   \n",
       "7068371    AbstractInferring attributes of discourse part...              0   \n",
       "13748556   AbstractUnsupervised neural machine translatio...              0   \n",
       "538820                                                     0              0   \n",
       "140282220                                                  0              0   \n",
       "417913     AbstractThe challenges of Named Entities Recog...              0   \n",
       "6743006    AbstractDiscourse segmentation is the first st...              0   \n",
       "17058294   AbstractThis paper describes a system designed...              0   \n",
       "267094     AbstractThe automatic analysis and categorizat...              0   \n",
       "\n",
       "           cnt_out_grobid  cnt_out_latex  \n",
       "13756507               40             41  \n",
       "14472576               27              0  \n",
       "21369026               29              0  \n",
       "2497200                 0              0  \n",
       "406684                 30              0  \n",
       "4964135                20              0  \n",
       "6222370                30              0  \n",
       "34547425               17             15  \n",
       "53079238               23              0  \n",
       "7431927                52              0  \n",
       "17981782               39              0  \n",
       "18147382               33              0  \n",
       "7068371                33              0  \n",
       "13748556               31             17  \n",
       "538820                 44              0  \n",
       "140282220               0              0  \n",
       "417913                 28              0  \n",
       "6743006                42              0  \n",
       "17058294                5              0  \n",
       "267094                 14              0  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = {\n",
    "    'ranlp':'Recent Advances in Natural Language Processing',\n",
    "    'anlp':'Applied Natural Language Processing',\n",
    "    'conll':'Computational Natural Language Learning',\n",
    "    'eacl':'European Chapter of the Association for Computational Linguistic',\n",
    "    'emnlp':'Empirical Methods in Natural Language Processing',\n",
    "    'naacl':'North American Chapter of the Association for Computational Linguistic',\n",
    "    'semeval':'Semantic Evaluation',#Lexical and Computational Semantics and \n",
    "    'tacl':'Transactions of the Association for Computational Linguistics',\n",
    "    'alta':'Australasian Language Technology Association',\n",
    "    'coling':'International Conference on Computational Linguistics',# aka ICCL\n",
    "    'hlt':'Human Language Technology',\n",
    "    'ijcnlp':'International Joint Conference on Natural Language Processing',\n",
    "    'jep-taln-recital':'JEP TALN RECITAL',\n",
    "    'lrec':'Language Resources and Evaluation',\n",
    "    'muc':'Message Understanding Conf',\n",
    "    'paclic':'Pacific Asia Conference on Language, Information and Computation',\n",
    "    'rocling-ijclclp':'Rocling Computation Linguistics',\n",
    "    'tinlap':'Theoretical Issues In Natural Language Processing',\n",
    "    'tipster':'TIPSTER',\n",
    "    'acl':'Association for Computational Linguistic',\n",
    "    'acl1':'Association of Computational Linguistic',\n",
    "    'cl':'Computational Linguistics Journal',\n",
    "    'senseval':'SENSEVAL',\n",
    "    'dialogue':'dialogue'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ArXiv'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = df_acl_features.venue.values[0]\n",
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArXiv']"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = val.replace(',',' ').split()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_in(list_toks,line):\n",
    "    flag = False\n",
    "    line = line.lower()\n",
    "    for tok in list_toks:\n",
    "        if tok.lower() in line:\n",
    "            flag = tok\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_keys = list(events.keys())+['arxiv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_values = {item.lower():key.lower() for key,item in events.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['recent advances in natural language processing', 'applied natural language processing', 'computational natural language learning', 'european chapter of the association for computational linguistic', 'empirical methods in natural language processing', 'north american chapter of the association for computational linguistic', 'semantic evaluation', 'transactions of the association for computational linguistics', 'australasian language technology association', 'international conference on computational linguistics', 'human language technology', 'international joint conference on natural language processing', 'jep taln recital', 'language resources and evaluation', 'message understanding conf', 'pacific asia conference on language, information and computation', 'rocling computation linguistics', 'theoretical issues in natural language processing', 'tipster', 'association for computational linguistic', 'association of computational linguistic', 'computational linguistics journal', 'senseval', 'dialogue'])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_values_keys = events_values.keys()\n",
    "events_values_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ICPR'\""
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "s = \"This must not be deleted, but the number at the end yes 134411\"\n",
    "s = \"ICPR'06\"\n",
    "s = re.sub(\"\\d+\", \"\", s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_venue_keys = ['journal','conference','workshop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21369026 TPDL --- Yes|word_upp = TPDL\n",
      "2497200 TOIS --- Yes|word_upp = TOIS\n",
      "406684 IEEE Transactions on Knowledge and Data Engineering --- Yes|word_upp = IEEE\n",
      "4964135 ICML '09 --- Yes|word_upp = ICML\n",
      "6222370 IEEE Transactions on Audio, Speech, and Language Processing --- Yes|word_upp = IEEE\n",
      "538820 <additional> journal of machine learning research\n",
      "140282220 nothing\n",
      "417913 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies --- Yes|mult_in = acl\n",
      "267094 Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications --- Yes|word_upp = NLP\n",
      "267094 <additional> proceedings of the third workshop on innovative use of nlp for building educational applications\n",
      "12547848 International Workshop On Paraphrasing IWP --- Yes|word_upp = IWP\n",
      "12547848 <additional> international workshop on paraphrasing iwp\n",
      "37306935 TAC --- Yes|word_upp = TAC\n",
      "60510120 nothing\n",
      "13748870 NeurIPS 2018 Proceedings --- Yes|word_upp = NeurIPS\n",
      "16669270 nothing\n",
      "14831093 SIGIR '11 --- Yes|word_upp = SIGIR\n",
      "17562582 IJCAI --- Yes|word_upp = IJCAI\n",
      "52153702 TPDL --- Yes|word_upp = TPDL\n",
      "1762679 other in Proc. Interspeech ’12\n",
      "2558305 CIKM '11 --- Yes|word_upp = CIKM\n",
      "28545836 BioNLP --- Yes|word_upp = BioNLP\n",
      "3104517 BMC bioinformatics --- Yes|word_upp = BMC\n",
      "144019269 other Discourse Studies\n",
      "2057420 ICML --- Yes|word_upp = ICML\n",
      "5499886 TIST --- Yes|word_upp = TIST\n",
      "202118060 nothing\n",
      "146241191 nothing\n",
      "15153890 CIKM '08 --- Yes|word_upp = CIKM\n",
      "9380509 PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE  --- Yes|word_upp = PROCEEDINGS\n",
      "9380509 <additional> proceedings of the twenty third international joint conference on artificial intelligence \n",
      "15855483 2011 IEEE Workshop on Automatic Speech Recognition & Understanding --- Yes|word_upp = IEEE\n",
      "15855483 <additional>  ieee workshop on automatic speech recognition & understanding\n",
      "972418 IEEE/ACM Transactions on Computational Biology and Bioinformatics --- Yes|word_upp = IEEE/ACM\n",
      "17748932 2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721) --- Yes|word_upp = IEEE\n",
      "17748932 <additional>  ieee workshop on automatic speech recognition and understanding  ieee cat. no.ex \n",
      "46684770 APWeb --- Yes|word_upp = APWeb\n",
      "62398164 nothing\n",
      "895559 NIPS --- Yes|word_upp = NIPS\n",
      "600271 INTERSPEECH --- Yes|word_upp = INTERSPEECH\n",
      "9993527 ICCPOL --- Yes|word_upp = ICCPOL\n",
      "11737218 WSDM '11 --- Yes|word_upp = WSDM\n",
      "145580646 other Language and Cognitive Processes\n",
      "16277970 other Summer School on Neural Networks\n",
      "44252376 ECSCW --- Yes|word_upp = ECSCW\n",
      "7431525 other Advances in Neural Information Processing Systems\n",
      "15328442 nothing\n",
      "15897401 ICEIS --- Yes|word_upp = ICEIS\n",
      "38078518 other Bioinformatics\n",
      "12205133 nothing\n",
      "14230599 2008 IEEE Spoken Language Technology Workshop --- Yes|word_upp = IEEE\n",
      "14230599 <additional>  ieee spoken language technology workshop\n",
      "6299901 nothing\n",
      "16092814 Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007) --- Yes|word_upp = ACIS\n",
      "16092814 <additional> eighth acis international conference on software engineering  artificial intelligence  networking  and parallel/distributed computing  snpd  \n",
      "27487641 ECIR --- Yes|word_upp = ECIR\n",
      "1122292 Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005. --- Yes|word_upp = ICASSP\n",
      "1122292 <additional> proceedings.  icassp ' . ieee international conference on acoustics  speech  and signal processing  .\n",
      "1332198 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions --- Yes|mult_in = acl\n",
      "14779001 other Transactions of the Japanese Society for Artificial Intelligence\n",
      "6108278 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies --- Yes|mult_in = acl\n",
      "6570913 nothing\n",
      "15689724 <additional> australasian conference on artificial intelligence\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for ind,val in zip(df_acl_features.index,df_acl_features.venue):\n",
    "    if cnt == 100:\n",
    "        break\n",
    "    cnt+=1\n",
    "    flag_continue = True\n",
    "    if val == 0:\n",
    "        print(ind,'nothing')\n",
    "        continue\n",
    "    if mult_in(events_keys,val.lower()):\n",
    "#         print(ind,val,'--- Yes|mult_in =',mult_in(events_keys,val.lower()))\n",
    "        continue\n",
    "    if mult_in(events_values_keys,val.lower()):\n",
    "        print(ind,val,'--- Yes|mult_in =',events_values[mult_in(events_values_keys,val.lower())])\n",
    "        continue\n",
    "    text = val.replace(',',' ').replace('-',' ').replace('+',' ').replace('(',' ').replace(')',' ').split()\n",
    "    for word in text:\n",
    "        word_upp_cnt = sum(map(str.isupper, word))\n",
    "        if not flag_continue:\n",
    "            break\n",
    "        if word_upp_cnt>1:\n",
    "            print(ind,val,'--- Yes|word_upp =',word)\n",
    "            flag_continue =False\n",
    "            \n",
    "    if mult_in(additional_venue_keys,val.lower()):\n",
    "        val = val.replace(',',' ').replace('-',' ').replace('+',' ').replace('(',' ').replace(')',' ')\n",
    "        val = re.sub(\"\\d+\", \"\", val)\n",
    "        print(ind,'<additional>',val.lower())\n",
    "        flag_continue =False\n",
    "    if flag_continue:\n",
    "        print(ind,'other',val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def venue2acl_events(x):\n",
    "    if x == 0:\n",
    "        return 'nothing'\n",
    "    if mult_in(events_keys,x.lower()):\n",
    "        return mult_in(events_keys,x.lower())\n",
    "    if mult_in(events_values_keys,x.lower()):\n",
    "        return events_values[mult_in(events_values_keys,x.lower())]\n",
    "    x = x.replace(',',' ').replace('-',' ').replace('+',' ').replace('(',' ').replace(')',' ').replace(\"’\",' ').replace(\":\",' ').replace(\"'\",' ').replace(\"/\",' ')\n",
    "    text = re.sub(\"\\d+\", \"\", x)\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        word_upp_cnt = sum(map(str.isupper, word))\n",
    "        if word_upp_cnt>1:\n",
    "            return word        \n",
    "    if mult_in(additional_venue_keys,x.lower()):\n",
    "        return x.lower()\n",
    "    return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ACL', 'acl')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 10\n",
    "df_acl_features.venue[k],venue2acl_events(df_acl_features.venue[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acl_features['new_venue'] = df_acl_features.venue.apply(venue2acl_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arxiv': 3450,\n",
       " 'other': 2679,\n",
       " 'acl': 2257,\n",
       " 'nothing': 1788,\n",
       " 'emnlp': 1602,\n",
       " 'naacl': 1315,\n",
       " 'coling': 1163,\n",
       " 'IEEE': 1144,\n",
       " 'lrec': 578,\n",
       " 'conll': 435,\n",
       " 'cl': 422,\n",
       " 'eacl': 406,\n",
       " 'AAAI': 406,\n",
       " 'ijcnlp': 402,\n",
       " 'SIGIR': 378,\n",
       " 'CIKM': 287,\n",
       " 'IJCAI': 254,\n",
       " 'WWW': 247,\n",
       " 'INTERSPEECH': 234,\n",
       " 'ICML': 229,\n",
       " 'NIPS': 211,\n",
       " 'KDD': 162,\n",
       " 'semeval': 153,\n",
       " 'SIGDIAL': 136,\n",
       " 'tacl': 124,\n",
       " 'ICWSM': 103,\n",
       " 'hlt': 98,\n",
       " 'acl1': 96,\n",
       " 'BMC': 96,\n",
       " 'WSDM': 93,\n",
       " 'JAMIA': 92,\n",
       " 'WMT': 86,\n",
       " 'ranlp': 84,\n",
       " 'BioNLP': 74,\n",
       " 'NLP': 72,\n",
       " 'CHI': 70,\n",
       " 'anlp': 67,\n",
       " 'AMIA': 65,\n",
       " 'IN': 64,\n",
       " 'TAC': 61,\n",
       " 'journal of biomedical informatics': 59,\n",
       " 'ENLG': 57,\n",
       " 'ECIR': 54,\n",
       " 'INLG': 49,\n",
       " 'dialogue': 47,\n",
       " 'ICSLP': 47,\n",
       " 'IWSLT': 46,\n",
       " 'paclic': 43,\n",
       " 'IWCS': 42,\n",
       " 'ACM': 42,\n",
       " 'CACM': 41,\n",
       " 'TREC': 41,\n",
       " 'alta': 40,\n",
       " 'workshop on statistical machine translation': 38,\n",
       " 'PloS': 38,\n",
       " 'IUI': 37,\n",
       " 'NODALIDA': 34,\n",
       " 'ICMI': 33,\n",
       " 'ECML': 33,\n",
       " 'FLAIRS': 32,\n",
       " 'journal of machine learning research': 32,\n",
       " 'AMTA': 32,\n",
       " 'proceedings of the sixth workshop on statistical machine translation': 31,\n",
       " 'IWPT': 31,\n",
       " 'TALIP': 30,\n",
       " 'TOIS': 30,\n",
       " 'AI': 29,\n",
       " 'ICASSP': 27,\n",
       " 'journal of memory and language': 26,\n",
       " 'CogSci': 26,\n",
       " 'workshop on speech and natural language': 26,\n",
       " 'SIGMOD': 26,\n",
       " 'TSLP': 25,\n",
       " 'CSCW': 24,\n",
       " 'AISTATS': 24,\n",
       " 'NTCIR': 22,\n",
       " 'SIGHAN': 22,\n",
       " 'proceedings of the third workshop on statistical machine translation': 22,\n",
       " 'proceedings of the fourth workshop on statistical machine translation': 22,\n",
       " 'MetricsMATR': 21,\n",
       " 'EUROSPEECH': 21,\n",
       " 'muc': 21,\n",
       " 'international semantic web conference': 21,\n",
       " 'EAMT': 20,\n",
       " 'SIAM': 20,\n",
       " 'PVLDB': 20,\n",
       " 'the journal of the acoustical society of america': 19,\n",
       " 'ESWC': 19,\n",
       " 'CSUR': 18,\n",
       " 'PROCEEDINGS': 18,\n",
       " 'RecSys': 18,\n",
       " 'ECCV': 18,\n",
       " 'CVPR': 18,\n",
       " 'JASIST': 17,\n",
       " 'VLDB': 16,\n",
       " 'IWP': 16,\n",
       " 'TextGraphs': 16,\n",
       " 'NEWS': 16,\n",
       " 'ECAI': 16,\n",
       " 'PAKDD': 15,\n",
       " 'JCDL': 15,\n",
       " 'journal of the american statistical association': 15,\n",
       " 'TAG': 15,\n",
       " 'senseval': 15,\n",
       " 'MT': 15,\n",
       " 'JOURNAL': 14,\n",
       " 'PLoS': 14,\n",
       " 'SIGF': 14,\n",
       " 'ICDM': 14,\n",
       " 'proceedings of the 2010 named entities workshop': 14,\n",
       " 'TIST': 14,\n",
       " 'KONVENS': 14,\n",
       " 'IBM': 13,\n",
       " 'TSD': 13,\n",
       " 'workshop on building and using parallel texts': 13,\n",
       " 'international journal of computer vision': 13,\n",
       " 'journal of personality and social psychology': 13,\n",
       " 'SIGMORPHON': 13,\n",
       " 'COLT': 12,\n",
       " 'journal of biomedical semantics': 12,\n",
       " 'ISMIR': 12,\n",
       " 'proceedings of the fifth international natural language generation conference': 12,\n",
       " 'MM': 12,\n",
       " 'workshop on very large corpora': 12,\n",
       " 'database   the journal of biological databases and curation': 12,\n",
       " 'DAARC': 11,\n",
       " 'TMI': 11,\n",
       " 'international journal of medical informatics': 11,\n",
       " 'II': 11,\n",
       " 'SDM': 11,\n",
       " 'IJCNN': 11,\n",
       " 'NLDB': 11,\n",
       " 'SAC': 11,\n",
       " 'HRI': 11,\n",
       " 'CVSC': 11,\n",
       " 'TLT': 11,\n",
       " 'EKAW': 10,\n",
       " 'tenth international conference on parsing technologies': 10,\n",
       " 'RIAO': 10,\n",
       " 'TASLP': 10,\n",
       " 'IVA': 10,\n",
       " 'DepLing': 10,\n",
       " 'proceedings of the international natural language generation conference': 10,\n",
       " 'international natural language generation conference': 10,\n",
       " 'journal of medical internet research': 10,\n",
       " 'VarDial': 10,\n",
       " 'journal of the american society for information science': 10,\n",
       " 'journal of pragmatics': 10,\n",
       " 'WebSci': 10,\n",
       " 'AIRS': 9,\n",
       " 'ISWC': 9,\n",
       " 'ICCC': 9,\n",
       " 'NLPRS': 9,\n",
       " 'AKBC': 9,\n",
       " 'UIST': 9,\n",
       " 'journal of the american society for information science and technology': 9,\n",
       " 'neural networks   the official journal of the international neural network society': 9,\n",
       " 'journal of intelligent information systems': 9,\n",
       " 'HT': 9,\n",
       " 'ICON': 9,\n",
       " 'international workshop on parsing technology': 9,\n",
       " 'EDBT': 8,\n",
       " 'SLaTE': 8,\n",
       " 'journal of psycholinguistic research': 8,\n",
       " 'workshop on frontiers in corpus annotation': 8,\n",
       " 'LAW': 8,\n",
       " 'BIRNDL@JCDL': 8,\n",
       " 'ICSE': 8,\n",
       " 'proceedings of the workshop on multiword expressions  from parsing and generation to the real world': 8,\n",
       " 'proceedings of the 4th workshop on building and using comparable corpora  comparable corpora and the web': 8,\n",
       " 'TextInfer': 8,\n",
       " 'MULTIMEDIA': 8,\n",
       " 'PASCAL': 8,\n",
       " '*SEM': 8,\n",
       " 'SKDD': 8,\n",
       " 'PKDD': 8,\n",
       " 'NLPCC': 8,\n",
       " 'EWNLG': 8,\n",
       " 'workshop on computational approaches to semitic languages': 8,\n",
       " 'WordNet': 8,\n",
       " 'STOC': 7,\n",
       " 'IEICE': 7,\n",
       " 'international conference on acoustics  speech  and signal processing ': 7,\n",
       " 'CAP': 7,\n",
       " 'NeuroImage': 7,\n",
       " 'ECDL': 7,\n",
       " 'CIPS': 7,\n",
       " 'LSM': 7,\n",
       " 'DARPA': 7,\n",
       " 'ICANN': 7,\n",
       " 'proceedings of the 5th linguistic annotation workshop': 7,\n",
       " 'linguistic annotation workshop': 7,\n",
       " 'JACM': 7,\n",
       " 'TALLIP': 7,\n",
       " 'workshop on statistical parsing of morphologically rich languages': 7,\n",
       " 'DUC': 7,\n",
       " 'TAL': 7,\n",
       " 'AIED': 7,\n",
       " 'international journal of computer applications': 7,\n",
       " 'workshop on text summarization branches out': 7,\n",
       " 'HICSS': 7,\n",
       " 'COMMA': 7,\n",
       " 'journal of cognitive neuroscience': 6,\n",
       " 'TIIS': 6,\n",
       " 'NLPBA': 6,\n",
       " 'LASM': 6,\n",
       " 'proceedings of the workshop on current trends in biomedical natural language processing': 6,\n",
       " 'journal of cheminformatics': 6,\n",
       " 'SSRN': 6,\n",
       " 'SGMD': 6,\n",
       " 'ISCA': 6,\n",
       " 'ICAART': 6,\n",
       " 'IALP': 6,\n",
       " 'proceedings of the workshop on geometrical models of natural language semantics': 6,\n",
       " 'workshop on multilingual summarization and question answering': 6,\n",
       " 'MLMI': 6,\n",
       " 'proceedings of the second workshop on language in social media': 6,\n",
       " 'american journal of political science': 6,\n",
       " 'proceedings of the workshop on monolingual text to text generation': 6,\n",
       " 'journal of child language': 6,\n",
       " 'IbPRIA': 6,\n",
       " 'ISMB': 6,\n",
       " 'EDM': 6,\n",
       " 'proceedings of the 2010 workshop on multiword expressions  from theory to applications': 6,\n",
       " 'HCI': 6,\n",
       " 'journal of computer science and technology': 6,\n",
       " 'WASSA': 6,\n",
       " 'MWE': 6,\n",
       " 'BUCC': 6,\n",
       " 'workshop on empirical modeling of semantic equivalence and entailment': 5,\n",
       " 'ICAIL': 5,\n",
       " '1995 international conference on acoustics  speech  and signal processing': 5,\n",
       " 'DASFAA': 5,\n",
       " 'KR': 5,\n",
       " 'D&D': 5,\n",
       " 'GEMS': 5,\n",
       " 'ARTIFICIAL': 5,\n",
       " 'AND': 5,\n",
       " 'workshop on text summarization': 5,\n",
       " 'ICSC': 5,\n",
       " 'ICME': 5,\n",
       " 'workshop on a broader perspective on multiword expressions': 5,\n",
       " 'computational lexical semantics workshop': 5,\n",
       " 'workshop on deep linguistic processing': 5,\n",
       " 'workshop on natural language processing in the biomedical domain': 5,\n",
       " 'journal of natural language processing': 5,\n",
       " 'proceedings of the workshop on figurative language processing': 5,\n",
       " 'LinkKDD': 5,\n",
       " 'JMIR': 5,\n",
       " 'workshop on sentiment and subjectivity in text': 5,\n",
       " '2009 3rd international conference on affective computing and intelligent interaction and workshops': 5,\n",
       " 'ICPR': 5,\n",
       " 'BMVC': 5,\n",
       " 'conference on reference resolution and its applications': 5,\n",
       " 'proceedings of the first workshop on computing news storylines': 5,\n",
       " 'proceedings of the joint conference on chinese language processing': 5,\n",
       " '2011 international conference on computer vision': 5,\n",
       " 'workshop on balto slavonic natural language processing': 5,\n",
       " 'PROPOR': 5,\n",
       " 'proceedings of the 2014 workshop on the use of computational methods in the study of endangered languages': 5,\n",
       " 'HCOMP': 5,\n",
       " 'UCNLG': 5,\n",
       " 'ASONAM': 5,\n",
       " 'journal of experimental psychology. general': 5,\n",
       " 'ICACCI': 5,\n",
       " 'workshop on tagging text with lexical semantics  why  what  and how?': 5,\n",
       " 'MICAI': 5,\n",
       " 'DiscoMT': 5,\n",
       " 'MIT': 5,\n",
       " 'PRICAI': 5,\n",
       " 'american journal of computational linguistics': 5,\n",
       " 'ICDAR': 5,\n",
       " 'proceedings of the workshop on negation and speculation in natural language processing': 5,\n",
       " 'proceedings of the 2010 workshop on biomedical natural language processing': 5,\n",
       " 'workshop on natural language processing in biomedicine': 5,\n",
       " 'FSKD': 5,\n",
       " 'JMLR': 5,\n",
       " 'ILP': 5,\n",
       " 'SIGLEX': 5,\n",
       " 'FinTAL': 5,\n",
       " 'workshop on linking biological literature  ontologies and databases  mining biological semantics': 5,\n",
       " 'proceedings of the 2010 workshop on domain adaptation for natural language processing': 5,\n",
       " 'VERification': 5,\n",
       " 'journal of bioinformatics and computational biology': 5,\n",
       " 'journal of affective disorders': 4,\n",
       " 'KES': 4,\n",
       " 'workshop on multiword expressions  analysis  acquisition and treatment': 4,\n",
       " 'SIGCSE': 4,\n",
       " 'TO': 4,\n",
       " 'workshop on incremental parsing  bringing engineering and cognition together': 4,\n",
       " 'IDEAL': 4,\n",
       " 'CRC': 4,\n",
       " 'international journal of corpus linguistics': 4,\n",
       " 'proceedings of the workshop on parsing german': 4,\n",
       " 'AAMAS': 4,\n",
       " 'DocEng': 4,\n",
       " 'workshop on operational factors in practical  robust anaphora resolution for unrestricted texts': 4,\n",
       " 'american journal of preventive medicine': 4,\n",
       " 'workshop on multiword expressions  identifying and exploiting underlying properties': 4,\n",
       " 'LNCS': 4,\n",
       " 'journal of child psychology and psychiatry  and allied disciplines': 4,\n",
       " 'proceedings of the third workshop on issues in teaching computational linguistics': 4,\n",
       " 'workshop on multiword expressions  integrating processing': 4,\n",
       " 'SoICT': 4,\n",
       " 'american journal of infection control': 4,\n",
       " 'proceedings of the workshop on integer linear programming for natural language processing': 4,\n",
       " 'L@S': 4,\n",
       " 'NeurIPS': 4,\n",
       " 'WebDB': 4,\n",
       " '2012 international conference on privacy  security  risk and trust and 2012 international confernece on social computing': 4,\n",
       " 'CALICO': 4,\n",
       " 'ICALP': 4,\n",
       " 'SPMRL@IWPT': 4,\n",
       " 'IC': 4,\n",
       " 'journal of logic  language and information': 4,\n",
       " 'GoTAL': 4,\n",
       " 'WIDM': 4,\n",
       " 'INTERNATIONAL': 4,\n",
       " 'journal of educational psychology': 4,\n",
       " 'SIGdial': 4,\n",
       " 'ESIRMT': 4,\n",
       " 'IDA': 4,\n",
       " 'SOMA': 4,\n",
       " 'ICTIR': 4,\n",
       " 'workshop on intelligent scalable text summarization': 4,\n",
       " 'DEXA': 4,\n",
       " 'TOIT': 4,\n",
       " 'REtrieval': 4,\n",
       " 'workshop on intrinsic and extrinsic evaluation measures for machine translation and or summarization': 4,\n",
       " 'UDW@NoDaLiDa': 4,\n",
       " 'EsTAL': 4,\n",
       " 'AWIC': 4,\n",
       " 'workshop on multi word expressions in a multilingual context': 4,\n",
       " 'TACC': 4,\n",
       " 'international conference on natural language processing and knowledge engineering  2003. proceedings. 2003': 4,\n",
       " 'journal of verbal learning and verbal behavior': 4,\n",
       " 'EURASIP': 4,\n",
       " 'JAIR': 4,\n",
       " 'workshop on building and using parallel texts  data driven machine translation and beyond': 4,\n",
       " 'TKDD': 4,\n",
       " 'proceedings of the eight international conference on computational semantics': 4,\n",
       " '2005 international conference on natural language processing and knowledge engineering': 4,\n",
       " 'workshop on discourse relations and discourse markers': 4,\n",
       " 'proceedings of the workshop on computational approaches to deception detection': 4,\n",
       " 'AI*IA': 4,\n",
       " 'STEP': 4,\n",
       " 'ACII': 4,\n",
       " 'proceedings of the fourth linguistic annotation workshop': 4,\n",
       " 'CSLI': 4,\n",
       " 'MIR': 4,\n",
       " 'ASE': 4,\n",
       " 'HIS': 4,\n",
       " 'HTL': 4,\n",
       " 'journal of documentation': 4,\n",
       " 'journal of statistical software': 4,\n",
       " 'TCHI': 3,\n",
       " 'international journal on digital libraries': 3,\n",
       " 'SocInfo': 3,\n",
       " 'GIS': 3,\n",
       " 'IRE': 3,\n",
       " 'journal of substance abuse treatment': 3,\n",
       " 'journal of language and social psychology': 3,\n",
       " 'proceedings of the 3rd workshop on continuous vector space models and their compositionality': 3,\n",
       " 'IRAL': 3,\n",
       " 'COMPUTER': 3,\n",
       " 'workshop on building educational applications using natural language processing': 3,\n",
       " 'ICMR': 3,\n",
       " 'workshop on automatic summarization': 3,\n",
       " 'PSYCHOLOGICAL': 3,\n",
       " 'behavior research methods  instruments  & computers   a journal of the psychonomic society  inc': 3,\n",
       " 'proceedings of the workshop on distributional semantics and compositionality': 3,\n",
       " 'ComAComA': 3,\n",
       " 'LTC': 3,\n",
       " 'journal of neurolinguistics': 3,\n",
       " 'AINL': 3,\n",
       " 'NLG': 3,\n",
       " 'LDV': 3,\n",
       " 'journal of second language writing': 3,\n",
       " 'CIDR': 3,\n",
       " 'ListLang@NoDaLiDa': 3,\n",
       " 'JSLHR': 3,\n",
       " 'COST': 3,\n",
       " 'workshop on text meaning and interpretation': 3,\n",
       " 'AHA!': 3,\n",
       " 'EC': 3,\n",
       " 'workshop on word sense disambiguation  recent successes and future directions': 3,\n",
       " 'ICRA': 3,\n",
       " 'workshop on unsupervised lexical acquisition': 3,\n",
       " 'DG.O': 3,\n",
       " 'tipster': 3,\n",
       " 'TPDL': 3,\n",
       " '2003 joint conference on digital libraries  2003. proceedings.': 3,\n",
       " 'workshop on unsupervised learning in natural language processing': 3,\n",
       " 'WI': 3,\n",
       " 'ICEIS': 3,\n",
       " 'journal of experimental psychology. learning  memory  and cognition': 3,\n",
       " 'proceedings of the first workshop on predicting and improving text readability for target reader populations': 3,\n",
       " 'EPIA': 3,\n",
       " 'ALT': 3,\n",
       " 'ASSETS': 3,\n",
       " 'DL': 3,\n",
       " 'NEWS@ACM': 3,\n",
       " 'journal of experimental psychology. human perception and performance': 3,\n",
       " 'DMNLP@PKDD': 3,\n",
       " 'AISB': 3,\n",
       " 'CIARP': 3,\n",
       " 'AICS': 3,\n",
       " 'SLPAT': 3,\n",
       " 'journal of the royal statistical society  series b  statistical methodology ': 3,\n",
       " 'ISER': 3,\n",
       " 'KI': 3,\n",
       " '#MSM': 3,\n",
       " 'proceedings of the first workshop on algorithms and resources for modelling of dialects and language varieties': 3,\n",
       " 'GROUP': 3,\n",
       " 'workshop on information extraction beyond the document': 3,\n",
       " 'in proceedings of the 17th international conference on world wide web': 3,\n",
       " 'proceedings of the workshop on computational approaches to linguistic creativity': 3,\n",
       " 'SIGGRAPH': 3,\n",
       " '2011 international conference on document analysis and recognition': 3,\n",
       " 'journal of neural engineering': 3,\n",
       " 'ICDE': 3,\n",
       " 'proceedings of the workshop on extra propositional aspects of meaning in computational linguistics': 3,\n",
       " 'autism research   official journal of the international society for autism research': 3,\n",
       " 'GIR': 3,\n",
       " 'proceedings of the 1st workshop on sense  concept and entity representations and their applications': 3,\n",
       " 'workshop on open domain question answering': 3,\n",
       " 'PAN': 3,\n",
       " 'SODA': 3,\n",
       " 'KDIR': 3,\n",
       " 'international conference on natural language generation': 3,\n",
       " 'IBERAMIA': 3,\n",
       " 'IAAI': 3,\n",
       " 'workshop on computational approaches to semitic languages  common issues and resources': 3,\n",
       " 'workshop on computational approaches to figurative language': 3,\n",
       " 'workshop on data driven methods in machine translation': 3,\n",
       " 'the british journal of social psychology': 3,\n",
       " 'IEA': 3,\n",
       " 'workshop on the acquisition of lexical knowledge from text': 3,\n",
       " 'ICGI': 3,\n",
       " 'SLTU': 3,\n",
       " 'ICSR': 3,\n",
       " 'PMLR': 3,\n",
       " 'workshop on coreference and its applications': 3,\n",
       " 'IberEval@SEPLN': 3,\n",
       " 'ICNN': 3,\n",
       " 'american educational research journal': 3,\n",
       " 'workshop on effective tools and methodologies for teaching natural language processing and computational linguistics': 3,\n",
       " 'CoopIS': 3,\n",
       " 'SBP': 3,\n",
       " 'WISDOM': 3,\n",
       " 'LDOW': 3,\n",
       " 'WA': 3,\n",
       " 'workshop on recent advances in dependency grammar': 3,\n",
       " 'OWLED': 3,\n",
       " 'GEometrical': 3,\n",
       " 'SPIRE': 3,\n",
       " 'FIRE': 3,\n",
       " 'AIRWeb': 3,\n",
       " 'HYPERTEXT': 3,\n",
       " 'in proceedings of the sixth international conference on theoretical and methodological issues in machine translation': 3,\n",
       " 'chinese language processing workshop': 3,\n",
       " 'proceedings of the 4th workshop on syntax and structure in statistical translation': 3,\n",
       " 'MLCW': 3,\n",
       " 'NEMLAR': 2,\n",
       " 'journal of school violence': 2,\n",
       " 'proceedings of the ninth workshop on statistical machine translation': 2,\n",
       " 'PLDI': 2,\n",
       " 'the american journal of psychology': 2,\n",
       " 'proceedings of the fifteenth workshop on computational research in phonetics  phonology  and morphology': 2,\n",
       " 'workshop on enhancing and using electronic dictionaries': 2,\n",
       " 'AVI': 2,\n",
       " 'journal of health and social behavior': 2,\n",
       " 'ADMA': 2,\n",
       " 'IIR': 2,\n",
       " 'TOPICS': 2,\n",
       " 'ICCV': 2,\n",
       " 'JOCCH': 2,\n",
       " 'NLPKE': 2,\n",
       " 'journal of abnormal psychology': 2,\n",
       " 'SRSL': 2,\n",
       " 'the journal of neuroscience   the official journal of the society for neuroscience': 2,\n",
       " 'ACML': 2,\n",
       " 'proceedings of the eighth workshop on asian language resouces': 2,\n",
       " 'international journal on artificial intelligence tools': 2,\n",
       " 'WOP': 2,\n",
       " 'HCOMP@AAAI': 2,\n",
       " 'DH': 2,\n",
       " 'workshop on cognitive modeling and computational linguistics': 2,\n",
       " 'the journal of philosophy': 2,\n",
       " 'UCS': 2,\n",
       " 'TOMCCAP': 2,\n",
       " 'TALN': 2,\n",
       " 'UbiComp': 2,\n",
       " 'BMJ': 2,\n",
       " 'WSM': 2,\n",
       " 'WOSN': 2,\n",
       " 'IUCS': 2,\n",
       " 'ASIST': 2,\n",
       " 'TOPL': 2,\n",
       " 'AIMSA': 2,\n",
       " 'bell system technical journal': 2,\n",
       " 'TSEM': 2,\n",
       " 'proceedings of the seventh workshop on statistical machine translation': 2,\n",
       " '2007 international conference on natural language processing and knowledge engineering': 2,\n",
       " 'TESOL': 2,\n",
       " 'ICCSA': 2,\n",
       " 'ICIP': 2,\n",
       " 'RO': 2,\n",
       " 'academic medicine   journal of the association of american medical colleges': 2,\n",
       " 'FSMNLP': 2,\n",
       " 'annual meeting of the association for compuational linguistics   student research workshop': 2,\n",
       " 'journal of semantics': 2,\n",
       " 'APWeb': 2,\n",
       " 'european journal of cognitive psychology': 2,\n",
       " 'MDS': 2,\n",
       " 'CNS': 2,\n",
       " 'in proceedings of the 15th conference of the european association for machine translation': 2,\n",
       " 'TheScientificWorldJournal': 2,\n",
       " 'workshop on comparing corpora': 2,\n",
       " 'ER': 2,\n",
       " 'proceedings of the workshop on automatic summarization for different genres  media  and languages': 2,\n",
       " 'international journal of advanced computer science and applications': 2,\n",
       " 'journal of speech and hearing research': 2,\n",
       " 'journal of japanese linguistics': 2,\n",
       " 'SMIR@SIGIR': 2,\n",
       " 'ATS': 2,\n",
       " 'workshop on text meaning': 2,\n",
       " 'ITL': 2,\n",
       " 'journal of the royal statistical society  series b  methodological ': 2,\n",
       " 'BIOINFORMATICS': 2,\n",
       " 'workshop on feature engineering for machine learning in natural language processing': 2,\n",
       " 'international conference on acoustics  speech  and signal processing': 2,\n",
       " 'OntoNotes': 2,\n",
       " 'PNAS': 2,\n",
       " 'in proceedings of the international workshop on parsing technologies': 2,\n",
       " 'journal of communication disorders': 2,\n",
       " '2008 international conference on natural language processing and knowledge engineering': 2,\n",
       " 'IICAI': 2,\n",
       " 'SBIA': 2,\n",
       " 'proceedings of the 2nd workshop on cognitive modeling and computational linguistics': 2,\n",
       " 'SemWiki': 2,\n",
       " 'proceedings 18th international conference on data engineering': 2,\n",
       " 'workshop on computationally hard problems and joint inference in speech and language processing': 2,\n",
       " 'workshop on cognitive aspects of computational language acquisition': 2,\n",
       " 'journal of the american medical informatics association': 2,\n",
       " 'WIMS': 2,\n",
       " 'ARES': 2,\n",
       " 'proceedings of workshop on evaluation metrics and system comparison for automatic summarization': 2,\n",
       " 'workshop on making sense of sense  bringing psycholinguistics and computational linguistics together': 2,\n",
       " 'NATURAL': 2,\n",
       " 'WSOM': 2,\n",
       " 'LAK': 2,\n",
       " 'WICOW': 2,\n",
       " 'ICUIMC': 2,\n",
       " 'KRAQ': 2,\n",
       " 'ESSLLI': 2,\n",
       " 'ExProM': 2,\n",
       " 'TIME': 2,\n",
       " 'MACHINE': 2,\n",
       " '2008 eighth international conference on intelligent systems design and applications': 2,\n",
       " 'international journal of man machine studies': 2,\n",
       " 'journal on multimodal user interfaces': 2,\n",
       " 'IJCSI': 2,\n",
       " 'SMT': 2,\n",
       " 'workshop on linking biological literature  ontologies and databases': 2,\n",
       " 'BIS': 2,\n",
       " 'workshop on analyzing conversations in text and speech': 2,\n",
       " '2013 12th international conference on machine learning and applications': 2,\n",
       " '<i>WORD<': 2,\n",
       " 'DSH': 2,\n",
       " 'SIGFSM': 2,\n",
       " 'DTMBIO': 2,\n",
       " 'SSW': 2,\n",
       " 'in proceedings of the eleventh national conference on artificial intelligence': 2,\n",
       " 'WikiSym': 2,\n",
       " 'ASAC': 2,\n",
       " 'ESEC': 2,\n",
       " 'UM': 2,\n",
       " 'LANGUAGE': 2,\n",
       " 'EPL': 2,\n",
       " 'COMPUTATIONAL': 2,\n",
       " 'ICWE': 2,\n",
       " 'CAiSE': 2,\n",
       " 'in proceedings of the 22nd annual conference of the cognitive science society': 2,\n",
       " 'SAAIP@IJCAI': 2,\n",
       " 'workshop on the computational treatment of nominals': 2,\n",
       " 'journal of sociolinguistics': 2,\n",
       " 'WISE': 2,\n",
       " 'australasian conference on artificial intelligence': 2,\n",
       " 'COGNITIVE': 2,\n",
       " '2013 international conference on asian language processing': 2,\n",
       " 'GECCO': 2,\n",
       " 'WVLC': 2,\n",
       " 'workshop on ontology learning and population  bridging the gap between text and knowledge': 2,\n",
       " 'CompuTerm': 2,\n",
       " 'ISCRAM': 2,\n",
       " 'InterSpeech': 2,\n",
       " 'NLPCS': 2,\n",
       " 'proceedings of the 35th annual hawaii international conference on system sciences': 2,\n",
       " 'AVEC': 2,\n",
       " 'the journal of symbolic logic': 2,\n",
       " 'journal of communication': 2,\n",
       " 'the journal of finance': 2,\n",
       " 'IceTAL': 2,\n",
       " 'workshop on breadth and depth of semantic lexicons': 2,\n",
       " 'ITCC': 2,\n",
       " 'IWSDS': 2,\n",
       " 'ISI': 2,\n",
       " 'journal of east asian linguistics': 2,\n",
       " 'workshop on task focused summarization and question answering': 2,\n",
       " 'PROC.': 2,\n",
       " 'ICEC': 2,\n",
       " 'proceedings of the fifth international workshop on inference in computational semantics': 2,\n",
       " 'EUSIPCO': 2,\n",
       " 'CBRecSys@RecSys': 2,\n",
       " 'ICPhS': 2,\n",
       " 'in international semantic web conference': 2,\n",
       " '2009 42nd hawaii international conference on system sciences': 2,\n",
       " 'proceedings of the joint workshop on social dynamics and personal attributes in social media': 2,\n",
       " 'workshop on how can computational linguistics improve information retrieval?': 2,\n",
       " 'proceedings of the tenth workshop on statistical machine translation': 2,\n",
       " 'tinlap': 2,\n",
       " 'journal of writing research': 2,\n",
       " 'ISSN': 2,\n",
       " 'journal of biomedical discovery and collaboration': 2,\n",
       " 'TODS': 2,\n",
       " 'workshop on very large corpora  academic and industrial perspectives': 2,\n",
       " 'Experimental&Theoretical': 2,\n",
       " 'proceedings of the 2010 workshop on cognitive modeling and computational linguistics': 2,\n",
       " 'ICMLA': 2,\n",
       " 'proceedings of the first workshop on multilingual surface realisation': 2,\n",
       " 'SNAKDD': 2,\n",
       " 'AFIPS': 2,\n",
       " 'ALR': 2,\n",
       " 'proceedings of the 38th annual hawaii international conference on system sciences': 2,\n",
       " 'workshop on speech to speech translation  algorithms and systems': 2,\n",
       " 'journal of banking & finance': 2,\n",
       " 'journal of applied social psychology': 2,\n",
       " 'journal of artificial intelligence research': 2,\n",
       " 'TIG': 2,\n",
       " 'journal of autism and developmental disorders': 2,\n",
       " '2012 11th international conference on machine learning and applications': 2,\n",
       " 'XIV': 2,\n",
       " 'CoDS': 2,\n",
       " 'genome informatics. workshop on genome informatics': 2,\n",
       " 'CEAS': 2,\n",
       " 'ADCS': 2,\n",
       " 'ECSQARU': 2,\n",
       " 'CSEDU': 2,\n",
       " 'journal of finance': 2,\n",
       " '2011 international conference on advances in social networks analysis and mining': 2,\n",
       " 'proceedings of the workshop on discontinuous structures in natural language processing': 2,\n",
       " 'journal of literacy research': 2,\n",
       " 'workshop on learning word meaning from non linguistic data': 2,\n",
       " 'IX': 2,\n",
       " 'journal of consumer marketing': 2,\n",
       " 'the journal of family practice': 2,\n",
       " 'IJDAR': 2,\n",
       " 'international journal on document analysis and recognition': 2,\n",
       " 'international journal of artificial intelligence in education': 2,\n",
       " 'american journal of respiratory and critical care medicine': 2,\n",
       " 'TWEB': 2,\n",
       " '2009 10th international conference on document analysis and recognition': 2,\n",
       " 'IASSE': 2,\n",
       " 'WAT': 2,\n",
       " 'ETRA': 2,\n",
       " 'HEALTHINF': 2,\n",
       " 'BioASQ': 2,\n",
       " 'ASRU': 2,\n",
       " 'international journal of social research methodology': 2,\n",
       " 'INFOS': 2,\n",
       " 'LINC': 2,\n",
       " 'VII': 2,\n",
       " 'the american journal of psychiatry': 2,\n",
       " 'proceedings of the workshop on semantic analysis in social media': 2,\n",
       " 'ESAIR': 2,\n",
       " 'journal of chinese language and computing': 2,\n",
       " 'workshop on learning structured information in natural language applications': 2,\n",
       " '2007 international conference on machine learning and cybernetics': 2,\n",
       " '2014 14th international conference on frontiers in handwriting recognition': 2,\n",
       " 'FAT*': 2,\n",
       " 'international journal of computer and communication engineering': 2,\n",
       " 'workshop on reading comprehension tests as evaluation for computer based language understanding systems': 1,\n",
       " 'DELTA': 1,\n",
       " 'SNS': 1,\n",
       " 'ENTER': 1,\n",
       " 'ISIE': 1,\n",
       " 'journalism practice': 1,\n",
       " 'in web content mining with human language technologies workshop on the 5th international semantic web': 1,\n",
       " 'ICNC': 1,\n",
       " 'SEQUENCES': 1,\n",
       " 'journal of abnormal child psychology': 1,\n",
       " 'international journal of advanced research in artificial intelligence': 1,\n",
       " 'journal of english for academic purposes': 1,\n",
       " 'NGCT': 1,\n",
       " 'ISITIA': 1,\n",
       " 'WebKDD': 1,\n",
       " 'workshop on morphological and phonological learning': 1,\n",
       " 'SLPAT@Interspeech': 1,\n",
       " 'information retrieval journal': 1,\n",
       " 'workshop on the balancing act  combining symbolic and statistical approaches to language': 1,\n",
       " 'ICCA': 1,\n",
       " 'ACE': 1,\n",
       " 'RECURRENT': 1,\n",
       " 'XTech': 1,\n",
       " 'proceedings of the twelfth european conference on machine learning   2001   freiburg  germany  491 502': 1,\n",
       " 'BCB': 1,\n",
       " '2009 international conference on machine learning and cybernetics': 1,\n",
       " 'in proceedings of the first international workshop on human computer conversations': 1,\n",
       " 'ICNLSP': 1,\n",
       " 'journal of software': 1,\n",
       " 'SGAI': 1,\n",
       " 'in proceedings of the fourth biennial conference of the australasian cognitive science society': 1,\n",
       " 'SMBM': 1,\n",
       " 'proceedings 17th international conference on data engineering': 1,\n",
       " 'in international journal of medical informatics': 1,\n",
       " 'ICIAR': 1,\n",
       " 'DATA': 1,\n",
       " 'PST': 1,\n",
       " 'journal of artificial intelligence research  volume 14  pages 253 302  2001': 1,\n",
       " 'ICSSE': 1,\n",
       " 'MUM': 1,\n",
       " 'in proc. of the corpus linguistics workshop on using corpora for natural language generation': 1,\n",
       " 'in proceedings of the workshop on treebanks and linguistic theories': 1,\n",
       " 'proceedings of the workshop on innovative hybrid approaches to the processing of textual data': 1,\n",
       " 'AVSP': 1,\n",
       " 'BTW': 1,\n",
       " 'journalism bulletin': 1,\n",
       " 'fifth international conference on information technology  new generations  itng 2008 ': 1,\n",
       " 'EMJ': 1,\n",
       " 'in proceeedings of the european conference on speech communication and technology': 1,\n",
       " '2008 the 28th international conference on distributed computing systems': 1,\n",
       " 'ELSNET': 1,\n",
       " 'in 4th workshop on knowledge and reasoning in practical dialog systems': 1,\n",
       " 'WAPA': 1,\n",
       " '2010 2nd international workshop on database technology and applications': 1,\n",
       " 'SEMANTIC': 1,\n",
       " 'in proceedings of the fourteenth annual conference of the cognitive science society': 1,\n",
       " 'LLC': 1,\n",
       " '2009 international joint conference on neural networks': 1,\n",
       " 'DMIN': 1,\n",
       " 'in third annual workshop on the weblogging ecosystem': 1,\n",
       " 'proceedings 1998 int l workshop on natural language generation  niagara on the lake  canada  august 1998': 1,\n",
       " 'BT': 1,\n",
       " 'applied artificial intelligence journal': 1,\n",
       " 'proceedings of the 2012 asia pacific signal and information processing association annual summit and conference': 1,\n",
       " 'SRMC': 1,\n",
       " 'journal of software  evolution and process': 1,\n",
       " 'NLP.': 1,\n",
       " 'journal of neurophysiology': 1,\n",
       " 'XXIX': 1,\n",
       " '2011 international conference on asian language processing': 1,\n",
       " '2012 international conference on computer science and service system': 1,\n",
       " 'journal of artificial intelligence research  volume 36  pages 129 163  2009': 1,\n",
       " 'ICIC': 1,\n",
       " '2008 fourth international conference on networked computing and advanced information management': 1,\n",
       " 'in proceedings of the eleventh european workshop on natural language generation': 1,\n",
       " '2009 international multiconference on computer science and information technology': 1,\n",
       " 'OM': 1,\n",
       " 'SAS': 1,\n",
       " 'journal of computing and information technology': 1,\n",
       " 'OAIR': 1,\n",
       " 'IJCTT': 1,\n",
       " 'R.L.': 1,\n",
       " 'proceedings of the 9th international conference on computational semantics  2011 ': 1,\n",
       " 'echallenges e 2015 conference': 1,\n",
       " 'journal of artificial intelligence research  volume 17  pages 35 55  2002': 1,\n",
       " 'ICCPOL': 1,\n",
       " 'MobiGIS': 1,\n",
       " 'in proceedings of the workshop on linguistic knowledge acquisition and representation  bootstrapping annotated language data': 1,\n",
       " 'ESAIM': 1,\n",
       " 'in proceedings of the seventh international conference on user modeling': 1,\n",
       " 'workshop on multilingual and mixed language named entity recognition': 1,\n",
       " 'DIS': 1,\n",
       " 'MUE': 1,\n",
       " 'in proceedings of the 1st workshop on learning language in logic': 1,\n",
       " 'WSCD': 1,\n",
       " 'MIXHS': 1,\n",
       " 'ScaNaLU': 1,\n",
       " 'AGENTS': 1,\n",
       " 'COMPLEX': 1,\n",
       " 'journal of educational computing research': 1,\n",
       " 'PIKM': 1,\n",
       " 'proceedings of sixth international conference on document analysis and recognition': 1,\n",
       " 'ADS': 1,\n",
       " 'MDAI': 1,\n",
       " 'ICONIP': 1,\n",
       " 'international journal of intelligence science': 1,\n",
       " 'XVI': 1,\n",
       " 'CW': 1,\n",
       " 'ITCS': 1,\n",
       " 'XII': 1,\n",
       " 'JIDM': 1,\n",
       " 'RIVF': 1,\n",
       " 'the international journal of press politics': 1,\n",
       " 'LABELS': 1,\n",
       " 'SIGPHON': 1,\n",
       " 'IWIC': 1,\n",
       " '2013 8th international conference on computer science & education': 1,\n",
       " 'LCR': 1,\n",
       " 'journal of artificial intelligence research  volume 44  pages 179 222  2012': 1,\n",
       " 'WEBKDD': 1,\n",
       " 'JDBP': 1,\n",
       " 'COMPUTING': 1,\n",
       " 'IS&NLG': 1,\n",
       " 'QUEUE': 1,\n",
       " 'PCM': 1,\n",
       " 'SAM': 1,\n",
       " 'in proceedings of workshop on speech and language technology for education': 1,\n",
       " 'information design journal': 1,\n",
       " 'NEC': 1,\n",
       " 'journal of consumer research': 1,\n",
       " 'journal of the korean society of marine engineering': 1,\n",
       " 'IMCSIT': 1,\n",
       " 'WADS': 1,\n",
       " 'journal of the royal statistical society  series b  royal statistical society  2009  71  3   pp.593 613': 1,\n",
       " 'SEMSEARCH': 1,\n",
       " 'DHN': 1,\n",
       " '2006 8th international conference on signal processing': 1,\n",
       " 'EWRL': 1,\n",
       " 'KNOWLEDGE': 1,\n",
       " 'WORD': 1,\n",
       " 'ICCSEE': 1,\n",
       " '8th asia pacific finance association annual conference': 1,\n",
       " 'in proceedings of 9th european conference on speech communication and technology': 1,\n",
       " 'ISEC': 1,\n",
       " 'workshop on grammar engineering and evaluation': 1,\n",
       " '2009 international conference on computational science and engineering': 1,\n",
       " 'PRIMA': 1,\n",
       " 'the journal of applied psychology': 1,\n",
       " 'US': 1,\n",
       " 'workshop on linguistic distances': 1,\n",
       " 'KNOW': 1,\n",
       " 'journal of artificial intelligence research  vol 3   1995   1 24': 1,\n",
       " 'journal of new music research': 1,\n",
       " 'workshop on scalable natural language understanding': 1,\n",
       " 'JSSP': 1,\n",
       " 'the elementary school journal': 1,\n",
       " '2008 international conference on machine learning and cybernetics': 1,\n",
       " 'proceedings of the 34th annual conference of the': 1,\n",
       " 'journal of the american college of surgeons': 1,\n",
       " '2008 international conference on audio  language and image processing': 1,\n",
       " 'journal of educational media': 1,\n",
       " 'computers and biomedical research  an international journal': 1,\n",
       " 'QI': 1,\n",
       " 'proceedings of the international conference on spoken language processing': 1,\n",
       " 'british journal of political science': 1,\n",
       " 'the journal of hospital infection': 1,\n",
       " 'workshop on multilingual linguistic resources': 1,\n",
       " 'journal of artificial intelligence research  volume 37  pages 397 435  2010': 1,\n",
       " '2005 international conference on machine learning and cybernetics': 1,\n",
       " '6th workshop on very large corpora  montreal  canada  1998': 1,\n",
       " 'proceedings of the third workshop on computational approaches to linguistic code switching  2018  138 147': 1,\n",
       " 'international journal of data mining and bioinformatics': 1,\n",
       " 'ICTA': 1,\n",
       " 'DELOS': 1,\n",
       " 'economics”  the journal of business': 1,\n",
       " 'in proc. of siam international conference on data mining': 1,\n",
       " 'MSR': 1,\n",
       " 'DSMM': 1,\n",
       " 'in proceedings of language technologies for digital humanities and cultural heritage workshop': 1,\n",
       " 'JASIS': 1,\n",
       " 'proceedings of the 33rd annual hawaii international conference on system sciences': 1,\n",
       " 'ERD': 1,\n",
       " 'in workshop on statistically based natural language processing techniques': 1,\n",
       " 'NEURAL': 1,\n",
       " '2009 international conference on industrial mechatronics and automation': 1,\n",
       " 'proceedings fourth international workshop on parsing technologies': 1,\n",
       " 'IWBS': 1,\n",
       " 'the 6th international conference on soft computing and intelligent systems  and the 13th international symposium on advanced intelligence systems': 1,\n",
       " 'SWID': 1,\n",
       " 'CoLogNET': 1,\n",
       " 'PUI': 1,\n",
       " 'WAC': 1,\n",
       " 'IntelligentTutoringSystems': 1,\n",
       " 'journal of technology  learning  and assessment': 1,\n",
       " 'in proceedings of the sixth international workshop on parsing technologies': 1,\n",
       " 'GHz': 1,\n",
       " 'journal of digital information  vol 11  no 1  2010 ': 1,\n",
       " 'WCPR': 1,\n",
       " 'journal of computer and system sciences  78 5  1460 1480  2012': 1,\n",
       " 'ETS': 1,\n",
       " 'journal of artificial intelligence research  volume 42  pages 851 886  2011': 1,\n",
       " 'in proceedings of the international conference on weblogs and social': 1,\n",
       " 'proceedings of the conference of the association for machine translation in the americas': 1,\n",
       " 'CTS': 1,\n",
       " 'international journal of research studies in education': 1,\n",
       " 'L.P.J.J.': 1,\n",
       " 'in proc. of 6th belgian dutch conference on machine learning': 1,\n",
       " 'journal of artificial intelligence research  volume 39  pages 217 268  2010': 1,\n",
       " 'the british journal of psychiatry   the journal of mental science': 1,\n",
       " 'journal of artificial intelligence research  volume 34  pages 637 674  2009': 1,\n",
       " 'workshop on predicting and improving text readability for target reader populations': 1,\n",
       " 'ICCTA': 1,\n",
       " 'canadian oncology nursing journal = revue canadienne de nursing oncologique': 1,\n",
       " 'INFORMATION': 1,\n",
       " 'workshop on computational approaches to arabic script based languages': 1,\n",
       " 'proceedings of the 1st deep machine translation workshop. prague  czech republic. 2015': 1,\n",
       " 'workshop on embedded machine translation systems': 1,\n",
       " 'eCOM@SIGIR': 1,\n",
       " 'journal of information technology & politics': 1,\n",
       " 'ain shams engineering journal': 1,\n",
       " 'journal of the american society for information science. american society for information science': 1,\n",
       " 'LAWS': 1,\n",
       " 'EuroVAST@EuroVis': 1,\n",
       " 'IAPR': 1,\n",
       " 'british journal of social psychology': 1,\n",
       " 'SAI': 1,\n",
       " 'IXth': 1,\n",
       " 'proceedings of the 34th annual hawaii international conference on system sciences': 1,\n",
       " 'LiLT': 1,\n",
       " 'ACCV': 1,\n",
       " 'COMAD': 1,\n",
       " 'RObust': 1,\n",
       " 'ICIDS': 1,\n",
       " 'american journal of sociology': 1,\n",
       " 'AKRR': 1,\n",
       " 'SOEN': 1,\n",
       " 'ISIM': 1,\n",
       " 'ICDEW': 1,\n",
       " 'ITASEC': 1,\n",
       " 'journal of artificial intelligence research  volume 11  pages 335 360  1999': 1,\n",
       " 'ADBIS': 1,\n",
       " 'PolTAL': 1,\n",
       " 'in proceedings of the workshop on sharable natural language resources': 1,\n",
       " 'WePS': 1,\n",
       " 'PERS': 1,\n",
       " 'WisconsinMadison': 1,\n",
       " 'workshop on motion and video computing  2002. proceedings.': 1,\n",
       " 'in proceedings of the world conference on artificial intelligence in education': 1,\n",
       " 'COGNITION': 1,\n",
       " 'in first workshop on computational terminology': 1,\n",
       " 'GEAF': 1,\n",
       " 'IJCINI': 1,\n",
       " 'ITRE': 1,\n",
       " 'in proceedings of the seventeenth international conference on machine learning': 1,\n",
       " 'journal of chemometrics': 1,\n",
       " 'journal of artificial intelligence research  volume 11  pages 131 167  1999': 1,\n",
       " 'ICBO': 1,\n",
       " 'PACMPL': 1,\n",
       " 'IET': 1,\n",
       " '2012 international conference on machine learning and cybernetics': 1,\n",
       " 'SemWebEval@ESWC': 1,\n",
       " 'journal of experimental psychology  learning  memory  and cognition': 1,\n",
       " 'journal of artificial intelligence research  volume 15  pages 31 90  2001': 1,\n",
       " 'in  international workshop on stat. relational artif. intelligence.  2012': 1,\n",
       " 'in proceedings of the sixth international conference on spoken language processing': 1,\n",
       " 'LDK': 1,\n",
       " 'IHCI': 1,\n",
       " 'UKSim': 1,\n",
       " 'in proceedings of the 11th international conference on electronic publishing  2007': 1,\n",
       " 'InfoLab': 1,\n",
       " 'proc. of international conference on artificial intelligence': 1,\n",
       " 'in proceedings of the seventh national conference on artificial intelligence  saint paul': 1,\n",
       " 'WACKY!': 1,\n",
       " 'AM': 1,\n",
       " 'CIAA': 1,\n",
       " 'in proceedings of the 2nd international conference on computer supported education': 1,\n",
       " 'journal of mental health': 1,\n",
       " '2014 22nd international conference on pattern recognition': 1,\n",
       " 'EICT': 1,\n",
       " '2011 international conference on semantic technology and information retrieval': 1,\n",
       " 'LATA': 1,\n",
       " 'SWAD': 1,\n",
       " 'JISIC': 1,\n",
       " 'journal of artificial intelligence research  volume 29  pages 153 190  2007': 1,\n",
       " 'in u. hahn & d. harman  eds.   proceedings of the workshop on automatic summarization': 1,\n",
       " 'journal of marketing research': 1,\n",
       " 'ITALLC': 1,\n",
       " 'international journal of computer trends and technology': 1,\n",
       " 'english world wide a journal of varieties of english': 1,\n",
       " '2011 5th international conference on bioinformatics and biomedical engineering': 1,\n",
       " 'USA': 1,\n",
       " 'JCP': 1,\n",
       " 'proceedings of the 12th international conference on database and expert systems applications': 1,\n",
       " 'journal of artificial societies and social simulation': 1,\n",
       " 'international conference on machine learning 2019': 1,\n",
       " 'IS': 1,\n",
       " 'SPEECH': 1,\n",
       " '2008 fourth international conference on semantics  knowledge and grid': 1,\n",
       " 'proc. workshop treebanks and lexical theories': 1,\n",
       " 'journal of the american society of information science.   in press   baayen': 1,\n",
       " 'workshop on computer mediated language assessment and evaluation in natural language processing': 1,\n",
       " 'in international conference on weblogs and social': 1,\n",
       " 'IMPACTS': 1,\n",
       " 'NLPAR@LPNMR': 1,\n",
       " 'DIR': 1,\n",
       " 'ICCS': 1,\n",
       " 'in proceedings of the 1st international workshop on mining social': 1,\n",
       " 'MAAMAW': 1,\n",
       " '2013 8th international workshop on semantic and social media adaptation and personalization': 1,\n",
       " 'WS': 1,\n",
       " 'BENELEARN': 1,\n",
       " '[FE]': 1,\n",
       " 'JIP': 1,\n",
       " 'european journal of operations research': 1,\n",
       " 'ACSC': 1,\n",
       " 'ICoS': 1,\n",
       " 'proc. of the 21th annual conference of the cognitive science society': 1,\n",
       " 'XRDS': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_venue_val_counts  = {ind:val for ind,val in zip(df_acl_features.new_venue.value_counts().index,df_acl_features.new_venue.value_counts())}\n",
    "new_venue_val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1659"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_venue_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(new_venue_val_counts) old = 1657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставим только конференции, которые были выше какого-то treshold hfp\n",
    "def venue_limitation(x,threshold=14):\n",
    "    if new_venue_val_counts[x]>=threshold:\n",
    "        return x\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACL'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.venue[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acl_features.venue = df_acl_features.new_venue.apply(venue_limitation)\n",
    "df_acl_features.drop('new_venue',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acl_features.drop('journal',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставим только конференции, которые были выше какого-то treshold hfp\n",
    "def venue_citation(x,threshold=14):\n",
    "    if x == 'other' or x == 'nothing':\n",
    "        return 0\n",
    "    return new_venue_val_counts[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acl_features['venue_cit_n'] = df_acl_features.venue.apply(venue_citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>s2_pdf_hash</th>\n",
       "      <th>year</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>acl_id</th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>cnt_out_grobid</th>\n",
       "      <th>cnt_out_latex</th>\n",
       "      <th>venue_cit_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13756507</th>\n",
       "      <td>13756507</td>\n",
       "      <td>3944c1740bb643415583363a0a35f12f8bd19a16</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>Recommendation systems play a vital role to ke...</td>\n",
       "      <td>Collaborative Memory Network for Recommendatio...</td>\n",
       "      <td>[Travis Ebesu, Bin Shen, Yi Fang]</td>\n",
       "      <td>ABSTRACTRecommendation systems play a vital ro...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>3450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>f3de408be7d2e2720a61451bd196ac7e1ed9363a</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>acl</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21369026</th>\n",
       "      <td>21369026</td>\n",
       "      <td>abd6baaf7e9d830feb3889e1019a989b6959ae89</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>On the Uses of Word Sense Change for Research ...</td>\n",
       "      <td>[Nina Tahmasebi, Thomas Risse]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497200</th>\n",
       "      <td>2497200</td>\n",
       "      <td>5e3cd43b9c0288a08c782b8df1311dd3e4f06c9b</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TOIS</td>\n",
       "      <td>We describe an approach to text classification...</td>\n",
       "      <td>Information extraction as a basis for high-pre...</td>\n",
       "      <td>[Ellen Riloff, Wendy Lehnert]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406684</th>\n",
       "      <td>406684</td>\n",
       "      <td>a68dc3c376272c5c08df211c6974f02380295ebd</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>In many practical data mining applications, su...</td>\n",
       "      <td>Tri-training: exploiting unlabeled data using ...</td>\n",
       "      <td>[Zhi-Hua Zhou, Ming Li]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          paper_id                               s2_pdf_hash  year  arxiv_id  \\\n",
       "13756507  13756507  3944c1740bb643415583363a0a35f12f8bd19a16  2018         1   \n",
       "14472576  14472576  f3de408be7d2e2720a61451bd196ac7e1ed9363a  2015         0   \n",
       "21369026  21369026  abd6baaf7e9d830feb3889e1019a989b6959ae89  2017         0   \n",
       "2497200    2497200  5e3cd43b9c0288a08c782b8df1311dd3e4f06c9b  1994         0   \n",
       "406684      406684  a68dc3c376272c5c08df211c6974f02380295ebd  2005         0   \n",
       "\n",
       "          acl_id  pmc_id  pubmed_id  doi  venue  \\\n",
       "13756507       0       0          0    1  arxiv   \n",
       "14472576       1       0          0    1    acl   \n",
       "21369026       0       0          0    1  other   \n",
       "2497200        0       0          0    1   TOIS   \n",
       "406684         0       0          0    1   IEEE   \n",
       "\n",
       "                                                   abstract  \\\n",
       "13756507  Recommendation systems play a vital role to ke...   \n",
       "14472576  How do we build a semantic parser in a new dom...   \n",
       "21369026                                                  0   \n",
       "2497200   We describe an approach to text classification...   \n",
       "406684    In many practical data mining applications, su...   \n",
       "\n",
       "                                                      title  \\\n",
       "13756507  Collaborative Memory Network for Recommendatio...   \n",
       "14472576               Building a Semantic Parser Overnight   \n",
       "21369026  On the Uses of Word Sense Change for Research ...   \n",
       "2497200   Information extraction as a basis for high-pre...   \n",
       "406684    Tri-training: exploiting unlabeled data using ...   \n",
       "\n",
       "                                             authors  \\\n",
       "13756507           [Travis Ebesu, Bin Shen, Yi Fang]   \n",
       "14472576  [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "21369026              [Nina Tahmasebi, Thomas Risse]   \n",
       "2497200                [Ellen Riloff, Wendy Lehnert]   \n",
       "406684                       [Zhi-Hua Zhou, Ming Li]   \n",
       "\n",
       "                                            abstract_grobid abstract_latex  \\\n",
       "13756507  ABSTRACTRecommendation systems play a vital ro...              0   \n",
       "14472576  AbstractHow do we build a semantic parser in a...              0   \n",
       "21369026                                                  0              0   \n",
       "2497200                                                   0              0   \n",
       "406684                                                    0              0   \n",
       "\n",
       "          cnt_out_grobid  cnt_out_latex  venue_cit_n  \n",
       "13756507              40             41         3450  \n",
       "14472576              27              0         2257  \n",
       "21369026              29              0            0  \n",
       "2497200                0              0           30  \n",
       "406684                30              0         1144  "
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### признак цитируемость статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dict_in/dict_in_new.json') as data_file:\n",
    "    dict_in = json.load(data_file)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['73647128',\n",
       " '3521928',\n",
       " '52799282',\n",
       " '13808052',\n",
       " '26463',\n",
       " '26599055',\n",
       " '4697457',\n",
       " '18665195',\n",
       " '11212020',\n",
       " '8180128']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict_in.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_in_cit = []\n",
    "dict_in_keys = {k:1 for k in dict_in.keys()}\n",
    "for num_artic,article in enumerate(all_out_articles):\n",
    "#     if num_artic > 20:\n",
    "#         break\n",
    "    if article['paper_id'] in dict_in_keys:\n",
    "        p_in_cit.append(len(dict_in[article['paper_id']]['in']))\n",
    "    else:\n",
    "        p_in_cit.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acl_features['in_cits'] = p_in_cit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>s2_pdf_hash</th>\n",
       "      <th>year</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>acl_id</th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>cnt_out_grobid</th>\n",
       "      <th>cnt_out_latex</th>\n",
       "      <th>venue_cit_n</th>\n",
       "      <th>in_cits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16579379</th>\n",
       "      <td>16579379</td>\n",
       "      <td>8a7c82d6efb26ad725a80ecdf97acecd09f0adab</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>naacl</td>\n",
       "      <td>0</td>\n",
       "      <td>MayoNLP at SemEval-2016 Task 1: Semantic Textu...</td>\n",
       "      <td>[Naveed Afzal, Yanshan Wang, Hongfang Liu]</td>\n",
       "      <td>AbstractGiven two sentences, participating sys...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1315</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565460</th>\n",
       "      <td>7565460</td>\n",
       "      <td>f0ec25cb2e7a17fed5eae185a5b779c1c0704719</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>acl</td>\n",
       "      <td>Community question answering (cQA) has become ...</td>\n",
       "      <td>Learning Continuous Word Embedding with Metada...</td>\n",
       "      <td>[Guangyou Zhou, Tingting He, Jun Zhao, Po Hu]</td>\n",
       "      <td>AbstractCommunity question answering (cQA) has...</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2257</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990641</th>\n",
       "      <td>2990641</td>\n",
       "      <td>9207dda4344cd2e07319ec1fdba3e1124dd9b09b</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ICWSM</td>\n",
       "      <td>0</td>\n",
       "      <td>Connecting Corresponding Identities across Com...</td>\n",
       "      <td>[Reza Zafarani, Huan Liu]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10103917</th>\n",
       "      <td>10103917</td>\n",
       "      <td>2d31c7839cdf0ba8f778688e7a2b0d513cce3e4c</td>\n",
       "      <td>2011</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>conll</td>\n",
       "      <td>0</td>\n",
       "      <td>Mention Detection: Heuristics for the OntoNote...</td>\n",
       "      <td>[Jonathan Kummerfeld, Mohit Bansal, David Burk...</td>\n",
       "      <td>AbstractOur submission was a reduced version o...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>435</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076914</th>\n",
       "      <td>2076914</td>\n",
       "      <td>378d8752535f9361db73c713eda2b91a05a8b42b</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>emnlp</td>\n",
       "      <td>One of the most desired information types when...</td>\n",
       "      <td>Geo-mining: Discovery of Road and Transport Ne...</td>\n",
       "      <td>[Dmitry Davidov, Ari Rappoport]</td>\n",
       "      <td>AbstractOne of the most desired information ty...</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1602</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3551784</th>\n",
       "      <td>3551784</td>\n",
       "      <td>f91d764f59a922e3d25ada4dd1b5f1dea7320662</td>\n",
       "      <td>2013</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>acl</td>\n",
       "      <td>0</td>\n",
       "      <td>Linking and Extending an Open Multilingual Wor...</td>\n",
       "      <td>[Francis Bond, Ryan Foster]</td>\n",
       "      <td>AbstractWe create an open multilingual wordnet...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2257</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793980</th>\n",
       "      <td>793980</td>\n",
       "      <td>0a995afa8d3c114b2b431c4e2737777a0e051bff</td>\n",
       "      <td>1992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>Accumulating neuropsychological, electrophysio...</td>\n",
       "      <td>Separate visual pathways for perception and ac...</td>\n",
       "      <td>[Melvyn Goodale, A. Milner]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14273237</th>\n",
       "      <td>14273237</td>\n",
       "      <td>acb68e759e50098642a347640e84f748ea56f10b</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "      <td>Abstract : Unlike read or laboratory speech, s...</td>\n",
       "      <td>Phonetic Consequences Of Speech Disfluency</td>\n",
       "      <td>[Elizabeth Shriberg]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557484</th>\n",
       "      <td>1557484</td>\n",
       "      <td>f5fb3dd7d59b06c1e2a2c32fbd58598eab82e8f2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nothing</td>\n",
       "      <td>0</td>\n",
       "      <td>Efficient induction of probabilistic word clas...</td>\n",
       "      <td>[Grzegorz Chrupała]</td>\n",
       "      <td>AbstractWord classes automatically induced fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18889305</th>\n",
       "      <td>18889305</td>\n",
       "      <td>ca0eb4e9f08cf6fed7cb5212451d9bc4026a3efc</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ijcnlp</td>\n",
       "      <td>0</td>\n",
       "      <td>Chinese Grammatical Error Diagnosis System Bas...</td>\n",
       "      <td>[Xiupeng Wu, Peijie Huang, Jundong Wang, Qingw...</td>\n",
       "      <td>AbstractThis paper describes our system in the...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          paper_id                               s2_pdf_hash  year  arxiv_id  \\\n",
       "16579379  16579379  8a7c82d6efb26ad725a80ecdf97acecd09f0adab  2016         0   \n",
       "7565460    7565460  f0ec25cb2e7a17fed5eae185a5b779c1c0704719  2015         0   \n",
       "2990641    2990641  9207dda4344cd2e07319ec1fdba3e1124dd9b09b  2009         0   \n",
       "10103917  10103917  2d31c7839cdf0ba8f778688e7a2b0d513cce3e4c  2011         0   \n",
       "2076914    2076914  378d8752535f9361db73c713eda2b91a05a8b42b  2009         0   \n",
       "3551784    3551784  f91d764f59a922e3d25ada4dd1b5f1dea7320662  2013         0   \n",
       "793980      793980  0a995afa8d3c114b2b431c4e2737777a0e051bff  1992         0   \n",
       "14273237  14273237  acb68e759e50098642a347640e84f748ea56f10b  1999         0   \n",
       "1557484    1557484  f5fb3dd7d59b06c1e2a2c32fbd58598eab82e8f2     0         0   \n",
       "18889305  18889305  ca0eb4e9f08cf6fed7cb5212451d9bc4026a3efc  2015         0   \n",
       "\n",
       "          acl_id  pmc_id  pubmed_id  doi    venue  \\\n",
       "16579379       1       0          0    1    naacl   \n",
       "7565460        1       0          0    1      acl   \n",
       "2990641        0       0          0    0    ICWSM   \n",
       "10103917       1       0          0    0    conll   \n",
       "2076914        1       0          0    1    emnlp   \n",
       "3551784        1       0          0    0      acl   \n",
       "793980         0       0          1    1    other   \n",
       "14273237       0       0          0    0    other   \n",
       "1557484        1       0          0    0  nothing   \n",
       "18889305       1       0          0    1   ijcnlp   \n",
       "\n",
       "                                                   abstract  \\\n",
       "16579379                                                  0   \n",
       "7565460   Community question answering (cQA) has become ...   \n",
       "2990641                                                   0   \n",
       "10103917                                                  0   \n",
       "2076914   One of the most desired information types when...   \n",
       "3551784                                                   0   \n",
       "793980    Accumulating neuropsychological, electrophysio...   \n",
       "14273237  Abstract : Unlike read or laboratory speech, s...   \n",
       "1557484                                                   0   \n",
       "18889305                                                  0   \n",
       "\n",
       "                                                      title  \\\n",
       "16579379  MayoNLP at SemEval-2016 Task 1: Semantic Textu...   \n",
       "7565460   Learning Continuous Word Embedding with Metada...   \n",
       "2990641   Connecting Corresponding Identities across Com...   \n",
       "10103917  Mention Detection: Heuristics for the OntoNote...   \n",
       "2076914   Geo-mining: Discovery of Road and Transport Ne...   \n",
       "3551784   Linking and Extending an Open Multilingual Wor...   \n",
       "793980    Separate visual pathways for perception and ac...   \n",
       "14273237         Phonetic Consequences Of Speech Disfluency   \n",
       "1557484   Efficient induction of probabilistic word clas...   \n",
       "18889305  Chinese Grammatical Error Diagnosis System Bas...   \n",
       "\n",
       "                                                    authors  \\\n",
       "16579379         [Naveed Afzal, Yanshan Wang, Hongfang Liu]   \n",
       "7565460       [Guangyou Zhou, Tingting He, Jun Zhao, Po Hu]   \n",
       "2990641                           [Reza Zafarani, Huan Liu]   \n",
       "10103917  [Jonathan Kummerfeld, Mohit Bansal, David Burk...   \n",
       "2076914                     [Dmitry Davidov, Ari Rappoport]   \n",
       "3551784                         [Francis Bond, Ryan Foster]   \n",
       "793980                          [Melvyn Goodale, A. Milner]   \n",
       "14273237                               [Elizabeth Shriberg]   \n",
       "1557484                                 [Grzegorz Chrupała]   \n",
       "18889305  [Xiupeng Wu, Peijie Huang, Jundong Wang, Qingw...   \n",
       "\n",
       "                                            abstract_grobid abstract_latex  \\\n",
       "16579379  AbstractGiven two sentences, participating sys...              0   \n",
       "7565460   AbstractCommunity question answering (cQA) has...              0   \n",
       "2990641                                                   0              0   \n",
       "10103917  AbstractOur submission was a reduced version o...              0   \n",
       "2076914   AbstractOne of the most desired information ty...              0   \n",
       "3551784   AbstractWe create an open multilingual wordnet...              0   \n",
       "793980                                                    0              0   \n",
       "14273237                                                  0              0   \n",
       "1557484   AbstractWord classes automatically induced fro...              0   \n",
       "18889305  AbstractThis paper describes our system in the...              0   \n",
       "\n",
       "          cnt_out_grobid  cnt_out_latex  venue_cit_n  in_cits  \n",
       "16579379              20              0         1315        4  \n",
       "7565460               30              0         2257       35  \n",
       "2990641                1              0          103        0  \n",
       "10103917               4              0          435        5  \n",
       "2076914               24              0         1602        4  \n",
       "3551784               42              0         2257       59  \n",
       "793980                 0              0            0        0  \n",
       "14273237              21              0            0        0  \n",
       "1557484               40              0            0        5  \n",
       "18889305              15              0          402        1  "
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dict_in\n",
    "del dict_in_keys\n",
    "del p_in_cit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заглушка для создания признака цитируемости автора "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выделение обзорной части статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Самый простой принцип построения - по максимальному количеству ссылок в абзаце:\n",
    " - **Решение**:\n",
    "    - подсчитать количество ссылок в каждой секции\n",
    "    - выбрать секцию с максимальным количеством ссылок (возможно ещё оставить ещё 1 секцию, в которой количество ссылок было больше половины чем в максимальной)\n",
    "    - для latex статей надо объединить текст одинаковых секций в 1 абзац\n",
    " - **Критерий**:\n",
    "    -  в части latex публикаций есть названия секций => после выделения обзорных часте можно посмотреть какие секции выделились: какие топ-3, сделать просмотр глазами и после этого решать что делать дальше.\n",
    "    - возможно логично сохранять для 2 максимального текста название статей\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [{'start': 956, 'end': 964, 'text': 'Figure 1', 'latex': None, 'ref_id': 'FIGREF0'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 451, 'end': 468, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 12, 'end': 20, 'text': 'Figure 2', 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "6 {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [{'start': 32, 'end': 58, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 59, 'end': 79, 'text': 'Rezende et al., 2014', 'latex': None, 'ref_id': 'BIBREF18'}, {'start': 184, 'end': 200, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2384, 'end': 2401, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2433, 'end': 2456, 'text': '(Bahdanau et al., 2015;', 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 2457, 'end': 2476, 'text': 'Luong et al., 2015)', 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "3 {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .', 'cite_spans': [{'start': 86, 'end': 102, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 1133, 'end': 1158, 'text': '(Dantzig and Thapa, 2006)', 'latex': None, 'ref_id': 'BIBREF3'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 107, 'end': 118, 'text': '(Lin, 2004)', 'latex': None, 'ref_id': 'BIBREF12'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "4 {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [{'start': 351, 'end': 365, 'text': '(Wasson, 1998)', 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 492, 'end': 512, 'text': '(Radev et al., 2000)', 'latex': None, 'ref_id': 'BIBREF17'}, {'start': 700, 'end': 723, 'text': '(Erkan and Radev, 2004)', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 737, 'end': 763, 'text': '(Mihalcea and Tarau, 2004)', 'latex': None, 'ref_id': 'BIBREF14'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "2 {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .', 'cite_spans': [{'start': 498, 'end': 518, 'text': '(Kingma and Ba, 2014', 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 652, 'end': 674, 'text': '(Bastien et al., 2012)', 'latex': None, 'ref_id': 'BIBREF1'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "2 {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.', 'cite_spans': [{'start': 690, 'end': 707, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 908, 'end': 925, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 77, 'end': 84, 'text': 'Table 1', 'latex': None, 'ref_id': 'TABREF0'}, {'start': 746, 'end': 753, 'text': 'Table 2', 'latex': None, 'ref_id': 'TABREF1'}, {'start': 1184, 'end': 1191, 'text': 'Table 3', 'latex': None, 'ref_id': 'TABREF2'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".', 'cite_spans': [], 'ref_spans': [{'start': 315, 'end': 322, 'text': 'Table 4', 'latex': None, 'ref_id': 'TABREF3'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ', 'cite_spans': [{'start': 517, 'end': 868, 'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world', 'latex': None, 'ref_id': None}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "grobid_parse_overview = dict()\n",
    "for num_sec,sections in enumerate(acl_only_articles[0]['grobid_parse']['body_text']):\n",
    "    grobid_parse_overview[num_sec] = sections\n",
    "    print(len(grobid_parse_overview[num_sec]['cite_spans']),sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "  'cite_spans': [{'start': 192,\n",
       "    'end': 216,\n",
       "    'text': '(Goldstein et al., 2000;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'},\n",
       "   {'start': 217,\n",
       "    'end': 239,\n",
       "    'text': 'Erkan and Radev, 2004;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 240,\n",
       "    'end': 257,\n",
       "    'text': 'Wan et al., 2007;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF19'},\n",
       "   {'start': 258,\n",
       "    'end': 284,\n",
       "    'text': 'Nenkova and McKeown, 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF16'},\n",
       "   {'start': 285,\n",
       "    'end': 302,\n",
       "    'text': 'Min et al., 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF15'},\n",
       "   {'start': 303,\n",
       "    'end': 319,\n",
       "    'text': 'Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 773,\n",
       "    'end': 797,\n",
       "    'text': '(Project Code: 14203414)',\n",
       "    'latex': None,\n",
       "    'ref_id': None},\n",
       "   {'start': 2288,\n",
       "    'end': 2305,\n",
       "    'text': '(Hu et al., 2008;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 2306,\n",
       "    'end': 2324,\n",
       "    'text': 'Yang et al., 2011)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF22'},\n",
       "   {'start': 2582,\n",
       "    'end': 2598,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 2911,\n",
       "    'end': 2927,\n",
       "    'text': 'Li et al. (2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 3069,\n",
       "    'end': 3095,\n",
       "    'text': '(Kingma and Welling, 2014;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'},\n",
       "   {'start': 3096,\n",
       "    'end': 3117,\n",
       "    'text': 'Rezende et al., 2014)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF18'}],\n",
       "  'ref_spans': [{'start': 956,\n",
       "    'end': 964,\n",
       "    'text': 'Figure 1',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF0'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 1: {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "  'cite_spans': [{'start': 451,\n",
       "    'end': 468,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'}],\n",
       "  'ref_spans': [{'start': 12,\n",
       "    'end': 20,\n",
       "    'text': 'Figure 2',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF2'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 2: {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "  'cite_spans': [{'start': 32,\n",
       "    'end': 58,\n",
       "    'text': '(Kingma and Welling, 2014;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'},\n",
       "   {'start': 59,\n",
       "    'end': 79,\n",
       "    'text': 'Rezende et al., 2014',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF18'},\n",
       "   {'start': 184,\n",
       "    'end': 200,\n",
       "    'text': 'Li et al. (2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 2384,\n",
       "    'end': 2401,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 2433,\n",
       "    'end': 2456,\n",
       "    'text': '(Bahdanau et al., 2015;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF0'},\n",
       "   {'start': 2457,\n",
       "    'end': 2476,\n",
       "    'text': 'Luong et al., 2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF13'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 3: {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "  'cite_spans': [{'start': 86,\n",
       "    'end': 102,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 971,\n",
       "    'end': 987,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 1133,\n",
       "    'end': 1158,\n",
       "    'text': '(Dantzig and Thapa, 2006)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF3'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 4: {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 5: {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 6: {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 7: {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 8: {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "  'cite_spans': [{'start': 107,\n",
       "    'end': 118,\n",
       "    'text': '(Lin, 2004)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF12'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 9: {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "  'cite_spans': [{'start': 351,\n",
       "    'end': 365,\n",
       "    'text': '(Wasson, 1998)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF20'},\n",
       "   {'start': 492,\n",
       "    'end': 512,\n",
       "    'text': '(Radev et al., 2000)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF17'},\n",
       "   {'start': 700,\n",
       "    'end': 723,\n",
       "    'text': '(Erkan and Radev, 2004)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 737,\n",
       "    'end': 763,\n",
       "    'text': '(Mihalcea and Tarau, 2004)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF14'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 10: {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "  'cite_spans': [{'start': 498,\n",
       "    'end': 518,\n",
       "    'text': '(Kingma and Ba, 2014',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF8'},\n",
       "   {'start': 652,\n",
       "    'end': 674,\n",
       "    'text': '(Bastien et al., 2012)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF1'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 11: {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "  'cite_spans': [{'start': 690,\n",
       "    'end': 707,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 908,\n",
       "    'end': 925,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'}],\n",
       "  'ref_spans': [{'start': 77,\n",
       "    'end': 84,\n",
       "    'text': 'Table 1',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF0'},\n",
       "   {'start': 746,\n",
       "    'end': 753,\n",
       "    'text': 'Table 2',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF1'},\n",
       "   {'start': 1184,\n",
       "    'end': 1191,\n",
       "    'text': 'Table 3',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF2'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 12: {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 315,\n",
       "    'end': 322,\n",
       "    'text': 'Table 4',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF3'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 13: {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "  'cite_spans': [{'start': 517,\n",
       "    'end': 868,\n",
       "    'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "    'latex': None,\n",
       "    'ref_id': None}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_parse_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'cite_spans': [], 'ref_spans': [{'start': 118, 'end': 125, 'text': None, 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'cite_spans': [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Overview 1\n",
      "{'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 19, 'end': 26, 'text': None, 'latex': None, 'ref_id': 'FIGREF7'}], 'eq_spans': [{'start': 212, 'end': 223, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 739, 'end': 750, 'text': 'ρ i ', 'latex': '\\\\rho _i', 'ref_id': None}, {'start': 774, 'end': 785, 'text': '𝐱 c i ', 'latex': '\\\\mathbf {x}_c^i', 'ref_id': None}, {'start': 807, 'end': 818, 'text': 'ρ∈ℝ n c  ', 'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}], 'section': 'Overview'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "{'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'cite_spans': [{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [{'start': 472, 'end': 483, 'text': 'p θ (𝐳)=𝒩(0,𝐈)', 'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})', 'ref_id': None}, {'start': 488, 'end': 499, 'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)', 'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})', 'ref_id': None}, {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None}, {'start': 524, 'end': 535, 'text': 'σ', 'latex': '\\\\sigma ', 'ref_id': None}, {'start': 799, 'end': 811, 'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\", 'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n', 'ref_id': 'EQREF9'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [{'start': 50, 'end': 56, 'text': None, 'latex': None, 'ref_id': 'EQREF9'}], 'eq_spans': [{'start': 131, 'end': 142, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 145, 'end': 157, 'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\", 'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ', 'ref_id': 'EQREF10'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': '𝐱', 'latex': '\\\\mathbf {x}', 'ref_id': None}, {'start': 76, 'end': 87, 'text': '𝐱 d ', 'latex': '\\\\mathbf {x}_d', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐱 c ', 'latex': '\\\\mathbf {x}_c', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 364, 'end': 375, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 444, 'end': 456, 'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )', 'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n', 'ref_id': 'EQREF11'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The calculation of INLINEFORM0 will be discussed later.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 19, 'end': 30, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 113, 'end': 124, 'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }', 'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ', 'ref_id': None}, {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 219, 'end': 230, 'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }', 'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ', 'ref_id': None}, {'start': 320, 'end': 331, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 335, 'end': 346, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 402, 'end': 413, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 416, 'end': 428, 'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )', 'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n', 'ref_id': 'EQREF12'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'cite_spans': [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [{'start': 304, 'end': 315, 'text': 's h i ', 'latex': 's^i_{h}', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'h d j ', 'latex': 'h^j_{d}', 'ref_id': None}, {'start': 401, 'end': 412, 'text': 'a d ∈ℝ n d  ', 'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}', 'ref_id': None}, {'start': 472, 'end': 483, 'text': 'h c j ', 'latex': 'h^j_{c}', 'ref_id': None}, {'start': 507, 'end': 518, 'text': 'a c ∈ℝ n c  ', 'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}, {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 672, 'end': 684, 'text': 'a ˜ c =a c ×ρ', 'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n', 'ref_id': 'EQREF13'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 30, 'end': 41, 'text': 'c d i ', 'latex': 'c_d^i', 'ref_id': None}, {'start': 79, 'end': 90, 'text': 'c c i ', 'latex': 'c_c^i', 'ref_id': None}, {'start': 240, 'end': 252, 'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )', 'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n', 'ref_id': 'EQREF14'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 64, 'end': 75, 'text': 's ˜ h i ', 'latex': '{{\\\\tilde{s}}_h^i}', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 15, 'end': 26, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 33, 'end': 44, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 220, 'end': 231, 'text': '𝐀 d ∈ℝ n d ×m ', 'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}', 'ref_id': None}, {'start': 297, 'end': 308, 'text': '𝐀 c ∈ℝ n c ×m ', 'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}', 'ref_id': None}, {'start': 569, 'end': 581, 'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )', 'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n', 'ref_id': 'EQREF15'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 170, 'end': 182, 'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )', 'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n', 'ref_id': 'EQREF16'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'Θ', 'latex': '\\\\Theta ', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐀 d ', 'latex': '\\\\mathbf {A}_d', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 434, 'end': 445, 'text': 'R∈ℝ n d ×n c  ', 'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}', 'ref_id': None}, {'start': 450, 'end': 462, 'text': 'R=X d ×X c T ', 'latex': '\\nR = X_d\\\\times X_c^T\\n', 'ref_id': 'EQREF17'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 93, 'end': 105, 'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]', 'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n', 'ref_id': 'EQREF18'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': '(0,1)', 'latex': '(0,1)', 'ref_id': None}, {'start': 84, 'end': 96, 'text': 'ρ=sigmoid(𝐫)', 'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n', 'ref_id': 'EQREF19'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 332, 'end': 343, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 346, 'end': 358, 'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ', 'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n', 'ref_id': 'EQREF20'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'ρ z ', 'latex': '\\\\rho _z', 'ref_id': None}, {'start': 22, 'end': 33, 'text': 'ρ x ', 'latex': '\\\\rho _x', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Summary Construction 2\n",
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'cite_spans': [{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [{'start': 485, 'end': 497, 'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },', 'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n', 'ref_id': 'EQREF22'}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Summary Construction 4\n",
      "{'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.', 'cite_spans': [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'α i ', 'latex': '\\\\alpha _i', 'ref_id': None}, {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None}, {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 126, 'end': 137, 'text': 'α ij ', 'latex': '\\\\alpha _{ij}', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'R ij ', 'latex': 'R_{ij}', 'ref_id': None}, {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 234, 'end': 245, 'text': 'P j ', 'latex': 'P_j', 'ref_id': None}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Data Description 0\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Description'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Properties 0\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}], 'eq_spans': [], 'section': 'Data Properties'}\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "{'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}], 'ref_spans': [{'start': 58, 'end': 66, 'text': None, 'latex': None, 'ref_id': 'SECREF28'}], 'eq_spans': [], 'section': 'Dataset and Metrics'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'cite_spans': [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'cite_spans': [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 2\n",
      "{'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Experimental Settings 2\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.', 'cite_spans': [{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': '|V|', 'latex': '|V|', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None}, {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 360, 'end': 371, 'text': 'm=5', 'latex': 'm = 5', 'ref_id': None}, {'start': 431, 'end': 442, 'text': 'd h =500', 'latex': 'd_h = 500', 'ref_id': None}, {'start': 463, 'end': 474, 'text': 'K=100', 'latex': 'K = 100', 'ref_id': None}, {'start': 495, 'end': 506, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 538, 'end': 549, 'text': 'λ p =0.2', 'latex': '\\\\lambda _p=0.2', 'ref_id': None}], 'section': 'Experimental Settings'}\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.', 'cite_spans': [], 'ref_spans': [{'start': 83, 'end': 91, 'text': None, 'latex': None, 'ref_id': 'TABREF40'}], 'eq_spans': [{'start': 240, 'end': 251, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Results on Our Dataset'}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', 'cite_spans': [{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'TABREF42'}], 'eq_spans': [{'start': 380, 'end': 391, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\", 'cite_spans': [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 305, 'end': 313, 'text': None, 'latex': None, 'ref_id': 'TABREF43'}], 'eq_spans': [], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Case Study 0\n",
      "{'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.', 'cite_spans': [], 'ref_spans': [{'start': 250, 'end': 258, 'text': None, 'latex': None, 'ref_id': 'TABREF45'}], 'eq_spans': [], 'section': 'Case Study'}\n",
      "====================\n",
      "Conclusions 0\n",
      "{'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusions'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "latex_parse_overview = dict()\n",
    "for sections in acl_only_articles[0]['latex_parse']['body_text']:\n",
    "    latex_parse_overview[sections['section']] = sections\n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.\n",
      "====================\n",
      "Introduction 0\n",
      "With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\n",
      "====================\n",
      "Introduction 3\n",
      "One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.\n",
      "====================\n",
      "Introduction 3\n",
      "Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.\n",
      "====================\n",
      "Introduction 0\n",
      "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.\n",
      "====================\n",
      "Introduction 0\n",
      "Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "====================\n",
      "Overview 1\n",
      "As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The calculation of INLINEFORM0 will be discussed later.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      " INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.\n",
      "====================\n",
      "Summary Construction 2\n",
      "In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 \n",
      "====================\n",
      "Summary Construction 4\n",
      "where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.\n",
      "====================\n",
      "Data Description 0\n",
      "In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.\n",
      "====================\n",
      "Background 0\n",
      "The definition of the terminology related to the dataset is given as follows.\n",
      "====================\n",
      "Background 0\n",
      "Topic: A topic refers to an event and it is composed of a set of news documents from different sources.\n",
      "====================\n",
      "Background 0\n",
      "Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.\n",
      "====================\n",
      "Background 0\n",
      "Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).\n",
      "====================\n",
      "Background 0\n",
      "Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.\n",
      "====================\n",
      "Background 0\n",
      "Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.\n",
      "====================\n",
      "Background 0\n",
      "Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.\n",
      "====================\n",
      "Data Collection 0\n",
      "The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .\n",
      "====================\n",
      "Data Collection 0\n",
      "For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.\n",
      "====================\n",
      "Data Collection 0\n",
      "After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.\n",
      "====================\n",
      "Data Properties 0\n",
      "The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n",
      "====================\n",
      "Comparative Methods 0\n",
      "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:\n",
      "====================\n",
      "Comparative Methods 1\n",
      "RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.\n",
      "====================\n",
      "Comparative Methods 2\n",
      "LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.\n",
      "====================\n",
      "Comparative Methods 0\n",
      "We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.\n",
      "====================\n",
      "Experimental Settings 2\n",
      "The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\n",
      "====================\n",
      "Case Study 0\n",
      "Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.\n",
      "====================\n",
      "Conclusions 0\n",
      "We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for article in acl_only_articles[0]['latex_parse']['body_text']:\n",
    "    print(article['section'],len(article['cite_spans']))\n",
    "    print(article['text'])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_only_articles[0]['latex_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'cite_spans': [], 'ref_spans': [{'start': 118, 'end': 125, 'text': None, 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'cite_spans': [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Overview 1\n",
      "{'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 19, 'end': 26, 'text': None, 'latex': None, 'ref_id': 'FIGREF7'}], 'eq_spans': [{'start': 212, 'end': 223, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 739, 'end': 750, 'text': 'ρ i ', 'latex': '\\\\rho _i', 'ref_id': None}, {'start': 774, 'end': 785, 'text': '𝐱 c i ', 'latex': '\\\\mathbf {x}_c^i', 'ref_id': None}, {'start': 807, 'end': 818, 'text': 'ρ∈ℝ n c  ', 'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}], 'section': 'Overview'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "{'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'cite_spans': [{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [{'start': 472, 'end': 483, 'text': 'p θ (𝐳)=𝒩(0,𝐈)', 'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})', 'ref_id': None}, {'start': 488, 'end': 499, 'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)', 'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})', 'ref_id': None}, {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None}, {'start': 524, 'end': 535, 'text': 'σ', 'latex': '\\\\sigma ', 'ref_id': None}, {'start': 799, 'end': 811, 'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\", 'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n', 'ref_id': 'EQREF9'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [{'start': 50, 'end': 56, 'text': None, 'latex': None, 'ref_id': 'EQREF9'}], 'eq_spans': [{'start': 131, 'end': 142, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 145, 'end': 157, 'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\", 'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ', 'ref_id': 'EQREF10'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': '𝐱', 'latex': '\\\\mathbf {x}', 'ref_id': None}, {'start': 76, 'end': 87, 'text': '𝐱 d ', 'latex': '\\\\mathbf {x}_d', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐱 c ', 'latex': '\\\\mathbf {x}_c', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 364, 'end': 375, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 444, 'end': 456, 'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )', 'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n', 'ref_id': 'EQREF11'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The calculation of INLINEFORM0 will be discussed later.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 19, 'end': 30, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 113, 'end': 124, 'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }', 'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ', 'ref_id': None}, {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 219, 'end': 230, 'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }', 'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ', 'ref_id': None}, {'start': 320, 'end': 331, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 335, 'end': 346, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 402, 'end': 413, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 416, 'end': 428, 'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )', 'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n', 'ref_id': 'EQREF12'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'cite_spans': [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [{'start': 304, 'end': 315, 'text': 's h i ', 'latex': 's^i_{h}', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'h d j ', 'latex': 'h^j_{d}', 'ref_id': None}, {'start': 401, 'end': 412, 'text': 'a d ∈ℝ n d  ', 'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}', 'ref_id': None}, {'start': 472, 'end': 483, 'text': 'h c j ', 'latex': 'h^j_{c}', 'ref_id': None}, {'start': 507, 'end': 518, 'text': 'a c ∈ℝ n c  ', 'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}, {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 672, 'end': 684, 'text': 'a ˜ c =a c ×ρ', 'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n', 'ref_id': 'EQREF13'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 30, 'end': 41, 'text': 'c d i ', 'latex': 'c_d^i', 'ref_id': None}, {'start': 79, 'end': 90, 'text': 'c c i ', 'latex': 'c_c^i', 'ref_id': None}, {'start': 240, 'end': 252, 'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )', 'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n', 'ref_id': 'EQREF14'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 64, 'end': 75, 'text': 's ˜ h i ', 'latex': '{{\\\\tilde{s}}_h^i}', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 15, 'end': 26, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 33, 'end': 44, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 220, 'end': 231, 'text': '𝐀 d ∈ℝ n d ×m ', 'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}', 'ref_id': None}, {'start': 297, 'end': 308, 'text': '𝐀 c ∈ℝ n c ×m ', 'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}', 'ref_id': None}, {'start': 569, 'end': 581, 'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )', 'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n', 'ref_id': 'EQREF15'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 170, 'end': 182, 'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )', 'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n', 'ref_id': 'EQREF16'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'Θ', 'latex': '\\\\Theta ', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐀 d ', 'latex': '\\\\mathbf {A}_d', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 434, 'end': 445, 'text': 'R∈ℝ n d ×n c  ', 'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}', 'ref_id': None}, {'start': 450, 'end': 462, 'text': 'R=X d ×X c T ', 'latex': '\\nR = X_d\\\\times X_c^T\\n', 'ref_id': 'EQREF17'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 93, 'end': 105, 'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]', 'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n', 'ref_id': 'EQREF18'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': '(0,1)', 'latex': '(0,1)', 'ref_id': None}, {'start': 84, 'end': 96, 'text': 'ρ=sigmoid(𝐫)', 'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n', 'ref_id': 'EQREF19'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 332, 'end': 343, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 346, 'end': 358, 'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ', 'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n', 'ref_id': 'EQREF20'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'ρ z ', 'latex': '\\\\rho _z', 'ref_id': None}, {'start': 22, 'end': 33, 'text': 'ρ x ', 'latex': '\\\\rho _x', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Summary Construction 2\n",
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'cite_spans': [{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [{'start': 485, 'end': 497, 'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },', 'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n', 'ref_id': 'EQREF22'}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Summary Construction 4\n",
      "{'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.', 'cite_spans': [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'α i ', 'latex': '\\\\alpha _i', 'ref_id': None}, {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None}, {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 126, 'end': 137, 'text': 'α ij ', 'latex': '\\\\alpha _{ij}', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'R ij ', 'latex': 'R_{ij}', 'ref_id': None}, {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 234, 'end': 245, 'text': 'P j ', 'latex': 'P_j', 'ref_id': None}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Data Description 0\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Description'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Properties 0\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}], 'eq_spans': [], 'section': 'Data Properties'}\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "{'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}], 'ref_spans': [{'start': 58, 'end': 66, 'text': None, 'latex': None, 'ref_id': 'SECREF28'}], 'eq_spans': [], 'section': 'Dataset and Metrics'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'cite_spans': [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'cite_spans': [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 2\n",
      "{'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Experimental Settings 2\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.', 'cite_spans': [{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': '|V|', 'latex': '|V|', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None}, {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 360, 'end': 371, 'text': 'm=5', 'latex': 'm = 5', 'ref_id': None}, {'start': 431, 'end': 442, 'text': 'd h =500', 'latex': 'd_h = 500', 'ref_id': None}, {'start': 463, 'end': 474, 'text': 'K=100', 'latex': 'K = 100', 'ref_id': None}, {'start': 495, 'end': 506, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 538, 'end': 549, 'text': 'λ p =0.2', 'latex': '\\\\lambda _p=0.2', 'ref_id': None}], 'section': 'Experimental Settings'}\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.', 'cite_spans': [], 'ref_spans': [{'start': 83, 'end': 91, 'text': None, 'latex': None, 'ref_id': 'TABREF40'}], 'eq_spans': [{'start': 240, 'end': 251, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Results on Our Dataset'}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', 'cite_spans': [{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'TABREF42'}], 'eq_spans': [{'start': 380, 'end': 391, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\", 'cite_spans': [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 305, 'end': 313, 'text': None, 'latex': None, 'ref_id': 'TABREF43'}], 'eq_spans': [], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Case Study 0\n",
      "{'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.', 'cite_spans': [], 'ref_spans': [{'start': 250, 'end': 258, 'text': None, 'latex': None, 'ref_id': 'TABREF45'}], 'eq_spans': [], 'section': 'Case Study'}\n",
      "====================\n",
      "Conclusions 0\n",
      "{'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusions'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "latex_parse_overview = dict()\n",
    "for sections in acl_only_articles[0]['latex_parse']['body_text']:\n",
    "    if sections['section'] in latex_parse_overview:\n",
    "        # если есть дублирование, такое бывает у первых часте\n",
    "        if latex_parse_overview[sections['section']] == sections:\n",
    "            continue\n",
    "        else:\n",
    "            latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "            latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "            latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "            latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "#             latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "    else:\n",
    "        latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                      'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                      'section':[sections['section']], \n",
    "                                                      'bib_entries':acl_only_articles[0]['latex_parse']}\n",
    "        \n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "  \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "  'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "  'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "  'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "  'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       " 'cite_spans': [[{'start': 193,\n",
       "    'end': 200,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF0'},\n",
       "   {'start': 203,\n",
       "    'end': 210,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF1'},\n",
       "   {'start': 213,\n",
       "    'end': 220,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 223,\n",
       "    'end': 230,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF3'},\n",
       "   {'start': 233,\n",
       "    'end': 240,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 243,\n",
       "    'end': 250,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF5'},\n",
       "   {'start': 253,\n",
       "    'end': 260,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'}],\n",
       "  [],\n",
       "  [{'start': 527,\n",
       "    'end': 534,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 537,\n",
       "    'end': 544,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF8'},\n",
       "   {'start': 802,\n",
       "    'end': 809,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'}],\n",
       "  [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "   {'start': 159,\n",
       "    'end': 167,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF10'},\n",
       "   {'start': 170,\n",
       "    'end': 178,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'}],\n",
       "  [],\n",
       "  []],\n",
       " 'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       " 'section': ['Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction'],\n",
       " 'bib_entries': {'abstract': [],\n",
       "  'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "    'cite_spans': [{'start': 193,\n",
       "      'end': 200,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 203,\n",
       "      'end': 210,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 213,\n",
       "      'end': 220,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 223,\n",
       "      'end': 230,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 233,\n",
       "      'end': 240,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 243,\n",
       "      'end': 250,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 253,\n",
       "      'end': 260,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 118,\n",
       "      'end': 125,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "    'cite_spans': [{'start': 527,\n",
       "      'end': 534,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 537,\n",
       "      'end': 544,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 802,\n",
       "      'end': 809,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 159,\n",
       "      'end': 167,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 170,\n",
       "      'end': 178,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "    'cite_spans': [{'start': 489,\n",
       "      'end': 496,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 19,\n",
       "      'end': 26,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF7'}],\n",
       "    'eq_spans': [{'start': 212,\n",
       "      'end': 223,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 228,\n",
       "      'end': 239,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 254,\n",
       "      'end': 265,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 285,\n",
       "      'end': 296,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 739,\n",
       "      'end': 750,\n",
       "      'text': 'ρ i ',\n",
       "      'latex': '\\\\rho _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 774,\n",
       "      'end': 785,\n",
       "      'text': '𝐱 c i ',\n",
       "      'latex': '\\\\mathbf {x}_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 807,\n",
       "      'end': 818,\n",
       "      'text': 'ρ∈ℝ n c  ',\n",
       "      'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Overview'},\n",
       "   {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 32,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 43,\n",
       "      'end': 51,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 154,\n",
       "      'end': 161,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "      'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 488,\n",
       "      'end': 499,\n",
       "      'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "      'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None},\n",
       "     {'start': 524,\n",
       "      'end': 535,\n",
       "      'text': 'σ',\n",
       "      'latex': '\\\\sigma ',\n",
       "      'ref_id': None},\n",
       "     {'start': 799,\n",
       "      'end': 811,\n",
       "      'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 50,\n",
       "      'end': 56,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'eq_spans': [{'start': 131,\n",
       "      'end': 142,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 145,\n",
       "      'end': 157,\n",
       "      'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "      'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "      'ref_id': 'EQREF10'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': '𝐱',\n",
       "      'latex': '\\\\mathbf {x}',\n",
       "      'ref_id': None},\n",
       "     {'start': 76,\n",
       "      'end': 87,\n",
       "      'text': '𝐱 d ',\n",
       "      'latex': '\\\\mathbf {x}_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐱 c ',\n",
       "      'latex': '\\\\mathbf {x}_c',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 364,\n",
       "      'end': 375,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 430,\n",
       "      'end': 441,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 444,\n",
       "      'end': 456,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "      'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "      'ref_id': 'EQREF11'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 19,\n",
       "      'end': 30,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 113,\n",
       "      'end': 124,\n",
       "      'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "      'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 219,\n",
       "      'end': 230,\n",
       "      'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "      'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 320,\n",
       "      'end': 331,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 335,\n",
       "      'end': 346,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 402,\n",
       "      'end': 413,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 416,\n",
       "      'end': 428,\n",
       "      'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "      'ref_id': 'EQREF12'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 7,\n",
       "      'end': 14,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 46,\n",
       "      'end': 54,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'},\n",
       "     {'start': 57,\n",
       "      'end': 65,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 304,\n",
       "      'end': 315,\n",
       "      'text': 's h i ',\n",
       "      'latex': 's^i_{h}',\n",
       "      'ref_id': None},\n",
       "     {'start': 366,\n",
       "      'end': 377,\n",
       "      'text': 'h d j ',\n",
       "      'latex': 'h^j_{d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 401,\n",
       "      'end': 412,\n",
       "      'text': 'a d ∈ℝ n d  ',\n",
       "      'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'h c j ',\n",
       "      'latex': 'h^j_{c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 507,\n",
       "      'end': 518,\n",
       "      'text': 'a c ∈ℝ n c  ',\n",
       "      'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 616,\n",
       "      'end': 627,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 672,\n",
       "      'end': 684,\n",
       "      'text': 'a ˜ c =a c ×ρ',\n",
       "      'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "      'ref_id': 'EQREF13'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 30,\n",
       "      'end': 41,\n",
       "      'text': 'c d i ',\n",
       "      'latex': 'c_d^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 79,\n",
       "      'end': 90,\n",
       "      'text': 'c c i ',\n",
       "      'latex': 'c_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 240,\n",
       "      'end': 252,\n",
       "      'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "      'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "      'ref_id': 'EQREF14'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 64,\n",
       "      'end': 75,\n",
       "      'text': 's ˜ h i ',\n",
       "      'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 1,\n",
       "      'end': 12,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 15,\n",
       "      'end': 26,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 33,\n",
       "      'end': 44,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "      'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 297,\n",
       "      'end': 308,\n",
       "      'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "      'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 569,\n",
       "      'end': 581,\n",
       "      'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "      'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "      'ref_id': 'EQREF15'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 170,\n",
       "      'end': 182,\n",
       "      'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "      'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "      'ref_id': 'EQREF16'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'Θ',\n",
       "      'latex': '\\\\Theta ',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐀 d ',\n",
       "      'latex': '\\\\mathbf {A}_d',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 199,\n",
       "      'end': 210,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 348,\n",
       "      'end': 359,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 390,\n",
       "      'end': 401,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 434,\n",
       "      'end': 445,\n",
       "      'text': 'R∈ℝ n d ×n c  ',\n",
       "      'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 450,\n",
       "      'end': 462,\n",
       "      'text': 'R=X d ×X c T ',\n",
       "      'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "      'ref_id': 'EQREF17'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 93,\n",
       "      'end': 105,\n",
       "      'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "      'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "      'ref_id': 'EQREF18'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': '(0,1)',\n",
       "      'latex': '(0,1)',\n",
       "      'ref_id': None},\n",
       "     {'start': 84,\n",
       "      'end': 96,\n",
       "      'text': 'ρ=sigmoid(𝐫)',\n",
       "      'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "      'ref_id': 'EQREF19'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 332,\n",
       "      'end': 343,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 346,\n",
       "      'end': 358,\n",
       "      'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "      'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "      'ref_id': 'EQREF20'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'ρ z ',\n",
       "      'latex': '\\\\rho _z',\n",
       "      'ref_id': None},\n",
       "     {'start': 22,\n",
       "      'end': 33,\n",
       "      'text': 'ρ x ',\n",
       "      'latex': '\\\\rho _x',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 82,\n",
       "      'end': 89,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 94,\n",
       "      'end': 101,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 485,\n",
       "      'end': 497,\n",
       "      'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "      'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "      'ref_id': 'EQREF22'}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "    'cite_spans': [{'start': 466,\n",
       "      'end': 474,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'},\n",
       "     {'start': 477,\n",
       "      'end': 484,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 491,\n",
       "      'end': 498,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 644,\n",
       "      'end': 652,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'α i ',\n",
       "      'latex': '\\\\alpha _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "     {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "     {'start': 112,\n",
       "      'end': 123,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 126,\n",
       "      'end': 137,\n",
       "      'text': 'α ij ',\n",
       "      'latex': '\\\\alpha _{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'R ij ',\n",
       "      'latex': 'R_{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 234,\n",
       "      'end': 245,\n",
       "      'text': 'P j ',\n",
       "      'latex': 'P_j',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Description'},\n",
       "   {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 222,\n",
       "      'end': 229,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF7'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Properties'},\n",
       "   {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "    'cite_spans': [{'start': 113,\n",
       "      'end': 121,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'}],\n",
       "    'ref_spans': [{'start': 58,\n",
       "      'end': 66,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF28'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Dataset and Metrics'},\n",
       "   {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "    'cite_spans': [{'start': 5,\n",
       "      'end': 13,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "    'cite_spans': [{'start': 9,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 29,\n",
       "      'end': 37,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "    'cite_spans': [{'start': 557,\n",
       "      'end': 565,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'},\n",
       "     {'start': 697,\n",
       "      'end': 705,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF21'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': '|V|',\n",
       "      'latex': '|V|',\n",
       "      'ref_id': None},\n",
       "     {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "     {'start': 194,\n",
       "      'end': 205,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 210,\n",
       "      'end': 221,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 360,\n",
       "      'end': 371,\n",
       "      'text': 'm=5',\n",
       "      'latex': 'm = 5',\n",
       "      'ref_id': None},\n",
       "     {'start': 431,\n",
       "      'end': 442,\n",
       "      'text': 'd h =500',\n",
       "      'latex': 'd_h = 500',\n",
       "      'ref_id': None},\n",
       "     {'start': 463,\n",
       "      'end': 474,\n",
       "      'text': 'K=100',\n",
       "      'latex': 'K = 100',\n",
       "      'ref_id': None},\n",
       "     {'start': 495,\n",
       "      'end': 506,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 538,\n",
       "      'end': 549,\n",
       "      'text': 'λ p =0.2',\n",
       "      'latex': '\\\\lambda _p=0.2',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Experimental Settings'},\n",
       "   {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 83,\n",
       "      'end': 91,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF40'}],\n",
       "    'eq_spans': [{'start': 240,\n",
       "      'end': 251,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Results on Our Dataset'},\n",
       "   {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "    'cite_spans': [{'start': 208,\n",
       "      'end': 215,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 260,\n",
       "      'end': 268,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF42'}],\n",
       "    'eq_spans': [{'start': 380,\n",
       "      'end': 391,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "    'cite_spans': [{'start': 33,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 305,\n",
       "      'end': 313,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF43'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 250,\n",
       "      'end': 258,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF45'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Case Study'},\n",
       "   {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Conclusions'}],\n",
       "  'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "    'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF9',\n",
       "    'type': 'equation'},\n",
       "   'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "    'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "    'ref_id': 'EQREF10',\n",
       "    'type': 'equation'},\n",
       "   'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "    'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "    'ref_id': 'EQREF11',\n",
       "    'type': 'equation'},\n",
       "   'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "    'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF12',\n",
       "    'type': 'equation'},\n",
       "   'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "    'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "    'ref_id': 'EQREF13',\n",
       "    'type': 'equation'},\n",
       "   'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "    'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "    'ref_id': 'EQREF14',\n",
       "    'type': 'equation'},\n",
       "   'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "    'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "    'ref_id': 'EQREF15',\n",
       "    'type': 'equation'},\n",
       "   'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "    'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "    'ref_id': 'EQREF16',\n",
       "    'type': 'equation'},\n",
       "   'EQREF17': {'text': 'R=X d ×X c T',\n",
       "    'latex': 'R = X_d\\\\times X_c^T',\n",
       "    'ref_id': 'EQREF17',\n",
       "    'type': 'equation'},\n",
       "   'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "    'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "    'ref_id': 'EQREF18',\n",
       "    'type': 'equation'},\n",
       "   'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "    'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "    'ref_id': 'EQREF19',\n",
       "    'type': 'equation'},\n",
       "   'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "    'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "    'ref_id': 'EQREF20',\n",
       "    'type': 'equation'},\n",
       "   'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "    'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "    'ref_id': 'EQREF22',\n",
       "    'type': 'equation'},\n",
       "   'FIGREF2': {'text': '1',\n",
       "    'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF2',\n",
       "    'type': 'figure'},\n",
       "   'FIGREF7': {'text': '2',\n",
       "    'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF7',\n",
       "    'type': 'figure'},\n",
       "   'TABREF40': {'text': '1',\n",
       "    'caption': 'Summarization performance.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF40',\n",
       "    'type': 'table'},\n",
       "   'TABREF42': {'text': '2',\n",
       "    'caption': 'Further investigation of RAVAESum.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF42',\n",
       "    'type': 'table'},\n",
       "   'TABREF43': {'text': '3',\n",
       "    'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF43',\n",
       "    'type': 'table'},\n",
       "   'TABREF45': {'text': '4',\n",
       "    'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF45',\n",
       "    'type': 'table'},\n",
       "   'TABREF46': {'text': '5',\n",
       "    'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF46',\n",
       "    'type': 'table'},\n",
       "   'SECREF1': {'text': 'Introduction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF1',\n",
       "    'type': 'section'},\n",
       "   'SECREF2': {'text': 'Framework',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF2',\n",
       "    'type': 'section'},\n",
       "   'SECREF6': {'text': 'Conclusions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF6',\n",
       "    'type': 'section'},\n",
       "   'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF8',\n",
       "    'type': 'section'},\n",
       "   'SECREF21': {'text': 'Summary Construction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF21',\n",
       "    'type': 'section'},\n",
       "   'SECREF3': {'text': 'Data Description',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF3',\n",
       "    'type': 'section'},\n",
       "   'SECREF24': {'text': 'Background',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF24',\n",
       "    'type': 'section'},\n",
       "   'SECREF26': {'text': 'Data Collection',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF26',\n",
       "    'type': 'section'},\n",
       "   'SECREF28': {'text': 'Data Properties',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF28',\n",
       "    'type': 'section'},\n",
       "   'SECREF4': {'text': 'Experimental Setup',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF4',\n",
       "    'type': 'section'},\n",
       "   'SECREF29': {'text': 'Dataset and Metrics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF29',\n",
       "    'type': 'section'},\n",
       "   'SECREF31': {'text': 'Comparative Methods',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF31',\n",
       "    'type': 'section'},\n",
       "   'SECREF37': {'text': 'Experimental Settings',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF37',\n",
       "    'type': 'section'},\n",
       "   'SECREF5': {'text': 'Results and Discussions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF5',\n",
       "    'type': 'section'},\n",
       "   'SECREF39': {'text': 'Results on Our Dataset',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF39',\n",
       "    'type': 'section'},\n",
       "   'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF41',\n",
       "    'type': 'section'},\n",
       "   'SECREF44': {'text': 'Case Study',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF44',\n",
       "    'type': 'section'},\n",
       "   'SECREF7': {'text': 'Topics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF7',\n",
       "    'type': 'section'}},\n",
       "  'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "    'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACL-ANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "    'title': 'Auto-encoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '15789289'},\n",
       "   'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "    'title': 'Effective approaches to attention-based neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "    'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_parse_overview['Introduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_only_articles[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### мы должны выделить только обзорную часть из текста, а все остальноё сохранить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "overview_papers[acl_only_articles[0]['paper_id']] = {\n",
    "    'paper_id':acl_only_articles[0]['paper_id'],   'metadata':acl_only_articles[0]['metadata'],\n",
    "    's2_pdf_hash':acl_only_articles[0]['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers[acl_only_articles[0]['paper_id']]['grobid_parse'] = {'abstract':acl_only_articles[0]['grobid_parse']['abstract'],\n",
    "                                                                'overview_text':grobid_parse_overview,  \n",
    "                                                                'bib_entries':acl_only_articles[0]['grobid_parse']['bib_entries']}\n",
    "overview_papers[acl_only_articles[0]['paper_id']]['latex_parse'] =  {'abstract':acl_only_articles[0]['latex_parse']['abstract'],\n",
    "                                                                'overview_text':latex_parse_overview,  \n",
    "                                                                'bib_entries':acl_only_articles[0]['latex_parse']['bib_entries']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10164018': {'paper_id': '10164018',\n",
       "  'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "   'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "   'year': '2017',\n",
       "   'arxiv_id': '1708.01065',\n",
       "   'acl_id': 'W17-4512',\n",
       "   'pmc_id': None,\n",
       "   'pubmed_id': None,\n",
       "   'doi': '10.18653/v1/w17-4512',\n",
       "   'venue': 'ArXiv',\n",
       "   'journal': 'ArXiv'},\n",
       "  's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       "  'grobid_parse': {'abstract': [{'text': 'AbstractWe investigate the problem of readeraware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online 1 .',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Abstract'}],\n",
       "   'overview_text': {0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "     'cite_spans': [{'start': 192,\n",
       "       'end': 216,\n",
       "       'text': '(Goldstein et al., 2000;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 217,\n",
       "       'end': 239,\n",
       "       'text': 'Erkan and Radev, 2004;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 240,\n",
       "       'end': 257,\n",
       "       'text': 'Wan et al., 2007;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF19'},\n",
       "      {'start': 258,\n",
       "       'end': 284,\n",
       "       'text': 'Nenkova and McKeown, 2012;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF16'},\n",
       "      {'start': 285,\n",
       "       'end': 302,\n",
       "       'text': 'Min et al., 2012;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF15'},\n",
       "      {'start': 303,\n",
       "       'end': 319,\n",
       "       'text': 'Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 773,\n",
       "       'end': 797,\n",
       "       'text': '(Project Code: 14203414)',\n",
       "       'latex': None,\n",
       "       'ref_id': None},\n",
       "      {'start': 2288,\n",
       "       'end': 2305,\n",
       "       'text': '(Hu et al., 2008;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF7'},\n",
       "      {'start': 2306,\n",
       "       'end': 2324,\n",
       "       'text': 'Yang et al., 2011)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF22'},\n",
       "      {'start': 2582,\n",
       "       'end': 2598,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 2911,\n",
       "       'end': 2927,\n",
       "       'text': 'Li et al. (2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 3069,\n",
       "       'end': 3095,\n",
       "       'text': '(Kingma and Welling, 2014;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 3096,\n",
       "       'end': 3117,\n",
       "       'text': 'Rezende et al., 2014)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'}],\n",
       "     'ref_spans': [{'start': 956,\n",
       "       'end': 964,\n",
       "       'text': 'Figure 1',\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF0'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    1: {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "     'cite_spans': [{'start': 451,\n",
       "       'end': 468,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     'ref_spans': [{'start': 12,\n",
       "       'end': 20,\n",
       "       'text': 'Figure 2',\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF2'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    2: {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "     'cite_spans': [{'start': 32,\n",
       "       'end': 58,\n",
       "       'text': '(Kingma and Welling, 2014;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 59,\n",
       "       'end': 79,\n",
       "       'text': 'Rezende et al., 2014',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'},\n",
       "      {'start': 184,\n",
       "       'end': 200,\n",
       "       'text': 'Li et al. (2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 2384,\n",
       "       'end': 2401,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 2433,\n",
       "       'end': 2456,\n",
       "       'text': '(Bahdanau et al., 2015;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF0'},\n",
       "      {'start': 2457,\n",
       "       'end': 2476,\n",
       "       'text': 'Luong et al., 2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF13'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    3: {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "     'cite_spans': [{'start': 86,\n",
       "       'end': 102,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 971,\n",
       "       'end': 987,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 1133,\n",
       "       'end': 1158,\n",
       "       'text': '(Dantzig and Thapa, 2006)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF3'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    4: {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    5: {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    6: {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    7: {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    8: {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "     'cite_spans': [{'start': 107,\n",
       "       'end': 118,\n",
       "       'text': '(Lin, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF12'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    9: {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "     'cite_spans': [{'start': 351,\n",
       "       'end': 365,\n",
       "       'text': '(Wasson, 1998)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF20'},\n",
       "      {'start': 492,\n",
       "       'end': 512,\n",
       "       'text': '(Radev et al., 2000)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF17'},\n",
       "      {'start': 700,\n",
       "       'end': 723,\n",
       "       'text': '(Erkan and Radev, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 737,\n",
       "       'end': 763,\n",
       "       'text': '(Mihalcea and Tarau, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF14'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    10: {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "     'cite_spans': [{'start': 498,\n",
       "       'end': 518,\n",
       "       'text': '(Kingma and Ba, 2014',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF8'},\n",
       "      {'start': 652,\n",
       "       'end': 674,\n",
       "       'text': '(Bastien et al., 2012)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    11: {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "     'cite_spans': [{'start': 690,\n",
       "       'end': 707,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 908,\n",
       "       'end': 925,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     'ref_spans': [{'start': 77,\n",
       "       'end': 84,\n",
       "       'text': 'Table 1',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF0'},\n",
       "      {'start': 746,\n",
       "       'end': 753,\n",
       "       'text': 'Table 2',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF1'},\n",
       "      {'start': 1184,\n",
       "       'end': 1191,\n",
       "       'text': 'Table 3',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF2'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    12: {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 315,\n",
       "       'end': 322,\n",
       "       'text': 'Table 4',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF3'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    13: {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "     'cite_spans': [{'start': 517,\n",
       "       'end': 868,\n",
       "       'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "       'latex': None,\n",
       "       'ref_id': None}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None}},\n",
       "   'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "     'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "     'authors': [{'first': 'Dzmitry',\n",
       "       'middle': [],\n",
       "       'last': 'Bahdanau',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "      {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '11212020'},\n",
       "    'BIBREF1': {'ref_id': 'b1',\n",
       "     'title': 'Theano: new features and speed improvements',\n",
       "     'authors': [{'first': 'Frédéric',\n",
       "       'middle': [],\n",
       "       'last': 'Bastien',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "      {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "      {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "      {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "      {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "      {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "      {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "      {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "     'links': '8180128'},\n",
       "    'BIBREF2': {'ref_id': 'b2',\n",
       "     'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "     'authors': [{'first': 'Lidong',\n",
       "       'middle': [],\n",
       "       'last': 'Bing',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "      {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1587--1597',\n",
       "     'other_ids': {},\n",
       "     'links': '8377315'},\n",
       "    'BIBREF3': {'ref_id': 'b3',\n",
       "     'title': 'Linear programming 1: introduction',\n",
       "     'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "      {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "     'year': 2006,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '53739754'},\n",
       "    'BIBREF4': {'ref_id': 'b4',\n",
       "     'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "     'authors': [{'first': 'Günes',\n",
       "       'middle': [],\n",
       "       'last': 'Erkan',\n",
       "       'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '4',\n",
       "     'issn': '',\n",
       "     'pages': '365--371',\n",
       "     'other_ids': {},\n",
       "     'links': '10418456'},\n",
       "    'BIBREF5': {'ref_id': 'b5',\n",
       "     'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "     'authors': [{'first': 'Shuhei',\n",
       "       'middle': [],\n",
       "       'last': 'Yoshida',\n",
       "       'suffix': ''}],\n",
       "     'year': None,\n",
       "     'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': None},\n",
       "    'BIBREF6': {'ref_id': 'b6',\n",
       "     'title': 'Multi-document summarization by sentence extraction',\n",
       "     'authors': [{'first': 'Jade',\n",
       "       'middle': [],\n",
       "       'last': 'Goldstein',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "      {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "      {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'NAACLANLPWorkshop',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '40--48',\n",
       "     'other_ids': {},\n",
       "     'links': '8294822'},\n",
       "    'BIBREF7': {'ref_id': 'b7',\n",
       "     'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "     'authors': [{'first': 'Meishan',\n",
       "       'middle': [],\n",
       "       'last': 'Hu',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "      {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "     'year': 2008,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '291--298',\n",
       "     'other_ids': {},\n",
       "     'links': '13723748'},\n",
       "    'BIBREF8': {'ref_id': 'b8',\n",
       "     'title': 'Adam: A method for stochastic optimization',\n",
       "     'authors': [{'first': 'Diederik',\n",
       "       'middle': [],\n",
       "       'last': 'Kingma',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '6628106'},\n",
       "    'BIBREF9': {'ref_id': 'b9',\n",
       "     'title': 'Autoencoding variational bayes',\n",
       "     'authors': [{'first': 'P',\n",
       "       'middle': [],\n",
       "       'last': 'Diederik',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': None},\n",
       "    'BIBREF10': {'ref_id': 'b10',\n",
       "     'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1270--1276',\n",
       "     'other_ids': {},\n",
       "     'links': '14777460'},\n",
       "    'BIBREF11': {'ref_id': 'b11',\n",
       "     'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "     'year': 2017,\n",
       "     'venue': 'AAAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '3497--3503',\n",
       "     'other_ids': {},\n",
       "     'links': '29562039'},\n",
       "    'BIBREF12': {'ref_id': 'b12',\n",
       "     'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "     'authors': [{'first': 'Chin-Yew',\n",
       "       'middle': [],\n",
       "       'last': 'Lin',\n",
       "       'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "     'volume': '8',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '964287'},\n",
       "    'BIBREF13': {'ref_id': 'b13',\n",
       "     'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "     'authors': [{'first': 'Minh-Thang',\n",
       "       'middle': [],\n",
       "       'last': 'Luong',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "      {'first': 'Christopher D',\n",
       "       'middle': [],\n",
       "       'last': 'Manning',\n",
       "       'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1412--1421',\n",
       "     'other_ids': {},\n",
       "     'links': '1998416'},\n",
       "    'BIBREF14': {'ref_id': 'b14',\n",
       "     'title': 'Textrank: Bringing order into texts',\n",
       "     'authors': [{'first': 'Rada',\n",
       "       'middle': [],\n",
       "       'last': 'Mihalcea',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '577937'},\n",
       "    'BIBREF15': {'ref_id': 'b15',\n",
       "     'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "     'authors': [{'first': 'Yen',\n",
       "       'middle': ['Kan'],\n",
       "       'last': 'Ziheng Lin Min',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'COLING',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '2093--2108',\n",
       "     'other_ids': {},\n",
       "     'links': '6317274'},\n",
       "    'BIBREF16': {'ref_id': 'b16',\n",
       "     'title': 'A survey of text summarization techniques',\n",
       "     'authors': [{'first': 'Ani',\n",
       "       'middle': [],\n",
       "       'last': 'Nenkova',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'Mining Text Data',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '43--76',\n",
       "     'other_ids': {},\n",
       "     'links': '556431'},\n",
       "    'BIBREF17': {'ref_id': 'b17',\n",
       "     'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "     'authors': [{'first': 'Hongyan',\n",
       "       'middle': [],\n",
       "       'last': 'Dragomir R Radev',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '21--30',\n",
       "     'other_ids': {},\n",
       "     'links': '1320'},\n",
       "    'BIBREF18': {'ref_id': 'b18',\n",
       "     'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "     'authors': [{'first': 'Danilo',\n",
       "       'middle': [],\n",
       "       'last': 'Jimenez Rezende',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "      {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICML',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1278--1286',\n",
       "     'other_ids': {},\n",
       "     'links': '16895865'},\n",
       "    'BIBREF19': {'ref_id': 'b19',\n",
       "     'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "     'authors': [{'first': 'Xiaojun',\n",
       "       'middle': [],\n",
       "       'last': 'Wan',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "     'year': 2007,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '7',\n",
       "     'issn': '',\n",
       "     'pages': '2903--2908',\n",
       "     'other_ids': {},\n",
       "     'links': '532313'},\n",
       "    'BIBREF20': {'ref_id': 'b20',\n",
       "     'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "     'authors': [{'first': 'Mark',\n",
       "       'middle': [],\n",
       "       'last': 'Wasson',\n",
       "       'suffix': ''}],\n",
       "     'year': 1998,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1364--1368',\n",
       "     'other_ids': {},\n",
       "     'links': '12681629'},\n",
       "    'BIBREF21': {'ref_id': 'b21',\n",
       "     'title': 'Multiple aspect summarization using integer linear programming',\n",
       "     'authors': [{'first': 'Kristian',\n",
       "       'middle': [],\n",
       "       'last': 'Woodsend',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'EMNLP-CNLL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '233--243',\n",
       "     'other_ids': {},\n",
       "     'links': '17497992'},\n",
       "    'BIBREF22': {'ref_id': 'b22',\n",
       "     'title': 'Social context summarization',\n",
       "     'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "      {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "      {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "      {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "      {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "     'year': 2011,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '255--264',\n",
       "     'other_ids': {},\n",
       "     'links': '704517'}}},\n",
       "  'latex_parse': {'abstract': [],\n",
       "   'overview_text': {'Introduction': {'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "      \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "      'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "      'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "      'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "      'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       "     'cite_spans': [[{'start': 193,\n",
       "        'end': 200,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF0'},\n",
       "       {'start': 203,\n",
       "        'end': 210,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF1'},\n",
       "       {'start': 213,\n",
       "        'end': 220,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF2'},\n",
       "       {'start': 223,\n",
       "        'end': 230,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF3'},\n",
       "       {'start': 233,\n",
       "        'end': 240,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF4'},\n",
       "       {'start': 243,\n",
       "        'end': 250,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 253,\n",
       "        'end': 260,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [],\n",
       "      [{'start': 527,\n",
       "        'end': 534,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF7'},\n",
       "       {'start': 537,\n",
       "        'end': 544,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF8'},\n",
       "       {'start': 802,\n",
       "        'end': 809,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 10,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'},\n",
       "       {'start': 159,\n",
       "        'end': 167,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF10'},\n",
       "       {'start': 170,\n",
       "        'end': 178,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF11'}],\n",
       "      [],\n",
       "      []],\n",
       "     'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       "     'section': ['Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Overview': {'text': ['As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.'],\n",
       "     'cite_spans': [[{'start': 489,\n",
       "        'end': 496,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}]],\n",
       "     'cite_span_lens': [1],\n",
       "     'section': ['Overview'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Reader-Aware Salience Estimation': {'text': ['Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "      'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "      'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "      'The calculation of INLINEFORM0 will be discussed later.',\n",
       "      'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "      'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "      'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "      'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "      ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "      'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "      'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "      'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "      'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.'],\n",
       "     'cite_spans': [[{'start': 32,\n",
       "        'end': 40,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF10'},\n",
       "       {'start': 43,\n",
       "        'end': 51,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF11'},\n",
       "       {'start': 154,\n",
       "        'end': 161,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [{'start': 7,\n",
       "        'end': 14,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'},\n",
       "       {'start': 46,\n",
       "        'end': 54,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF12'},\n",
       "       {'start': 57,\n",
       "        'end': 65,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF13'}],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      []],\n",
       "     'cite_span_lens': [3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "     'section': ['Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Summary Construction': {'text': ['In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.'],\n",
       "     'cite_spans': [[{'start': 82,\n",
       "        'end': 89,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 94,\n",
       "        'end': 101,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 466,\n",
       "        'end': 474,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF14'},\n",
       "       {'start': 477,\n",
       "        'end': 484,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 491,\n",
       "        'end': 498,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'},\n",
       "       {'start': 644,\n",
       "        'end': 652,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF15'}]],\n",
       "     'cite_span_lens': [2, 4],\n",
       "     'section': ['Summary Construction', 'Summary Construction'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Description': {'text': ['In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Data Description'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Background': {'text': ['The definition of the terminology related to the dataset is given as follows.',\n",
       "      'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "      'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "      'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "      'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "      'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "      'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.'],\n",
       "     'cite_spans': [[], [], [], [], [], [], []],\n",
       "     'cite_span_lens': [0, 0, 0, 0, 0, 0, 0],\n",
       "     'section': ['Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Collection': {'text': ['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "      'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "      'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "      'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.'],\n",
       "     'cite_spans': [[], [], [], []],\n",
       "     'cite_span_lens': [0, 0, 0, 0],\n",
       "     'section': ['Data Collection',\n",
       "      'Data Collection',\n",
       "      'Data Collection',\n",
       "      'Data Collection'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Properties': {'text': ['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Data Properties'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Dataset and Metrics': {'text': ['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.'],\n",
       "     'cite_spans': [[{'start': 113,\n",
       "        'end': 121,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF16'}]],\n",
       "     'cite_span_lens': [1],\n",
       "     'section': ['Dataset and Metrics'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Comparative Methods': {'text': ['To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "      'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "      'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "      'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "      'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "      'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "      'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.'],\n",
       "     'cite_spans': [[],\n",
       "      [{'start': 10,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 5,\n",
       "        'end': 13,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF17'}],\n",
       "      [{'start': 9,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF18'}],\n",
       "      [{'start': 8,\n",
       "        'end': 15,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF1'},\n",
       "       {'start': 29,\n",
       "        'end': 37,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF19'}],\n",
       "      [{'start': 8,\n",
       "        'end': 15,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'}],\n",
       "      []],\n",
       "     'cite_span_lens': [0, 1, 1, 1, 2, 1, 0],\n",
       "     'section': ['Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Experimental Settings': {'text': ['The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.'],\n",
       "     'cite_spans': [[{'start': 557,\n",
       "        'end': 565,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF20'},\n",
       "       {'start': 697,\n",
       "        'end': 705,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF21'}]],\n",
       "     'cite_span_lens': [2],\n",
       "     'section': ['Experimental Settings'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Results on Our Dataset': {'text': ['The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Results on Our Dataset'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Further Investigation of Our Framework ': {'text': ['To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "      \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\"],\n",
       "     'cite_spans': [[{'start': 208,\n",
       "        'end': 215,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [{'start': 33,\n",
       "        'end': 40,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}]],\n",
       "     'cite_span_lens': [1, 1],\n",
       "     'section': ['Further Investigation of Our Framework ',\n",
       "      'Further Investigation of Our Framework '],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Case Study': {'text': ['Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Case Study'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Conclusions': {'text': ['We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Conclusions'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}}},\n",
       "   'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "     'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "     'authors': [{'first': 'Lidong',\n",
       "       'middle': [],\n",
       "       'last': 'Bing',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "      {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1587--1597',\n",
       "     'other_ids': {},\n",
       "     'links': '8377315'},\n",
       "    'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "     'title': 'Linear programming 1: introduction',\n",
       "     'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "      {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "     'year': 2006,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '53739754'},\n",
       "    'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "     'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "     'authors': [{'first': 'Günes',\n",
       "       'middle': [],\n",
       "       'last': 'Erkan',\n",
       "       'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '4',\n",
       "     'issn': '',\n",
       "     'pages': '365--371',\n",
       "     'other_ids': {},\n",
       "     'links': '10418456'},\n",
       "    'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "     'title': 'Multi-document summarization by sentence extraction',\n",
       "     'authors': [{'first': 'Jade',\n",
       "       'middle': [],\n",
       "       'last': 'Goldstein',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "      {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "      {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'NAACL-ANLPWorkshop',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '40--48',\n",
       "     'other_ids': {},\n",
       "     'links': '8294822'},\n",
       "    'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "     'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "     'authors': [{'first': 'Meishan',\n",
       "       'middle': [],\n",
       "       'last': 'Hu',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "      {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "     'year': 2008,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '291--298',\n",
       "     'other_ids': {},\n",
       "     'links': '13723748'},\n",
       "    'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "     'title': 'Adam: A method for stochastic optimization',\n",
       "     'authors': [{'first': 'Diederik',\n",
       "       'middle': [],\n",
       "       'last': 'Kingma',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '6628106'},\n",
       "    'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "     'title': 'Auto-encoding variational bayes',\n",
       "     'authors': [{'first': 'P',\n",
       "       'middle': [],\n",
       "       'last': 'Diederik',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '15789289'},\n",
       "    'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "     'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1270--1276',\n",
       "     'other_ids': {},\n",
       "     'links': '14777460'},\n",
       "    'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "     'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "     'year': 2017,\n",
       "     'venue': 'AAAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '3497--3503',\n",
       "     'other_ids': {},\n",
       "     'links': '29562039'},\n",
       "    'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "     'title': 'Effective approaches to attention-based neural machine translation',\n",
       "     'authors': [{'first': 'Minh-Thang',\n",
       "       'middle': [],\n",
       "       'last': 'Luong',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "      {'first': 'Christopher D',\n",
       "       'middle': [],\n",
       "       'last': 'Manning',\n",
       "       'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1412--1421',\n",
       "     'other_ids': {},\n",
       "     'links': '1998416'},\n",
       "    'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "     'title': 'Textrank: Bringing order into texts',\n",
       "     'authors': [{'first': 'Rada',\n",
       "       'middle': [],\n",
       "       'last': 'Mihalcea',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '577937'},\n",
       "    'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "     'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "     'authors': [{'first': 'Yen',\n",
       "       'middle': ['Kan'],\n",
       "       'last': 'Ziheng Lin Min',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'COLING',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '2093--2108',\n",
       "     'other_ids': {},\n",
       "     'links': '6317274'},\n",
       "    'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "     'title': 'A survey of text summarization techniques',\n",
       "     'authors': [{'first': 'Ani',\n",
       "       'middle': [],\n",
       "       'last': 'Nenkova',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'Mining Text Data',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '43--76',\n",
       "     'other_ids': {},\n",
       "     'links': '556431'},\n",
       "    'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "     'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "     'authors': [{'first': 'Hongyan',\n",
       "       'middle': [],\n",
       "       'last': 'Dragomir R Radev',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '21--30',\n",
       "     'other_ids': {},\n",
       "     'links': '1320'},\n",
       "    'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "     'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "     'authors': [{'first': 'Danilo',\n",
       "       'middle': [],\n",
       "       'last': 'Jimenez Rezende',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "      {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICML',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1278--1286',\n",
       "     'other_ids': {},\n",
       "     'links': '16895865'},\n",
       "    'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "     'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "     'authors': [{'first': 'Mark',\n",
       "       'middle': [],\n",
       "       'last': 'Wasson',\n",
       "       'suffix': ''}],\n",
       "     'year': 1998,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1364--1368',\n",
       "     'other_ids': {},\n",
       "     'links': '12681629'},\n",
       "    'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "     'title': 'Multiple aspect summarization using integer linear programming',\n",
       "     'authors': [{'first': 'Kristian',\n",
       "       'middle': [],\n",
       "       'last': 'Woodsend',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'EMNLP-CNLL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '233--243',\n",
       "     'other_ids': {},\n",
       "     'links': '17497992'},\n",
       "    'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "     'title': 'Social context summarization',\n",
       "     'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "      {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "      {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "      {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "      {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "     'year': 2011,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '255--264',\n",
       "     'other_ids': {},\n",
       "     'links': '704517'}}}}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения глазами ссылка на [статью](https://arxiv.org/pdf/1708.01065.pdf)\n",
    "\n",
    "Все супер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Introduction 13\n",
      "1 Overview 1\n",
      "2 Reader-Aware Salience Estimation 6\n",
      "3 Summary Construction 6\n",
      "4 Data Description 0\n",
      "5 Background 0\n",
      "6 Data Collection 0\n",
      "7 Data Properties 0\n",
      "8 Dataset and Metrics 1\n",
      "9 Comparative Methods 6\n",
      "10 Experimental Settings 2\n",
      "11 Results on Our Dataset 0\n",
      "12 Further Investigation of Our Framework  2\n",
      "13 Case Study 0\n",
      "14 Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for num_tex,(k,v) in enumerate(latex_parse_overview.items()):\n",
    "    print(num_tex,k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "1 1\n",
      "2 6\n",
      "3 3\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 1\n",
      "9 4\n",
      "10 2\n",
      "11 2\n",
      "12 0\n",
      "13 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in grobid_parse_overview.items():\n",
    "    print(k,len(v['cite_spans']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упорядочим по количеству ссылок и возьмём абзац с максимальным значением, а также со 2 максимальным значением если количество ссылок в нём больше половины от максимального кол-ва "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), \n",
    "                                                 key=lambda item: len(item[1]['cite_spans']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cite_span_sum = 0\n",
    "max_grobid_parse_overview = dict()\n",
    "for k,v in grobid_parse_overview.items():\n",
    "    if max_cite_span_sum < len(v['cite_spans']):\n",
    "        max_cite_span_sum = len(v['cite_spans'])\n",
    "        max_grobid_parse_overview[k] = v\n",
    "    elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "        max_grobid_parse_overview[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), \n",
    "                                                 key=lambda item: sum(item[1]['cite_span_lens']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cite_span_sum = 0\n",
    "max_latex_parse_overview = dict()\n",
    "for k,v in latex_parse_overview.items():\n",
    "    if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "        max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "        max_latex_parse_overview[k] = v\n",
    "    elif (max_cite_span_sum>7) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "        max_latex_parse_overview[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 13\n",
      "Reader-Aware Salience Estimation 6\n",
      "Summary Construction 6\n",
      "Comparative Methods 6\n",
      "Experimental Settings 2\n",
      "Further Investigation of Our Framework  2\n",
      "Overview 1\n",
      "Dataset and Metrics 1\n",
      "Data Description 0\n",
      "Background 0\n",
      "Data Collection 0\n",
      "Data Properties 0\n",
      "Results on Our Dataset 0\n",
      "Case Study 0\n",
      "Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for k,v in latex_parse_overview.items():\n",
    "    print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим для всех \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделим обзорные части статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "for num_artic,article in enumerate(acl_only_articles):\n",
    "    # проверяем что у статьи есть grobid_parse и latex_parse и естб текст\n",
    "    if (article['grobid_parse'] and article['grobid_parse']['body_text']) or (article['latex_parse'] and article['latex_parse']['body_text']):\n",
    "        # задаем шаблон отражения статьи в укороченном формате (чтобы занимать меньше памяти)\n",
    "        overview_papers[article['paper_id']] = { 'paper_id':article['paper_id'],   'metadata':article['metadata'],\n",
    "                                                 's2_pdf_hash':article['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}\n",
    "        \n",
    "        grobid_parse_overview = None\n",
    "        # если у статьи есть article['grobid_parse']['body_text']\n",
    "        if article['grobid_parse'] and article['grobid_parse']['body_text']:\n",
    "            grobid_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "                grobid_parse_overview[num_sec] = sections\n",
    "            \n",
    "            # отсортируем по количеству цитат абзацы\n",
    "            grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), key=lambda item: len(item[1]['cite_spans']), reverse=True)}\n",
    "            \n",
    "#             # найдем 1 , пока без 2 максимума по обзорной части\n",
    "            max_cite_span_sum = 0\n",
    "            max_grobid_parse_overview = dict()\n",
    "            for k,v in grobid_parse_overview.items():\n",
    "                if max_cite_span_sum < len(v['cite_spans']):\n",
    "                    max_cite_span_sum = len(v['cite_spans'])\n",
    "                    max_grobid_parse_overview[k] = v\n",
    "                    \n",
    "#                 # записываем 2 максимум, если количество ссылок в егочасти больше половины от максимального \n",
    "#                 elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "#                     max_grobid_parse_overview[k] = v\n",
    "                    \n",
    "            \n",
    "            grobid_parse_overview = max_grobid_parse_overview\n",
    "            \n",
    "        latex_parse_overview = None\n",
    "        # если у статьи есть article['latex_parse']['body_text']\n",
    "        if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "            latex_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            # в latex_parse \n",
    "            for sections in article['latex_parse']['body_text']:\n",
    "                if sections['section'] in latex_parse_overview:\n",
    "                    if latex_parse_overview[sections['section']] == sections:\n",
    "                        continue\n",
    "                    else:\n",
    "                        latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "                        latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "                        latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "                        latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "                else:\n",
    "                    latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                                  'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                                  'section':[sections['section']]}\n",
    "            latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), key=lambda item: item[1]['cite_span_lens'], reverse=True)}\n",
    "        \n",
    "        \n",
    "            max_cite_span_sum = 0\n",
    "            max_latex_parse_overview = dict()\n",
    "            for k,v in latex_parse_overview.items():\n",
    "                if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "                    max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "                    max_latex_parse_overview[k] = v\n",
    "#                 elif (max_cite_span_sum>0) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "#                     max_latex_parse_overview[k] = v\n",
    "            \n",
    "            latex_parse_overview = max_latex_parse_overview\n",
    "        \n",
    "\n",
    "        if grobid_parse_overview:\n",
    "            overview_papers[article['paper_id']]['grobid_parse'] = {'abstract':None,\n",
    "                                                        'overview_text':grobid_parse_overview,  \n",
    "                                                        'bib_entries':None}\n",
    "            if article['grobid_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['abstract'] = article['grobid_parse']['abstract']\n",
    "            if article['grobid_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['bib_entries'] = article['grobid_parse']['bib_entries']\n",
    "            \n",
    "        if latex_parse_overview:            \n",
    "            overview_papers[article['paper_id']]['latex_parse'] = {'abstract':None,\n",
    "                                                                    'overview_text':latex_parse_overview,  \n",
    "                                                                    'bib_entries':None}\n",
    "            if article['latex_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['abstract'] = article['latex_parse']['abstract']\n",
    "            if article['latex_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['bib_entries'] = article['latex_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers_list = np.array(list(overview_papers.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overview_papers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'acl_scenario/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acl_scenario/acl_overview_0.json\n",
      "acl_scenario/acl_overview_5000.json\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(overview_papers_list)\n",
    "# Shuffle at the start of epoch\n",
    "indices = np.arange(n_samples)\n",
    "batch_size = 5000\n",
    "for start in range(0, n_samples, batch_size):\n",
    "    name_json = dir_name+'acl_overview_'+str(start)+'.json' \n",
    "    print(name_json)\n",
    "    end = min(start + batch_size, n_samples)\n",
    "    batch_idx = indices[start:end]\n",
    "    batch_overview = list(overview_papers_list[batch_idx])\n",
    "    with open(name_json, 'w') as json_fil:\n",
    "        json.dump(batch_overview, json_fil)\n",
    "    json_fil.close()\n",
    "    del batch_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018']['grobid_parse']['overview_text'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Introduction'])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018']['latex_parse']['overview_text'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выделим стать, у которых есть latex_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers_w_latex = {k:v for k,v in overview_papers.items() if v['latex_parse']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1058"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overview_papers_w_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([k for k,v in overview_papers.items() if v['grobid_parse']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_in(list_toks,line):\n",
    "    flag = False\n",
    "    line = line.lower()\n",
    "    for tok in list_toks:\n",
    "        if tok.lower() in line:\n",
    "            flag = tok\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение порядка упоминания статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018',\n",
       " '14472576',\n",
       " '14323173',\n",
       " '442560',\n",
       " '44112954',\n",
       " '22103840',\n",
       " '15281193',\n",
       " '10335977',\n",
       " '5084110',\n",
       " '1368861']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(overview_papers.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])\n",
      "dict_keys(['abstract', 'overview_text', 'bib_entries'])\n",
      "dict_keys(['abstract', 'overview_text', 'bib_entries'])\n",
      "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])\n",
      "dict_keys(['text', 'cite_spans', 'cite_span_lens', 'section'])\n"
     ]
    }
   ],
   "source": [
    "print(overview_papers['10164018'].keys())\n",
    "print(overview_papers['10164018']['grobid_parse'].keys())\n",
    "print(overview_papers['10164018']['latex_parse'].keys())\n",
    "print(overview_papers['10164018']['grobid_parse']['overview_text'][0].keys())\n",
    "print(overview_papers['10164018']['latex_parse']['overview_text']['Introduction'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_id = '10164018'#'442560'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "  'cite_spans': [{'start': 192,\n",
       "    'end': 216,\n",
       "    'text': '(Goldstein et al., 2000;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'},\n",
       "   {'start': 217,\n",
       "    'end': 239,\n",
       "    'text': 'Erkan and Radev, 2004;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 240,\n",
       "    'end': 257,\n",
       "    'text': 'Wan et al., 2007;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF19'},\n",
       "   {'start': 258,\n",
       "    'end': 284,\n",
       "    'text': 'Nenkova and McKeown, 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF16'},\n",
       "   {'start': 285,\n",
       "    'end': 302,\n",
       "    'text': 'Min et al., 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF15'},\n",
       "   {'start': 303,\n",
       "    'end': 319,\n",
       "    'text': 'Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 773,\n",
       "    'end': 797,\n",
       "    'text': '(Project Code: 14203414)',\n",
       "    'latex': None,\n",
       "    'ref_id': None},\n",
       "   {'start': 2288,\n",
       "    'end': 2305,\n",
       "    'text': '(Hu et al., 2008;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 2306,\n",
       "    'end': 2324,\n",
       "    'text': 'Yang et al., 2011)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF22'},\n",
       "   {'start': 2582,\n",
       "    'end': 2598,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 2911,\n",
       "    'end': 2927,\n",
       "    'text': 'Li et al. (2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 3069,\n",
       "    'end': 3095,\n",
       "    'text': '(Kingma and Welling, 2014;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'},\n",
       "   {'start': 3096,\n",
       "    'end': 3117,\n",
       "    'text': 'Rezende et al., 2014)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF18'}],\n",
       "  'ref_spans': [{'start': 956,\n",
       "    'end': 964,\n",
       "    'text': 'Figure 1',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF0'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse']['overview_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'b0',\n",
       "  'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "  'authors': [{'first': 'Dzmitry',\n",
       "    'middle': [],\n",
       "    'last': 'Bahdanau',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "   {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '11212020'},\n",
       " 'BIBREF1': {'ref_id': 'b1',\n",
       "  'title': 'Theano: new features and speed improvements',\n",
       "  'authors': [{'first': 'Frédéric',\n",
       "    'middle': [],\n",
       "    'last': 'Bastien',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "   {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "   {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "   {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "   {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "   {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "   {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "   {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "  'links': '8180128'},\n",
       " 'BIBREF2': {'ref_id': 'b2',\n",
       "  'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "  'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "   {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1587--1597',\n",
       "  'other_ids': {},\n",
       "  'links': '8377315'},\n",
       " 'BIBREF3': {'ref_id': 'b3',\n",
       "  'title': 'Linear programming 1: introduction',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "   {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "  'year': 2006,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '53739754'},\n",
       " 'BIBREF4': {'ref_id': 'b4',\n",
       "  'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "  'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '4',\n",
       "  'issn': '',\n",
       "  'pages': '365--371',\n",
       "  'other_ids': {},\n",
       "  'links': '10418456'},\n",
       " 'BIBREF5': {'ref_id': 'b5',\n",
       "  'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "  'authors': [{'first': 'Shuhei',\n",
       "    'middle': [],\n",
       "    'last': 'Yoshida',\n",
       "    'suffix': ''}],\n",
       "  'year': None,\n",
       "  'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF6': {'ref_id': 'b6',\n",
       "  'title': 'Multi-document summarization by sentence extraction',\n",
       "  'authors': [{'first': 'Jade',\n",
       "    'middle': [],\n",
       "    'last': 'Goldstein',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "   {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "   {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'NAACLANLPWorkshop',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '40--48',\n",
       "  'other_ids': {},\n",
       "  'links': '8294822'},\n",
       " 'BIBREF7': {'ref_id': 'b7',\n",
       "  'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "  'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "   {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "   {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '291--298',\n",
       "  'other_ids': {},\n",
       "  'links': '13723748'},\n",
       " 'BIBREF8': {'ref_id': 'b8',\n",
       "  'title': 'Adam: A method for stochastic optimization',\n",
       "  'authors': [{'first': 'Diederik',\n",
       "    'middle': [],\n",
       "    'last': 'Kingma',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '6628106'},\n",
       " 'BIBREF9': {'ref_id': 'b9',\n",
       "  'title': 'Autoencoding variational bayes',\n",
       "  'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "   {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF10': {'ref_id': 'b10',\n",
       "  'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1270--1276',\n",
       "  'other_ids': {},\n",
       "  'links': '14777460'},\n",
       " 'BIBREF11': {'ref_id': 'b11',\n",
       "  'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'AAAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '3497--3503',\n",
       "  'other_ids': {},\n",
       "  'links': '29562039'},\n",
       " 'BIBREF12': {'ref_id': 'b12',\n",
       "  'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "  'authors': [{'first': 'Chin-Yew',\n",
       "    'middle': [],\n",
       "    'last': 'Lin',\n",
       "    'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "  'volume': '8',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '964287'},\n",
       " 'BIBREF13': {'ref_id': 'b13',\n",
       "  'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "  'authors': [{'first': 'Minh-Thang',\n",
       "    'middle': [],\n",
       "    'last': 'Luong',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "   {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1412--1421',\n",
       "  'other_ids': {},\n",
       "  'links': '1998416'},\n",
       " 'BIBREF14': {'ref_id': 'b14',\n",
       "  'title': 'Textrank: Bringing order into texts',\n",
       "  'authors': [{'first': 'Rada',\n",
       "    'middle': [],\n",
       "    'last': 'Mihalcea',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '577937'},\n",
       " 'BIBREF15': {'ref_id': 'b15',\n",
       "  'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "  'authors': [{'first': 'Yen',\n",
       "    'middle': ['Kan'],\n",
       "    'last': 'Ziheng Lin Min',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'COLING',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '2093--2108',\n",
       "  'other_ids': {},\n",
       "  'links': '6317274'},\n",
       " 'BIBREF16': {'ref_id': 'b16',\n",
       "  'title': 'A survey of text summarization techniques',\n",
       "  'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "   {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Mining Text Data',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '43--76',\n",
       "  'other_ids': {},\n",
       "  'links': '556431'},\n",
       " 'BIBREF17': {'ref_id': 'b17',\n",
       "  'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "  'authors': [{'first': 'Hongyan',\n",
       "    'middle': [],\n",
       "    'last': 'Dragomir R Radev',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '21--30',\n",
       "  'other_ids': {},\n",
       "  'links': '1320'},\n",
       " 'BIBREF18': {'ref_id': 'b18',\n",
       "  'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "  'authors': [{'first': 'Danilo',\n",
       "    'middle': [],\n",
       "    'last': 'Jimenez Rezende',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "   {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICML',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1278--1286',\n",
       "  'other_ids': {},\n",
       "  'links': '16895865'},\n",
       " 'BIBREF19': {'ref_id': 'b19',\n",
       "  'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "  'authors': [{'first': 'Xiaojun', 'middle': [], 'last': 'Wan', 'suffix': ''},\n",
       "   {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "  'year': 2007,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '7',\n",
       "  'issn': '',\n",
       "  'pages': '2903--2908',\n",
       "  'other_ids': {},\n",
       "  'links': '532313'},\n",
       " 'BIBREF20': {'ref_id': 'b20',\n",
       "  'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "  'authors': [{'first': 'Mark', 'middle': [], 'last': 'Wasson', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1364--1368',\n",
       "  'other_ids': {},\n",
       "  'links': '12681629'},\n",
       " 'BIBREF21': {'ref_id': 'b21',\n",
       "  'title': 'Multiple aspect summarization using integer linear programming',\n",
       "  'authors': [{'first': 'Kristian',\n",
       "    'middle': [],\n",
       "    'last': 'Woodsend',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'EMNLP-CNLL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '233--243',\n",
       "  'other_ids': {},\n",
       "  'links': '17497992'},\n",
       " 'BIBREF22': {'ref_id': 'b22',\n",
       "  'title': 'Social context summarization',\n",
       "  'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "   {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "   {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "   {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "   {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '255--264',\n",
       "  'other_ids': {},\n",
       "  'links': '704517'}}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 192,\n",
       "  'end': 216,\n",
       "  'text': '(Goldstein et al., 2000;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF6'},\n",
       " {'start': 217,\n",
       "  'end': 239,\n",
       "  'text': 'Erkan and Radev, 2004;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF4'},\n",
       " {'start': 240,\n",
       "  'end': 257,\n",
       "  'text': 'Wan et al., 2007;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF19'},\n",
       " {'start': 258,\n",
       "  'end': 284,\n",
       "  'text': 'Nenkova and McKeown, 2012;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF16'},\n",
       " {'start': 285,\n",
       "  'end': 302,\n",
       "  'text': 'Min et al., 2012;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF15'},\n",
       " {'start': 303,\n",
       "  'end': 319,\n",
       "  'text': 'Li et al., 2017)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF11'},\n",
       " {'start': 773,\n",
       "  'end': 797,\n",
       "  'text': '(Project Code: 14203414)',\n",
       "  'latex': None,\n",
       "  'ref_id': None},\n",
       " {'start': 2288,\n",
       "  'end': 2305,\n",
       "  'text': '(Hu et al., 2008;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF7'},\n",
       " {'start': 2306,\n",
       "  'end': 2324,\n",
       "  'text': 'Yang et al., 2011)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF22'},\n",
       " {'start': 2582,\n",
       "  'end': 2598,\n",
       "  'text': 'Li et al. (2015)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF2'},\n",
       " {'start': 2911,\n",
       "  'end': 2927,\n",
       "  'text': 'Li et al. (2017)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF11'},\n",
       " {'start': 3069,\n",
       "  'end': 3095,\n",
       "  'text': '(Kingma and Welling, 2014;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF9'},\n",
       " {'start': 3096,\n",
       "  'end': 3117,\n",
       "  'text': 'Rezende et al., 2014)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF18'}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_grobid = list(overview_papers[key_id]['grobid_parse']['overview_text'].keys())[0]\n",
    "overview_papers[key_id]['grobid_parse']['overview_text'][key_grobid]['cite_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIBREF6 8294822 Multi-document summarization by sentence extraction\n",
      "BIBREF4 10418456 Lexpagerank: Prestige in multi-document text summarization\n",
      "BIBREF19 532313 Manifold-ranking based topic-focused multidocument summarization\n",
      "BIBREF16 556431 A survey of text summarization techniques\n",
      "BIBREF15 6317274 Exploiting category-specific information for multidocument summarization\n",
      "BIBREF11 29562039 Salience estimation via variational auto-encoders for multi-document summarization\n",
      "kek None None\n",
      "BIBREF7 13723748 Comments-oriented document summarization: Understanding documents with readers' feedback\n",
      "BIBREF22 704517 Social context summarization\n",
      "BIBREF2 8377315 Abstractive multidocument summarization via phrase selection and merging\n",
      "BIBREF11 29562039 Salience estimation via variational auto-encoders for multi-document summarization\n",
      "BIBREF9 None Autoencoding variational bayes\n",
      "BIBREF18 16895865 Stochastic backpropagation and approximate inference in deep generative models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BIBREF11    2\n",
       "BIBREF16    1\n",
       "BIBREF18    1\n",
       "BIBREF2     1\n",
       "BIBREF7     1\n",
       "BIBREF9     1\n",
       "BIBREF19    1\n",
       "BIBREF15    1\n",
       "BIBREF6     1\n",
       "BIBREF22    1\n",
       "BIBREF4     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_entries_grobid = dict()\n",
    "bib_refs = []\n",
    "ref_order = []\n",
    "paper = overview_papers[key_id]\n",
    "\n",
    "key_grobid = list(paper['grobid_parse']['overview_text'].keys())[0]\n",
    "\n",
    "for cite_span in paper['grobid_parse']['overview_text'][key_grobid]['cite_spans']:\n",
    "    cite_ref = cite_span['ref_id']\n",
    "    bib_refs.append(cite_ref)\n",
    "    cited_paper_id = None\n",
    "    if cite_ref in paper['grobid_parse']['bib_entries']:\n",
    "        cited_paper_id = paper['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "    if cited_paper_id:\n",
    "        ref_order.append(cited_paper_id)\n",
    "        print(cite_ref,cited_paper_id,paper['grobid_parse']['bib_entries'][cite_ref]['title'])\n",
    "        bib_entries_grobid[paper['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "    elif cite_ref  in paper['grobid_parse']['bib_entries']:\n",
    "        print(cite_ref,cited_paper_id,paper['grobid_parse']['bib_entries'][cite_ref]['title'])\n",
    "        bib_entries_grobid[paper['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "    else:\n",
    "        # неверно распознается, это редко статья\n",
    "        print('kek',cite_ref,cited_paper_id)\n",
    "pd.Series(bib_refs).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8294822',\n",
       " '10418456',\n",
       " '532313',\n",
       " '556431',\n",
       " '6317274',\n",
       " '29562039',\n",
       " '13723748',\n",
       " '704517',\n",
       " '8377315',\n",
       " '29562039',\n",
       " '16895865']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': '8294822',\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': '10418456',\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': '532313',\n",
       " 'A survey of text summarization techniques': '556431',\n",
       " 'Exploiting category-specific information for multidocument summarization': '6317274',\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': '29562039',\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": '13723748',\n",
       " 'Social context summarization': '704517',\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': '8377315',\n",
       " 'Autoencoding variational bayes': None,\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': '16895865'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_entries_grobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       " \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       " 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       " 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       " 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       " 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['latex_parse']['overview_text']['Introduction']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'},\n",
       "  {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'},\n",
       "  {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'},\n",
       "  {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'},\n",
       "  {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'},\n",
       "  {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'},\n",
       "  {'start': 253,\n",
       "   'end': 260,\n",
       "   'text': None,\n",
       "   'latex': None,\n",
       "   'ref_id': 'BIBREF6'}],\n",
       " [],\n",
       " [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'},\n",
       "  {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'},\n",
       "  {'start': 802,\n",
       "   'end': 809,\n",
       "   'text': None,\n",
       "   'latex': None,\n",
       "   'ref_id': 'BIBREF9'}],\n",
       " [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "  {'start': 159,\n",
       "   'end': 167,\n",
       "   'text': None,\n",
       "   'latex': None,\n",
       "   'ref_id': 'BIBREF10'},\n",
       "  {'start': 170,\n",
       "   'end': 178,\n",
       "   'text': None,\n",
       "   'latex': None,\n",
       "   'ref_id': 'BIBREF11'}],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['latex_parse']['overview_text']['Introduction']['cite_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIBREF0 8294822 Multi-document summarization by sentence extraction\n",
      "BIBREF1 10418456 Lexpagerank: Prestige in multi-document text summarization\n",
      "kek BIBREF2 None\n",
      "BIBREF3 556431 A survey of text summarization techniques\n",
      "BIBREF4 6317274 Exploiting category-specific information for multi-document summarization\n",
      "BIBREF5 8377315 Abstractive multi-document summarization via phrase selection and merging\n",
      "BIBREF6 29562039 Salience estimation via variational auto-encoders for multi-document summarization\n",
      "BIBREF7 13723748 Comments-oriented document summarization: Understanding documents with readers' feedback\n",
      "BIBREF8 704517 Social context summarization\n",
      "BIBREF9 14777460 Reader-aware multi-document summarization via sparse coding\n",
      "BIBREF6 29562039 Salience estimation via variational auto-encoders for multi-document summarization\n",
      "BIBREF10 15789289 Auto-encoding variational bayes\n",
      "BIBREF11 16895865 Stochastic backpropagation and approximate inference in deep generative models\n"
     ]
    }
   ],
   "source": [
    "article = overview_papers[key_id]\n",
    "\n",
    "bib_entries_latex = dict()\n",
    "for cite_spans in article['latex_parse']['overview_text']['Introduction']['cite_spans']:\n",
    "    if len(cite_spans)>1:\n",
    "        for cite_span in cite_spans:\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_article_id = None\n",
    "            if cite_ref in article['latex_parse']['bib_entries']:\n",
    "                cited_article_id = article['latex_parse']['bib_entries'][cite_ref]['links']\n",
    "                \n",
    "            if cited_article_id:\n",
    "                print(cite_ref,cited_article_id,article['latex_parse']['bib_entries'][cite_ref]['title'])\n",
    "                bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "            elif cite_ref in article['latex_parse']['bib_entries']:\n",
    "                print(cite_ref,cited_article_id)\n",
    "                \n",
    "                bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "            else:\n",
    "                print('kek',cite_ref,cited_article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': '8294822',\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': '10418456',\n",
       " 'A survey of text summarization techniques': '556431',\n",
       " 'Exploiting category-specific information for multi-document summarization': '6317274',\n",
       " 'Abstractive multi-document summarization via phrase selection and merging': '8377315',\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': '29562039',\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": '13723748',\n",
       " 'Social context summarization': '704517',\n",
       " 'Reader-aware multi-document summarization via sparse coding': '14777460',\n",
       " 'Auto-encoding variational bayes': '15789289',\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': '16895865'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_entries_latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': '8294822',\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': '10418456',\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': '532313',\n",
       " 'A survey of text summarization techniques': '556431',\n",
       " 'Exploiting category-specific information for multidocument summarization': '6317274',\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': '29562039',\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": '13723748',\n",
       " 'Social context summarization': '704517',\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': '8377315',\n",
       " 'Autoencoding variational bayes': None,\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': '16895865'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_entries_grobid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пока заглушка\n",
    "Начинаем обход с bib_entries_grobid смотрим, что у нас всё одинаково матчится с bib_entries_latex\n",
    "если есть различия то bib_entries_latex добавляем, пока не заматчится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_bib_entries = dict()\n",
    "# if bib_entries_latex:\n",
    "#     for k,v in bib_entries_latex.items():\n",
    "#         all_bib_entries[k] = [v]\n",
    "# if bib_entries_grobid:\n",
    "#     for k,v in bib_entries_grobid.items():\n",
    "#         if k in all_bib_entries:\n",
    "#             if all_bib_entries[k][0] == v:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 all_bib_entries[k].append(v)\n",
    "#         else:\n",
    "#             all_bib_entries[k] = [v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пока делаю только для статей с grobid_parse\n",
    "\n",
    "тк статей с latex в 7 раз меньше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'authors', 'abstract', 'year', 'arxiv_id', 'acl_id', 'pmc_id', 'pubmed_id', 'doi', 'venue', 'journal'])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018']['metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018']['metadata']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       " {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       " {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018']['metadata']['authors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piji Li\n",
      "Lidong Bing\n",
      "Wai Lam\n"
     ]
    }
   ],
   "source": [
    "for author in overview_papers['10164018']['metadata']['authors']:\n",
    "    if author['first']:\n",
    "        print(author['first']+' '+author['last'])\n",
    "    else:\n",
    "        print(author['last'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'AbstractWe investigate the problem of readeraware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online 1 .',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': 'Abstract'}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018']['grobid_parse']['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7a3b364eed410f9f98e3c64da0c73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7259), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "article_bibs = {}\n",
    "\n",
    "p_ref_ind = []\n",
    "p_ref_title = []\n",
    "p_ref_abstract = []\n",
    "p_ref_abstract_grobid = []\n",
    "p_ref_abstract_latex = []\n",
    "p_ref_order = []\n",
    "p_ref_authors = []\n",
    "\n",
    "for article in tqdm_notebook(overview_papers):\n",
    "# for num_artic,article in enumerate(overview_papers):\n",
    "#     if num_artic >10:\n",
    "#         break\n",
    "#     print(num_artic,article)\n",
    "    article = overview_papers[article]\n",
    "    # проверка наличия определленных значений статьи\n",
    "    if not ((article['grobid_parse'] and article['grobid_parse']['bib_entries']) or (article['latex_parse'] and article['latex_parse']['bib_entries'])):\n",
    "#         article_bibs[article['paper_id']] = 'None'\n",
    "        continue\n",
    "        \n",
    "    bib_entries_grobid = None\n",
    "    if article['grobid_parse'] and article['grobid_parse']['bib_entries'] and article['grobid_parse']['overview_text']:\n",
    "        bib_entries_grobid = dict()\n",
    "        ref_order = []\n",
    "        key_grobid = list(article['grobid_parse']['overview_text'].keys())[0]\n",
    "        for cite_span in article['grobid_parse']['overview_text'][key_grobid]['cite_spans']:\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_paper_id = None\n",
    "            if cite_ref in article['grobid_parse']['bib_entries']:\n",
    "                cited_paper_id = article['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "            if cited_paper_id:\n",
    "                bib_entries_grobid[article['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "                ref_order.append(cited_paper_id)\n",
    "            elif cite_ref  in article['grobid_parse']['bib_entries']:\n",
    "                bib_entries_grobid[article['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "        \n",
    "        \n",
    "        \n",
    "#     bib_entries_latex = None\n",
    "#     if article['latex_parse'] and article['latex_parse']['overview_text'] and article['latex_parse']['bib_entries']:\n",
    "#         bib_entries_latex = dict()\n",
    "        \n",
    "#         key_tex = list(article['latex_parse']['overview_text'].keys())[0]\n",
    "#         for cite_spans in article['latex_parse']['overview_text'][key_tex]['cite_spans']:\n",
    "#             if len(cite_spans)>1:\n",
    "#                 for cite_span in cite_spans:\n",
    "#                     cite_ref = cite_span['ref_id']\n",
    "#                     cited_article_id = None\n",
    "#                     if cite_ref in article['latex_parse']['bib_entries']:\n",
    "#                         cited_article_id = article['latex_parse']['bib_entries'][cite_ref]['links']\n",
    "#                     if cited_article_id:\n",
    "#                         bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "#                     elif cite_ref in article['latex_parse']['bib_entries']:\n",
    "#                         bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "    \n",
    "    \n",
    "    all_bib_entries = dict()\n",
    "#     if bib_entries_latex:\n",
    "#         for k,v in bib_entries_latex.items():\n",
    "#             if k == '':\n",
    "#                 continue\n",
    "                \n",
    "#             if v:\n",
    "#                 all_bib_entries[k] = [v]\n",
    "#             else:\n",
    "#                 all_bib_entries[k] = []\n",
    "\n",
    "    if bib_entries_grobid:\n",
    "        for k,v in bib_entries_grobid.items():\n",
    "            if k == '':\n",
    "                continue\n",
    "                \n",
    "            if k in all_bib_entries:\n",
    "                if len(all_bib_entries[k]) == 0:\n",
    "                    if v:\n",
    "                        all_bib_entries[k] = [v]\n",
    "                    else:\n",
    "                        all_bib_entries[k] = []\n",
    "                else:\n",
    "                    if all_bib_entries[k][0] == v:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if v:\n",
    "                            all_bib_entries[k].append(v)\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                if v:\n",
    "                    all_bib_entries[k] = [v]\n",
    "                else:\n",
    "                    all_bib_entries[k] = []\n",
    "        article_bibs[article['paper_id']] = all_bib_entries\n",
    "        \n",
    "        p_ref_order.append(ref_order)\n",
    "        p_ref_ind.append(article['paper_id'])\n",
    "        \n",
    "#         key_grobid = list(article['grobid_parse']['abstract'].keys())[0]\n",
    "        if article['metadata']['abstract']:\n",
    "            p_ref_abstract.append(article['metadata']['abstract'])\n",
    "        else:\n",
    "            p_ref_abstract.append(0)\n",
    "        if article['metadata']['title']:\n",
    "            p_ref_title.append(article['metadata']['title'])\n",
    "        else:\n",
    "            p_ref_title.append(0)\n",
    "        if article['grobid_parse'] and article['grobid_parse']['abstract']:\n",
    "            p_ref_abstract_grobid.append(article['grobid_parse']['abstract'][0]['text'])\n",
    "        else:\n",
    "            p_ref_abstract_grobid.append(0)\n",
    "        if article['latex_parse'] and article['latex_parse']['abstract']:\n",
    "            p_ref_abstract_latex.append(article['latex_parse']['abstract'][0]['text'])\n",
    "        else:\n",
    "            p_ref_abstract_latex.append(0)\n",
    "        \n",
    "        ref_author = []\n",
    "        if article['metadata']['authors']:\n",
    "            for author in article['metadata']['authors']:\n",
    "                if author['first']:\n",
    "                    ref_author.append(author['first']+' '+author['last'])\n",
    "                elif author['suffix'] or author['middle']:\n",
    "                    print('CAn be mistake',article['metadata']['authors'],article['paper_id'])\n",
    "                    break\n",
    "                else:\n",
    "                    ref_author.append(author['last'])\n",
    "        p_ref_authors.append(ref_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018', '14472576', '14323173', '442560', '44112954']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(article_bibs.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': ['8294822'],\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': ['10418456'],\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': ['532313'],\n",
       " 'A survey of text summarization techniques': ['556431'],\n",
       " 'Exploiting category-specific information for multidocument summarization': ['6317274'],\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': ['29562039'],\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": ['13723748'],\n",
       " 'Social context summarization': ['704517'],\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': ['8377315'],\n",
       " 'Autoencoding variational bayes': [],\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': ['16895865']}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_bibs['10164018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8294822',\n",
       " '10418456',\n",
       " '532313',\n",
       " '556431',\n",
       " '6317274',\n",
       " '29562039',\n",
       " '13723748',\n",
       " '704517',\n",
       " '8377315',\n",
       " '29562039',\n",
       " '16895865']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ref_order[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29562039    2\n",
       "16895865    1\n",
       "10418456    1\n",
       "13723748    1\n",
       "8294822     1\n",
       "704517      1\n",
       "6317274     1\n",
       "556431      1\n",
       "532313      1\n",
       "8377315     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(p_ref_order[0]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.196445791431326"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(p) for p in p_ref_order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10164018'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ref_ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = {'acl_p_id': p_ref_ind,'title':p_ref_title,'authors':p_ref_authors,'abstract':p_ref_abstract,\n",
    "          'abstract_grobid':p_ref_abstract_grobid,'abstract_latex':p_ref_abstract_latex,'ref_order': p_ref_order}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.DataFrame(target)\n",
    "df_target.index = p_ref_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_p_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>ref_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Reader-Aware Multi-Document Summarization: An ...</td>\n",
       "      <td>[Piji Li, Lidong Bing, Wai Lam]</td>\n",
       "      <td>We investigate the problem of reader-aware mul...</td>\n",
       "      <td>AbstractWe investigate the problem of readeraw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8294822, 10418456, 532313, 556431, 6317274, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2265838, 14341841, 6401679, 6216733, 2408319,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14323173</th>\n",
       "      <td>14323173</td>\n",
       "      <td>Speculation and negation annotation in natural...</td>\n",
       "      <td>[Veronika Vincze]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[5261517, 8410430, 18343028, 16285026, 4068646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442560</th>\n",
       "      <td>442560</td>\n",
       "      <td>HYENA-live: Fine-Grained Online Entity Type Cl...</td>\n",
       "      <td>[Mohamed Yosef, Sandro Bauer, Johannes Hoffart...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractRecent research has shown progress in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[10977241, 8722811, 9345159, 6430811, 5570507,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44112954</th>\n",
       "      <td>44112954</td>\n",
       "      <td>UNAM at SemEval-2018 Task 10: Unsupervised Sem...</td>\n",
       "      <td>[Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractIn this paper we report an unsupervise...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1919756, 17389337, 16694779, 63259515, 149407...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acl_p_id                                              title  \\\n",
       "10164018  10164018  Reader-Aware Multi-Document Summarization: An ...   \n",
       "14472576  14472576               Building a Semantic Parser Overnight   \n",
       "14323173  14323173  Speculation and negation annotation in natural...   \n",
       "442560      442560  HYENA-live: Fine-Grained Online Entity Type Cl...   \n",
       "44112954  44112954  UNAM at SemEval-2018 Task 10: Unsupervised Sem...   \n",
       "\n",
       "                                                    authors  \\\n",
       "10164018                    [Piji Li, Lidong Bing, Wai Lam]   \n",
       "14472576         [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "14323173                                  [Veronika Vincze]   \n",
       "442560    [Mohamed Yosef, Sandro Bauer, Johannes Hoffart...   \n",
       "44112954  [Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...   \n",
       "\n",
       "                                                   abstract  \\\n",
       "10164018  We investigate the problem of reader-aware mul...   \n",
       "14472576  How do we build a semantic parser in a new dom...   \n",
       "14323173                                                  0   \n",
       "442560                                                    0   \n",
       "44112954                                                  0   \n",
       "\n",
       "                                            abstract_grobid  abstract_latex  \\\n",
       "10164018  AbstractWe investigate the problem of readeraw...               0   \n",
       "14472576  AbstractHow do we build a semantic parser in a...               0   \n",
       "14323173                                                  0               0   \n",
       "442560    AbstractRecent research has shown progress in ...               0   \n",
       "44112954  AbstractIn this paper we report an unsupervise...               0   \n",
       "\n",
       "                                                  ref_order  \n",
       "10164018  [8294822, 10418456, 532313, 556431, 6317274, 2...  \n",
       "14472576  [2265838, 14341841, 6401679, 6216733, 2408319,...  \n",
       "14323173  [5261517, 8410430, 18343028, 16285026, 4068646...  \n",
       "442560    [10977241, 8722811, 9345159, 6430811, 5570507,...  \n",
       "44112954  [1919756, 17389337, 16694779, 63259515, 149407...  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ids_dict = {ids:1 for ids in p_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2575762',\n",
       " '2428290',\n",
       " '4248450',\n",
       " '2612947',\n",
       " '44063762',\n",
       " '51979529',\n",
       " '17703143',\n",
       " '908539',\n",
       " '49210174',\n",
       " '20587930',\n",
       " '40100965',\n",
       " '3626819',\n",
       " '52010710']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.ref_order.values[893]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_targets_not_in_OUT_papers(ref_order):\n",
    "    new_ref_order = []\n",
    "    for num,ref in enumerate(ref_order):\n",
    "        if not ref in p_ids_dict:\n",
    "            print(num,ref)\n",
    "        else:\n",
    "            new_ref_order.append(ref)\n",
    "    return new_ref_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 908539\n",
      "7 57352402\n",
      "6 908539\n",
      "4 908539\n",
      "14 908539\n",
      "8 908539\n",
      "11 908539\n"
     ]
    }
   ],
   "source": [
    "df_target['ref_order'] = df_target['ref_order'].apply(del_targets_not_in_OUT_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ref_order in df_target['ref_order'].values:\n",
    "    for ref in ref_order:\n",
    "        if not ref in p_ids_dict:\n",
    "            print(num,ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_p_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>ref_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Reader-Aware Multi-Document Summarization: An ...</td>\n",
       "      <td>[Piji Li, Lidong Bing, Wai Lam]</td>\n",
       "      <td>We investigate the problem of reader-aware mul...</td>\n",
       "      <td>AbstractWe investigate the problem of readeraw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8294822, 10418456, 532313, 556431, 6317274, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2265838, 14341841, 6401679, 6216733, 2408319,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14323173</th>\n",
       "      <td>14323173</td>\n",
       "      <td>Speculation and negation annotation in natural...</td>\n",
       "      <td>[Veronika Vincze]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[5261517, 8410430, 18343028, 16285026, 4068646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442560</th>\n",
       "      <td>442560</td>\n",
       "      <td>HYENA-live: Fine-Grained Online Entity Type Cl...</td>\n",
       "      <td>[Mohamed Yosef, Sandro Bauer, Johannes Hoffart...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractRecent research has shown progress in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[10977241, 8722811, 9345159, 6430811, 5570507,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44112954</th>\n",
       "      <td>44112954</td>\n",
       "      <td>UNAM at SemEval-2018 Task 10: Unsupervised Sem...</td>\n",
       "      <td>[Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractIn this paper we report an unsupervise...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1919756, 17389337, 16694779, 63259515, 149407...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acl_p_id                                              title  \\\n",
       "10164018  10164018  Reader-Aware Multi-Document Summarization: An ...   \n",
       "14472576  14472576               Building a Semantic Parser Overnight   \n",
       "14323173  14323173  Speculation and negation annotation in natural...   \n",
       "442560      442560  HYENA-live: Fine-Grained Online Entity Type Cl...   \n",
       "44112954  44112954  UNAM at SemEval-2018 Task 10: Unsupervised Sem...   \n",
       "\n",
       "                                                    authors  \\\n",
       "10164018                    [Piji Li, Lidong Bing, Wai Lam]   \n",
       "14472576         [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "14323173                                  [Veronika Vincze]   \n",
       "442560    [Mohamed Yosef, Sandro Bauer, Johannes Hoffart...   \n",
       "44112954  [Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...   \n",
       "\n",
       "                                                   abstract  \\\n",
       "10164018  We investigate the problem of reader-aware mul...   \n",
       "14472576  How do we build a semantic parser in a new dom...   \n",
       "14323173                                                  0   \n",
       "442560                                                    0   \n",
       "44112954                                                  0   \n",
       "\n",
       "                                            abstract_grobid  abstract_latex  \\\n",
       "10164018  AbstractWe investigate the problem of readeraw...               0   \n",
       "14472576  AbstractHow do we build a semantic parser in a...               0   \n",
       "14323173                                                  0               0   \n",
       "442560    AbstractRecent research has shown progress in ...               0   \n",
       "44112954  AbstractIn this paper we report an unsupervise...               0   \n",
       "\n",
       "                                                  ref_order  \n",
       "10164018  [8294822, 10418456, 532313, 556431, 6317274, 2...  \n",
       "14472576  [2265838, 14341841, 6401679, 6216733, 2408319,...  \n",
       "14323173  [5261517, 8410430, 18343028, 16285026, 4068646...  \n",
       "442560    [10977241, 8722811, 9345159, 6430811, 5570507,...  \n",
       "44112954  [1919756, 17389337, 16694779, 63259515, 149407...  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>s2_pdf_hash</th>\n",
       "      <th>year</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>acl_id</th>\n",
       "      <th>pmc_id</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>venue</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>cnt_out_grobid</th>\n",
       "      <th>cnt_out_latex</th>\n",
       "      <th>venue_cit_n</th>\n",
       "      <th>in_cits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13756507</th>\n",
       "      <td>13756507</td>\n",
       "      <td>3944c1740bb643415583363a0a35f12f8bd19a16</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>arxiv</td>\n",
       "      <td>Recommendation systems play a vital role to ke...</td>\n",
       "      <td>Collaborative Memory Network for Recommendatio...</td>\n",
       "      <td>[Travis Ebesu, Bin Shen, Yi Fang]</td>\n",
       "      <td>ABSTRACTRecommendation systems play a vital ro...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>3450</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>f3de408be7d2e2720a61451bd196ac7e1ed9363a</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>acl</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2257</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21369026</th>\n",
       "      <td>21369026</td>\n",
       "      <td>abd6baaf7e9d830feb3889e1019a989b6959ae89</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>On the Uses of Word Sense Change for Research ...</td>\n",
       "      <td>[Nina Tahmasebi, Thomas Risse]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2497200</th>\n",
       "      <td>2497200</td>\n",
       "      <td>5e3cd43b9c0288a08c782b8df1311dd3e4f06c9b</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TOIS</td>\n",
       "      <td>We describe an approach to text classification...</td>\n",
       "      <td>Information extraction as a basis for high-pre...</td>\n",
       "      <td>[Ellen Riloff, Wendy Lehnert]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406684</th>\n",
       "      <td>406684</td>\n",
       "      <td>a68dc3c376272c5c08df211c6974f02380295ebd</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IEEE</td>\n",
       "      <td>In many practical data mining applications, su...</td>\n",
       "      <td>Tri-training: exploiting unlabeled data using ...</td>\n",
       "      <td>[Zhi-Hua Zhou, Ming Li]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          paper_id                               s2_pdf_hash  year  arxiv_id  \\\n",
       "13756507  13756507  3944c1740bb643415583363a0a35f12f8bd19a16  2018         1   \n",
       "14472576  14472576  f3de408be7d2e2720a61451bd196ac7e1ed9363a  2015         0   \n",
       "21369026  21369026  abd6baaf7e9d830feb3889e1019a989b6959ae89  2017         0   \n",
       "2497200    2497200  5e3cd43b9c0288a08c782b8df1311dd3e4f06c9b  1994         0   \n",
       "406684      406684  a68dc3c376272c5c08df211c6974f02380295ebd  2005         0   \n",
       "\n",
       "          acl_id  pmc_id  pubmed_id  doi  venue  \\\n",
       "13756507       0       0          0    1  arxiv   \n",
       "14472576       1       0          0    1    acl   \n",
       "21369026       0       0          0    1  other   \n",
       "2497200        0       0          0    1   TOIS   \n",
       "406684         0       0          0    1   IEEE   \n",
       "\n",
       "                                                   abstract  \\\n",
       "13756507  Recommendation systems play a vital role to ke...   \n",
       "14472576  How do we build a semantic parser in a new dom...   \n",
       "21369026                                                  0   \n",
       "2497200   We describe an approach to text classification...   \n",
       "406684    In many practical data mining applications, su...   \n",
       "\n",
       "                                                      title  \\\n",
       "13756507  Collaborative Memory Network for Recommendatio...   \n",
       "14472576               Building a Semantic Parser Overnight   \n",
       "21369026  On the Uses of Word Sense Change for Research ...   \n",
       "2497200   Information extraction as a basis for high-pre...   \n",
       "406684    Tri-training: exploiting unlabeled data using ...   \n",
       "\n",
       "                                             authors  \\\n",
       "13756507           [Travis Ebesu, Bin Shen, Yi Fang]   \n",
       "14472576  [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "21369026              [Nina Tahmasebi, Thomas Risse]   \n",
       "2497200                [Ellen Riloff, Wendy Lehnert]   \n",
       "406684                       [Zhi-Hua Zhou, Ming Li]   \n",
       "\n",
       "                                            abstract_grobid abstract_latex  \\\n",
       "13756507  ABSTRACTRecommendation systems play a vital ro...              0   \n",
       "14472576  AbstractHow do we build a semantic parser in a...              0   \n",
       "21369026                                                  0              0   \n",
       "2497200                                                   0              0   \n",
       "406684                                                    0              0   \n",
       "\n",
       "          cnt_out_grobid  cnt_out_latex  venue_cit_n  in_cits  \n",
       "13756507              40             41         3450       13  \n",
       "14472576              27              0         2257       92  \n",
       "21369026              29              0            0        0  \n",
       "2497200                0              0           30        0  \n",
       "406684                30              0         1144        0  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7259, 8), (26887, 18))"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.shape,df_acl_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_target.to_excel(dir_name+'df_target.xlsx')\n",
    "# df_target.to_csv(dir_name+'df_target.csv',encoding='utf8')\n",
    "# df_acl_features.to_excel(dir_name+'df_acl_features.xlsx')\n",
    "# df_acl_features.to_csv(dir_name+'df_acl_features.csv',encoding='utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_target удаляем дублирования в таргет и записываем как признак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8294822',\n",
       " '10418456',\n",
       " '532313',\n",
       " '556431',\n",
       " '6317274',\n",
       " '29562039',\n",
       " '13723748',\n",
       " '704517',\n",
       " '8377315',\n",
       " '16895865']"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.iloc[0]['ref_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_order_uniq = []\n",
    "doubled_values = []\n",
    "for ref in df_target.iloc[0]['ref_order']:\n",
    "    if ref in ref_order_uniq:\n",
    "        doubled_values.append(ref)\n",
    "    else:\n",
    "        ref_order_uniq.append(ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doubled_values(x):\n",
    "    ref_order_uniq = []\n",
    "    doubled_value = []\n",
    "    for ref in x:\n",
    "        if ref in ref_order_uniq:\n",
    "            doubled_value.append(ref)\n",
    "        else:\n",
    "            ref_order_uniq.append(ref)\n",
    "    return doubled_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doubled_values(x):\n",
    "    ref_order_uniq = []\n",
    "    doubled_value = []\n",
    "    for ref in x:\n",
    "        if ref in ref_order_uniq:\n",
    "            doubled_value.append(ref)\n",
    "        else:\n",
    "            ref_order_uniq.append(ref)\n",
    "    return ref_order_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['doubl_ids'] = df_target.ref_order.apply(doubled_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['ref_order'] = df_target.ref_order.apply(remove_doubled_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_p_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>ref_order</th>\n",
       "      <th>doubl_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Reader-Aware Multi-Document Summarization: An ...</td>\n",
       "      <td>[Piji Li, Lidong Bing, Wai Lam]</td>\n",
       "      <td>We investigate the problem of reader-aware mul...</td>\n",
       "      <td>AbstractWe investigate the problem of readeraw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8294822, 10418456, 532313, 556431, 6317274, 2...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2265838, 14341841, 6401679, 6216733, 2408319,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14323173</th>\n",
       "      <td>14323173</td>\n",
       "      <td>Speculation and negation annotation in natural...</td>\n",
       "      <td>[Veronika Vincze]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[5261517, 8410430, 18343028, 16285026, 4068646...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442560</th>\n",
       "      <td>442560</td>\n",
       "      <td>HYENA-live: Fine-Grained Online Entity Type Cl...</td>\n",
       "      <td>[Mohamed Yosef, Sandro Bauer, Johannes Hoffart...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractRecent research has shown progress in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[10977241, 8722811, 9345159, 6430811, 5570507,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44112954</th>\n",
       "      <td>44112954</td>\n",
       "      <td>UNAM at SemEval-2018 Task 10: Unsupervised Sem...</td>\n",
       "      <td>[Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractIn this paper we report an unsupervise...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1919756, 17389337, 16694779, 63259515, 149407...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acl_p_id                                              title  \\\n",
       "10164018  10164018  Reader-Aware Multi-Document Summarization: An ...   \n",
       "14472576  14472576               Building a Semantic Parser Overnight   \n",
       "14323173  14323173  Speculation and negation annotation in natural...   \n",
       "442560      442560  HYENA-live: Fine-Grained Online Entity Type Cl...   \n",
       "44112954  44112954  UNAM at SemEval-2018 Task 10: Unsupervised Sem...   \n",
       "\n",
       "                                                    authors  \\\n",
       "10164018                    [Piji Li, Lidong Bing, Wai Lam]   \n",
       "14472576         [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "14323173                                  [Veronika Vincze]   \n",
       "442560    [Mohamed Yosef, Sandro Bauer, Johannes Hoffart...   \n",
       "44112954  [Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...   \n",
       "\n",
       "                                                   abstract  \\\n",
       "10164018  We investigate the problem of reader-aware mul...   \n",
       "14472576  How do we build a semantic parser in a new dom...   \n",
       "14323173                                                  0   \n",
       "442560                                                    0   \n",
       "44112954                                                  0   \n",
       "\n",
       "                                            abstract_grobid  abstract_latex  \\\n",
       "10164018  AbstractWe investigate the problem of readeraw...               0   \n",
       "14472576  AbstractHow do we build a semantic parser in a...               0   \n",
       "14323173                                                  0               0   \n",
       "442560    AbstractRecent research has shown progress in ...               0   \n",
       "44112954  AbstractIn this paper we report an unsupervise...               0   \n",
       "\n",
       "                                                  ref_order doubl_ids  \n",
       "10164018  [8294822, 10418456, 532313, 556431, 6317274, 2...        []  \n",
       "14472576  [2265838, 14341841, 6401679, 6216733, 2408319,...        []  \n",
       "14323173  [5261517, 8410430, 18343028, 16285026, 4068646...        []  \n",
       "442560    [10977241, 8722811, 9345159, 6430811, 5570507,...        []  \n",
       "44112954  [1919756, 17389337, 16694779, 63259515, 149407...        []  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "также удаляем дублирование в df_acl_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26887, 18)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26886, 18)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.loc[~df_acl_features.index.duplicated(keep='first')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acl_features = df_acl_features.loc[~df_acl_features.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26886, 18)"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baseline model - Ranking by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009.4243285221805"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features[df_acl_features.year != 0].year.astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2011.0"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acl_features[df_acl_features.year != 0].year.astype(int).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_by_year(x,df_acl_features=df_acl_features,reverse=False):\n",
    "    year_list_tups = []\n",
    "    for out_paper_id in x:\n",
    "        year = int(df_acl_features.loc[out_paper_id]['year']) \n",
    "        if year == 0:\n",
    "            year_list_tups.append( (2011,out_paper_id) )\n",
    "        else:\n",
    "            year_list_tups.append( (year,out_paper_id) )\n",
    "    year_list_tups = sorted(year_list_tups, key=lambda tup: tup[0],reverse=reverse)\n",
    "    year_list_tups = list(map(lambda x: x[1],year_list_tups))\n",
    "    return year_list_tups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['14680675',\n",
       "  '14775471',\n",
       "  '10253749',\n",
       "  '15029034',\n",
       "  '10608783',\n",
       "  '9481819',\n",
       "  '3201232',\n",
       "  '15763200',\n",
       "  '11125179',\n",
       "  '2436152',\n",
       "  '14476855',\n",
       "  '5570507',\n",
       "  '6272433',\n",
       "  '9345159',\n",
       "  '442560',\n",
       "  '18169483',\n",
       "  '8789606',\n",
       "  '11174540',\n",
       "  '4357791',\n",
       "  '8760686',\n",
       "  '7461625',\n",
       "  '16915526',\n",
       "  '7124381'],\n",
       " 23)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## x = df_target.iloc[0]['ref_order']\n",
    "x,len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2006, '14680675'), (2012, '14775471'), (2010, '10253749'), (2013, '15029034'), (2013, '10608783'), (2014, '9481819'), (2018, '3201232'), (1992, '15763200'), (2014, '11125179'), (2012, '2436152'), (2013, '14476855'), (2002, '5570507'), (2010, '6272433'), (2012, '9345159'), (2013, '442560'), (2012, '18169483'), (2008, '8789606'), (2010, '11174540'), (2009, '4357791'), (2010, '8760686'), (2012, '7461625'), (2014, '16915526'), (2014, '7124381')]\n",
      "[(1992, '15763200'), (2002, '5570507'), (2006, '14680675'), (2008, '8789606'), (2009, '4357791'), (2010, '10253749'), (2010, '6272433'), (2010, '11174540'), (2010, '8760686'), (2012, '14775471'), (2012, '2436152'), (2012, '9345159'), (2012, '18169483'), (2012, '7461625'), (2013, '15029034'), (2013, '10608783'), (2013, '14476855'), (2013, '442560'), (2014, '9481819'), (2014, '11125179'), (2014, '16915526'), (2014, '7124381'), (2018, '3201232')]\n",
      "['15763200', '5570507', '14680675', '8789606', '4357791', '10253749', '6272433', '11174540', '8760686', '14775471', '2436152', '9345159', '18169483', '7461625', '15029034', '10608783', '14476855', '442560', '9481819', '11125179', '16915526', '7124381', '3201232']\n"
     ]
    }
   ],
   "source": [
    "year_list_tups = []\n",
    "for out_paper_id in x:\n",
    "    year_list_tups.append( (int(df_acl_features.loc[out_paper_id]['year']),out_paper_id) )\n",
    "print(year_list_tups)\n",
    "year_list_tups = sorted(year_list_tups, key=lambda tup: tup[0],reverse=False)\n",
    "print(year_list_tups)\n",
    "year_list_tups = list(map(lambda x: x[1],year_list_tups))\n",
    "print(year_list_tups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['rank_by_year'] = df_target.ref_order.apply(ranking_by_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_target.ref_order.apply(len) == df_target.rank_by_year.apply(len)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_p_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>ref_order</th>\n",
       "      <th>doubl_ids</th>\n",
       "      <th>rank_by_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Reader-Aware Multi-Document Summarization: An ...</td>\n",
       "      <td>[Piji Li, Lidong Bing, Wai Lam]</td>\n",
       "      <td>We investigate the problem of reader-aware mul...</td>\n",
       "      <td>AbstractWe investigate the problem of readeraw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8294822, 10418456, 532313, 556431, 6317274, 2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[8294822, 10418456, 532313, 13723748, 704517, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2265838, 14341841, 6401679, 6216733, 2408319,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[62727207, 2498523, 9337134, 5667590, 10228634...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14323173</th>\n",
       "      <td>14323173</td>\n",
       "      <td>Speculation and negation annotation in natural...</td>\n",
       "      <td>[Veronika Vincze]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[5261517, 8410430, 18343028, 16285026, 4068646...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[8410430, 18343028, 5261517, 16285026, 4068646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442560</th>\n",
       "      <td>442560</td>\n",
       "      <td>HYENA-live: Fine-Grained Online Entity Type Cl...</td>\n",
       "      <td>[Mohamed Yosef, Sandro Bauer, Johannes Hoffart...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractRecent research has shown progress in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[10977241, 8722811, 9345159, 6430811, 5570507,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[5570507, 10977241, 33008026, 8722811, 6430811...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44112954</th>\n",
       "      <td>44112954</td>\n",
       "      <td>UNAM at SemEval-2018 Task 10: Unsupervised Sem...</td>\n",
       "      <td>[Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractIn this paper we report an unsupervise...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1919756, 17389337, 16694779, 63259515, 149407...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[6708547, 10677711, 14940757, 10301835, 141718...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acl_p_id                                              title  \\\n",
       "10164018  10164018  Reader-Aware Multi-Document Summarization: An ...   \n",
       "14472576  14472576               Building a Semantic Parser Overnight   \n",
       "14323173  14323173  Speculation and negation annotation in natural...   \n",
       "442560      442560  HYENA-live: Fine-Grained Online Entity Type Cl...   \n",
       "44112954  44112954  UNAM at SemEval-2018 Task 10: Unsupervised Sem...   \n",
       "\n",
       "                                                    authors  \\\n",
       "10164018                    [Piji Li, Lidong Bing, Wai Lam]   \n",
       "14472576         [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "14323173                                  [Veronika Vincze]   \n",
       "442560    [Mohamed Yosef, Sandro Bauer, Johannes Hoffart...   \n",
       "44112954  [Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...   \n",
       "\n",
       "                                                   abstract  \\\n",
       "10164018  We investigate the problem of reader-aware mul...   \n",
       "14472576  How do we build a semantic parser in a new dom...   \n",
       "14323173                                                  0   \n",
       "442560                                                    0   \n",
       "44112954                                                  0   \n",
       "\n",
       "                                            abstract_grobid  abstract_latex  \\\n",
       "10164018  AbstractWe investigate the problem of readeraw...               0   \n",
       "14472576  AbstractHow do we build a semantic parser in a...               0   \n",
       "14323173                                                  0               0   \n",
       "442560    AbstractRecent research has shown progress in ...               0   \n",
       "44112954  AbstractIn this paper we report an unsupervise...               0   \n",
       "\n",
       "                                                  ref_order doubl_ids  \\\n",
       "10164018  [8294822, 10418456, 532313, 556431, 6317274, 2...        []   \n",
       "14472576  [2265838, 14341841, 6401679, 6216733, 2408319,...        []   \n",
       "14323173  [5261517, 8410430, 18343028, 16285026, 4068646...        []   \n",
       "442560    [10977241, 8722811, 9345159, 6430811, 5570507,...        []   \n",
       "44112954  [1919756, 17389337, 16694779, 63259515, 149407...        []   \n",
       "\n",
       "                                               rank_by_year  \n",
       "10164018  [8294822, 10418456, 532313, 13723748, 704517, ...  \n",
       "14472576  [62727207, 2498523, 9337134, 5667590, 10228634...  \n",
       "14323173  [8410430, 18343028, 5261517, 16285026, 4068646...  \n",
       "442560    [5570507, 10977241, 33008026, 8722811, 6430811...  \n",
       "44112954  [6708547, 10677711, 14940757, 10301835, 141718...  "
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем ранговый коэффициент корреляции Кендалла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1,x2 = np.array(df_target.iloc[0]['ref_order']).astype(int),np.array(df_target.iloc[0]['rank_by_year']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau, _ = stats.kendalltau(x1, x2)\n",
    "# Kendall’s tau is a measure of the correspondence between two rankings. \n",
    "# Values close to 1 indicate strong agreement, values close to -1 indicate strong disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_kendalltau_betw_2_str_lists(x):\n",
    "    ref_order = np.array(x['ref_order']).astype(int)\n",
    "    rank_by_year = np.array(x['rank_by_year']).astype(int)\n",
    "    tau, _ = stats.kendalltau(ref_order, rank_by_year)\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_kendalltau_betw_2_str_lists(df_target.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['baseline_kendalltau'] = df_target.apply( measure_kendalltau_betw_2_str_lists, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acl_p_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract_grobid</th>\n",
       "      <th>abstract_latex</th>\n",
       "      <th>ref_order</th>\n",
       "      <th>doubl_ids</th>\n",
       "      <th>rank_by_year</th>\n",
       "      <th>baseline_kendalltau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Reader-Aware Multi-Document Summarization: An ...</td>\n",
       "      <td>[Piji Li, Lidong Bing, Wai Lam]</td>\n",
       "      <td>We investigate the problem of reader-aware mul...</td>\n",
       "      <td>AbstractWe investigate the problem of readeraw...</td>\n",
       "      <td>0</td>\n",
       "      <td>[8294822, 10418456, 532313, 556431, 6317274, 2...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[8294822, 10418456, 532313, 13723748, 704517, ...</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>14472576</td>\n",
       "      <td>Building a Semantic Parser Overnight</td>\n",
       "      <td>[Yushi Wang, Jonathan Berant, Percy Liang]</td>\n",
       "      <td>How do we build a semantic parser in a new dom...</td>\n",
       "      <td>AbstractHow do we build a semantic parser in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2265838, 14341841, 6401679, 6216733, 2408319,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[62727207, 2498523, 9337134, 5667590, 10228634...</td>\n",
       "      <td>-0.098039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14323173</th>\n",
       "      <td>14323173</td>\n",
       "      <td>Speculation and negation annotation in natural...</td>\n",
       "      <td>[Veronika Vincze]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[5261517, 8410430, 18343028, 16285026, 4068646...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[8410430, 18343028, 5261517, 16285026, 4068646...</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442560</th>\n",
       "      <td>442560</td>\n",
       "      <td>HYENA-live: Fine-Grained Online Entity Type Cl...</td>\n",
       "      <td>[Mohamed Yosef, Sandro Bauer, Johannes Hoffart...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractRecent research has shown progress in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[10977241, 8722811, 9345159, 6430811, 5570507,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[5570507, 10977241, 33008026, 8722811, 6430811...</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44112954</th>\n",
       "      <td>44112954</td>\n",
       "      <td>UNAM at SemEval-2018 Task 10: Unsupervised Sem...</td>\n",
       "      <td>[Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...</td>\n",
       "      <td>0</td>\n",
       "      <td>AbstractIn this paper we report an unsupervise...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1919756, 17389337, 16694779, 63259515, 149407...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[6708547, 10677711, 14940757, 10301835, 141718...</td>\n",
       "      <td>-0.010989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          acl_p_id                                              title  \\\n",
       "10164018  10164018  Reader-Aware Multi-Document Summarization: An ...   \n",
       "14472576  14472576               Building a Semantic Parser Overnight   \n",
       "14323173  14323173  Speculation and negation annotation in natural...   \n",
       "442560      442560  HYENA-live: Fine-Grained Online Entity Type Cl...   \n",
       "44112954  44112954  UNAM at SemEval-2018 Task 10: Unsupervised Sem...   \n",
       "\n",
       "                                                    authors  \\\n",
       "10164018                    [Piji Li, Lidong Bing, Wai Lam]   \n",
       "14472576         [Yushi Wang, Jonathan Berant, Percy Liang]   \n",
       "14323173                                  [Veronika Vincze]   \n",
       "442560    [Mohamed Yosef, Sandro Bauer, Johannes Hoffart...   \n",
       "44112954  [Ignacio Arroyo-Fernández, Iván Meza, Carlos-F...   \n",
       "\n",
       "                                                   abstract  \\\n",
       "10164018  We investigate the problem of reader-aware mul...   \n",
       "14472576  How do we build a semantic parser in a new dom...   \n",
       "14323173                                                  0   \n",
       "442560                                                    0   \n",
       "44112954                                                  0   \n",
       "\n",
       "                                            abstract_grobid  abstract_latex  \\\n",
       "10164018  AbstractWe investigate the problem of readeraw...               0   \n",
       "14472576  AbstractHow do we build a semantic parser in a...               0   \n",
       "14323173                                                  0               0   \n",
       "442560    AbstractRecent research has shown progress in ...               0   \n",
       "44112954  AbstractIn this paper we report an unsupervise...               0   \n",
       "\n",
       "                                                  ref_order doubl_ids  \\\n",
       "10164018  [8294822, 10418456, 532313, 556431, 6317274, 2...        []   \n",
       "14472576  [2265838, 14341841, 6401679, 6216733, 2408319,...        []   \n",
       "14323173  [5261517, 8410430, 18343028, 16285026, 4068646...        []   \n",
       "442560    [10977241, 8722811, 9345159, 6430811, 5570507,...        []   \n",
       "44112954  [1919756, 17389337, 16694779, 63259515, 149407...        []   \n",
       "\n",
       "                                               rank_by_year  \\\n",
       "10164018  [8294822, 10418456, 532313, 13723748, 704517, ...   \n",
       "14472576  [62727207, 2498523, 9337134, 5667590, 10228634...   \n",
       "14323173  [8410430, 18343028, 5261517, 16285026, 4068646...   \n",
       "442560    [5570507, 10977241, 33008026, 8722811, 6430811...   \n",
       "44112954  [6708547, 10677711, 14940757, 10301835, 141718...   \n",
       "\n",
       "          baseline_kendalltau  \n",
       "10164018             0.066667  \n",
       "14472576            -0.098039  \n",
       "14323173             0.466667  \n",
       "442560               0.047619  \n",
       "44112954            -0.010989  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10070598365623208, 0.06666666666666667)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_target.baseline_kendalltau),np.median(df_target.baseline_kendalltau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
