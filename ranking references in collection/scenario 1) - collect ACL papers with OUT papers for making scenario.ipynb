{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "f_zips = []\n",
    "for (dirpath, dirnames, filenames) in walk('../../gorc/'):\n",
    "    f_zips.extend(filenames)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002,\n",
       " ['0.jsonl.gz',\n",
       "  '1.jsonl.gz',\n",
       "  '10.jsonl.gz',\n",
       "  '100.jsonl.gz',\n",
       "  '1000.jsonl.gz',\n",
       "  '1001.jsonl.gz',\n",
       "  '1002.jsonl.gz',\n",
       "  '1003.jsonl.gz',\n",
       "  '1004.jsonl.gz',\n",
       "  '1005.jsonl.gz',\n",
       "  '1006.jsonl.gz',\n",
       "  '1007.jsonl.gz',\n",
       "  '1008.jsonl.gz',\n",
       "  '1009.jsonl.gz',\n",
       "  '101.jsonl.gz',\n",
       "  '1010.jsonl.gz',\n",
       "  '1011.jsonl.gz',\n",
       "  '1012.jsonl.gz',\n",
       "  '1013.jsonl.gz',\n",
       "  '1014.jsonl.gz',\n",
       "  '1015.jsonl.gz',\n",
       "  '1016.jsonl.gz',\n",
       "  '1017.jsonl.gz',\n",
       "  '1018.jsonl.gz',\n",
       "  '1019.jsonl.gz',\n",
       "  '102.jsonl.gz',\n",
       "  '1020.jsonl.gz',\n",
       "  '1021.jsonl.gz',\n",
       "  '1022.jsonl.gz',\n",
       "  '1023.jsonl.gz',\n",
       "  '1024.jsonl.gz',\n",
       "  '1025.jsonl.gz',\n",
       "  '1026.jsonl.gz',\n",
       "  '1027.jsonl.gz',\n",
       "  '1028.jsonl.gz',\n",
       "  '1029.jsonl.gz',\n",
       "  '103.jsonl.gz',\n",
       "  '1030.jsonl.gz',\n",
       "  '1031.jsonl.gz',\n",
       "  '1032.jsonl.gz',\n",
       "  '1033.jsonl.gz',\n",
       "  '1034.jsonl.gz',\n",
       "  '1035.jsonl.gz',\n",
       "  '1036.jsonl.gz',\n",
       "  '1037.jsonl.gz',\n",
       "  '1038.jsonl.gz',\n",
       "  '1039.jsonl.gz',\n",
       "  '104.jsonl.gz',\n",
       "  '1040.jsonl.gz',\n",
       "  '1041.jsonl.gz',\n",
       "  '1042.jsonl.gz',\n",
       "  '1043.jsonl.gz',\n",
       "  '1044.jsonl.gz',\n",
       "  '1045.jsonl.gz',\n",
       "  '1046.jsonl.gz',\n",
       "  '1047.jsonl.gz',\n",
       "  '1048.jsonl.gz',\n",
       "  '1049.jsonl.gz',\n",
       "  '105.jsonl.gz',\n",
       "  '1050.jsonl.gz',\n",
       "  '1051.jsonl.gz',\n",
       "  '1052.jsonl.gz',\n",
       "  '1053.jsonl.gz',\n",
       "  '1054.jsonl.gz',\n",
       "  '1055.jsonl.gz',\n",
       "  '1056.jsonl.gz',\n",
       "  '1057.jsonl.gz',\n",
       "  '1058.jsonl.gz',\n",
       "  '1059.jsonl.gz',\n",
       "  '106.jsonl.gz',\n",
       "  '1060.jsonl.gz',\n",
       "  '1061.jsonl.gz',\n",
       "  '1062.jsonl.gz',\n",
       "  '1063.jsonl.gz',\n",
       "  '1064.jsonl.gz',\n",
       "  '1065.jsonl.gz',\n",
       "  '1066.jsonl.gz',\n",
       "  '1067.jsonl.gz',\n",
       "  '1068.jsonl.gz',\n",
       "  '1069.jsonl.gz',\n",
       "  '107.jsonl.gz',\n",
       "  '1070.jsonl.gz',\n",
       "  '1071.jsonl.gz',\n",
       "  '1072.jsonl.gz',\n",
       "  '1073.jsonl.gz',\n",
       "  '1074.jsonl.gz',\n",
       "  '1075.jsonl.gz',\n",
       "  '1076.jsonl.gz',\n",
       "  '1077.jsonl.gz',\n",
       "  '1078.jsonl.gz',\n",
       "  '1079.jsonl.gz',\n",
       "  '108.jsonl.gz',\n",
       "  '1080.jsonl.gz',\n",
       "  '1081.jsonl.gz',\n",
       "  '1082.jsonl.gz',\n",
       "  '1083.jsonl.gz',\n",
       "  '1084.jsonl.gz',\n",
       "  '1085.jsonl.gz',\n",
       "  '1086.jsonl.gz',\n",
       "  '1087.jsonl.gz',\n",
       "  '1088.jsonl.gz',\n",
       "  '1089.jsonl.gz',\n",
       "  '109.jsonl.gz',\n",
       "  '1090.jsonl.gz',\n",
       "  '1091.jsonl.gz',\n",
       "  '1092.jsonl.gz',\n",
       "  '1093.jsonl.gz',\n",
       "  '1094.jsonl.gz',\n",
       "  '1095.jsonl.gz',\n",
       "  '1096.jsonl.gz',\n",
       "  '1097.jsonl.gz',\n",
       "  '1098.jsonl.gz',\n",
       "  '1099.jsonl.gz',\n",
       "  '11.jsonl.gz',\n",
       "  '110.jsonl.gz',\n",
       "  '1100.jsonl.gz',\n",
       "  '1101.jsonl.gz',\n",
       "  '1102.jsonl.gz',\n",
       "  '1103.jsonl.gz',\n",
       "  '1104.jsonl.gz',\n",
       "  '1105.jsonl.gz',\n",
       "  '1106.jsonl.gz',\n",
       "  '1107.jsonl.gz',\n",
       "  '1108.jsonl.gz',\n",
       "  '1109.jsonl.gz',\n",
       "  '111.jsonl.gz',\n",
       "  '1110.jsonl.gz',\n",
       "  '1111.jsonl.gz',\n",
       "  '1112.jsonl.gz',\n",
       "  '1113.jsonl.gz',\n",
       "  '1114.jsonl.gz',\n",
       "  '1115.jsonl.gz',\n",
       "  '1116.jsonl.gz',\n",
       "  '1117.jsonl.gz',\n",
       "  '1118.jsonl.gz',\n",
       "  '1119.jsonl.gz',\n",
       "  '112.jsonl.gz',\n",
       "  '1120.jsonl.gz',\n",
       "  '1121.jsonl.gz',\n",
       "  '1122.jsonl.gz',\n",
       "  '1123.jsonl.gz',\n",
       "  '1124.jsonl.gz',\n",
       "  '1125.jsonl.gz',\n",
       "  '1126.jsonl.gz',\n",
       "  '1127.jsonl.gz',\n",
       "  '1128.jsonl.gz',\n",
       "  '1129.jsonl.gz',\n",
       "  '113.jsonl.gz',\n",
       "  '1130.jsonl.gz',\n",
       "  '1131.jsonl.gz',\n",
       "  '1132.jsonl.gz',\n",
       "  '1133.jsonl.gz',\n",
       "  '1134.jsonl.gz',\n",
       "  '1135.jsonl.gz',\n",
       "  '1136.jsonl.gz',\n",
       "  '1137.jsonl.gz',\n",
       "  '1138.jsonl.gz',\n",
       "  '1139.jsonl.gz',\n",
       "  '114.jsonl.gz',\n",
       "  '1140.jsonl.gz',\n",
       "  '1141.jsonl.gz',\n",
       "  '1142.jsonl.gz',\n",
       "  '1143.jsonl.gz',\n",
       "  '1144.jsonl.gz',\n",
       "  '1145.jsonl.gz',\n",
       "  '1146.jsonl.gz',\n",
       "  '1147.jsonl.gz',\n",
       "  '1148.jsonl.gz',\n",
       "  '1149.jsonl.gz',\n",
       "  '115.jsonl.gz',\n",
       "  '1150.jsonl.gz',\n",
       "  '1151.jsonl.gz',\n",
       "  '1152.jsonl.gz',\n",
       "  '1153.jsonl.gz',\n",
       "  '1154.jsonl.gz',\n",
       "  '1155.jsonl.gz',\n",
       "  '1156.jsonl.gz',\n",
       "  '1157.jsonl.gz',\n",
       "  '1158.jsonl.gz',\n",
       "  '1159.jsonl.gz',\n",
       "  '116.jsonl.gz',\n",
       "  '1160.jsonl.gz',\n",
       "  '1161.jsonl.gz',\n",
       "  '1162.jsonl.gz',\n",
       "  '1163.jsonl.gz',\n",
       "  '1164.jsonl.gz',\n",
       "  '1165.jsonl.gz',\n",
       "  '1166.jsonl.gz',\n",
       "  '1167.jsonl.gz',\n",
       "  '1168.jsonl.gz',\n",
       "  '1169.jsonl.gz',\n",
       "  '117.jsonl.gz',\n",
       "  '1170.jsonl.gz',\n",
       "  '1171.jsonl.gz',\n",
       "  '1172.jsonl.gz',\n",
       "  '1173.jsonl.gz',\n",
       "  '1174.jsonl.gz',\n",
       "  '1175.jsonl.gz',\n",
       "  '1176.jsonl.gz',\n",
       "  '1177.jsonl.gz',\n",
       "  '1178.jsonl.gz',\n",
       "  '1179.jsonl.gz',\n",
       "  '118.jsonl.gz',\n",
       "  '1180.jsonl.gz',\n",
       "  '1181.jsonl.gz',\n",
       "  '1182.jsonl.gz',\n",
       "  '1183.jsonl.gz',\n",
       "  '1184.jsonl.gz',\n",
       "  '1185.jsonl.gz',\n",
       "  '1186.jsonl.gz',\n",
       "  '1187.jsonl.gz',\n",
       "  '1188.jsonl.gz',\n",
       "  '1189.jsonl.gz',\n",
       "  '119.jsonl.gz',\n",
       "  '1190.jsonl.gz',\n",
       "  '1191.jsonl.gz',\n",
       "  '1192.jsonl.gz',\n",
       "  '1193.jsonl.gz',\n",
       "  '1194.jsonl.gz',\n",
       "  '1195.jsonl.gz',\n",
       "  '1196.jsonl.gz',\n",
       "  '1197.jsonl.gz',\n",
       "  '1198.jsonl.gz',\n",
       "  '1199.jsonl.gz',\n",
       "  '12.jsonl.gz',\n",
       "  '120.jsonl.gz',\n",
       "  '1200.jsonl.gz',\n",
       "  '1201.jsonl.gz',\n",
       "  '1202.jsonl.gz',\n",
       "  '1203.jsonl.gz',\n",
       "  '1204.jsonl.gz',\n",
       "  '1205.jsonl.gz',\n",
       "  '1206.jsonl.gz',\n",
       "  '1207.jsonl.gz',\n",
       "  '1208.jsonl.gz',\n",
       "  '1209.jsonl.gz',\n",
       "  '121.jsonl.gz',\n",
       "  '1210.jsonl.gz',\n",
       "  '1211.jsonl.gz',\n",
       "  '1212.jsonl.gz',\n",
       "  '1213.jsonl.gz',\n",
       "  '1214.jsonl.gz',\n",
       "  '1215.jsonl.gz',\n",
       "  '1216.jsonl.gz',\n",
       "  '1217.jsonl.gz',\n",
       "  '1218.jsonl.gz',\n",
       "  '1219.jsonl.gz',\n",
       "  '122.jsonl.gz',\n",
       "  '1220.jsonl.gz',\n",
       "  '1221.jsonl.gz',\n",
       "  '1222.jsonl.gz',\n",
       "  '1223.jsonl.gz',\n",
       "  '1224.jsonl.gz',\n",
       "  '1225.jsonl.gz',\n",
       "  '1226.jsonl.gz',\n",
       "  '1227.jsonl.gz',\n",
       "  '1228.jsonl.gz',\n",
       "  '1229.jsonl.gz',\n",
       "  '123.jsonl.gz',\n",
       "  '1230.jsonl.gz',\n",
       "  '1231.jsonl.gz',\n",
       "  '1232.jsonl.gz',\n",
       "  '1233.jsonl.gz',\n",
       "  '1234.jsonl.gz',\n",
       "  '1235.jsonl.gz',\n",
       "  '1236.jsonl.gz',\n",
       "  '1237.jsonl.gz',\n",
       "  '1238.jsonl.gz',\n",
       "  '1239.jsonl.gz',\n",
       "  '124.jsonl.gz',\n",
       "  '1240.jsonl.gz',\n",
       "  '1241.jsonl.gz',\n",
       "  '1242.jsonl.gz',\n",
       "  '1243.jsonl.gz',\n",
       "  '1244.jsonl.gz',\n",
       "  '1245.jsonl.gz',\n",
       "  '1246.jsonl.gz',\n",
       "  '1247.jsonl.gz',\n",
       "  '1248.jsonl.gz',\n",
       "  '1249.jsonl.gz',\n",
       "  '125.jsonl.gz',\n",
       "  '1250.jsonl.gz',\n",
       "  '1251.jsonl.gz',\n",
       "  '1252.jsonl.gz',\n",
       "  '1253.jsonl.gz',\n",
       "  '1254.jsonl.gz',\n",
       "  '1255.jsonl.gz',\n",
       "  '1256.jsonl.gz',\n",
       "  '1257.jsonl.gz',\n",
       "  '1258.jsonl.gz',\n",
       "  '1259.jsonl.gz',\n",
       "  '126.jsonl.gz',\n",
       "  '1260.jsonl.gz',\n",
       "  '1261.jsonl.gz',\n",
       "  '1262.jsonl.gz',\n",
       "  '1263.jsonl.gz',\n",
       "  '1264.jsonl.gz',\n",
       "  '1265.jsonl.gz',\n",
       "  '1266.jsonl.gz',\n",
       "  '1267.jsonl.gz',\n",
       "  '1268.jsonl.gz',\n",
       "  '1269.jsonl.gz',\n",
       "  '127.jsonl.gz',\n",
       "  '1270.jsonl.gz',\n",
       "  '1271.jsonl.gz',\n",
       "  '1272.jsonl.gz',\n",
       "  '1273.jsonl.gz',\n",
       "  '1274.jsonl.gz',\n",
       "  '1275.jsonl.gz',\n",
       "  '1276.jsonl.gz',\n",
       "  '1277.jsonl.gz',\n",
       "  '1278.jsonl.gz',\n",
       "  '1279.jsonl.gz',\n",
       "  '128.jsonl.gz',\n",
       "  '1280.jsonl.gz',\n",
       "  '1281.jsonl.gz',\n",
       "  '1282.jsonl.gz',\n",
       "  '1283.jsonl.gz',\n",
       "  '1284.jsonl.gz',\n",
       "  '1285.jsonl.gz',\n",
       "  '1286.jsonl.gz',\n",
       "  '1287.jsonl.gz',\n",
       "  '1288.jsonl.gz',\n",
       "  '1289.jsonl.gz',\n",
       "  '129.jsonl.gz',\n",
       "  '1290.jsonl.gz',\n",
       "  '1291.jsonl.gz',\n",
       "  '1292.jsonl.gz',\n",
       "  '1293.jsonl.gz',\n",
       "  '1294.jsonl.gz',\n",
       "  '1295.jsonl.gz',\n",
       "  '1296.jsonl.gz',\n",
       "  '1297.jsonl.gz',\n",
       "  '1298.jsonl.gz',\n",
       "  '1299.jsonl.gz',\n",
       "  '13.jsonl.gz',\n",
       "  '130.jsonl.gz',\n",
       "  '1300.jsonl.gz',\n",
       "  '1301.jsonl.gz',\n",
       "  '1302.jsonl.gz',\n",
       "  '1303.jsonl.gz',\n",
       "  '1304.jsonl.gz',\n",
       "  '1305.jsonl.gz',\n",
       "  '1306.jsonl.gz',\n",
       "  '1307.jsonl.gz',\n",
       "  '1308.jsonl.gz',\n",
       "  '1309.jsonl.gz',\n",
       "  '131.jsonl.gz',\n",
       "  '1310.jsonl.gz',\n",
       "  '1311.jsonl.gz',\n",
       "  '1312.jsonl.gz',\n",
       "  '1313.jsonl.gz',\n",
       "  '1314.jsonl.gz',\n",
       "  '1315.jsonl.gz',\n",
       "  '1316.jsonl.gz',\n",
       "  '1317.jsonl.gz',\n",
       "  '1318.jsonl.gz',\n",
       "  '1319.jsonl.gz',\n",
       "  '132.jsonl.gz',\n",
       "  '1320.jsonl.gz',\n",
       "  '1321.jsonl.gz',\n",
       "  '1322.jsonl.gz',\n",
       "  '1323.jsonl.gz',\n",
       "  '1324.jsonl.gz',\n",
       "  '1325.jsonl.gz',\n",
       "  '1326.jsonl.gz',\n",
       "  '1327.jsonl.gz',\n",
       "  '1328.jsonl.gz',\n",
       "  '1329.jsonl.gz',\n",
       "  '133.jsonl.gz',\n",
       "  '1330.jsonl.gz',\n",
       "  '1331.jsonl.gz',\n",
       "  '1332.jsonl.gz',\n",
       "  '1333.jsonl.gz',\n",
       "  '1334.jsonl.gz',\n",
       "  '1335.jsonl.gz',\n",
       "  '1336.jsonl.gz',\n",
       "  '1337.jsonl.gz',\n",
       "  '1338.jsonl.gz',\n",
       "  '1339.jsonl.gz',\n",
       "  '134.jsonl.gz',\n",
       "  '1340.jsonl.gz',\n",
       "  '1341.jsonl.gz',\n",
       "  '1342.jsonl.gz',\n",
       "  '1343.jsonl.gz',\n",
       "  '1344.jsonl.gz',\n",
       "  '1345.jsonl.gz',\n",
       "  '1346.jsonl.gz',\n",
       "  '1347.jsonl.gz',\n",
       "  '1348.jsonl.gz',\n",
       "  '1349.jsonl.gz',\n",
       "  '135.jsonl.gz',\n",
       "  '1350.jsonl.gz',\n",
       "  '1351.jsonl.gz',\n",
       "  '1352.jsonl.gz',\n",
       "  '1353.jsonl.gz',\n",
       "  '1354.jsonl.gz',\n",
       "  '1355.jsonl.gz',\n",
       "  '1356.jsonl.gz',\n",
       "  '1357.jsonl.gz',\n",
       "  '1358.jsonl.gz',\n",
       "  '1359.jsonl.gz',\n",
       "  '136.jsonl.gz',\n",
       "  '1360.jsonl.gz',\n",
       "  '1361.jsonl.gz',\n",
       "  '1362.jsonl.gz',\n",
       "  '1363.jsonl.gz',\n",
       "  '1364.jsonl.gz',\n",
       "  '1365.jsonl.gz',\n",
       "  '1366.jsonl.gz',\n",
       "  '1367.jsonl.gz',\n",
       "  '1368.jsonl.gz',\n",
       "  '1369.jsonl.gz',\n",
       "  '137.jsonl.gz',\n",
       "  '1370.jsonl.gz',\n",
       "  '1371.jsonl.gz',\n",
       "  '1372.jsonl.gz',\n",
       "  '1373.jsonl.gz',\n",
       "  '1374.jsonl.gz',\n",
       "  '1375.jsonl.gz',\n",
       "  '1376.jsonl.gz',\n",
       "  '1377.jsonl.gz',\n",
       "  '1378.jsonl.gz',\n",
       "  '1379.jsonl.gz',\n",
       "  '138.jsonl.gz',\n",
       "  '1380.jsonl.gz',\n",
       "  '1381.jsonl.gz',\n",
       "  '1382.jsonl.gz',\n",
       "  '1383.jsonl.gz',\n",
       "  '1384.jsonl.gz',\n",
       "  '1385.jsonl.gz',\n",
       "  '1386.jsonl.gz',\n",
       "  '1387.jsonl.gz',\n",
       "  '1388.jsonl.gz',\n",
       "  '1389.jsonl.gz',\n",
       "  '139.jsonl.gz',\n",
       "  '1390.jsonl.gz',\n",
       "  '1391.jsonl.gz',\n",
       "  '1392.jsonl.gz',\n",
       "  '1393.jsonl.gz',\n",
       "  '1394.jsonl.gz',\n",
       "  '1395.jsonl.gz',\n",
       "  '1396.jsonl.gz',\n",
       "  '1397.jsonl.gz',\n",
       "  '1398.jsonl.gz',\n",
       "  '1399.jsonl.gz',\n",
       "  '14.jsonl.gz',\n",
       "  '140.jsonl.gz',\n",
       "  '1400.jsonl.gz',\n",
       "  '1401.jsonl.gz',\n",
       "  '1402.jsonl.gz',\n",
       "  '1403.jsonl.gz',\n",
       "  '1404.jsonl.gz',\n",
       "  '1405.jsonl.gz',\n",
       "  '1406.jsonl.gz',\n",
       "  '1407.jsonl.gz',\n",
       "  '1408.jsonl.gz',\n",
       "  '1409.jsonl.gz',\n",
       "  '141.jsonl.gz',\n",
       "  '1410.jsonl.gz',\n",
       "  '1411.jsonl.gz',\n",
       "  '1412.jsonl.gz',\n",
       "  '1413.jsonl.gz',\n",
       "  '1414.jsonl.gz',\n",
       "  '1415.jsonl.gz',\n",
       "  '1416.jsonl.gz',\n",
       "  '1417.jsonl.gz',\n",
       "  '1418.jsonl.gz',\n",
       "  '1419.jsonl.gz',\n",
       "  '142.jsonl.gz',\n",
       "  '1420.jsonl.gz',\n",
       "  '1421.jsonl.gz',\n",
       "  '1422.jsonl.gz',\n",
       "  '1423.jsonl.gz',\n",
       "  '1424.jsonl.gz',\n",
       "  '1425.jsonl.gz',\n",
       "  '1426.jsonl.gz',\n",
       "  '1427.jsonl.gz',\n",
       "  '1428.jsonl.gz',\n",
       "  '1429.jsonl.gz',\n",
       "  '143.jsonl.gz',\n",
       "  '1430.jsonl.gz',\n",
       "  '1431.jsonl.gz',\n",
       "  '1432.jsonl.gz',\n",
       "  '1433.jsonl.gz',\n",
       "  '1434.jsonl.gz',\n",
       "  '1435.jsonl.gz',\n",
       "  '1436.jsonl.gz',\n",
       "  '1437.jsonl.gz',\n",
       "  '1438.jsonl.gz',\n",
       "  '1439.jsonl.gz',\n",
       "  '144.jsonl.gz',\n",
       "  '1440.jsonl.gz',\n",
       "  '1441.jsonl.gz',\n",
       "  '1442.jsonl.gz',\n",
       "  '1443.jsonl.gz',\n",
       "  '1444.jsonl.gz',\n",
       "  '1445.jsonl.gz',\n",
       "  '1446.jsonl.gz',\n",
       "  '1447.jsonl.gz',\n",
       "  '1448.jsonl.gz',\n",
       "  '1449.jsonl.gz',\n",
       "  '145.jsonl.gz',\n",
       "  '1450.jsonl.gz',\n",
       "  '1451.jsonl.gz',\n",
       "  '1452.jsonl.gz',\n",
       "  '1453.jsonl.gz',\n",
       "  '1454.jsonl.gz',\n",
       "  '1455.jsonl.gz',\n",
       "  '1456.jsonl.gz',\n",
       "  '1457.jsonl.gz',\n",
       "  '1458.jsonl.gz',\n",
       "  '1459.jsonl.gz',\n",
       "  '146.jsonl.gz',\n",
       "  '1460.jsonl.gz',\n",
       "  '1461.jsonl.gz',\n",
       "  '1462.jsonl.gz',\n",
       "  '1463.jsonl.gz',\n",
       "  '1464.jsonl.gz',\n",
       "  '1465.jsonl.gz',\n",
       "  '1466.jsonl.gz',\n",
       "  '1467.jsonl.gz',\n",
       "  '1468.jsonl.gz',\n",
       "  '1469.jsonl.gz',\n",
       "  '147.jsonl.gz',\n",
       "  '1470.jsonl.gz',\n",
       "  '1471.jsonl.gz',\n",
       "  '1472.jsonl.gz',\n",
       "  '1473.jsonl.gz',\n",
       "  '1474.jsonl.gz',\n",
       "  '1475.jsonl.gz',\n",
       "  '1476.jsonl.gz',\n",
       "  '1477.jsonl.gz',\n",
       "  '1478.jsonl.gz',\n",
       "  '1479.jsonl.gz',\n",
       "  '148.jsonl.gz',\n",
       "  '1480.jsonl.gz',\n",
       "  '1481.jsonl.gz',\n",
       "  '1482.jsonl.gz',\n",
       "  '1483.jsonl.gz',\n",
       "  '1484.jsonl.gz',\n",
       "  '1485.jsonl.gz',\n",
       "  '1486.jsonl.gz',\n",
       "  '1487.jsonl.gz',\n",
       "  '1488.jsonl.gz',\n",
       "  '1489.jsonl.gz',\n",
       "  '149.jsonl.gz',\n",
       "  '1490.jsonl.gz',\n",
       "  '1491.jsonl.gz',\n",
       "  '1492.jsonl.gz',\n",
       "  '1493.jsonl.gz',\n",
       "  '1494.jsonl.gz',\n",
       "  '1495.jsonl.gz',\n",
       "  '1496.jsonl.gz',\n",
       "  '1497.jsonl.gz',\n",
       "  '1498.jsonl.gz',\n",
       "  '1499.jsonl.gz',\n",
       "  '15.jsonl.gz',\n",
       "  '150.jsonl.gz',\n",
       "  '1500.jsonl.gz',\n",
       "  '1501.jsonl.gz',\n",
       "  '1502.jsonl.gz',\n",
       "  '1503.jsonl.gz',\n",
       "  '1504.jsonl.gz',\n",
       "  '1505.jsonl.gz',\n",
       "  '1506.jsonl.gz',\n",
       "  '1507.jsonl.gz',\n",
       "  '1508.jsonl.gz',\n",
       "  '1509.jsonl.gz',\n",
       "  '151.jsonl.gz',\n",
       "  '1510.jsonl.gz',\n",
       "  '1511.jsonl.gz',\n",
       "  '1512.jsonl.gz',\n",
       "  '1513.jsonl.gz',\n",
       "  '1514.jsonl.gz',\n",
       "  '1515.jsonl.gz',\n",
       "  '1516.jsonl.gz',\n",
       "  '1517.jsonl.gz',\n",
       "  '1518.jsonl.gz',\n",
       "  '1519.jsonl.gz',\n",
       "  '152.jsonl.gz',\n",
       "  '1520.jsonl.gz',\n",
       "  '1521.jsonl.gz',\n",
       "  '1522.jsonl.gz',\n",
       "  '1523.jsonl.gz',\n",
       "  '1524.jsonl.gz',\n",
       "  '1525.jsonl.gz',\n",
       "  '1526.jsonl.gz',\n",
       "  '1527.jsonl.gz',\n",
       "  '1528.jsonl.gz',\n",
       "  '1529.jsonl.gz',\n",
       "  '153.jsonl.gz',\n",
       "  '1530.jsonl.gz',\n",
       "  '1531.jsonl.gz',\n",
       "  '1532.jsonl.gz',\n",
       "  '1533.jsonl.gz',\n",
       "  '1534.jsonl.gz',\n",
       "  '1535.jsonl.gz',\n",
       "  '1536.jsonl.gz',\n",
       "  '1537.jsonl.gz',\n",
       "  '1538.jsonl.gz',\n",
       "  '1539.jsonl.gz',\n",
       "  '154.jsonl.gz',\n",
       "  '1540.jsonl.gz',\n",
       "  '1541.jsonl.gz',\n",
       "  '1542.jsonl.gz',\n",
       "  '1543.jsonl.gz',\n",
       "  '1544.jsonl.gz',\n",
       "  '1545.jsonl.gz',\n",
       "  '1546.jsonl.gz',\n",
       "  '1547.jsonl.gz',\n",
       "  '1548.jsonl.gz',\n",
       "  '1549.jsonl.gz',\n",
       "  '155.jsonl.gz',\n",
       "  '1550.jsonl.gz',\n",
       "  '1551.jsonl.gz',\n",
       "  '1552.jsonl.gz',\n",
       "  '1553.jsonl.gz',\n",
       "  '1554.jsonl.gz',\n",
       "  '1555.jsonl.gz',\n",
       "  '1556.jsonl.gz',\n",
       "  '1557.jsonl.gz',\n",
       "  '1558.jsonl.gz',\n",
       "  '1559.jsonl.gz',\n",
       "  '156.jsonl.gz',\n",
       "  '1560.jsonl.gz',\n",
       "  '1561.jsonl.gz',\n",
       "  '1562.jsonl.gz',\n",
       "  '1563.jsonl.gz',\n",
       "  '1564.jsonl.gz',\n",
       "  '1565.jsonl.gz',\n",
       "  '1566.jsonl.gz',\n",
       "  '1567.jsonl.gz',\n",
       "  '1568.jsonl.gz',\n",
       "  '1569.jsonl.gz',\n",
       "  '157.jsonl.gz',\n",
       "  '1570.jsonl.gz',\n",
       "  '1571.jsonl.gz',\n",
       "  '1572.jsonl.gz',\n",
       "  '1573.jsonl.gz',\n",
       "  '1574.jsonl.gz',\n",
       "  '1575.jsonl.gz',\n",
       "  '1576.jsonl.gz',\n",
       "  '1577.jsonl.gz',\n",
       "  '1578.jsonl.gz',\n",
       "  '1579.jsonl.gz',\n",
       "  '158.jsonl.gz',\n",
       "  '1580.jsonl.gz',\n",
       "  '1581.jsonl.gz',\n",
       "  '1582.jsonl.gz',\n",
       "  '1583.jsonl.gz',\n",
       "  '1584.jsonl.gz',\n",
       "  '1585.jsonl.gz',\n",
       "  '1586.jsonl.gz',\n",
       "  '1587.jsonl.gz',\n",
       "  '1588.jsonl.gz',\n",
       "  '1589.jsonl.gz',\n",
       "  '159.jsonl.gz',\n",
       "  '1590.jsonl.gz',\n",
       "  '1591.jsonl.gz',\n",
       "  '1592.jsonl.gz',\n",
       "  '1593.jsonl.gz',\n",
       "  '1594.jsonl.gz',\n",
       "  '1595.jsonl.gz',\n",
       "  '1596.jsonl.gz',\n",
       "  '1597.jsonl.gz',\n",
       "  '1598.jsonl.gz',\n",
       "  '1599.jsonl.gz',\n",
       "  '16.jsonl.gz',\n",
       "  '160.jsonl.gz',\n",
       "  '1600.jsonl.gz',\n",
       "  '1601.jsonl.gz',\n",
       "  '1602.jsonl.gz',\n",
       "  '1603.jsonl.gz',\n",
       "  '1604.jsonl.gz',\n",
       "  '1605.jsonl.gz',\n",
       "  '1606.jsonl.gz',\n",
       "  '1607.jsonl.gz',\n",
       "  '1608.jsonl.gz',\n",
       "  '1609.jsonl.gz',\n",
       "  '161.jsonl.gz',\n",
       "  '1610.jsonl.gz',\n",
       "  '1611.jsonl.gz',\n",
       "  '1612.jsonl.gz',\n",
       "  '1613.jsonl.gz',\n",
       "  '1614.jsonl.gz',\n",
       "  '1615.jsonl.gz',\n",
       "  '1616.jsonl.gz',\n",
       "  '1617.jsonl.gz',\n",
       "  '1618.jsonl.gz',\n",
       "  '1619.jsonl.gz',\n",
       "  '162.jsonl.gz',\n",
       "  '1620.jsonl.gz',\n",
       "  '1621.jsonl.gz',\n",
       "  '1622.jsonl.gz',\n",
       "  '1623.jsonl.gz',\n",
       "  '1624.jsonl.gz',\n",
       "  '1625.jsonl.gz',\n",
       "  '1626.jsonl.gz',\n",
       "  '1627.jsonl.gz',\n",
       "  '1628.jsonl.gz',\n",
       "  '1629.jsonl.gz',\n",
       "  '163.jsonl.gz',\n",
       "  '1630.jsonl.gz',\n",
       "  '1631.jsonl.gz',\n",
       "  '1632.jsonl.gz',\n",
       "  '1633.jsonl.gz',\n",
       "  '1634.jsonl.gz',\n",
       "  '1635.jsonl.gz',\n",
       "  '1636.jsonl.gz',\n",
       "  '1637.jsonl.gz',\n",
       "  '1638.jsonl.gz',\n",
       "  '1639.jsonl.gz',\n",
       "  '164.jsonl.gz',\n",
       "  '1640.jsonl.gz',\n",
       "  '1641.jsonl.gz',\n",
       "  '1642.jsonl.gz',\n",
       "  '1643.jsonl.gz',\n",
       "  '1644.jsonl.gz',\n",
       "  '1645.jsonl.gz',\n",
       "  '1646.jsonl.gz',\n",
       "  '1647.jsonl.gz',\n",
       "  '1648.jsonl.gz',\n",
       "  '1649.jsonl.gz',\n",
       "  '165.jsonl.gz',\n",
       "  '1650.jsonl.gz',\n",
       "  '1651.jsonl.gz',\n",
       "  '1652.jsonl.gz',\n",
       "  '1653.jsonl.gz',\n",
       "  '1654.jsonl.gz',\n",
       "  '1655.jsonl.gz',\n",
       "  '1656.jsonl.gz',\n",
       "  '1657.jsonl.gz',\n",
       "  '1658.jsonl.gz',\n",
       "  '1659.jsonl.gz',\n",
       "  '166.jsonl.gz',\n",
       "  '1660.jsonl.gz',\n",
       "  '1661.jsonl.gz',\n",
       "  '1662.jsonl.gz',\n",
       "  '1663.jsonl.gz',\n",
       "  '1664.jsonl.gz',\n",
       "  '1665.jsonl.gz',\n",
       "  '1666.jsonl.gz',\n",
       "  '1667.jsonl.gz',\n",
       "  '1668.jsonl.gz',\n",
       "  '1669.jsonl.gz',\n",
       "  '167.jsonl.gz',\n",
       "  '1670.jsonl.gz',\n",
       "  '1671.jsonl.gz',\n",
       "  '1672.jsonl.gz',\n",
       "  '1673.jsonl.gz',\n",
       "  '1674.jsonl.gz',\n",
       "  '1675.jsonl.gz',\n",
       "  '1676.jsonl.gz',\n",
       "  '1677.jsonl.gz',\n",
       "  '1678.jsonl.gz',\n",
       "  '1679.jsonl.gz',\n",
       "  '168.jsonl.gz',\n",
       "  '1680.jsonl.gz',\n",
       "  '1681.jsonl.gz',\n",
       "  '1682.jsonl.gz',\n",
       "  '1683.jsonl.gz',\n",
       "  '1684.jsonl.gz',\n",
       "  '1685.jsonl.gz',\n",
       "  '1686.jsonl.gz',\n",
       "  '1687.jsonl.gz',\n",
       "  '1688.jsonl.gz',\n",
       "  '1689.jsonl.gz',\n",
       "  '169.jsonl.gz',\n",
       "  '1690.jsonl.gz',\n",
       "  '1691.jsonl.gz',\n",
       "  '1692.jsonl.gz',\n",
       "  '1693.jsonl.gz',\n",
       "  '1694.jsonl.gz',\n",
       "  '1695.jsonl.gz',\n",
       "  '1696.jsonl.gz',\n",
       "  '1697.jsonl.gz',\n",
       "  '1698.jsonl.gz',\n",
       "  '1699.jsonl.gz',\n",
       "  '17.jsonl.gz',\n",
       "  '170.jsonl.gz',\n",
       "  '1700.jsonl.gz',\n",
       "  '1701.jsonl.gz',\n",
       "  '1702.jsonl.gz',\n",
       "  '1703.jsonl.gz',\n",
       "  '1704.jsonl.gz',\n",
       "  '1705.jsonl.gz',\n",
       "  '1706.jsonl.gz',\n",
       "  '1707.jsonl.gz',\n",
       "  '1708.jsonl.gz',\n",
       "  '1709.jsonl.gz',\n",
       "  '171.jsonl.gz',\n",
       "  '1710.jsonl.gz',\n",
       "  '1711.jsonl.gz',\n",
       "  '1712.jsonl.gz',\n",
       "  '1713.jsonl.gz',\n",
       "  '1714.jsonl.gz',\n",
       "  '1715.jsonl.gz',\n",
       "  '1716.jsonl.gz',\n",
       "  '1717.jsonl.gz',\n",
       "  '1718.jsonl.gz',\n",
       "  '1719.jsonl.gz',\n",
       "  '172.jsonl.gz',\n",
       "  '1720.jsonl.gz',\n",
       "  '1721.jsonl.gz',\n",
       "  '1722.jsonl.gz',\n",
       "  '1723.jsonl.gz',\n",
       "  '1724.jsonl.gz',\n",
       "  '1725.jsonl.gz',\n",
       "  '1726.jsonl.gz',\n",
       "  '1727.jsonl.gz',\n",
       "  '1728.jsonl.gz',\n",
       "  '1729.jsonl.gz',\n",
       "  '173.jsonl.gz',\n",
       "  '1730.jsonl.gz',\n",
       "  '1731.jsonl.gz',\n",
       "  '1732.jsonl.gz',\n",
       "  '1733.jsonl.gz',\n",
       "  '1734.jsonl.gz',\n",
       "  '1735.jsonl.gz',\n",
       "  '1736.jsonl.gz',\n",
       "  '1737.jsonl.gz',\n",
       "  '1738.jsonl.gz',\n",
       "  '1739.jsonl.gz',\n",
       "  '174.jsonl.gz',\n",
       "  '1740.jsonl.gz',\n",
       "  '1741.jsonl.gz',\n",
       "  '1742.jsonl.gz',\n",
       "  '1743.jsonl.gz',\n",
       "  '1744.jsonl.gz',\n",
       "  '1745.jsonl.gz',\n",
       "  '1746.jsonl.gz',\n",
       "  '1747.jsonl.gz',\n",
       "  '1748.jsonl.gz',\n",
       "  '1749.jsonl.gz',\n",
       "  '175.jsonl.gz',\n",
       "  '1750.jsonl.gz',\n",
       "  '1751.jsonl.gz',\n",
       "  '1752.jsonl.gz',\n",
       "  '1753.jsonl.gz',\n",
       "  '1754.jsonl.gz',\n",
       "  '1755.jsonl.gz',\n",
       "  '1756.jsonl.gz',\n",
       "  '1757.jsonl.gz',\n",
       "  '1758.jsonl.gz',\n",
       "  '1759.jsonl.gz',\n",
       "  '176.jsonl.gz',\n",
       "  '1760.jsonl.gz',\n",
       "  '1761.jsonl.gz',\n",
       "  '1762.jsonl.gz',\n",
       "  '1763.jsonl.gz',\n",
       "  '1764.jsonl.gz',\n",
       "  '1765.jsonl.gz',\n",
       "  '1766.jsonl.gz',\n",
       "  '1767.jsonl.gz',\n",
       "  '1768.jsonl.gz',\n",
       "  '1769.jsonl.gz',\n",
       "  '177.jsonl.gz',\n",
       "  '1770.jsonl.gz',\n",
       "  '1771.jsonl.gz',\n",
       "  '1772.jsonl.gz',\n",
       "  '1773.jsonl.gz',\n",
       "  '1774.jsonl.gz',\n",
       "  '1775.jsonl.gz',\n",
       "  '1776.jsonl.gz',\n",
       "  '1777.jsonl.gz',\n",
       "  '1778.jsonl.gz',\n",
       "  '1779.jsonl.gz',\n",
       "  '178.jsonl.gz',\n",
       "  '1780.jsonl.gz',\n",
       "  '1781.jsonl.gz',\n",
       "  '1782.jsonl.gz',\n",
       "  '1783.jsonl.gz',\n",
       "  '1784.jsonl.gz',\n",
       "  '1785.jsonl.gz',\n",
       "  '1786.jsonl.gz',\n",
       "  '1787.jsonl.gz',\n",
       "  '1788.jsonl.gz',\n",
       "  '1789.jsonl.gz',\n",
       "  '179.jsonl.gz',\n",
       "  '1790.jsonl.gz',\n",
       "  '1791.jsonl.gz',\n",
       "  '1792.jsonl.gz',\n",
       "  '1793.jsonl.gz',\n",
       "  '1794.jsonl.gz',\n",
       "  '1795.jsonl.gz',\n",
       "  '1796.jsonl.gz',\n",
       "  '1797.jsonl.gz',\n",
       "  '1798.jsonl.gz',\n",
       "  '1799.jsonl.gz',\n",
       "  '18.jsonl.gz',\n",
       "  '180.jsonl.gz',\n",
       "  '1800.jsonl.gz',\n",
       "  '1801.jsonl.gz',\n",
       "  '1802.jsonl.gz',\n",
       "  '1803.jsonl.gz',\n",
       "  '1804.jsonl.gz',\n",
       "  '1805.jsonl.gz',\n",
       "  '1806.jsonl.gz',\n",
       "  '1807.jsonl.gz',\n",
       "  '1808.jsonl.gz',\n",
       "  '1809.jsonl.gz',\n",
       "  '181.jsonl.gz',\n",
       "  '1810.jsonl.gz',\n",
       "  '1811.jsonl.gz',\n",
       "  '1812.jsonl.gz',\n",
       "  '1813.jsonl.gz',\n",
       "  '1814.jsonl.gz',\n",
       "  '1815.jsonl.gz',\n",
       "  '1816.jsonl.gz',\n",
       "  '1817.jsonl.gz',\n",
       "  '1818.jsonl.gz',\n",
       "  '1819.jsonl.gz',\n",
       "  '182.jsonl.gz',\n",
       "  '1820.jsonl.gz',\n",
       "  '1821.jsonl.gz',\n",
       "  '1822.jsonl.gz',\n",
       "  '1823.jsonl.gz',\n",
       "  '1824.jsonl.gz',\n",
       "  '1825.jsonl.gz',\n",
       "  '1826.jsonl.gz',\n",
       "  '1827.jsonl.gz',\n",
       "  '1828.jsonl.gz',\n",
       "  '1829.jsonl.gz',\n",
       "  '183.jsonl.gz',\n",
       "  '1830.jsonl.gz',\n",
       "  '1831.jsonl.gz',\n",
       "  '1832.jsonl.gz',\n",
       "  '1833.jsonl.gz',\n",
       "  '1834.jsonl.gz',\n",
       "  '1835.jsonl.gz',\n",
       "  '1836.jsonl.gz',\n",
       "  '1837.jsonl.gz',\n",
       "  '1838.jsonl.gz',\n",
       "  '1839.jsonl.gz',\n",
       "  '184.jsonl.gz',\n",
       "  '1840.jsonl.gz',\n",
       "  '1841.jsonl.gz',\n",
       "  '1842.jsonl.gz',\n",
       "  '1843.jsonl.gz',\n",
       "  '1844.jsonl.gz',\n",
       "  '1845.jsonl.gz',\n",
       "  '1846.jsonl.gz',\n",
       "  '1847.jsonl.gz',\n",
       "  '1848.jsonl.gz',\n",
       "  '1849.jsonl.gz',\n",
       "  '185.jsonl.gz',\n",
       "  '1850.jsonl.gz',\n",
       "  '1851.jsonl.gz',\n",
       "  '1852.jsonl.gz',\n",
       "  '1853.jsonl.gz',\n",
       "  '1854.jsonl.gz',\n",
       "  '1855.jsonl.gz',\n",
       "  '1856.jsonl.gz',\n",
       "  '1857.jsonl.gz',\n",
       "  '1858.jsonl.gz',\n",
       "  '1859.jsonl.gz',\n",
       "  '186.jsonl.gz',\n",
       "  '1860.jsonl.gz',\n",
       "  '1861.jsonl.gz',\n",
       "  '1862.jsonl.gz',\n",
       "  '1863.jsonl.gz',\n",
       "  '1864.jsonl.gz',\n",
       "  '1865.jsonl.gz',\n",
       "  '1866.jsonl.gz',\n",
       "  '1867.jsonl.gz',\n",
       "  '1868.jsonl.gz',\n",
       "  '1869.jsonl.gz',\n",
       "  '187.jsonl.gz',\n",
       "  '1870.jsonl.gz',\n",
       "  '1871.jsonl.gz',\n",
       "  '1872.jsonl.gz',\n",
       "  '1873.jsonl.gz',\n",
       "  '1874.jsonl.gz',\n",
       "  '1875.jsonl.gz',\n",
       "  '1876.jsonl.gz',\n",
       "  '1877.jsonl.gz',\n",
       "  '1878.jsonl.gz',\n",
       "  '1879.jsonl.gz',\n",
       "  '188.jsonl.gz',\n",
       "  '1880.jsonl.gz',\n",
       "  '1881.jsonl.gz',\n",
       "  '1882.jsonl.gz',\n",
       "  '1883.jsonl.gz',\n",
       "  '1884.jsonl.gz',\n",
       "  '1885.jsonl.gz',\n",
       "  '1886.jsonl.gz',\n",
       "  '1887.jsonl.gz',\n",
       "  '1888.jsonl.gz',\n",
       "  '1889.jsonl.gz',\n",
       "  '189.jsonl.gz',\n",
       "  '1890.jsonl.gz',\n",
       "  '1891.jsonl.gz',\n",
       "  '1892.jsonl.gz',\n",
       "  '1893.jsonl.gz',\n",
       "  '1894.jsonl.gz',\n",
       "  '1895.jsonl.gz',\n",
       "  '1896.jsonl.gz',\n",
       "  '1897.jsonl.gz',\n",
       "  '1898.jsonl.gz',\n",
       "  ...])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_zips),f_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s2orc-master.zip']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in f_zips if '.gz' not in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загрузим все  acl статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41660\n"
     ]
    }
   ],
   "source": [
    "with open(\"acl_only_json_list_10000.json\", \"r\") as read_file:\n",
    "    acl_only_articles = json.load(read_file)\n",
    "print(len(acl_only_articles))\n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41660"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выберем статьи из acl\n",
    "чтобы по ним выкачать статьи из reference c аннотацией, которые больше всего подходят для формирования обучающей выборки сценариев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41439"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in acl_only_articles if article['grobid_parse']['body_text'] or (article['latex_parse'] and article['latex_parse']['body_text'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Удалим статьи,у которых нет body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_body_text = [article['paper_id'] for article in acl_only_articles if not article['grobid_parse']['body_text'] or not article['grobid_parse']['bib_entries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_ids_not_body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_items_from_papers(acl_ids_not_body_text,acl_only_articles):\n",
    "    del_items = []\n",
    "    del_num_items = []\n",
    "    for num_artic, article in enumerate(acl_only_articles):\n",
    "        if article['paper_id'] in acl_ids_not_body_text:\n",
    "            del_items.append(article['paper_id'])\n",
    "            del_num_items.append(num_artic)\n",
    "            \n",
    "    del_num_items = np.array(del_num_items)\n",
    "    acl_only_articles = np.array(acl_only_articles)\n",
    "    acl_only_articles = np.delete(acl_only_articles,del_num_items)\n",
    "    return acl_only_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41660"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_only_articles = delete_items_from_papers(acl_ids_not_body_text,acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39763"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### удалим дублированные статьи по acl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39763"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_paper_ids = [article['metadata']['acl_id'] for article in acl_only_articles if article['metadata']['acl_id']]\n",
    "len(acl_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubled_acl_id_papers = []\n",
    "for ind,cnt_of_acl_id in zip(pd.Series(acl_paper_ids).value_counts().index,pd.Series(acl_paper_ids).value_counts()):\n",
    "    if cnt_of_acl_id >=2:\n",
    "        doubled_acl_id_papers.append(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doubled_acl_id_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_items = []\n",
    "del_num_items = []\n",
    "for num_artic, article in enumerate(acl_only_articles):\n",
    "    if article['metadata']['acl_id'] in doubled_acl_id_papers:\n",
    "        del_items.append(article['paper_id'])\n",
    "        del_num_items.append(num_artic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(del_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_only_articles = np.delete(acl_only_articles,del_num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_items = []\n",
    "del_num_items = []\n",
    "for num_artic, article in enumerate(acl_only_articles):\n",
    "    if article['metadata']['acl_id'] in doubled_acl_id_papers:\n",
    "        del_items.append(article['paper_id'])\n",
    "        del_num_items.append(num_artic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(del_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39191"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_paper_ids = [article['metadata']['acl_id'] for article in acl_only_articles if article['metadata']['acl_id']]\n",
    "len(acl_paper_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделим обзорные части статей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "for num_artic,article in enumerate(acl_only_articles):\n",
    "    # проверяем что у статьи есть grobid_parse и latex_parse и естб текст\n",
    "    if (article['grobid_parse'] and article['grobid_parse']['body_text']) or (article['latex_parse'] and article['latex_parse']['body_text']):\n",
    "        # задаем шаблон отражения статьи в укороченном формате (чтобы занимать меньше памяти)\n",
    "        overview_papers[article['paper_id']] = { 'paper_id':article['paper_id'],   'metadata':article['metadata'],\n",
    "                                                 's2_pdf_hash':article['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}\n",
    "        \n",
    "        grobid_parse_overview = None\n",
    "        # если у статьи есть article['grobid_parse']['body_text']\n",
    "        if article['grobid_parse'] and article['grobid_parse']['body_text']:\n",
    "            grobid_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "                grobid_parse_overview[num_sec] = sections\n",
    "            \n",
    "            # отсортируем по количеству цитат абзацы\n",
    "            \n",
    "            grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), key=lambda item: len(item[1]['cite_spans']), reverse=True)}\n",
    "            \n",
    "#             # найдем 1 , пока без 2 максимума по обзорной части\n",
    "            max_cite_span_sum = 0\n",
    "            max_grobid_parse_overview = dict()\n",
    "            for k,v in grobid_parse_overview.items():\n",
    "                if max_cite_span_sum < len(v['cite_spans']):\n",
    "                    max_cite_span_sum = len(v['cite_spans'])\n",
    "                    max_grobid_parse_overview[k] = v\n",
    "                    \n",
    "#                 # записываем 2 максимум, если количество ссылок в егочасти больше половины от максимального \n",
    "#                 elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "#                     max_grobid_parse_overview[k] = v\n",
    "                    \n",
    "            \n",
    "            grobid_parse_overview = max_grobid_parse_overview\n",
    "            \n",
    "        latex_parse_overview = None\n",
    "        # если у статьи есть article['latex_parse']['body_text']\n",
    "        if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "            latex_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            # в latex_parse \n",
    "            for sections in article['latex_parse']['body_text']:\n",
    "                if sections['section'] in latex_parse_overview:\n",
    "                    if latex_parse_overview[sections['section']] == sections:\n",
    "                        continue\n",
    "                    else:\n",
    "                        latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "                        latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "                        latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "                        latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "                else:\n",
    "                    latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                                  'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                                  'section':[sections['section']]}\n",
    "            latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), key=lambda item: item[1]['cite_span_lens'], reverse=True)}\n",
    "        \n",
    "        \n",
    "            max_cite_span_sum = 0\n",
    "            max_latex_parse_overview = dict()\n",
    "            for k,v in latex_parse_overview.items():\n",
    "                if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "                    max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "                    max_latex_parse_overview[k] = v\n",
    "#                 elif (max_cite_span_sum>0) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "#                     max_latex_parse_overview[k] = v\n",
    "            \n",
    "            latex_parse_overview = max_latex_parse_overview\n",
    "        \n",
    "\n",
    "        if grobid_parse_overview:\n",
    "            overview_papers[article['paper_id']]['grobid_parse'] = {'abstract':None,\n",
    "                                                        'overview_text':grobid_parse_overview,  \n",
    "                                                        'bib_entries':None}\n",
    "            if article['grobid_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['abstract'] = article['grobid_parse']['abstract']\n",
    "            if article['grobid_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['bib_entries'] = article['grobid_parse']['bib_entries']\n",
    "            \n",
    "        if latex_parse_overview:            \n",
    "            overview_papers[article['paper_id']]['latex_parse'] = {'abstract':None,\n",
    "                                                                    'overview_text':latex_parse_overview,  \n",
    "                                                                    'bib_entries':None}\n",
    "            if article['latex_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['abstract'] = article['latex_parse']['abstract']\n",
    "            if article['latex_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['bib_entries'] = article['latex_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39191, 39191)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overview_papers),len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018',\n",
       " '14472576',\n",
       " '17302615',\n",
       " '3243536',\n",
       " '3248240',\n",
       " '2223737',\n",
       " '488',\n",
       " '14323173',\n",
       " '15251605',\n",
       " '8260435']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(overview_papers.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '10164018',\n",
       " 'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "  'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "  'year': '2017',\n",
       "  'arxiv_id': '1708.01065',\n",
       "  'acl_id': 'W17-4512',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.18653/v1/w17-4512',\n",
       "  'venue': 'ArXiv',\n",
       "  'journal': 'ArXiv'},\n",
       " 's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       " 'grobid_parse': {'abstract': [{'text': 'AbstractWe investigate the problem of readeraware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online 1 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Abstract'}],\n",
       "  'overview_text': {0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [{'start': 192,\n",
       "      'end': 216,\n",
       "      'text': '(Goldstein et al., 2000;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 217,\n",
       "      'end': 239,\n",
       "      'text': 'Erkan and Radev, 2004;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 240,\n",
       "      'end': 257,\n",
       "      'text': 'Wan et al., 2007;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 258,\n",
       "      'end': 284,\n",
       "      'text': 'Nenkova and McKeown, 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'},\n",
       "     {'start': 285,\n",
       "      'end': 302,\n",
       "      'text': 'Min et al., 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'},\n",
       "     {'start': 303,\n",
       "      'end': 319,\n",
       "      'text': 'Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 773,\n",
       "      'end': 797,\n",
       "      'text': '(Project Code: 14203414)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2288,\n",
       "      'end': 2305,\n",
       "      'text': '(Hu et al., 2008;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 2306,\n",
       "      'end': 2324,\n",
       "      'text': 'Yang et al., 2011)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF22'},\n",
       "     {'start': 2582,\n",
       "      'end': 2598,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 2911,\n",
       "      'end': 2927,\n",
       "      'text': 'Li et al. (2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 3069,\n",
       "      'end': 3095,\n",
       "      'text': '(Kingma and Welling, 2014;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 3096,\n",
       "      'end': 3117,\n",
       "      'text': 'Rezende et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [{'start': 956,\n",
       "      'end': 964,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF0'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "    'authors': [{'first': 'Dzmitry',\n",
       "      'middle': [],\n",
       "      'last': 'Bahdanau',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "     {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '11212020'},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Theano: new features and speed improvements',\n",
       "    'authors': [{'first': 'Frédéric',\n",
       "      'middle': [],\n",
       "      'last': 'Bastien',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "     {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "     {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "     {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "     {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "     {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "     {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "     {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "    'links': '8180128'},\n",
       "   'BIBREF2': {'ref_id': 'b2',\n",
       "    'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF3': {'ref_id': 'b3',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "    'authors': [{'first': 'Shuhei',\n",
       "      'middle': [],\n",
       "      'last': 'Yoshida',\n",
       "      'suffix': ''}],\n",
       "    'year': None,\n",
       "    'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACLANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Autoencoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF11': {'ref_id': 'b11',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "    'authors': [{'first': 'Chin-Yew',\n",
       "      'middle': [],\n",
       "      'last': 'Lin',\n",
       "      'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '964287'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF14': {'ref_id': 'b14',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "    'authors': [{'first': 'Xiaojun',\n",
       "      'middle': [],\n",
       "      'last': 'Wan',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "    'year': 2007,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '7',\n",
       "    'issn': '',\n",
       "    'pages': '2903--2908',\n",
       "    'other_ids': {},\n",
       "    'links': '532313'},\n",
       "   'BIBREF20': {'ref_id': 'b20',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}},\n",
       " 'latex_parse': {'abstract': None,\n",
       "  'overview_text': {'Introduction': {'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "     \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "     'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "     'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "     'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "     'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       "    'cite_spans': [[{'start': 193,\n",
       "       'end': 200,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF0'},\n",
       "      {'start': 203,\n",
       "       'end': 210,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'},\n",
       "      {'start': 213,\n",
       "       'end': 220,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 223,\n",
       "       'end': 230,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF3'},\n",
       "      {'start': 233,\n",
       "       'end': 240,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 243,\n",
       "       'end': 250,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'},\n",
       "      {'start': 253,\n",
       "       'end': 260,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     [],\n",
       "     [{'start': 527,\n",
       "       'end': 534,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF7'},\n",
       "      {'start': 537,\n",
       "       'end': 544,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF8'},\n",
       "      {'start': 802,\n",
       "       'end': 809,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'}],\n",
       "     [{'start': 10,\n",
       "       'end': 17,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 159,\n",
       "       'end': 167,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF10'},\n",
       "      {'start': 170,\n",
       "       'end': 178,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     [],\n",
       "     []],\n",
       "    'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       "    'section': ['Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction']}},\n",
       "  'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "    'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACL-ANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "    'title': 'Auto-encoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '15789289'},\n",
       "   'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "    'title': 'Effective approaches to attention-based neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "    'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['10164018']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделим количество ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08c5dfc19454a61b22b557e4fb4fde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "article_bibs = {}\n",
    "for num_artic,article in tqdm_notebook(enumerate(overview_papers)):\n",
    "# for num_artic,article in enumerate(overview_papers):\n",
    "#     if num_artic >10:\n",
    "#         break\n",
    "#     print(num_artic,article)\n",
    "    article = overview_papers[article]\n",
    "    # проверка наличия определленных значений статьи\n",
    "    # если нет в grobid_parse bib_entries, то нет смысла рассматривать статью, тк отсутствуют cite_spans\n",
    "    if not ((article['grobid_parse'] and article['grobid_parse']['bib_entries']) or (article['latex_parse'] and article['latex_parse']['bib_entries'])):\n",
    "#         article_bibs[article['paper_id']] = 'None'\n",
    "#         print(num_artic,article['paper_id'])\n",
    "        continue\n",
    "        \n",
    "    bib_entries_grobid = None\n",
    "    if article['grobid_parse'] and article['grobid_parse']['bib_entries'] and article['grobid_parse']['overview_text']:\n",
    "        bib_entries_grobid = dict()\n",
    "        \n",
    "        key_grobid = list(article['grobid_parse']['overview_text'].keys())[0]\n",
    "        for cite_span in article['grobid_parse']['overview_text'][key_grobid]['cite_spans']:\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_paper_id = None\n",
    "            if cite_ref in article['grobid_parse']['bib_entries']:\n",
    "                cited_paper_id = article['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "            if cited_paper_id:\n",
    "                bib_entries_grobid[article['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "            elif cite_ref  in article['grobid_parse']['bib_entries']:\n",
    "                bib_entries_grobid[article['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "        \n",
    "        \n",
    "        \n",
    "#     bib_entries_latex = None\n",
    "#     if article['latex_parse'] and article['latex_parse']['overview_text'] and article['latex_parse']['bib_entries']:\n",
    "#         bib_entries_latex = dict()\n",
    "        \n",
    "#         key_tex = list(article['latex_parse']['overview_text'].keys())[0]\n",
    "#         for cite_spans in article['latex_parse']['overview_text'][key_tex]['cite_spans']:\n",
    "#             if len(cite_spans)>1:\n",
    "#                 for cite_span in cite_spans:\n",
    "#                     cite_ref = cite_span['ref_id']\n",
    "#                     cited_article_id = None\n",
    "#                     if cite_ref in article['latex_parse']['bib_entries']:\n",
    "#                         cited_article_id = article['latex_parse']['bib_entries'][cite_ref]['links']\n",
    "#                     if cited_article_id:\n",
    "#                         bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "#                     elif cite_ref in article['latex_parse']['bib_entries']:\n",
    "#                         bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "    \n",
    "    \n",
    "    all_bib_entries = dict()\n",
    "#     if bib_entries_latex:\n",
    "#         for k,v in bib_entries_latex.items():\n",
    "#             if k == '':\n",
    "#                 continue\n",
    "                \n",
    "#             if v:\n",
    "#                 all_bib_entries[k] = [v]\n",
    "#             else:\n",
    "#                 all_bib_entries[k] = []\n",
    "\n",
    "    if bib_entries_grobid:\n",
    "        for k,v in bib_entries_grobid.items():\n",
    "            if k == '':\n",
    "                continue\n",
    "                \n",
    "            if k in all_bib_entries:\n",
    "                if len(all_bib_entries[k]) == 0:\n",
    "                    if v:\n",
    "                        all_bib_entries[k] = [v]\n",
    "                    else:\n",
    "                        all_bib_entries[k] = []\n",
    "                else:\n",
    "                    if all_bib_entries[k][0] == v:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if v:\n",
    "                            all_bib_entries[k].append(v)\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                if v:\n",
    "                    all_bib_entries[k] = [v]\n",
    "                else:\n",
    "                    all_bib_entries[k] = []\n",
    "    article_bibs[article['paper_id']] = all_bib_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38824"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_bibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bibs_keys = {val:1 for val in article_bibs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_bibs = [article['paper_id'] for article in acl_only_articles if article['paper_id'] not in article_bibs_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(367, ['65364185', '18414606', '8825607', '1294235', '18312930'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_ids_not_bibs),acl_ids_not_bibs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_only_articles = delete_items_from_papers(acl_ids_not_bibs,acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38824"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018', '14472576', '17302615', '3243536', '3248240', '2223737']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(article_bibs_keys.keys())[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': ['8294822'],\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': ['10418456'],\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': ['532313'],\n",
       " 'A survey of text summarization techniques': ['556431'],\n",
       " 'Exploiting category-specific information for multidocument summarization': ['6317274'],\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': ['29562039'],\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": ['13723748'],\n",
       " 'Social context summarization': ['704517'],\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': ['8377315'],\n",
       " 'Autoencoding variational bayes': [],\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': ['16895865']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_bibs['10164018']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 ячейки ниже просто чтобы работало "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_in_paper.txt', encoding=\"utf8\") as fin:\n",
    "    content = fin.readlines()\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict_in_ids = [x.strip() for x in content]\n",
    "del content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986243"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_in_ids_all = {val:1 for val in all_dict_in_ids}\n",
    "len(dict_in_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "num_all_bibs = []\n",
    "num_all_bibs_with_links = []\n",
    "num_all_bibs_w_l_in_dict_in = []\n",
    "for num_artic,(key,val_dict) in enumerate(article_bibs.items()):\n",
    "    index.append(key)\n",
    "    num_bibs = 0\n",
    "    num_bibs_with_links = 0\n",
    "    num_bibs_w_l_in_dict_in = 0\n",
    "    try:\n",
    "        for k,val in val_dict.items():\n",
    "            num_bibs +=1\n",
    "            if len(val)>0:\n",
    "                num_bibs_with_links+=1\n",
    "            for link in val:\n",
    "                if link in dict_in_ids_all:\n",
    "                    num_bibs_w_l_in_dict_in+=1\n",
    "                    break\n",
    "    except:\n",
    "        print(num_artic)\n",
    "        print(key)\n",
    "        break\n",
    "    num_all_bibs.append(num_bibs)\n",
    "    num_all_bibs_with_links.append(num_bibs_with_links)\n",
    "    num_all_bibs_w_l_in_dict_in.append(num_bibs_w_l_in_dict_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_overview = {'#bibs':num_all_bibs,'#bibs_with_links':num_all_bibs_with_links,'#bibs_with_links_in_dict':num_all_bibs_w_l_in_dict_in}\n",
    "df_covering_overview =  pd.DataFrame(d_overview)\n",
    "df_covering_overview.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covering_overview['%_in_dict2bibs'] = df_covering_overview['#bibs_with_links_in_dict']/df_covering_overview['#bibs']\n",
    "df_covering_overview['%_in_dict2bibs_w_l'] = df_covering_overview['#bibs_with_links_in_dict']/df_covering_overview['#bibs_with_links']\n",
    "df_covering_overview['%bibs_with_links2bibs'] = df_covering_overview['#bibs_with_links']/df_covering_overview['#bibs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#bibs</th>\n",
       "      <th>#bibs_with_links</th>\n",
       "      <th>#bibs_with_links_in_dict</th>\n",
       "      <th>%_in_dict2bibs</th>\n",
       "      <th>%_in_dict2bibs_w_l</th>\n",
       "      <th>%bibs_with_links2bibs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17302615</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243536</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248240</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          #bibs  #bibs_with_links  #bibs_with_links_in_dict  %_in_dict2bibs  \\\n",
       "10164018     11                10                         5        0.454545   \n",
       "14472576     18                18                        16        0.888889   \n",
       "17302615      3                 1                         0        0.000000   \n",
       "3243536       2                 1                         1        0.500000   \n",
       "3248240       4                 3                         3        0.750000   \n",
       "\n",
       "          %_in_dict2bibs_w_l  %bibs_with_links2bibs  \n",
       "10164018            0.500000               0.909091  \n",
       "14472576            0.888889               1.000000  \n",
       "17302615            0.000000               0.333333  \n",
       "3243536             1.000000               0.500000  \n",
       "3248240             1.000000               0.750000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7064105122470907"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview['%bibs_with_links2bibs'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10020, 6)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview[df_covering_overview['%bibs_with_links2bibs']>=0.9].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7259, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview[(df_covering_overview['%bibs_with_links2bibs']>=0.9) & (df_covering_overview['#bibs']>=5)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17750, 6)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview[df_covering_overview['%bibs_with_links2bibs']>=0.8].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14989, 6)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview[(df_covering_overview['%bibs_with_links2bibs']>=0.8) & (df_covering_overview['#bibs']>=5)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_for_out = df_covering_overview[(df_covering_overview['%bibs_with_links2bibs']>=0.9) & (df_covering_overview['#bibs']>=5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_for_out_index = list(acl_ids_for_out.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_for_out_index_keys = {val:1 for val in acl_ids_for_out_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_for_out = [article['paper_id'] for article in acl_only_articles if article['paper_id'] not in acl_ids_for_out_index_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_only_articles = delete_items_from_papers(acl_ids_not_for_out,acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[article['paper_id'] for article in acl_only_articles if article['paper_id'] not in acl_ids_for_out_index_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '10164018',\n",
       " 'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "  'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "  'year': '2017',\n",
       "  'arxiv_id': '1708.01065',\n",
       "  'acl_id': 'W17-4512',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.18653/v1/w17-4512',\n",
       "  'venue': 'ArXiv',\n",
       "  'journal': 'ArXiv'},\n",
       " 's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       " 'grobid_parse': {'abstract': [{'text': 'AbstractWe investigate the problem of readeraware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online 1 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Abstract'}],\n",
       "  'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [{'start': 192,\n",
       "      'end': 216,\n",
       "      'text': '(Goldstein et al., 2000;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 217,\n",
       "      'end': 239,\n",
       "      'text': 'Erkan and Radev, 2004;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 240,\n",
       "      'end': 257,\n",
       "      'text': 'Wan et al., 2007;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 258,\n",
       "      'end': 284,\n",
       "      'text': 'Nenkova and McKeown, 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'},\n",
       "     {'start': 285,\n",
       "      'end': 302,\n",
       "      'text': 'Min et al., 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'},\n",
       "     {'start': 303,\n",
       "      'end': 319,\n",
       "      'text': 'Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 773,\n",
       "      'end': 797,\n",
       "      'text': '(Project Code: 14203414)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2288,\n",
       "      'end': 2305,\n",
       "      'text': '(Hu et al., 2008;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 2306,\n",
       "      'end': 2324,\n",
       "      'text': 'Yang et al., 2011)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF22'},\n",
       "     {'start': 2582,\n",
       "      'end': 2598,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 2911,\n",
       "      'end': 2927,\n",
       "      'text': 'Li et al. (2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 3069,\n",
       "      'end': 3095,\n",
       "      'text': '(Kingma and Welling, 2014;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 3096,\n",
       "      'end': 3117,\n",
       "      'text': 'Rezende et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [{'start': 956,\n",
       "      'end': 964,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF0'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "    'cite_spans': [{'start': 451,\n",
       "      'end': 468,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [{'start': 12,\n",
       "      'end': 20,\n",
       "      'text': 'Figure 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "    'cite_spans': [{'start': 32,\n",
       "      'end': 58,\n",
       "      'text': '(Kingma and Welling, 2014;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 59,\n",
       "      'end': 79,\n",
       "      'text': 'Rezende et al., 2014',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 184,\n",
       "      'end': 200,\n",
       "      'text': 'Li et al. (2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2384,\n",
       "      'end': 2401,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2433,\n",
       "      'end': 2456,\n",
       "      'text': '(Bahdanau et al., 2015;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 2457,\n",
       "      'end': 2476,\n",
       "      'text': 'Luong et al., 2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "    'cite_spans': [{'start': 86,\n",
       "      'end': 102,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 971,\n",
       "      'end': 987,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1133,\n",
       "      'end': 1158,\n",
       "      'text': '(Dantzig and Thapa, 2006)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "    'cite_spans': [{'start': 107,\n",
       "      'end': 118,\n",
       "      'text': '(Lin, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "    'cite_spans': [{'start': 351,\n",
       "      'end': 365,\n",
       "      'text': '(Wasson, 1998)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'},\n",
       "     {'start': 492,\n",
       "      'end': 512,\n",
       "      'text': '(Radev et al., 2000)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'},\n",
       "     {'start': 700,\n",
       "      'end': 723,\n",
       "      'text': '(Erkan and Radev, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 737,\n",
       "      'end': 763,\n",
       "      'text': '(Mihalcea and Tarau, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "    'cite_spans': [{'start': 498,\n",
       "      'end': 518,\n",
       "      'text': '(Kingma and Ba, 2014',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 652,\n",
       "      'end': 674,\n",
       "      'text': '(Bastien et al., 2012)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "    'cite_spans': [{'start': 690,\n",
       "      'end': 707,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 908,\n",
       "      'end': 925,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [{'start': 77,\n",
       "      'end': 84,\n",
       "      'text': 'Table 1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF0'},\n",
       "     {'start': 746,\n",
       "      'end': 753,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF1'},\n",
       "     {'start': 1184,\n",
       "      'end': 1191,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 315,\n",
       "      'end': 322,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF3'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "    'cite_spans': [{'start': 517,\n",
       "      'end': 868,\n",
       "      'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {'FIGREF0': {'text': 'Figure 1: Reader comments of the news \"The most important announcements from Google\\'s big developers\\' conference (May, 2017)\".',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF2': {'text': 'Figure 2: Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence x d and comment sentence x c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. A d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'TABREF0': {'text': 'Summarization performance.',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF1': {'text': 'Further investigation of RAVAESum.',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF2': {'text': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF3': {'text': 'Generated summaries for the topic \"Sony Virtual Reality PS4\".A virtual reality headset that\\'s coming to the PlayStation 4. Today announced the development of \"Project Mor- pheus\" (Morpheus) \"a virtual reality (VR) system that takes the PlayStation4 (PS4)\". Shuhei Yoshida, presi- dent of Sony Computer Entertainment, revealed a proto- type of Morpheus at the Game Developers Conference in San Francisco on Tuesday. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. The camera on the Playstation 4 using sensors that track the player\\'s head movements.',\n",
       "    'latex': None,\n",
       "    'type': 'table'}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "    'authors': [{'first': 'Dzmitry',\n",
       "      'middle': [],\n",
       "      'last': 'Bahdanau',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "     {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '11212020'},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Theano: new features and speed improvements',\n",
       "    'authors': [{'first': 'Frédéric',\n",
       "      'middle': [],\n",
       "      'last': 'Bastien',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "     {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "     {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "     {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "     {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "     {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "     {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "     {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "    'links': '8180128'},\n",
       "   'BIBREF2': {'ref_id': 'b2',\n",
       "    'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF3': {'ref_id': 'b3',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "    'authors': [{'first': 'Shuhei',\n",
       "      'middle': [],\n",
       "      'last': 'Yoshida',\n",
       "      'suffix': ''}],\n",
       "    'year': None,\n",
       "    'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACLANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Autoencoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF11': {'ref_id': 'b11',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "    'authors': [{'first': 'Chin-Yew',\n",
       "      'middle': [],\n",
       "      'last': 'Lin',\n",
       "      'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '964287'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF14': {'ref_id': 'b14',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "    'authors': [{'first': 'Xiaojun',\n",
       "      'middle': [],\n",
       "      'last': 'Wan',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "    'year': 2007,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '7',\n",
       "    'issn': '',\n",
       "    'pages': '2903--2908',\n",
       "    'other_ids': {},\n",
       "    'links': '532313'},\n",
       "   'BIBREF20': {'ref_id': 'b20',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}},\n",
       " 'latex_parse': {'abstract': [],\n",
       "  'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "    'cite_spans': [{'start': 193,\n",
       "      'end': 200,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 203,\n",
       "      'end': 210,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 213,\n",
       "      'end': 220,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 223,\n",
       "      'end': 230,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 233,\n",
       "      'end': 240,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 243,\n",
       "      'end': 250,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 253,\n",
       "      'end': 260,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 118,\n",
       "      'end': 125,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "    'cite_spans': [{'start': 527,\n",
       "      'end': 534,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 537,\n",
       "      'end': 544,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 802,\n",
       "      'end': 809,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 159,\n",
       "      'end': 167,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 170,\n",
       "      'end': 178,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "    'cite_spans': [{'start': 489,\n",
       "      'end': 496,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 19,\n",
       "      'end': 26,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF7'}],\n",
       "    'eq_spans': [{'start': 212,\n",
       "      'end': 223,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 228,\n",
       "      'end': 239,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 254,\n",
       "      'end': 265,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 285,\n",
       "      'end': 296,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 739,\n",
       "      'end': 750,\n",
       "      'text': 'ρ i ',\n",
       "      'latex': '\\\\rho _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 774,\n",
       "      'end': 785,\n",
       "      'text': '𝐱 c i ',\n",
       "      'latex': '\\\\mathbf {x}_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 807,\n",
       "      'end': 818,\n",
       "      'text': 'ρ∈ℝ n c  ',\n",
       "      'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Overview'},\n",
       "   {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 32,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 43,\n",
       "      'end': 51,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 154,\n",
       "      'end': 161,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "      'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 488,\n",
       "      'end': 499,\n",
       "      'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "      'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None},\n",
       "     {'start': 524,\n",
       "      'end': 535,\n",
       "      'text': 'σ',\n",
       "      'latex': '\\\\sigma ',\n",
       "      'ref_id': None},\n",
       "     {'start': 799,\n",
       "      'end': 811,\n",
       "      'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 50,\n",
       "      'end': 56,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'eq_spans': [{'start': 131,\n",
       "      'end': 142,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 145,\n",
       "      'end': 157,\n",
       "      'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "      'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "      'ref_id': 'EQREF10'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': '𝐱',\n",
       "      'latex': '\\\\mathbf {x}',\n",
       "      'ref_id': None},\n",
       "     {'start': 76,\n",
       "      'end': 87,\n",
       "      'text': '𝐱 d ',\n",
       "      'latex': '\\\\mathbf {x}_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐱 c ',\n",
       "      'latex': '\\\\mathbf {x}_c',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 364,\n",
       "      'end': 375,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 430,\n",
       "      'end': 441,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 444,\n",
       "      'end': 456,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "      'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "      'ref_id': 'EQREF11'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 19,\n",
       "      'end': 30,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 113,\n",
       "      'end': 124,\n",
       "      'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "      'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 219,\n",
       "      'end': 230,\n",
       "      'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "      'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 320,\n",
       "      'end': 331,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 335,\n",
       "      'end': 346,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 402,\n",
       "      'end': 413,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 416,\n",
       "      'end': 428,\n",
       "      'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "      'ref_id': 'EQREF12'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 7,\n",
       "      'end': 14,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 46,\n",
       "      'end': 54,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'},\n",
       "     {'start': 57,\n",
       "      'end': 65,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 304,\n",
       "      'end': 315,\n",
       "      'text': 's h i ',\n",
       "      'latex': 's^i_{h}',\n",
       "      'ref_id': None},\n",
       "     {'start': 366,\n",
       "      'end': 377,\n",
       "      'text': 'h d j ',\n",
       "      'latex': 'h^j_{d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 401,\n",
       "      'end': 412,\n",
       "      'text': 'a d ∈ℝ n d  ',\n",
       "      'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'h c j ',\n",
       "      'latex': 'h^j_{c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 507,\n",
       "      'end': 518,\n",
       "      'text': 'a c ∈ℝ n c  ',\n",
       "      'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 616,\n",
       "      'end': 627,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 672,\n",
       "      'end': 684,\n",
       "      'text': 'a ˜ c =a c ×ρ',\n",
       "      'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "      'ref_id': 'EQREF13'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 30,\n",
       "      'end': 41,\n",
       "      'text': 'c d i ',\n",
       "      'latex': 'c_d^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 79,\n",
       "      'end': 90,\n",
       "      'text': 'c c i ',\n",
       "      'latex': 'c_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 240,\n",
       "      'end': 252,\n",
       "      'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "      'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "      'ref_id': 'EQREF14'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 64,\n",
       "      'end': 75,\n",
       "      'text': 's ˜ h i ',\n",
       "      'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 1,\n",
       "      'end': 12,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 15,\n",
       "      'end': 26,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 33,\n",
       "      'end': 44,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "      'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 297,\n",
       "      'end': 308,\n",
       "      'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "      'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 569,\n",
       "      'end': 581,\n",
       "      'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "      'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "      'ref_id': 'EQREF15'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 170,\n",
       "      'end': 182,\n",
       "      'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "      'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "      'ref_id': 'EQREF16'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'Θ',\n",
       "      'latex': '\\\\Theta ',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐀 d ',\n",
       "      'latex': '\\\\mathbf {A}_d',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 199,\n",
       "      'end': 210,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 348,\n",
       "      'end': 359,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 390,\n",
       "      'end': 401,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 434,\n",
       "      'end': 445,\n",
       "      'text': 'R∈ℝ n d ×n c  ',\n",
       "      'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 450,\n",
       "      'end': 462,\n",
       "      'text': 'R=X d ×X c T ',\n",
       "      'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "      'ref_id': 'EQREF17'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 93,\n",
       "      'end': 105,\n",
       "      'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "      'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "      'ref_id': 'EQREF18'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': '(0,1)',\n",
       "      'latex': '(0,1)',\n",
       "      'ref_id': None},\n",
       "     {'start': 84,\n",
       "      'end': 96,\n",
       "      'text': 'ρ=sigmoid(𝐫)',\n",
       "      'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "      'ref_id': 'EQREF19'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 332,\n",
       "      'end': 343,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 346,\n",
       "      'end': 358,\n",
       "      'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "      'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "      'ref_id': 'EQREF20'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'ρ z ',\n",
       "      'latex': '\\\\rho _z',\n",
       "      'ref_id': None},\n",
       "     {'start': 22,\n",
       "      'end': 33,\n",
       "      'text': 'ρ x ',\n",
       "      'latex': '\\\\rho _x',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 82,\n",
       "      'end': 89,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 94,\n",
       "      'end': 101,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 485,\n",
       "      'end': 497,\n",
       "      'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "      'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "      'ref_id': 'EQREF22'}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "    'cite_spans': [{'start': 466,\n",
       "      'end': 474,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'},\n",
       "     {'start': 477,\n",
       "      'end': 484,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 491,\n",
       "      'end': 498,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 644,\n",
       "      'end': 652,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'α i ',\n",
       "      'latex': '\\\\alpha _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "     {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "     {'start': 112,\n",
       "      'end': 123,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 126,\n",
       "      'end': 137,\n",
       "      'text': 'α ij ',\n",
       "      'latex': '\\\\alpha _{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'R ij ',\n",
       "      'latex': 'R_{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 234,\n",
       "      'end': 245,\n",
       "      'text': 'P j ',\n",
       "      'latex': 'P_j',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Description'},\n",
       "   {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 222,\n",
       "      'end': 229,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF7'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Properties'},\n",
       "   {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "    'cite_spans': [{'start': 113,\n",
       "      'end': 121,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'}],\n",
       "    'ref_spans': [{'start': 58,\n",
       "      'end': 66,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF28'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Dataset and Metrics'},\n",
       "   {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "    'cite_spans': [{'start': 5,\n",
       "      'end': 13,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "    'cite_spans': [{'start': 9,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 29,\n",
       "      'end': 37,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "    'cite_spans': [{'start': 557,\n",
       "      'end': 565,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'},\n",
       "     {'start': 697,\n",
       "      'end': 705,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF21'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': '|V|',\n",
       "      'latex': '|V|',\n",
       "      'ref_id': None},\n",
       "     {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "     {'start': 194,\n",
       "      'end': 205,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 210,\n",
       "      'end': 221,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 360,\n",
       "      'end': 371,\n",
       "      'text': 'm=5',\n",
       "      'latex': 'm = 5',\n",
       "      'ref_id': None},\n",
       "     {'start': 431,\n",
       "      'end': 442,\n",
       "      'text': 'd h =500',\n",
       "      'latex': 'd_h = 500',\n",
       "      'ref_id': None},\n",
       "     {'start': 463,\n",
       "      'end': 474,\n",
       "      'text': 'K=100',\n",
       "      'latex': 'K = 100',\n",
       "      'ref_id': None},\n",
       "     {'start': 495,\n",
       "      'end': 506,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 538,\n",
       "      'end': 549,\n",
       "      'text': 'λ p =0.2',\n",
       "      'latex': '\\\\lambda _p=0.2',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Experimental Settings'},\n",
       "   {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 83,\n",
       "      'end': 91,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF40'}],\n",
       "    'eq_spans': [{'start': 240,\n",
       "      'end': 251,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Results on Our Dataset'},\n",
       "   {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "    'cite_spans': [{'start': 208,\n",
       "      'end': 215,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 260,\n",
       "      'end': 268,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF42'}],\n",
       "    'eq_spans': [{'start': 380,\n",
       "      'end': 391,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "    'cite_spans': [{'start': 33,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 305,\n",
       "      'end': 313,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF43'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 250,\n",
       "      'end': 258,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF45'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Case Study'},\n",
       "   {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Conclusions'}],\n",
       "  'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "    'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF9',\n",
       "    'type': 'equation'},\n",
       "   'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "    'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "    'ref_id': 'EQREF10',\n",
       "    'type': 'equation'},\n",
       "   'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "    'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "    'ref_id': 'EQREF11',\n",
       "    'type': 'equation'},\n",
       "   'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "    'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF12',\n",
       "    'type': 'equation'},\n",
       "   'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "    'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "    'ref_id': 'EQREF13',\n",
       "    'type': 'equation'},\n",
       "   'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "    'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "    'ref_id': 'EQREF14',\n",
       "    'type': 'equation'},\n",
       "   'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "    'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "    'ref_id': 'EQREF15',\n",
       "    'type': 'equation'},\n",
       "   'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "    'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "    'ref_id': 'EQREF16',\n",
       "    'type': 'equation'},\n",
       "   'EQREF17': {'text': 'R=X d ×X c T',\n",
       "    'latex': 'R = X_d\\\\times X_c^T',\n",
       "    'ref_id': 'EQREF17',\n",
       "    'type': 'equation'},\n",
       "   'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "    'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "    'ref_id': 'EQREF18',\n",
       "    'type': 'equation'},\n",
       "   'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "    'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "    'ref_id': 'EQREF19',\n",
       "    'type': 'equation'},\n",
       "   'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "    'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "    'ref_id': 'EQREF20',\n",
       "    'type': 'equation'},\n",
       "   'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "    'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "    'ref_id': 'EQREF22',\n",
       "    'type': 'equation'},\n",
       "   'FIGREF2': {'text': '1',\n",
       "    'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF2',\n",
       "    'type': 'figure'},\n",
       "   'FIGREF7': {'text': '2',\n",
       "    'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF7',\n",
       "    'type': 'figure'},\n",
       "   'TABREF40': {'text': '1',\n",
       "    'caption': 'Summarization performance.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF40',\n",
       "    'type': 'table'},\n",
       "   'TABREF42': {'text': '2',\n",
       "    'caption': 'Further investigation of RAVAESum.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF42',\n",
       "    'type': 'table'},\n",
       "   'TABREF43': {'text': '3',\n",
       "    'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF43',\n",
       "    'type': 'table'},\n",
       "   'TABREF45': {'text': '4',\n",
       "    'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF45',\n",
       "    'type': 'table'},\n",
       "   'TABREF46': {'text': '5',\n",
       "    'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF46',\n",
       "    'type': 'table'},\n",
       "   'SECREF1': {'text': 'Introduction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF1',\n",
       "    'type': 'section'},\n",
       "   'SECREF2': {'text': 'Framework',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF2',\n",
       "    'type': 'section'},\n",
       "   'SECREF6': {'text': 'Conclusions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF6',\n",
       "    'type': 'section'},\n",
       "   'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF8',\n",
       "    'type': 'section'},\n",
       "   'SECREF21': {'text': 'Summary Construction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF21',\n",
       "    'type': 'section'},\n",
       "   'SECREF3': {'text': 'Data Description',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF3',\n",
       "    'type': 'section'},\n",
       "   'SECREF24': {'text': 'Background',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF24',\n",
       "    'type': 'section'},\n",
       "   'SECREF26': {'text': 'Data Collection',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF26',\n",
       "    'type': 'section'},\n",
       "   'SECREF28': {'text': 'Data Properties',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF28',\n",
       "    'type': 'section'},\n",
       "   'SECREF4': {'text': 'Experimental Setup',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF4',\n",
       "    'type': 'section'},\n",
       "   'SECREF29': {'text': 'Dataset and Metrics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF29',\n",
       "    'type': 'section'},\n",
       "   'SECREF31': {'text': 'Comparative Methods',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF31',\n",
       "    'type': 'section'},\n",
       "   'SECREF37': {'text': 'Experimental Settings',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF37',\n",
       "    'type': 'section'},\n",
       "   'SECREF5': {'text': 'Results and Discussions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF5',\n",
       "    'type': 'section'},\n",
       "   'SECREF39': {'text': 'Results on Our Dataset',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF39',\n",
       "    'type': 'section'},\n",
       "   'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF41',\n",
       "    'type': 'section'},\n",
       "   'SECREF44': {'text': 'Case Study',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF44',\n",
       "    'type': 'section'},\n",
       "   'SECREF7': {'text': 'Topics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF7',\n",
       "    'type': 'section'}},\n",
       "  'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "    'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACL-ANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "    'title': 'Auto-encoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '15789289'},\n",
       "   'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "    'title': 'Effective approaches to attention-based neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "    'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}}}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_only_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': ['8294822'],\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': ['10418456'],\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': ['532313'],\n",
       " 'A survey of text summarization techniques': ['556431'],\n",
       " 'Exploiting category-specific information for multidocument summarization': ['6317274'],\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': ['29562039'],\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": ['13723748'],\n",
       " 'Social context summarization': ['704517'],\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': ['8377315'],\n",
       " 'Autoencoding variational bayes': [],\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': ['16895865']}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_bibs['10164018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Large-scale semantic parsing via schema matching and lexicon extension': ['2265838'],\n",
       " 'Scaling semantic parsers with on-the-fly ontology matching': ['14341841'],\n",
       " 'Semantic parsing on Freebase from question-answer pairs': ['6401679'],\n",
       " 'Using semantic unification to generate regular expressions from natural language': ['6216733'],\n",
       " 'A joint model of language and perception for grounded attribute learning': ['2408319'],\n",
       " 'Understanding natural language commands for robotic navigation and mobile manipulation': ['1078533'],\n",
       " 'Jointly learning to parse and perceive: Connecting natural language to the physical world': ['10250712'],\n",
       " 'Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars': ['449252'],\n",
       " 'Learning synchronous grammars for semantic parsing with lambda calculus': ['9337134'],\n",
       " \"Driving semantic parsing from the world's response\": ['5667590'],\n",
       " 'Learning dependency-based compositional semantics': ['340852'],\n",
       " 'Bootstrapping semantic parsers from conversations': ['1140108'],\n",
       " 'Largescale semantic parsing without question-answer pairs': ['15324422'],\n",
       " 'Paraphrase-driven learning for open question answering': ['8893912'],\n",
       " 'Semantic parsing via paraphrasing': ['1336493'],\n",
       " 'The lunar sciences natural language information system: Final report': ['62727207'],\n",
       " 'An efficient easily adaptable system for interpreting natural language queries': ['2498523'],\n",
       " 'Controlled natural languages for knowledge representation': ['10228634']}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_bibs['14472576']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('acl_scenario/acl_only_articles_0_9.json', 'w+') as json_fil:\n",
    "    json.dump(list(acl_only_articles), json_fil, separators=(',', ':'))\n",
    "json_fil.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_paper_ids = {article['paper_id']:1 for article in acl_only_articles }\n",
    "len(acl_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers_list = [v for k,v in overview_papers.items() if k in acl_paper_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7259"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overview_papers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('acl_scenario/overview_papers_0_9.json', 'w+') as json_fil:\n",
    "    json.dump(overview_papers_list, json_fil, separators=(',', ':'))\n",
    "json_fil.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_out_ids = []\n",
    "for paper in acl_only_articles:\n",
    "#     print(paper['paper_id'])\n",
    "    for out in article_bibs[paper['paper_id']].values():\n",
    "        if len(out)>0:\n",
    "            acl_out_ids+=out\n",
    "#             print(out)\n",
    "#     print(acl_out_ids)\n",
    "#     print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82636, 26887)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_out_ids),len(set(acl_out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_out_ids_keys = {val:1 for val in set(acl_out_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('acl_scenario/acl_out_ids.txt', 'w') as f_in_file:\n",
    "    for item in set(acl_out_ids):\n",
    "        f_in_file.write(\"%s\\n\" % item)\n",
    "f_in_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_only_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_only_articles[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'b0',\n",
       "  'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "  'authors': [{'first': 'Dzmitry',\n",
       "    'middle': [],\n",
       "    'last': 'Bahdanau',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "   {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '11212020'},\n",
       " 'BIBREF1': {'ref_id': 'b1',\n",
       "  'title': 'Theano: new features and speed improvements',\n",
       "  'authors': [{'first': 'Frédéric',\n",
       "    'middle': [],\n",
       "    'last': 'Bastien',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "   {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "   {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "   {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "   {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "   {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "   {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "   {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "  'links': '8180128'},\n",
       " 'BIBREF2': {'ref_id': 'b2',\n",
       "  'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "  'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "   {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1587--1597',\n",
       "  'other_ids': {},\n",
       "  'links': '8377315'},\n",
       " 'BIBREF3': {'ref_id': 'b3',\n",
       "  'title': 'Linear programming 1: introduction',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "   {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "  'year': 2006,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '53739754'},\n",
       " 'BIBREF4': {'ref_id': 'b4',\n",
       "  'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "  'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '4',\n",
       "  'issn': '',\n",
       "  'pages': '365--371',\n",
       "  'other_ids': {},\n",
       "  'links': '10418456'},\n",
       " 'BIBREF5': {'ref_id': 'b5',\n",
       "  'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "  'authors': [{'first': 'Shuhei',\n",
       "    'middle': [],\n",
       "    'last': 'Yoshida',\n",
       "    'suffix': ''}],\n",
       "  'year': None,\n",
       "  'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF6': {'ref_id': 'b6',\n",
       "  'title': 'Multi-document summarization by sentence extraction',\n",
       "  'authors': [{'first': 'Jade',\n",
       "    'middle': [],\n",
       "    'last': 'Goldstein',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "   {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "   {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'NAACLANLPWorkshop',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '40--48',\n",
       "  'other_ids': {},\n",
       "  'links': '8294822'},\n",
       " 'BIBREF7': {'ref_id': 'b7',\n",
       "  'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "  'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "   {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "   {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '291--298',\n",
       "  'other_ids': {},\n",
       "  'links': '13723748'},\n",
       " 'BIBREF8': {'ref_id': 'b8',\n",
       "  'title': 'Adam: A method for stochastic optimization',\n",
       "  'authors': [{'first': 'Diederik',\n",
       "    'middle': [],\n",
       "    'last': 'Kingma',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '6628106'},\n",
       " 'BIBREF9': {'ref_id': 'b9',\n",
       "  'title': 'Autoencoding variational bayes',\n",
       "  'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "   {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF10': {'ref_id': 'b10',\n",
       "  'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1270--1276',\n",
       "  'other_ids': {},\n",
       "  'links': '14777460'},\n",
       " 'BIBREF11': {'ref_id': 'b11',\n",
       "  'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'AAAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '3497--3503',\n",
       "  'other_ids': {},\n",
       "  'links': '29562039'},\n",
       " 'BIBREF12': {'ref_id': 'b12',\n",
       "  'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "  'authors': [{'first': 'Chin-Yew',\n",
       "    'middle': [],\n",
       "    'last': 'Lin',\n",
       "    'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "  'volume': '8',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '964287'},\n",
       " 'BIBREF13': {'ref_id': 'b13',\n",
       "  'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "  'authors': [{'first': 'Minh-Thang',\n",
       "    'middle': [],\n",
       "    'last': 'Luong',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "   {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1412--1421',\n",
       "  'other_ids': {},\n",
       "  'links': '1998416'},\n",
       " 'BIBREF14': {'ref_id': 'b14',\n",
       "  'title': 'Textrank: Bringing order into texts',\n",
       "  'authors': [{'first': 'Rada',\n",
       "    'middle': [],\n",
       "    'last': 'Mihalcea',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '577937'},\n",
       " 'BIBREF15': {'ref_id': 'b15',\n",
       "  'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "  'authors': [{'first': 'Yen',\n",
       "    'middle': ['Kan'],\n",
       "    'last': 'Ziheng Lin Min',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'COLING',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '2093--2108',\n",
       "  'other_ids': {},\n",
       "  'links': '6317274'},\n",
       " 'BIBREF16': {'ref_id': 'b16',\n",
       "  'title': 'A survey of text summarization techniques',\n",
       "  'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "   {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Mining Text Data',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '43--76',\n",
       "  'other_ids': {},\n",
       "  'links': '556431'},\n",
       " 'BIBREF17': {'ref_id': 'b17',\n",
       "  'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "  'authors': [{'first': 'Hongyan',\n",
       "    'middle': [],\n",
       "    'last': 'Dragomir R Radev',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '21--30',\n",
       "  'other_ids': {},\n",
       "  'links': '1320'},\n",
       " 'BIBREF18': {'ref_id': 'b18',\n",
       "  'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "  'authors': [{'first': 'Danilo',\n",
       "    'middle': [],\n",
       "    'last': 'Jimenez Rezende',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "   {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICML',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1278--1286',\n",
       "  'other_ids': {},\n",
       "  'links': '16895865'},\n",
       " 'BIBREF19': {'ref_id': 'b19',\n",
       "  'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "  'authors': [{'first': 'Xiaojun', 'middle': [], 'last': 'Wan', 'suffix': ''},\n",
       "   {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "  'year': 2007,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '7',\n",
       "  'issn': '',\n",
       "  'pages': '2903--2908',\n",
       "  'other_ids': {},\n",
       "  'links': '532313'},\n",
       " 'BIBREF20': {'ref_id': 'b20',\n",
       "  'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "  'authors': [{'first': 'Mark', 'middle': [], 'last': 'Wasson', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1364--1368',\n",
       "  'other_ids': {},\n",
       "  'links': '12681629'},\n",
       " 'BIBREF21': {'ref_id': 'b21',\n",
       "  'title': 'Multiple aspect summarization using integer linear programming',\n",
       "  'authors': [{'first': 'Kristian',\n",
       "    'middle': [],\n",
       "    'last': 'Woodsend',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'EMNLP-CNLL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '233--243',\n",
       "  'other_ids': {},\n",
       "  'links': '17497992'},\n",
       " 'BIBREF22': {'ref_id': 'b22',\n",
       "  'title': 'Social context summarization',\n",
       "  'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "   {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "   {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "   {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "   {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '255--264',\n",
       "  'other_ids': {},\n",
       "  'links': '704517'}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_only_articles[0]['grobid_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cоберем данные статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### нам для обучения сценария, кроме метаданных ничего не нужно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'acl_scenario/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "del overview_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c222f37369b6469db014ed9181f35559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 | 5\n",
      "epoch = 2000 | 5327\n",
      "epoch = 4000 | 5290\n",
      "epoch = 6000 | 5441\n",
      "ERROR epoch=7369\n",
      "epoch = 8000 | 5405\n",
      "epoch = 10000 | 5419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = -1\n",
    "all_articles = []\n",
    "for file in tqdm_notebook([file for file in f_zips if '.gz' in file]):\n",
    "    txt_input = \"../../gorc/\"+file\n",
    "    \n",
    "    zip_num = file.split('.')[0]\n",
    "    epoch+=1\n",
    "    \n",
    "    with gzip.open(txt_input, \"r\") as read_file:\n",
    "#         print(txt_input)\n",
    "        try:\n",
    "            json_list = list(read_file)\n",
    "        except:\n",
    "#             epoch-=1\n",
    "            print(f'ERROR epoch={epoch}')\n",
    "            \n",
    "    for num, jsonchik in enumerate(json_list):\n",
    "        try:\n",
    "            result = json.loads(jsonchik)\n",
    "    #             context_dict = get_out_citation(result,context_dict)\n",
    "            if result['paper_id'] in acl_out_ids_keys:\n",
    "                latex_parse_abstract = None\n",
    "                latex_parse_bib_entries = None\n",
    "                \n",
    "                grobid_parse_abstract = None\n",
    "                grobid_parse_bib_entries = None\n",
    "                if result['grobid_parse']:\n",
    "                    if result['grobid_parse']['abstract']:#and result['grobid_parse']['body_text']\n",
    "                        grobid_parse_abstract = result['grobid_parse']['abstract']\n",
    "                    if result['grobid_parse']['bib_entries']:\n",
    "                        grobid_parse_bib_entries = result['grobid_parse']['bib_entries']\n",
    "                if result['latex_parse']:\n",
    "                    if result['latex_parse']['abstract']:# and result['latex_parse']['body_text']:\n",
    "                        latex_parse_abstract = result['latex_parse']['abstract']\n",
    "                    if result['latex_parse']['bib_entries']:\n",
    "                        latex_parse_bib_entries = result['latex_parse']['bib_entries']\n",
    "                out_paper = {'paper_id': result['paper_id'], 'metadata':result['metadata'],'s2_pdf_hash':result['s2_pdf_hash'],\n",
    "                             'grobid_parse_abstract':grobid_parse_abstract,'latex_parse_abstract':latex_parse_abstract,\n",
    "                             'grobid_parse_bib_entries':grobid_parse_bib_entries,'latex_parse_bib_entries':latex_parse_bib_entries\n",
    "                            }\n",
    "                all_articles.append(out_paper)\n",
    "        except:\n",
    "                print('EXCEPT')\n",
    "                continue\n",
    "#     context_dict = get__in_out_papers(all_articles, zip_num, context_dict)\n",
    "    \n",
    "    read_file.close()\n",
    "    if epoch % 2000 == 0:\n",
    "        name_json = dir_name+'acl_out_new_'+str(epoch)+'.json' \n",
    "        with open(name_json, 'w') as json_fil:\n",
    "            json.dump(all_articles, json_fil)\n",
    "        json_fil.close()\n",
    "        \n",
    "        print('epoch =',epoch,'|',len(all_articles))\n",
    "        all_articles = []  # main for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e9ee9da4ce46c6afa30446b10d7371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 | 7\n",
      "epoch = 1000 | 4305\n",
      "epoch = 2000 | 4248\n",
      "epoch = 3000 | 4300\n",
      "epoch = 4000 | 4258\n",
      "epoch = 5000 | 4427\n",
      "epoch = 6000 | 4312\n",
      "epoch = 7000 | 4326\n",
      "ERROR epoch=7369\n",
      "epoch = 8000 | 4406\n",
      "epoch = 9000 | 4321\n",
      "epoch = 10000 | 4445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = -1\n",
    "all_articles = []\n",
    "for file in tqdm_notebook([file for file in f_zips if '.gz' in file]):\n",
    "    txt_input = \"../../gorc/\"+file\n",
    "    \n",
    "    zip_num = file.split('.')[0]\n",
    "    epoch+=1\n",
    "    \n",
    "    with gzip.open(txt_input, \"r\") as read_file:\n",
    "#         print(txt_input)\n",
    "        try:\n",
    "            json_list = list(read_file)\n",
    "        except:\n",
    "#             epoch-=1\n",
    "            print(f'ERROR epoch={epoch}')\n",
    "            \n",
    "    for num, jsonchik in enumerate(json_list):\n",
    "        try:\n",
    "            result = json.loads(jsonchik)\n",
    "    #             context_dict = get_out_citation(result,context_dict)\n",
    "            if result['paper_id'] in acl_out_ids_keys:\n",
    "                if result['grobid_parse'] and result['grobid_parse']['body_text']:\n",
    "                    all_articles.append(result)\n",
    "                elif result['latex_parse'] and result['latex_parse']['body_text']:\n",
    "                    all_articles.append(result)\n",
    "                all_articles.append(out_paper)\n",
    "        except:\n",
    "                print('EXCEPT')\n",
    "                continue\n",
    "#     context_dict = get__in_out_papers(all_articles, zip_num, context_dict)\n",
    "    \n",
    "    read_file.close()\n",
    "    if epoch % 1000 == 0:\n",
    "        name_json = dir_name+'acl_entire_papers_out'+str(epoch)+'.json' \n",
    "        with open(name_json, 'w') as json_fil:\n",
    "            json.dump(all_articles, json_fil)\n",
    "        json_fil.close()\n",
    "        \n",
    "        print('epoch =',epoch,'|',len(all_articles))\n",
    "        all_articles = []  # main for batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ещё"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "del all_articles\n",
    "del result\n",
    "del acl_only_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Не запускал"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41660\n"
     ]
    }
   ],
   "source": [
    "with open(\"acl_only_json_list_10000.json\", \"r\") as read_file:\n",
    "    acl_only_articles = json.load(read_file)\n",
    "print(len(acl_only_articles))\n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_for_out = df_covering_overview[(df_covering_overview['%bibs_with_links2bibs']>=0.8) & (df_covering_overview['#bibs']>=5)]\n",
    "acl_ids_for_out_index = list(acl_ids_for_out.index)\n",
    "acl_ids_for_out_index_keys = {val:1 for val in acl_ids_for_out_index}\n",
    "acl_ids_not_for_out = [article['paper_id'] for article in acl_only_articles if article['paper_id'] not in acl_ids_for_out_index_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14989, 26671, 41660)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_ids_for_out),len(acl_ids_not_for_out),len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_only_articles = delete_items_from_papers(acl_ids_not_for_out,acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14989"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_only_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_out_ids = []\n",
    "for paper in acl_only_articles:\n",
    "#     print(paper['paper_id'])\n",
    "    for out in article_bibs[paper['paper_id']].values():\n",
    "        if len(out)>0:\n",
    "            acl_out_ids+=out\n",
    "#             print(out)\n",
    "#     print(acl_out_ids)\n",
    "#     print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158049, 42748)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_out_ids),len(set(acl_out_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('acl_scenario/acl_out_ids_0_8.txt', 'w') as f_in_file:\n",
    "    for item in set(acl_out_ids):\n",
    "        f_in_file.write(\"%s\\n\" % item)\n",
    "f_in_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('acl_scenario/acl_only_articles_0_8.json', 'w+') as json_fil:\n",
    "    json.dump(list(acl_only_articles), json_fil, separators=(',', ':'))\n",
    "json_fil.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_out_ids_keys = {val:1 for val in acl_out_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42748"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_out_ids_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192c7a00fed6473aa55e805f9218e9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 | 7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = -1\n",
    "all_articles = []\n",
    "for file in tqdm_notebook([file for file in f_zips if '.gz' in file]):\n",
    "    txt_input = \"../../gorc/\"+file\n",
    "    \n",
    "    zip_num = file.split('.')[0]\n",
    "    epoch+=1\n",
    "    \n",
    "    with gzip.open(txt_input, \"r\") as read_file:\n",
    "#         print(txt_input)\n",
    "        try:\n",
    "            json_list = list(read_file)\n",
    "        except:\n",
    "#             epoch-=1\n",
    "            print(f'ERROR epoch={epoch}')\n",
    "            \n",
    "    for num, jsonchik in enumerate(json_list):\n",
    "        try:\n",
    "            result = json.loads(jsonchik)\n",
    "    #             context_dict = get_out_citation(result,context_dict)\n",
    "            if result['paper_id'] in acl_out_ids_keys:\n",
    "                latex_parse_abstract = None\n",
    "                latex_parse_bib_entries = None\n",
    "                \n",
    "                grobid_parse_abstract = None\n",
    "                grobid_parse_bib_entries = None\n",
    "                if result['grobid_parse']:\n",
    "                    if result['grobid_parse']['abstract']:#and result['grobid_parse']['body_text']\n",
    "                        grobid_parse_abstract = result['grobid_parse']['abstract']\n",
    "                    if result['grobid_parse']['bib_entries']:\n",
    "                        grobid_parse_bib_entries = result['grobid_parse']['bib_entries']\n",
    "                if result['latex_parse']:\n",
    "                    if result['latex_parse']['abstract']:# and result['latex_parse']['body_text']:\n",
    "                        latex_parse_abstract = result['latex_parse']['abstract']\n",
    "                    if result['latex_parse']['bib_entries']:\n",
    "                        latex_parse_bib_entries = result['latex_parse']['bib_entries']\n",
    "                out_paper = {'paper_id': result['paper_id'], 'metadata':result['metadata'],'s2_pdf_hash':result['s2_pdf_hash'],\n",
    "                             'grobid_parse_abstract':grobid_parse_abstract,'latex_parse_abstract':latex_parse_abstract,\n",
    "                             'grobid_parse_bib_entries':grobid_parse_bib_entries,'latex_parse_bib_entries':latex_parse_bib_entries\n",
    "                            }\n",
    "                all_articles.append(out_paper)\n",
    "        except:\n",
    "                print('EXCEPT')\n",
    "                continue\n",
    "#     context_dict = get__in_out_papers(all_articles, zip_num, context_dict)\n",
    "    \n",
    "    read_file.close()\n",
    "    if epoch % 2000 == 0:\n",
    "        name_json = dir_name+'acl_out_0_8_'+str(epoch)+'.json' \n",
    "        with open(name_json, 'w') as json_fil:\n",
    "            json.dump(all_articles, json_fil)\n",
    "        json_fil.close()\n",
    "        \n",
    "        print('epoch =',epoch,'|',len(all_articles))\n",
    "        all_articles = []  # main for batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
