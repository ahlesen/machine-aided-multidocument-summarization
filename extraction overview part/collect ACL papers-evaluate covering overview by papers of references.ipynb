{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_parse_overview = dict()\n",
    "for sections in all_articles[0]['latex_parse']['body_text']:\n",
    "    if sections['section'] in latex_parse_overview:\n",
    "        # если есть дублирование, такое бывает у первых частo\n",
    "        if latex_parse_overview[sections['section']] == sections:\n",
    "            continue\n",
    "        else:\n",
    "            latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "            latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "            latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "            latex_parse_overview[sections['section']]['cite_spans_start'].append(sections['section'])\n",
    "            latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "#             latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "    else:\n",
    "        latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                      'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                      'cite_spans_start':list(map(lambda x: int(x['start']),sections['cite_spans'])),\n",
    "                                                      'section':[sections['section']], \n",
    "                                                      'bib_entries':all_articles[0]['latex_parse']}\n",
    "       \n",
    "    \n",
    "for sections in latex_parse_overview:\n",
    "    sections_dict = latex_parse_overview[sections]\n",
    "    print(sections,len(sections_dict['cite_spans']),sections_dict['cite_span_lens'])\n",
    "    print(sections_dict)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def get_citation_contexts(paper: Dict, toks_in_context=10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve citation contexts from GORC paper\n",
    "    :param paper:\n",
    "    :param toks_in_context:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not paper:\n",
    "        return []\n",
    "\n",
    "    if not paper['grobid_parse']:\n",
    "        return []\n",
    "\n",
    "    if not paper['grobid_parse']['body_text']:\n",
    "        return []\n",
    "\n",
    "    contexts = []\n",
    "\n",
    "    for paragraph in paper['grobid_parse']['body_text']:\n",
    "        for cite_span in paragraph['cite_spans']:\n",
    "            # get cited paper id, skip if none\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_paper_id = None\n",
    "            if cite_ref in paper['grobid_parse']['bib_entries']:\n",
    "                cited_paper_id = paper['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "            if not cited_paper_id:\n",
    "                continue\n",
    "\n",
    "            # get pre and post tokens\n",
    "            pre_span_tokens = paragraph['text'][:cite_span['start']].split(' ')[-toks_in_context:]\n",
    "            post_span_tokens = paragraph['text'][cite_span['end']:].split(' ')[:toks_in_context]\n",
    "            pre_string = ' '.join(pre_span_tokens)\n",
    "            post_string = ' '.join(post_span_tokens)\n",
    "            full_context = pre_string + cite_span['text'] + post_string\n",
    "\n",
    "            contexts.append({\n",
    "                \"paper_id\": paper['paper_id'],\n",
    "                \"context_string\": full_context,\n",
    "                \"cite_start\": len(pre_string),\n",
    "                \"cite_end\": len(pre_string) + len(cite_span['text']),\n",
    "                \"cite_str\": cite_span['text'],\n",
    "                \"cited_paper_id\": cited_paper_id\n",
    "            })\n",
    "\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_DATA_FILE = 'data/example_papers.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_contexts = []\n",
    "all_papers = []\n",
    "context_dict = dict()\n",
    "with open(EXAMPLE_DATA_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        gorc_obj = json.loads(line)\n",
    "        all_papers.append(gorc_obj)\n",
    "        all_contexts += get_citation_contexts(gorc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "f_zips = []\n",
    "for (dirpath, dirnames, filenames) in walk('../../gorc/'):\n",
    "    f_zips.extend(filenames)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002,\n",
       " ['0.jsonl.gz',\n",
       "  '1.jsonl.gz',\n",
       "  '10.jsonl.gz',\n",
       "  '100.jsonl.gz',\n",
       "  '1000.jsonl.gz',\n",
       "  '1001.jsonl.gz',\n",
       "  '1002.jsonl.gz',\n",
       "  '1003.jsonl.gz',\n",
       "  '1004.jsonl.gz',\n",
       "  '1005.jsonl.gz',\n",
       "  '1006.jsonl.gz',\n",
       "  '1007.jsonl.gz',\n",
       "  '1008.jsonl.gz',\n",
       "  '1009.jsonl.gz',\n",
       "  '101.jsonl.gz',\n",
       "  '1010.jsonl.gz',\n",
       "  '1011.jsonl.gz',\n",
       "  '1012.jsonl.gz',\n",
       "  '1013.jsonl.gz',\n",
       "  '1014.jsonl.gz',\n",
       "  '1015.jsonl.gz',\n",
       "  '1016.jsonl.gz',\n",
       "  '1017.jsonl.gz',\n",
       "  '1018.jsonl.gz',\n",
       "  '1019.jsonl.gz',\n",
       "  '102.jsonl.gz',\n",
       "  '1020.jsonl.gz',\n",
       "  '1021.jsonl.gz',\n",
       "  '1022.jsonl.gz',\n",
       "  '1023.jsonl.gz',\n",
       "  '1024.jsonl.gz',\n",
       "  '1025.jsonl.gz',\n",
       "  '1026.jsonl.gz',\n",
       "  '1027.jsonl.gz',\n",
       "  '1028.jsonl.gz',\n",
       "  '1029.jsonl.gz',\n",
       "  '103.jsonl.gz',\n",
       "  '1030.jsonl.gz',\n",
       "  '1031.jsonl.gz',\n",
       "  '1032.jsonl.gz',\n",
       "  '1033.jsonl.gz',\n",
       "  '1034.jsonl.gz',\n",
       "  '1035.jsonl.gz',\n",
       "  '1036.jsonl.gz',\n",
       "  '1037.jsonl.gz',\n",
       "  '1038.jsonl.gz',\n",
       "  '1039.jsonl.gz',\n",
       "  '104.jsonl.gz',\n",
       "  '1040.jsonl.gz',\n",
       "  '1041.jsonl.gz',\n",
       "  '1042.jsonl.gz',\n",
       "  '1043.jsonl.gz',\n",
       "  '1044.jsonl.gz',\n",
       "  '1045.jsonl.gz',\n",
       "  '1046.jsonl.gz',\n",
       "  '1047.jsonl.gz',\n",
       "  '1048.jsonl.gz',\n",
       "  '1049.jsonl.gz',\n",
       "  '105.jsonl.gz',\n",
       "  '1050.jsonl.gz',\n",
       "  '1051.jsonl.gz',\n",
       "  '1052.jsonl.gz',\n",
       "  '1053.jsonl.gz',\n",
       "  '1054.jsonl.gz',\n",
       "  '1055.jsonl.gz',\n",
       "  '1056.jsonl.gz',\n",
       "  '1057.jsonl.gz',\n",
       "  '1058.jsonl.gz',\n",
       "  '1059.jsonl.gz',\n",
       "  '106.jsonl.gz',\n",
       "  '1060.jsonl.gz',\n",
       "  '1061.jsonl.gz',\n",
       "  '1062.jsonl.gz',\n",
       "  '1063.jsonl.gz',\n",
       "  '1064.jsonl.gz',\n",
       "  '1065.jsonl.gz',\n",
       "  '1066.jsonl.gz',\n",
       "  '1067.jsonl.gz',\n",
       "  '1068.jsonl.gz',\n",
       "  '1069.jsonl.gz',\n",
       "  '107.jsonl.gz',\n",
       "  '1070.jsonl.gz',\n",
       "  '1071.jsonl.gz',\n",
       "  '1072.jsonl.gz',\n",
       "  '1073.jsonl.gz',\n",
       "  '1074.jsonl.gz',\n",
       "  '1075.jsonl.gz',\n",
       "  '1076.jsonl.gz',\n",
       "  '1077.jsonl.gz',\n",
       "  '1078.jsonl.gz',\n",
       "  '1079.jsonl.gz',\n",
       "  '108.jsonl.gz',\n",
       "  '1080.jsonl.gz',\n",
       "  '1081.jsonl.gz',\n",
       "  '1082.jsonl.gz',\n",
       "  '1083.jsonl.gz',\n",
       "  '1084.jsonl.gz',\n",
       "  '1085.jsonl.gz',\n",
       "  '1086.jsonl.gz',\n",
       "  '1087.jsonl.gz',\n",
       "  '1088.jsonl.gz',\n",
       "  '1089.jsonl.gz',\n",
       "  '109.jsonl.gz',\n",
       "  '1090.jsonl.gz',\n",
       "  '1091.jsonl.gz',\n",
       "  '1092.jsonl.gz',\n",
       "  '1093.jsonl.gz',\n",
       "  '1094.jsonl.gz',\n",
       "  '1095.jsonl.gz',\n",
       "  '1096.jsonl.gz',\n",
       "  '1097.jsonl.gz',\n",
       "  '1098.jsonl.gz',\n",
       "  '1099.jsonl.gz',\n",
       "  '11.jsonl.gz',\n",
       "  '110.jsonl.gz',\n",
       "  '1100.jsonl.gz',\n",
       "  '1101.jsonl.gz',\n",
       "  '1102.jsonl.gz',\n",
       "  '1103.jsonl.gz',\n",
       "  '1104.jsonl.gz',\n",
       "  '1105.jsonl.gz',\n",
       "  '1106.jsonl.gz',\n",
       "  '1107.jsonl.gz',\n",
       "  '1108.jsonl.gz',\n",
       "  '1109.jsonl.gz',\n",
       "  '111.jsonl.gz',\n",
       "  '1110.jsonl.gz',\n",
       "  '1111.jsonl.gz',\n",
       "  '1112.jsonl.gz',\n",
       "  '1113.jsonl.gz',\n",
       "  '1114.jsonl.gz',\n",
       "  '1115.jsonl.gz',\n",
       "  '1116.jsonl.gz',\n",
       "  '1117.jsonl.gz',\n",
       "  '1118.jsonl.gz',\n",
       "  '1119.jsonl.gz',\n",
       "  '112.jsonl.gz',\n",
       "  '1120.jsonl.gz',\n",
       "  '1121.jsonl.gz',\n",
       "  '1122.jsonl.gz',\n",
       "  '1123.jsonl.gz',\n",
       "  '1124.jsonl.gz',\n",
       "  '1125.jsonl.gz',\n",
       "  '1126.jsonl.gz',\n",
       "  '1127.jsonl.gz',\n",
       "  '1128.jsonl.gz',\n",
       "  '1129.jsonl.gz',\n",
       "  '113.jsonl.gz',\n",
       "  '1130.jsonl.gz',\n",
       "  '1131.jsonl.gz',\n",
       "  '1132.jsonl.gz',\n",
       "  '1133.jsonl.gz',\n",
       "  '1134.jsonl.gz',\n",
       "  '1135.jsonl.gz',\n",
       "  '1136.jsonl.gz',\n",
       "  '1137.jsonl.gz',\n",
       "  '1138.jsonl.gz',\n",
       "  '1139.jsonl.gz',\n",
       "  '114.jsonl.gz',\n",
       "  '1140.jsonl.gz',\n",
       "  '1141.jsonl.gz',\n",
       "  '1142.jsonl.gz',\n",
       "  '1143.jsonl.gz',\n",
       "  '1144.jsonl.gz',\n",
       "  '1145.jsonl.gz',\n",
       "  '1146.jsonl.gz',\n",
       "  '1147.jsonl.gz',\n",
       "  '1148.jsonl.gz',\n",
       "  '1149.jsonl.gz',\n",
       "  '115.jsonl.gz',\n",
       "  '1150.jsonl.gz',\n",
       "  '1151.jsonl.gz',\n",
       "  '1152.jsonl.gz',\n",
       "  '1153.jsonl.gz',\n",
       "  '1154.jsonl.gz',\n",
       "  '1155.jsonl.gz',\n",
       "  '1156.jsonl.gz',\n",
       "  '1157.jsonl.gz',\n",
       "  '1158.jsonl.gz',\n",
       "  '1159.jsonl.gz',\n",
       "  '116.jsonl.gz',\n",
       "  '1160.jsonl.gz',\n",
       "  '1161.jsonl.gz',\n",
       "  '1162.jsonl.gz',\n",
       "  '1163.jsonl.gz',\n",
       "  '1164.jsonl.gz',\n",
       "  '1165.jsonl.gz',\n",
       "  '1166.jsonl.gz',\n",
       "  '1167.jsonl.gz',\n",
       "  '1168.jsonl.gz',\n",
       "  '1169.jsonl.gz',\n",
       "  '117.jsonl.gz',\n",
       "  '1170.jsonl.gz',\n",
       "  '1171.jsonl.gz',\n",
       "  '1172.jsonl.gz',\n",
       "  '1173.jsonl.gz',\n",
       "  '1174.jsonl.gz',\n",
       "  '1175.jsonl.gz',\n",
       "  '1176.jsonl.gz',\n",
       "  '1177.jsonl.gz',\n",
       "  '1178.jsonl.gz',\n",
       "  '1179.jsonl.gz',\n",
       "  '118.jsonl.gz',\n",
       "  '1180.jsonl.gz',\n",
       "  '1181.jsonl.gz',\n",
       "  '1182.jsonl.gz',\n",
       "  '1183.jsonl.gz',\n",
       "  '1184.jsonl.gz',\n",
       "  '1185.jsonl.gz',\n",
       "  '1186.jsonl.gz',\n",
       "  '1187.jsonl.gz',\n",
       "  '1188.jsonl.gz',\n",
       "  '1189.jsonl.gz',\n",
       "  '119.jsonl.gz',\n",
       "  '1190.jsonl.gz',\n",
       "  '1191.jsonl.gz',\n",
       "  '1192.jsonl.gz',\n",
       "  '1193.jsonl.gz',\n",
       "  '1194.jsonl.gz',\n",
       "  '1195.jsonl.gz',\n",
       "  '1196.jsonl.gz',\n",
       "  '1197.jsonl.gz',\n",
       "  '1198.jsonl.gz',\n",
       "  '1199.jsonl.gz',\n",
       "  '12.jsonl.gz',\n",
       "  '120.jsonl.gz',\n",
       "  '1200.jsonl.gz',\n",
       "  '1201.jsonl.gz',\n",
       "  '1202.jsonl.gz',\n",
       "  '1203.jsonl.gz',\n",
       "  '1204.jsonl.gz',\n",
       "  '1205.jsonl.gz',\n",
       "  '1206.jsonl.gz',\n",
       "  '1207.jsonl.gz',\n",
       "  '1208.jsonl.gz',\n",
       "  '1209.jsonl.gz',\n",
       "  '121.jsonl.gz',\n",
       "  '1210.jsonl.gz',\n",
       "  '1211.jsonl.gz',\n",
       "  '1212.jsonl.gz',\n",
       "  '1213.jsonl.gz',\n",
       "  '1214.jsonl.gz',\n",
       "  '1215.jsonl.gz',\n",
       "  '1216.jsonl.gz',\n",
       "  '1217.jsonl.gz',\n",
       "  '1218.jsonl.gz',\n",
       "  '1219.jsonl.gz',\n",
       "  '122.jsonl.gz',\n",
       "  '1220.jsonl.gz',\n",
       "  '1221.jsonl.gz',\n",
       "  '1222.jsonl.gz',\n",
       "  '1223.jsonl.gz',\n",
       "  '1224.jsonl.gz',\n",
       "  '1225.jsonl.gz',\n",
       "  '1226.jsonl.gz',\n",
       "  '1227.jsonl.gz',\n",
       "  '1228.jsonl.gz',\n",
       "  '1229.jsonl.gz',\n",
       "  '123.jsonl.gz',\n",
       "  '1230.jsonl.gz',\n",
       "  '1231.jsonl.gz',\n",
       "  '1232.jsonl.gz',\n",
       "  '1233.jsonl.gz',\n",
       "  '1234.jsonl.gz',\n",
       "  '1235.jsonl.gz',\n",
       "  '1236.jsonl.gz',\n",
       "  '1237.jsonl.gz',\n",
       "  '1238.jsonl.gz',\n",
       "  '1239.jsonl.gz',\n",
       "  '124.jsonl.gz',\n",
       "  '1240.jsonl.gz',\n",
       "  '1241.jsonl.gz',\n",
       "  '1242.jsonl.gz',\n",
       "  '1243.jsonl.gz',\n",
       "  '1244.jsonl.gz',\n",
       "  '1245.jsonl.gz',\n",
       "  '1246.jsonl.gz',\n",
       "  '1247.jsonl.gz',\n",
       "  '1248.jsonl.gz',\n",
       "  '1249.jsonl.gz',\n",
       "  '125.jsonl.gz',\n",
       "  '1250.jsonl.gz',\n",
       "  '1251.jsonl.gz',\n",
       "  '1252.jsonl.gz',\n",
       "  '1253.jsonl.gz',\n",
       "  '1254.jsonl.gz',\n",
       "  '1255.jsonl.gz',\n",
       "  '1256.jsonl.gz',\n",
       "  '1257.jsonl.gz',\n",
       "  '1258.jsonl.gz',\n",
       "  '1259.jsonl.gz',\n",
       "  '126.jsonl.gz',\n",
       "  '1260.jsonl.gz',\n",
       "  '1261.jsonl.gz',\n",
       "  '1262.jsonl.gz',\n",
       "  '1263.jsonl.gz',\n",
       "  '1264.jsonl.gz',\n",
       "  '1265.jsonl.gz',\n",
       "  '1266.jsonl.gz',\n",
       "  '1267.jsonl.gz',\n",
       "  '1268.jsonl.gz',\n",
       "  '1269.jsonl.gz',\n",
       "  '127.jsonl.gz',\n",
       "  '1270.jsonl.gz',\n",
       "  '1271.jsonl.gz',\n",
       "  '1272.jsonl.gz',\n",
       "  '1273.jsonl.gz',\n",
       "  '1274.jsonl.gz',\n",
       "  '1275.jsonl.gz',\n",
       "  '1276.jsonl.gz',\n",
       "  '1277.jsonl.gz',\n",
       "  '1278.jsonl.gz',\n",
       "  '1279.jsonl.gz',\n",
       "  '128.jsonl.gz',\n",
       "  '1280.jsonl.gz',\n",
       "  '1281.jsonl.gz',\n",
       "  '1282.jsonl.gz',\n",
       "  '1283.jsonl.gz',\n",
       "  '1284.jsonl.gz',\n",
       "  '1285.jsonl.gz',\n",
       "  '1286.jsonl.gz',\n",
       "  '1287.jsonl.gz',\n",
       "  '1288.jsonl.gz',\n",
       "  '1289.jsonl.gz',\n",
       "  '129.jsonl.gz',\n",
       "  '1290.jsonl.gz',\n",
       "  '1291.jsonl.gz',\n",
       "  '1292.jsonl.gz',\n",
       "  '1293.jsonl.gz',\n",
       "  '1294.jsonl.gz',\n",
       "  '1295.jsonl.gz',\n",
       "  '1296.jsonl.gz',\n",
       "  '1297.jsonl.gz',\n",
       "  '1298.jsonl.gz',\n",
       "  '1299.jsonl.gz',\n",
       "  '13.jsonl.gz',\n",
       "  '130.jsonl.gz',\n",
       "  '1300.jsonl.gz',\n",
       "  '1301.jsonl.gz',\n",
       "  '1302.jsonl.gz',\n",
       "  '1303.jsonl.gz',\n",
       "  '1304.jsonl.gz',\n",
       "  '1305.jsonl.gz',\n",
       "  '1306.jsonl.gz',\n",
       "  '1307.jsonl.gz',\n",
       "  '1308.jsonl.gz',\n",
       "  '1309.jsonl.gz',\n",
       "  '131.jsonl.gz',\n",
       "  '1310.jsonl.gz',\n",
       "  '1311.jsonl.gz',\n",
       "  '1312.jsonl.gz',\n",
       "  '1313.jsonl.gz',\n",
       "  '1314.jsonl.gz',\n",
       "  '1315.jsonl.gz',\n",
       "  '1316.jsonl.gz',\n",
       "  '1317.jsonl.gz',\n",
       "  '1318.jsonl.gz',\n",
       "  '1319.jsonl.gz',\n",
       "  '132.jsonl.gz',\n",
       "  '1320.jsonl.gz',\n",
       "  '1321.jsonl.gz',\n",
       "  '1322.jsonl.gz',\n",
       "  '1323.jsonl.gz',\n",
       "  '1324.jsonl.gz',\n",
       "  '1325.jsonl.gz',\n",
       "  '1326.jsonl.gz',\n",
       "  '1327.jsonl.gz',\n",
       "  '1328.jsonl.gz',\n",
       "  '1329.jsonl.gz',\n",
       "  '133.jsonl.gz',\n",
       "  '1330.jsonl.gz',\n",
       "  '1331.jsonl.gz',\n",
       "  '1332.jsonl.gz',\n",
       "  '1333.jsonl.gz',\n",
       "  '1334.jsonl.gz',\n",
       "  '1335.jsonl.gz',\n",
       "  '1336.jsonl.gz',\n",
       "  '1337.jsonl.gz',\n",
       "  '1338.jsonl.gz',\n",
       "  '1339.jsonl.gz',\n",
       "  '134.jsonl.gz',\n",
       "  '1340.jsonl.gz',\n",
       "  '1341.jsonl.gz',\n",
       "  '1342.jsonl.gz',\n",
       "  '1343.jsonl.gz',\n",
       "  '1344.jsonl.gz',\n",
       "  '1345.jsonl.gz',\n",
       "  '1346.jsonl.gz',\n",
       "  '1347.jsonl.gz',\n",
       "  '1348.jsonl.gz',\n",
       "  '1349.jsonl.gz',\n",
       "  '135.jsonl.gz',\n",
       "  '1350.jsonl.gz',\n",
       "  '1351.jsonl.gz',\n",
       "  '1352.jsonl.gz',\n",
       "  '1353.jsonl.gz',\n",
       "  '1354.jsonl.gz',\n",
       "  '1355.jsonl.gz',\n",
       "  '1356.jsonl.gz',\n",
       "  '1357.jsonl.gz',\n",
       "  '1358.jsonl.gz',\n",
       "  '1359.jsonl.gz',\n",
       "  '136.jsonl.gz',\n",
       "  '1360.jsonl.gz',\n",
       "  '1361.jsonl.gz',\n",
       "  '1362.jsonl.gz',\n",
       "  '1363.jsonl.gz',\n",
       "  '1364.jsonl.gz',\n",
       "  '1365.jsonl.gz',\n",
       "  '1366.jsonl.gz',\n",
       "  '1367.jsonl.gz',\n",
       "  '1368.jsonl.gz',\n",
       "  '1369.jsonl.gz',\n",
       "  '137.jsonl.gz',\n",
       "  '1370.jsonl.gz',\n",
       "  '1371.jsonl.gz',\n",
       "  '1372.jsonl.gz',\n",
       "  '1373.jsonl.gz',\n",
       "  '1374.jsonl.gz',\n",
       "  '1375.jsonl.gz',\n",
       "  '1376.jsonl.gz',\n",
       "  '1377.jsonl.gz',\n",
       "  '1378.jsonl.gz',\n",
       "  '1379.jsonl.gz',\n",
       "  '138.jsonl.gz',\n",
       "  '1380.jsonl.gz',\n",
       "  '1381.jsonl.gz',\n",
       "  '1382.jsonl.gz',\n",
       "  '1383.jsonl.gz',\n",
       "  '1384.jsonl.gz',\n",
       "  '1385.jsonl.gz',\n",
       "  '1386.jsonl.gz',\n",
       "  '1387.jsonl.gz',\n",
       "  '1388.jsonl.gz',\n",
       "  '1389.jsonl.gz',\n",
       "  '139.jsonl.gz',\n",
       "  '1390.jsonl.gz',\n",
       "  '1391.jsonl.gz',\n",
       "  '1392.jsonl.gz',\n",
       "  '1393.jsonl.gz',\n",
       "  '1394.jsonl.gz',\n",
       "  '1395.jsonl.gz',\n",
       "  '1396.jsonl.gz',\n",
       "  '1397.jsonl.gz',\n",
       "  '1398.jsonl.gz',\n",
       "  '1399.jsonl.gz',\n",
       "  '14.jsonl.gz',\n",
       "  '140.jsonl.gz',\n",
       "  '1400.jsonl.gz',\n",
       "  '1401.jsonl.gz',\n",
       "  '1402.jsonl.gz',\n",
       "  '1403.jsonl.gz',\n",
       "  '1404.jsonl.gz',\n",
       "  '1405.jsonl.gz',\n",
       "  '1406.jsonl.gz',\n",
       "  '1407.jsonl.gz',\n",
       "  '1408.jsonl.gz',\n",
       "  '1409.jsonl.gz',\n",
       "  '141.jsonl.gz',\n",
       "  '1410.jsonl.gz',\n",
       "  '1411.jsonl.gz',\n",
       "  '1412.jsonl.gz',\n",
       "  '1413.jsonl.gz',\n",
       "  '1414.jsonl.gz',\n",
       "  '1415.jsonl.gz',\n",
       "  '1416.jsonl.gz',\n",
       "  '1417.jsonl.gz',\n",
       "  '1418.jsonl.gz',\n",
       "  '1419.jsonl.gz',\n",
       "  '142.jsonl.gz',\n",
       "  '1420.jsonl.gz',\n",
       "  '1421.jsonl.gz',\n",
       "  '1422.jsonl.gz',\n",
       "  '1423.jsonl.gz',\n",
       "  '1424.jsonl.gz',\n",
       "  '1425.jsonl.gz',\n",
       "  '1426.jsonl.gz',\n",
       "  '1427.jsonl.gz',\n",
       "  '1428.jsonl.gz',\n",
       "  '1429.jsonl.gz',\n",
       "  '143.jsonl.gz',\n",
       "  '1430.jsonl.gz',\n",
       "  '1431.jsonl.gz',\n",
       "  '1432.jsonl.gz',\n",
       "  '1433.jsonl.gz',\n",
       "  '1434.jsonl.gz',\n",
       "  '1435.jsonl.gz',\n",
       "  '1436.jsonl.gz',\n",
       "  '1437.jsonl.gz',\n",
       "  '1438.jsonl.gz',\n",
       "  '1439.jsonl.gz',\n",
       "  '144.jsonl.gz',\n",
       "  '1440.jsonl.gz',\n",
       "  '1441.jsonl.gz',\n",
       "  '1442.jsonl.gz',\n",
       "  '1443.jsonl.gz',\n",
       "  '1444.jsonl.gz',\n",
       "  '1445.jsonl.gz',\n",
       "  '1446.jsonl.gz',\n",
       "  '1447.jsonl.gz',\n",
       "  '1448.jsonl.gz',\n",
       "  '1449.jsonl.gz',\n",
       "  '145.jsonl.gz',\n",
       "  '1450.jsonl.gz',\n",
       "  '1451.jsonl.gz',\n",
       "  '1452.jsonl.gz',\n",
       "  '1453.jsonl.gz',\n",
       "  '1454.jsonl.gz',\n",
       "  '1455.jsonl.gz',\n",
       "  '1456.jsonl.gz',\n",
       "  '1457.jsonl.gz',\n",
       "  '1458.jsonl.gz',\n",
       "  '1459.jsonl.gz',\n",
       "  '146.jsonl.gz',\n",
       "  '1460.jsonl.gz',\n",
       "  '1461.jsonl.gz',\n",
       "  '1462.jsonl.gz',\n",
       "  '1463.jsonl.gz',\n",
       "  '1464.jsonl.gz',\n",
       "  '1465.jsonl.gz',\n",
       "  '1466.jsonl.gz',\n",
       "  '1467.jsonl.gz',\n",
       "  '1468.jsonl.gz',\n",
       "  '1469.jsonl.gz',\n",
       "  '147.jsonl.gz',\n",
       "  '1470.jsonl.gz',\n",
       "  '1471.jsonl.gz',\n",
       "  '1472.jsonl.gz',\n",
       "  '1473.jsonl.gz',\n",
       "  '1474.jsonl.gz',\n",
       "  '1475.jsonl.gz',\n",
       "  '1476.jsonl.gz',\n",
       "  '1477.jsonl.gz',\n",
       "  '1478.jsonl.gz',\n",
       "  '1479.jsonl.gz',\n",
       "  '148.jsonl.gz',\n",
       "  '1480.jsonl.gz',\n",
       "  '1481.jsonl.gz',\n",
       "  '1482.jsonl.gz',\n",
       "  '1483.jsonl.gz',\n",
       "  '1484.jsonl.gz',\n",
       "  '1485.jsonl.gz',\n",
       "  '1486.jsonl.gz',\n",
       "  '1487.jsonl.gz',\n",
       "  '1488.jsonl.gz',\n",
       "  '1489.jsonl.gz',\n",
       "  '149.jsonl.gz',\n",
       "  '1490.jsonl.gz',\n",
       "  '1491.jsonl.gz',\n",
       "  '1492.jsonl.gz',\n",
       "  '1493.jsonl.gz',\n",
       "  '1494.jsonl.gz',\n",
       "  '1495.jsonl.gz',\n",
       "  '1496.jsonl.gz',\n",
       "  '1497.jsonl.gz',\n",
       "  '1498.jsonl.gz',\n",
       "  '1499.jsonl.gz',\n",
       "  '15.jsonl.gz',\n",
       "  '150.jsonl.gz',\n",
       "  '1500.jsonl.gz',\n",
       "  '1501.jsonl.gz',\n",
       "  '1502.jsonl.gz',\n",
       "  '1503.jsonl.gz',\n",
       "  '1504.jsonl.gz',\n",
       "  '1505.jsonl.gz',\n",
       "  '1506.jsonl.gz',\n",
       "  '1507.jsonl.gz',\n",
       "  '1508.jsonl.gz',\n",
       "  '1509.jsonl.gz',\n",
       "  '151.jsonl.gz',\n",
       "  '1510.jsonl.gz',\n",
       "  '1511.jsonl.gz',\n",
       "  '1512.jsonl.gz',\n",
       "  '1513.jsonl.gz',\n",
       "  '1514.jsonl.gz',\n",
       "  '1515.jsonl.gz',\n",
       "  '1516.jsonl.gz',\n",
       "  '1517.jsonl.gz',\n",
       "  '1518.jsonl.gz',\n",
       "  '1519.jsonl.gz',\n",
       "  '152.jsonl.gz',\n",
       "  '1520.jsonl.gz',\n",
       "  '1521.jsonl.gz',\n",
       "  '1522.jsonl.gz',\n",
       "  '1523.jsonl.gz',\n",
       "  '1524.jsonl.gz',\n",
       "  '1525.jsonl.gz',\n",
       "  '1526.jsonl.gz',\n",
       "  '1527.jsonl.gz',\n",
       "  '1528.jsonl.gz',\n",
       "  '1529.jsonl.gz',\n",
       "  '153.jsonl.gz',\n",
       "  '1530.jsonl.gz',\n",
       "  '1531.jsonl.gz',\n",
       "  '1532.jsonl.gz',\n",
       "  '1533.jsonl.gz',\n",
       "  '1534.jsonl.gz',\n",
       "  '1535.jsonl.gz',\n",
       "  '1536.jsonl.gz',\n",
       "  '1537.jsonl.gz',\n",
       "  '1538.jsonl.gz',\n",
       "  '1539.jsonl.gz',\n",
       "  '154.jsonl.gz',\n",
       "  '1540.jsonl.gz',\n",
       "  '1541.jsonl.gz',\n",
       "  '1542.jsonl.gz',\n",
       "  '1543.jsonl.gz',\n",
       "  '1544.jsonl.gz',\n",
       "  '1545.jsonl.gz',\n",
       "  '1546.jsonl.gz',\n",
       "  '1547.jsonl.gz',\n",
       "  '1548.jsonl.gz',\n",
       "  '1549.jsonl.gz',\n",
       "  '155.jsonl.gz',\n",
       "  '1550.jsonl.gz',\n",
       "  '1551.jsonl.gz',\n",
       "  '1552.jsonl.gz',\n",
       "  '1553.jsonl.gz',\n",
       "  '1554.jsonl.gz',\n",
       "  '1555.jsonl.gz',\n",
       "  '1556.jsonl.gz',\n",
       "  '1557.jsonl.gz',\n",
       "  '1558.jsonl.gz',\n",
       "  '1559.jsonl.gz',\n",
       "  '156.jsonl.gz',\n",
       "  '1560.jsonl.gz',\n",
       "  '1561.jsonl.gz',\n",
       "  '1562.jsonl.gz',\n",
       "  '1563.jsonl.gz',\n",
       "  '1564.jsonl.gz',\n",
       "  '1565.jsonl.gz',\n",
       "  '1566.jsonl.gz',\n",
       "  '1567.jsonl.gz',\n",
       "  '1568.jsonl.gz',\n",
       "  '1569.jsonl.gz',\n",
       "  '157.jsonl.gz',\n",
       "  '1570.jsonl.gz',\n",
       "  '1571.jsonl.gz',\n",
       "  '1572.jsonl.gz',\n",
       "  '1573.jsonl.gz',\n",
       "  '1574.jsonl.gz',\n",
       "  '1575.jsonl.gz',\n",
       "  '1576.jsonl.gz',\n",
       "  '1577.jsonl.gz',\n",
       "  '1578.jsonl.gz',\n",
       "  '1579.jsonl.gz',\n",
       "  '158.jsonl.gz',\n",
       "  '1580.jsonl.gz',\n",
       "  '1581.jsonl.gz',\n",
       "  '1582.jsonl.gz',\n",
       "  '1583.jsonl.gz',\n",
       "  '1584.jsonl.gz',\n",
       "  '1585.jsonl.gz',\n",
       "  '1586.jsonl.gz',\n",
       "  '1587.jsonl.gz',\n",
       "  '1588.jsonl.gz',\n",
       "  '1589.jsonl.gz',\n",
       "  '159.jsonl.gz',\n",
       "  '1590.jsonl.gz',\n",
       "  '1591.jsonl.gz',\n",
       "  '1592.jsonl.gz',\n",
       "  '1593.jsonl.gz',\n",
       "  '1594.jsonl.gz',\n",
       "  '1595.jsonl.gz',\n",
       "  '1596.jsonl.gz',\n",
       "  '1597.jsonl.gz',\n",
       "  '1598.jsonl.gz',\n",
       "  '1599.jsonl.gz',\n",
       "  '16.jsonl.gz',\n",
       "  '160.jsonl.gz',\n",
       "  '1600.jsonl.gz',\n",
       "  '1601.jsonl.gz',\n",
       "  '1602.jsonl.gz',\n",
       "  '1603.jsonl.gz',\n",
       "  '1604.jsonl.gz',\n",
       "  '1605.jsonl.gz',\n",
       "  '1606.jsonl.gz',\n",
       "  '1607.jsonl.gz',\n",
       "  '1608.jsonl.gz',\n",
       "  '1609.jsonl.gz',\n",
       "  '161.jsonl.gz',\n",
       "  '1610.jsonl.gz',\n",
       "  '1611.jsonl.gz',\n",
       "  '1612.jsonl.gz',\n",
       "  '1613.jsonl.gz',\n",
       "  '1614.jsonl.gz',\n",
       "  '1615.jsonl.gz',\n",
       "  '1616.jsonl.gz',\n",
       "  '1617.jsonl.gz',\n",
       "  '1618.jsonl.gz',\n",
       "  '1619.jsonl.gz',\n",
       "  '162.jsonl.gz',\n",
       "  '1620.jsonl.gz',\n",
       "  '1621.jsonl.gz',\n",
       "  '1622.jsonl.gz',\n",
       "  '1623.jsonl.gz',\n",
       "  '1624.jsonl.gz',\n",
       "  '1625.jsonl.gz',\n",
       "  '1626.jsonl.gz',\n",
       "  '1627.jsonl.gz',\n",
       "  '1628.jsonl.gz',\n",
       "  '1629.jsonl.gz',\n",
       "  '163.jsonl.gz',\n",
       "  '1630.jsonl.gz',\n",
       "  '1631.jsonl.gz',\n",
       "  '1632.jsonl.gz',\n",
       "  '1633.jsonl.gz',\n",
       "  '1634.jsonl.gz',\n",
       "  '1635.jsonl.gz',\n",
       "  '1636.jsonl.gz',\n",
       "  '1637.jsonl.gz',\n",
       "  '1638.jsonl.gz',\n",
       "  '1639.jsonl.gz',\n",
       "  '164.jsonl.gz',\n",
       "  '1640.jsonl.gz',\n",
       "  '1641.jsonl.gz',\n",
       "  '1642.jsonl.gz',\n",
       "  '1643.jsonl.gz',\n",
       "  '1644.jsonl.gz',\n",
       "  '1645.jsonl.gz',\n",
       "  '1646.jsonl.gz',\n",
       "  '1647.jsonl.gz',\n",
       "  '1648.jsonl.gz',\n",
       "  '1649.jsonl.gz',\n",
       "  '165.jsonl.gz',\n",
       "  '1650.jsonl.gz',\n",
       "  '1651.jsonl.gz',\n",
       "  '1652.jsonl.gz',\n",
       "  '1653.jsonl.gz',\n",
       "  '1654.jsonl.gz',\n",
       "  '1655.jsonl.gz',\n",
       "  '1656.jsonl.gz',\n",
       "  '1657.jsonl.gz',\n",
       "  '1658.jsonl.gz',\n",
       "  '1659.jsonl.gz',\n",
       "  '166.jsonl.gz',\n",
       "  '1660.jsonl.gz',\n",
       "  '1661.jsonl.gz',\n",
       "  '1662.jsonl.gz',\n",
       "  '1663.jsonl.gz',\n",
       "  '1664.jsonl.gz',\n",
       "  '1665.jsonl.gz',\n",
       "  '1666.jsonl.gz',\n",
       "  '1667.jsonl.gz',\n",
       "  '1668.jsonl.gz',\n",
       "  '1669.jsonl.gz',\n",
       "  '167.jsonl.gz',\n",
       "  '1670.jsonl.gz',\n",
       "  '1671.jsonl.gz',\n",
       "  '1672.jsonl.gz',\n",
       "  '1673.jsonl.gz',\n",
       "  '1674.jsonl.gz',\n",
       "  '1675.jsonl.gz',\n",
       "  '1676.jsonl.gz',\n",
       "  '1677.jsonl.gz',\n",
       "  '1678.jsonl.gz',\n",
       "  '1679.jsonl.gz',\n",
       "  '168.jsonl.gz',\n",
       "  '1680.jsonl.gz',\n",
       "  '1681.jsonl.gz',\n",
       "  '1682.jsonl.gz',\n",
       "  '1683.jsonl.gz',\n",
       "  '1684.jsonl.gz',\n",
       "  '1685.jsonl.gz',\n",
       "  '1686.jsonl.gz',\n",
       "  '1687.jsonl.gz',\n",
       "  '1688.jsonl.gz',\n",
       "  '1689.jsonl.gz',\n",
       "  '169.jsonl.gz',\n",
       "  '1690.jsonl.gz',\n",
       "  '1691.jsonl.gz',\n",
       "  '1692.jsonl.gz',\n",
       "  '1693.jsonl.gz',\n",
       "  '1694.jsonl.gz',\n",
       "  '1695.jsonl.gz',\n",
       "  '1696.jsonl.gz',\n",
       "  '1697.jsonl.gz',\n",
       "  '1698.jsonl.gz',\n",
       "  '1699.jsonl.gz',\n",
       "  '17.jsonl.gz',\n",
       "  '170.jsonl.gz',\n",
       "  '1700.jsonl.gz',\n",
       "  '1701.jsonl.gz',\n",
       "  '1702.jsonl.gz',\n",
       "  '1703.jsonl.gz',\n",
       "  '1704.jsonl.gz',\n",
       "  '1705.jsonl.gz',\n",
       "  '1706.jsonl.gz',\n",
       "  '1707.jsonl.gz',\n",
       "  '1708.jsonl.gz',\n",
       "  '1709.jsonl.gz',\n",
       "  '171.jsonl.gz',\n",
       "  '1710.jsonl.gz',\n",
       "  '1711.jsonl.gz',\n",
       "  '1712.jsonl.gz',\n",
       "  '1713.jsonl.gz',\n",
       "  '1714.jsonl.gz',\n",
       "  '1715.jsonl.gz',\n",
       "  '1716.jsonl.gz',\n",
       "  '1717.jsonl.gz',\n",
       "  '1718.jsonl.gz',\n",
       "  '1719.jsonl.gz',\n",
       "  '172.jsonl.gz',\n",
       "  '1720.jsonl.gz',\n",
       "  '1721.jsonl.gz',\n",
       "  '1722.jsonl.gz',\n",
       "  '1723.jsonl.gz',\n",
       "  '1724.jsonl.gz',\n",
       "  '1725.jsonl.gz',\n",
       "  '1726.jsonl.gz',\n",
       "  '1727.jsonl.gz',\n",
       "  '1728.jsonl.gz',\n",
       "  '1729.jsonl.gz',\n",
       "  '173.jsonl.gz',\n",
       "  '1730.jsonl.gz',\n",
       "  '1731.jsonl.gz',\n",
       "  '1732.jsonl.gz',\n",
       "  '1733.jsonl.gz',\n",
       "  '1734.jsonl.gz',\n",
       "  '1735.jsonl.gz',\n",
       "  '1736.jsonl.gz',\n",
       "  '1737.jsonl.gz',\n",
       "  '1738.jsonl.gz',\n",
       "  '1739.jsonl.gz',\n",
       "  '174.jsonl.gz',\n",
       "  '1740.jsonl.gz',\n",
       "  '1741.jsonl.gz',\n",
       "  '1742.jsonl.gz',\n",
       "  '1743.jsonl.gz',\n",
       "  '1744.jsonl.gz',\n",
       "  '1745.jsonl.gz',\n",
       "  '1746.jsonl.gz',\n",
       "  '1747.jsonl.gz',\n",
       "  '1748.jsonl.gz',\n",
       "  '1749.jsonl.gz',\n",
       "  '175.jsonl.gz',\n",
       "  '1750.jsonl.gz',\n",
       "  '1751.jsonl.gz',\n",
       "  '1752.jsonl.gz',\n",
       "  '1753.jsonl.gz',\n",
       "  '1754.jsonl.gz',\n",
       "  '1755.jsonl.gz',\n",
       "  '1756.jsonl.gz',\n",
       "  '1757.jsonl.gz',\n",
       "  '1758.jsonl.gz',\n",
       "  '1759.jsonl.gz',\n",
       "  '176.jsonl.gz',\n",
       "  '1760.jsonl.gz',\n",
       "  '1761.jsonl.gz',\n",
       "  '1762.jsonl.gz',\n",
       "  '1763.jsonl.gz',\n",
       "  '1764.jsonl.gz',\n",
       "  '1765.jsonl.gz',\n",
       "  '1766.jsonl.gz',\n",
       "  '1767.jsonl.gz',\n",
       "  '1768.jsonl.gz',\n",
       "  '1769.jsonl.gz',\n",
       "  '177.jsonl.gz',\n",
       "  '1770.jsonl.gz',\n",
       "  '1771.jsonl.gz',\n",
       "  '1772.jsonl.gz',\n",
       "  '1773.jsonl.gz',\n",
       "  '1774.jsonl.gz',\n",
       "  '1775.jsonl.gz',\n",
       "  '1776.jsonl.gz',\n",
       "  '1777.jsonl.gz',\n",
       "  '1778.jsonl.gz',\n",
       "  '1779.jsonl.gz',\n",
       "  '178.jsonl.gz',\n",
       "  '1780.jsonl.gz',\n",
       "  '1781.jsonl.gz',\n",
       "  '1782.jsonl.gz',\n",
       "  '1783.jsonl.gz',\n",
       "  '1784.jsonl.gz',\n",
       "  '1785.jsonl.gz',\n",
       "  '1786.jsonl.gz',\n",
       "  '1787.jsonl.gz',\n",
       "  '1788.jsonl.gz',\n",
       "  '1789.jsonl.gz',\n",
       "  '179.jsonl.gz',\n",
       "  '1790.jsonl.gz',\n",
       "  '1791.jsonl.gz',\n",
       "  '1792.jsonl.gz',\n",
       "  '1793.jsonl.gz',\n",
       "  '1794.jsonl.gz',\n",
       "  '1795.jsonl.gz',\n",
       "  '1796.jsonl.gz',\n",
       "  '1797.jsonl.gz',\n",
       "  '1798.jsonl.gz',\n",
       "  '1799.jsonl.gz',\n",
       "  '18.jsonl.gz',\n",
       "  '180.jsonl.gz',\n",
       "  '1800.jsonl.gz',\n",
       "  '1801.jsonl.gz',\n",
       "  '1802.jsonl.gz',\n",
       "  '1803.jsonl.gz',\n",
       "  '1804.jsonl.gz',\n",
       "  '1805.jsonl.gz',\n",
       "  '1806.jsonl.gz',\n",
       "  '1807.jsonl.gz',\n",
       "  '1808.jsonl.gz',\n",
       "  '1809.jsonl.gz',\n",
       "  '181.jsonl.gz',\n",
       "  '1810.jsonl.gz',\n",
       "  '1811.jsonl.gz',\n",
       "  '1812.jsonl.gz',\n",
       "  '1813.jsonl.gz',\n",
       "  '1814.jsonl.gz',\n",
       "  '1815.jsonl.gz',\n",
       "  '1816.jsonl.gz',\n",
       "  '1817.jsonl.gz',\n",
       "  '1818.jsonl.gz',\n",
       "  '1819.jsonl.gz',\n",
       "  '182.jsonl.gz',\n",
       "  '1820.jsonl.gz',\n",
       "  '1821.jsonl.gz',\n",
       "  '1822.jsonl.gz',\n",
       "  '1823.jsonl.gz',\n",
       "  '1824.jsonl.gz',\n",
       "  '1825.jsonl.gz',\n",
       "  '1826.jsonl.gz',\n",
       "  '1827.jsonl.gz',\n",
       "  '1828.jsonl.gz',\n",
       "  '1829.jsonl.gz',\n",
       "  '183.jsonl.gz',\n",
       "  '1830.jsonl.gz',\n",
       "  '1831.jsonl.gz',\n",
       "  '1832.jsonl.gz',\n",
       "  '1833.jsonl.gz',\n",
       "  '1834.jsonl.gz',\n",
       "  '1835.jsonl.gz',\n",
       "  '1836.jsonl.gz',\n",
       "  '1837.jsonl.gz',\n",
       "  '1838.jsonl.gz',\n",
       "  '1839.jsonl.gz',\n",
       "  '184.jsonl.gz',\n",
       "  '1840.jsonl.gz',\n",
       "  '1841.jsonl.gz',\n",
       "  '1842.jsonl.gz',\n",
       "  '1843.jsonl.gz',\n",
       "  '1844.jsonl.gz',\n",
       "  '1845.jsonl.gz',\n",
       "  '1846.jsonl.gz',\n",
       "  '1847.jsonl.gz',\n",
       "  '1848.jsonl.gz',\n",
       "  '1849.jsonl.gz',\n",
       "  '185.jsonl.gz',\n",
       "  '1850.jsonl.gz',\n",
       "  '1851.jsonl.gz',\n",
       "  '1852.jsonl.gz',\n",
       "  '1853.jsonl.gz',\n",
       "  '1854.jsonl.gz',\n",
       "  '1855.jsonl.gz',\n",
       "  '1856.jsonl.gz',\n",
       "  '1857.jsonl.gz',\n",
       "  '1858.jsonl.gz',\n",
       "  '1859.jsonl.gz',\n",
       "  '186.jsonl.gz',\n",
       "  '1860.jsonl.gz',\n",
       "  '1861.jsonl.gz',\n",
       "  '1862.jsonl.gz',\n",
       "  '1863.jsonl.gz',\n",
       "  '1864.jsonl.gz',\n",
       "  '1865.jsonl.gz',\n",
       "  '1866.jsonl.gz',\n",
       "  '1867.jsonl.gz',\n",
       "  '1868.jsonl.gz',\n",
       "  '1869.jsonl.gz',\n",
       "  '187.jsonl.gz',\n",
       "  '1870.jsonl.gz',\n",
       "  '1871.jsonl.gz',\n",
       "  '1872.jsonl.gz',\n",
       "  '1873.jsonl.gz',\n",
       "  '1874.jsonl.gz',\n",
       "  '1875.jsonl.gz',\n",
       "  '1876.jsonl.gz',\n",
       "  '1877.jsonl.gz',\n",
       "  '1878.jsonl.gz',\n",
       "  '1879.jsonl.gz',\n",
       "  '188.jsonl.gz',\n",
       "  '1880.jsonl.gz',\n",
       "  '1881.jsonl.gz',\n",
       "  '1882.jsonl.gz',\n",
       "  '1883.jsonl.gz',\n",
       "  '1884.jsonl.gz',\n",
       "  '1885.jsonl.gz',\n",
       "  '1886.jsonl.gz',\n",
       "  '1887.jsonl.gz',\n",
       "  '1888.jsonl.gz',\n",
       "  '1889.jsonl.gz',\n",
       "  '189.jsonl.gz',\n",
       "  '1890.jsonl.gz',\n",
       "  '1891.jsonl.gz',\n",
       "  '1892.jsonl.gz',\n",
       "  '1893.jsonl.gz',\n",
       "  '1894.jsonl.gz',\n",
       "  '1895.jsonl.gz',\n",
       "  '1896.jsonl.gz',\n",
       "  '1897.jsonl.gz',\n",
       "  '1898.jsonl.gz',\n",
       "  ...])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_zips),f_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s2orc-master.zip']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in f_zips if '.gz' not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'b0',\n",
       "  'title': 'Tilt change recorded by broadband seismometer prior to 476 small phreatic explosion of Meakan-dake volcano',\n",
       "  'authors': [{'first': 'H', 'middle': [], 'last': 'Aoyama', 'suffix': ''},\n",
       "   {'first': 'H', 'middle': [], 'last': 'Oshima', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'Geophys. Res. Lett',\n",
       "  'volume': '477',\n",
       "  'issn': '6',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF1': {'ref_id': 'b1',\n",
       "  'title': 'Very long period conduit oscillations induced by rockfalls at 479',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF4': {'ref_id': 'b4',\n",
       "  'title': 'Source mechanisms of explosions at Stromboli Volcano',\n",
       "  'authors': [{'first': 'G', 'middle': [], 'last': 'Milana', 'suffix': ''},\n",
       "   {'first': 'R', 'middle': [], 'last': 'Scarpa', 'suffix': ''}],\n",
       "  'year': 2003,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF5': {'ref_id': 'b5',\n",
       "  'title': 'determined from moment-tensor inversions of very-long-period data',\n",
       "  'authors': [{'first': '', 'middle': [], 'last': 'Italy', 'suffix': ''}],\n",
       "  'year': None,\n",
       "  'venue': 'J. Geophys. 483 Res',\n",
       "  'volume': '108',\n",
       "  'issn': 'B1',\n",
       "  'pages': '',\n",
       "  'other_ids': {'doi': ['10.1029/2002JB001919']},\n",
       "  'links': None},\n",
       " 'BIBREF6': {'ref_id': 'b6',\n",
       "  'title': 'A multi-decadal view of seismic methods for detecting 485 precursors of magma movement and eruption',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'R', 'middle': ['S'], 'last': 'Matoza', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': 'J. Volcanol. Geotherm. Res',\n",
       "  'volume': '252',\n",
       "  'issn': '',\n",
       "  'pages': '108--175',\n",
       "  'other_ids': {'doi': ['10.1016/j.jvolgeores.2012.11.013']},\n",
       "  'links': '129636416'},\n",
       " 'BIBREF7': {'ref_id': 'b7',\n",
       "  'title': 'Network Sensitivity Solutions for Regional 488 Moment-Tensor Inversions',\n",
       "  'authors': [{'first': 'S', 'middle': ['R'], 'last': 'Ford', 'suffix': ''},\n",
       "   {'first': 'D', 'middle': ['S'], 'last': 'Dreger', 'suffix': ''},\n",
       "   {'first': 'W', 'middle': ['R'], 'last': 'Walter', 'suffix': ''}],\n",
       "  'year': 1962,\n",
       "  'venue': 'Bull. Seism. Soc. Am',\n",
       "  'volume': '100',\n",
       "  'issn': '5A',\n",
       "  'pages': '',\n",
       "  'other_ids': {'doi': ['10.1785/0120090140']},\n",
       "  'links': '73647128'},\n",
       " 'BIBREF8': {'ref_id': 'b8',\n",
       "  'title': 'Classical Mechanics',\n",
       "  'authors': [{'first': 'H', 'middle': [], 'last': 'Goldstein', 'suffix': ''},\n",
       "   {'first': 'C', 'middle': ['P J'], 'last': 'Poole', 'suffix': ''},\n",
       "   {'first': 'L', 'middle': ['L'], 'last': 'Safko', 'suffix': ''}],\n",
       "  'year': 2001,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF9': {'ref_id': 'b9',\n",
       "  'title': 'Dynamics of explosive volcanism at Fuego volcano imaged 492 with very long period seismicity',\n",
       "  'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "   {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '116',\n",
       "  'issn': 'B9',\n",
       "  'pages': '',\n",
       "  'other_ids': {'doi': ['10.1029/2011jb008521']},\n",
       "  'links': '129058472'},\n",
       " 'BIBREF10': {'ref_id': 'b10',\n",
       "  'title': 'Tilt prior to explosions and the 494 effect of topography on ultra-long-period seismic records at Fuego volcano',\n",
       "  'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "   {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "   {'first': 'M', 'middle': [], 'last': 'Ichihara', 'suffix': ''},\n",
       "   {'first': 'J', 'middle': ['M'], 'last': 'Lees', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF12': {'ref_id': 'b12',\n",
       "  'title': 'Very-long-period pulses at Asama volcano, central Japan, 497 inferred from dense seismic observations',\n",
       "  'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "   {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '185',\n",
       "  'issn': '1',\n",
       "  'pages': '265--282',\n",
       "  'other_ids': {'doi': ['10.1111/j.1365-246X.2011.04938.x']},\n",
       "  'links': '131264104'},\n",
       " 'BIBREF13': {'ref_id': 'b13',\n",
       "  'title': 'A waveform inversion including tilt: method and 500 simple tests',\n",
       "  'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "   {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''},\n",
       "   {'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '184',\n",
       "  'issn': '2',\n",
       "  'pages': '907--918',\n",
       "  'other_ids': {'doi': ['10.1111/j.1365-246X.2010.04892.x']},\n",
       "  'links': None},\n",
       " 'BIBREF15': {'ref_id': 'b15',\n",
       "  'title': 'Source mechanism of small long-period events at 503',\n",
       "  'authors': [{'first': 'T', 'middle': ['D'], 'last': 'Moran', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Mikesell', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF16': {'ref_id': 'b16',\n",
       "  'title': 'Helens in July 2005 using template matching, phase-weighted stacking, and 504 full-waveform inversion',\n",
       "  'authors': [{'first': 'Mount', 'middle': [], 'last': 'St', 'suffix': ''}],\n",
       "  'year': None,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '120',\n",
       "  'issn': '9',\n",
       "  'pages': '6351--6364',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF17': {'ref_id': 'b17',\n",
       "  'title': 'Geophysical Data Analysis: Discrete Inverse Theory',\n",
       "  'authors': [{'first': 'W', 'middle': [], 'last': 'Menke', 'suffix': ''}],\n",
       "  'year': 1989,\n",
       "  'venue': '',\n",
       "  'volume': '507',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '117957106'},\n",
       " 'BIBREF18': {'ref_id': 'b18',\n",
       "  'title': 'A free-surface boundary condition for including 3D 509 topography in the finite-difference method',\n",
       "  'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "   {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''}],\n",
       "  'year': 1997,\n",
       "  'venue': 'Bull. Seism. Soc. Am',\n",
       "  'volume': '87',\n",
       "  'issn': '2',\n",
       "  'pages': '494--515',\n",
       "  'other_ids': {},\n",
       "  'links': '129694378'},\n",
       " 'BIBREF19': {'ref_id': 'b19',\n",
       "  'title': 'Waveform inversion of very long 511 period impulsive signals associated with magmatic injection beneath Kilauea Volcano',\n",
       "  'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "   {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''},\n",
       "   {'first': 'S', 'middle': [], 'last': 'Kedar', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF21': {'ref_id': 'b21',\n",
       "  'title': 'The response of the horizontal pendulum seismometer to Rayleigh and 514 Love waves, tilt, and free oscillations of the Earth',\n",
       "  'authors': [{'first': 'P', 'middle': ['W'], 'last': 'Rogers', 'suffix': ''}],\n",
       "  'year': 1968,\n",
       "  'venue': 'Bull. Seism. Soc. Am',\n",
       "  'volume': '58',\n",
       "  'issn': '5',\n",
       "  'pages': '1384--515',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF22': {'ref_id': 'b22',\n",
       "  'title': 'Noise reduction and detection of weak, coherent signals 517 through phase-weighted stacks',\n",
       "  'authors': [{'first': 'M', 'middle': [], 'last': 'Schimmel', 'suffix': ''},\n",
       "   {'first': 'H', 'middle': [], 'last': 'Paulssen', 'suffix': ''}],\n",
       "  'year': 1997,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '130',\n",
       "  'issn': '',\n",
       "  'pages': '497--505',\n",
       "  'other_ids': {},\n",
       "  'links': '41492712'},\n",
       " 'BIBREF23': {'ref_id': 'b23',\n",
       "  'title': 'A geometric setting for moment tensors',\n",
       "  'authors': [{'first': 'W', 'middle': [], 'last': 'Tape', 'suffix': ''},\n",
       "   {'first': 'C', 'middle': [], 'last': 'Tape', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '190',\n",
       "  'issn': '',\n",
       "  'pages': '476--498',\n",
       "  'other_ids': {'doi': ['10.1111/j.1365-246X.2012.05491.x']},\n",
       "  'links': '122638175'},\n",
       " 'BIBREF24': {'ref_id': 'b24',\n",
       "  'title': 'Very-Long-Period Seismicity at Active Volcanoes: Source Mechanisms',\n",
       "  'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': '522 Encyclopedia of Earthquake Engineering',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '135418931'},\n",
       " 'BIBREF26': {'ref_id': 'b26',\n",
       "  'title': 'Eruption dynamics at Mount St. Helens 526 imaged from broadband seismic waveforms: Interaction of the shallow magmatic and 527 hydrothermal systems',\n",
       "  'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "   {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': ['B'], 'last': 'Dawson', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '113',\n",
       "  'issn': 'B2',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '14550325'},\n",
       " 'BIBREF27': {'ref_id': 'b27',\n",
       "  'title': 'Variability in eruption style and associated 529 very-long-period earthquakes at Fuego volcano, Guatemala',\n",
       "  'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "   {'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': ['A'], 'last': 'Nadeau', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '118',\n",
       "  'issn': '4',\n",
       "  'pages': '1526--1533',\n",
       "  'other_ids': {'doi': ['10.1002/jgrb.50075']},\n",
       "  'links': None},\n",
       " 'BIBREF28': {'ref_id': 'b28',\n",
       "  'title': 'Spherical-spline parameterization of three-dimensional Earth 532 models',\n",
       "  'authors': [{'first': 'Z', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'F', 'middle': ['A'], 'last': 'Dahlen', 'suffix': ''}],\n",
       "  'year': 1995,\n",
       "  'venue': 'Geophys. Res. Lett',\n",
       "  'volume': '22',\n",
       "  'issn': '22',\n",
       "  'pages': '3099--3102',\n",
       "  'other_ids': {'doi': ['10.1029/95GL03080']},\n",
       "  'links': '129880072'},\n",
       " 'BIBREF29': {'ref_id': 'b29',\n",
       "  'title': 'Data (black), free-inversion synthetics (red), and best-fitting constrained inversion 651 synthetics (blue) for each of the four bandpasses: a) 30-10 s; b) 90-10 s; c) 120-10 s',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '6',\n",
       "  'issn': '',\n",
       "  'pages': '400--652',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF30': {'ref_id': 'b30',\n",
       "  'title': 'The free-inversion results are generally better than the constrained inversion, as expected',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF31': {'ref_id': 'b31',\n",
       "  'title': 'Source time functions from free inversions in the a) 30-10, b) 90-10, c) 120-10',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF32': {'ref_id': 'b32',\n",
       "  'title': '400-60 s pass bands are largely consistent, although the 90-10 s band has notable differences',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF33': {'ref_id': 'b33',\n",
       "  'title': 'Note the difference in amplitude and time scales for each of the four plots',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF34': {'ref_id': 'b34',\n",
       "  'title': 'Error (E 2 ) by fixed moment-tensor solution plotted using the indicated color scale for 710 each of the bandpasses in Figure 7: a) 30-10, b) 90-10, c) 120-10, and d) 400-60 s. The γ−δ pairs 711 computed from the point-by-point eigenvalue analysis for the best free inversion for each of four 712 bandpasses are plotted in red. The lack of consistency in the free inversion for the 90-10 s band 713 is reflected in the lack of a resolved moment-tensor type',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '8',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF35': {'ref_id': 'b35',\n",
       "  'title': 'The combined crack-pipe tensor of Lyons and Waite',\n",
       "  'authors': [],\n",
       "  'year': 2011,\n",
       "  'venue': '',\n",
       "  'volume': '715',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]['grobid_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]['grobid_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = all_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for paper in all_papers if paper['metadata']['acl_id']!=None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41660\n"
     ]
    }
   ],
   "source": [
    "with open(\"acl_only_json_list_10000.json\", \"r\") as read_file:\n",
    "    all_articles = json.load(read_file)\n",
    "print(len(all_articles))\n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ подборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### проверка наличия названия секции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_paper_ids = [article['paper_id'] for article in all_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### количество  всех статьей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41439"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_articles if article['grobid_parse']['body_text'] or (article['latex_parse'] and article['latex_parse']['body_text'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### проверка наличия текста и названия секций во всех статьях в grobid части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41439"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_articles if article['grobid_parse']['body_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_bofy_text = [article['paper_id'] for article in all_articles if not article['grobid_parse']['body_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 {'paper_id': '60131735', 'metadata': {'title': 'Breadth and D e p t h of Semant ic Lexicons Proceedings of a Workshop Sponsored by the Special Interest Group on the Lexicon of the Associat ion for Computat ional Linguistics', 'authors': [{'first': 'Evelyne', 'middle': [], 'last': 'Viegas', 'suffix': ''}], 'abstract': 'Preface. Contributors. Introduction: E. Viegas. I. Lexical Rules and Underspecification. II. Breadth of Semantic Lexicons. III. Depth of Semantic Lexicons. IV. Lexical Semantics and Pragmatics. Subject Index. Author Index.', 'year': '1999', 'arxiv_id': None, 'acl_id': 'W96-0300', 'pmc_id': None, 'pubmed_id': None, 'doi': '10.1007/978-94-017-0952-1', 'venue': 'Text, Speech and Language Technology', 'journal': 'Text, Speech and Language Technology'}, 's2_pdf_hash': 'fef5115fca1124e5f994cd49414c743ca005b853', 'grobid_parse': {'abstract': [], 'body_text': [], 'ref_entries': {'TABREF0': {'text': 'Introduction', 'latex': None, 'type': 'table'}}, 'bib_entries': {}}, 'latex_parse': None}\n",
      "====================\n",
      "186 {'paper_id': '7473534', 'metadata': {'title': 'Interfacing Ontologies and Lexical Resources', 'authors': [{'first': 'Laurent', 'middle': [], 'last': 'Prévot', 'suffix': ''}, {'first': 'Stefano', 'middle': [], 'last': 'Borgo', 'suffix': ''}, {'first': 'Alessandro', 'middle': [], 'last': 'Oltramari', 'suffix': ''}], 'abstract': 'During the last few years, a number of works aiming at interfacing ontologies and lexical resources have been initiated. This paper aims at clarifying the current picture of this domain. It compares ontologies built following different methodologies and analyses their combination with lexical resources. A point defended in the paper is that different methodologies lead to very different characteristics for the resulting resources. We classify these methodologies show how actual projects fit into this classification.', 'year': '2005', 'arxiv_id': None, 'acl_id': 'I05-7013', 'pmc_id': None, 'pubmed_id': None, 'doi': '10.1017/CBO9780511676536.011', 'venue': 'Proceedings of OntoLex 2005 - Ontologies and Lexical Resources', 'journal': 'Ontology and the Lexicon'}, 's2_pdf_hash': '01c53413fa384b32929e074fb60d2dfa0318d7b9', 'grobid_parse': {'abstract': [], 'body_text': [], 'ref_entries': {}, 'bib_entries': {}}, 'latex_parse': None}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for num,paper_id in enumerate(acl_ids_not_bofy_text):\n",
    "    if num == 2:\n",
    "        break\n",
    "    id_lst = acl_paper_ids.index(paper_id)\n",
    "    print(id_lst,all_articles[id_lst])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '7473534',\n",
       " 'metadata': {'title': 'Interfacing Ontologies and Lexical Resources',\n",
       "  'authors': [{'first': 'Laurent',\n",
       "    'middle': [],\n",
       "    'last': 'Prévot',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Stefano', 'middle': [], 'last': 'Borgo', 'suffix': ''},\n",
       "   {'first': 'Alessandro', 'middle': [], 'last': 'Oltramari', 'suffix': ''}],\n",
       "  'abstract': 'During the last few years, a number of works aiming at interfacing ontologies and lexical resources have been initiated. This paper aims at clarifying the current picture of this domain. It compares ontologies built following different methodologies and analyses their combination with lexical resources. A point defended in the paper is that different methodologies lead to very different characteristics for the resulting resources. We classify these methodologies show how actual projects fit into this classification.',\n",
       "  'year': '2005',\n",
       "  'arxiv_id': None,\n",
       "  'acl_id': 'I05-7013',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.1017/CBO9780511676536.011',\n",
       "  'venue': 'Proceedings of OntoLex 2005 - Ontologies and Lexical Resources',\n",
       "  'journal': 'Ontology and the Lexicon'},\n",
       " 's2_pdf_hash': '01c53413fa384b32929e074fb60d2dfa0318d7b9',\n",
       " 'grobid_parse': {'abstract': [],\n",
       "  'body_text': [],\n",
       "  'ref_entries': {},\n",
       "  'bib_entries': {}},\n",
       " 'latex_parse': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_with_sect = dict()\n",
    "for article in all_articles:\n",
    "    for sections in article['grobid_parse']['body_text']:\n",
    "        if sections['section']:\n",
    "            if article['paper_id'] in article_with_sect:\n",
    "                article_with_sect[article['paper_id']] +=1\n",
    "            else:\n",
    "                article_with_sect[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_with_sect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### как видим нет названия секций у grobid_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '14472576',\n",
       " 'metadata': {'title': 'Building a Semantic Parser Overnight',\n",
       "  'authors': [{'first': 'Yushi', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Jonathan', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "   {'first': 'Percy', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "  'abstract': 'How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.',\n",
       "  'year': '2015',\n",
       "  'arxiv_id': None,\n",
       "  'acl_id': 'P15-1129',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.3115/v1/P15-1129',\n",
       "  'venue': 'ACL',\n",
       "  'journal': 'Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)'},\n",
       " 's2_pdf_hash': 'f3de408be7d2e2720a61451bd196ac7e1ed9363a',\n",
       " 'grobid_parse': {'abstract': [{'text': 'AbstractHow do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Abstract'}],\n",
       "  'body_text': [{'text': 'By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Kushman and Barzilay, 2013) . Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain-for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start?In this paper, we advocate a functionalitydriven process for rapidly building a semantic * Both authors equally contributed to the paper. ...',\n",
       "    'cite_spans': [{'start': 173,\n",
       "      'end': 197,\n",
       "      'text': '(Zelle and Mooney, 1996;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF25'},\n",
       "     {'start': 198,\n",
       "      'end': 228,\n",
       "      'text': 'Zettlemoyer and Collins, 2005;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 229,\n",
       "      'end': 248,\n",
       "      'text': 'Liang et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 249,\n",
       "      'end': 269,\n",
       "      'text': 'Berant et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 270,\n",
       "      'end': 295,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 296,\n",
       "      'end': 323,\n",
       "      'text': 'Kushman and Barzilay, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': '(1) by builder (∼30 minutes)(2) via domain-general grammar (3) via crowdsourcing (∼5 hours) (4) by training a paraphrasing model Figure 1 : Functionality-driven process for building semantic parsers. The two red boxes are the domain-specific parts provided by the builder of the semantic parser, and the other two are generated by the framework.parser in a new domain. At a high-level, we seek to minimize the amount of work needed for a new domain by factoring out the domaingeneral aspects (done by our framework) from the domain-specific ones (done by the builder of the semantic parser). We assume that the builder already has the desired functionality of the semantic parser in mind-e.g., the publications database is set up and the schema is fixed. Figure 1 depicts the functionality-driven process: First, the builder writes a seed lexicon specifying a canonical phrase (\"publication date\") for each predicate (publicationDate).Second, our framework uses a domain-general grammar, along with the seed lexicon and the database, to automatically generate a few hundred canonical utterances paired with their logical forms (e.g., \"article that has the largest publication date\" and arg max(type.article, publicationDate)). These utterances need not be the most elegant, but they should retain the semantics of the logical forms. Third, the builder leverages crowdsourcing to paraphrase each canonical utterance into a few natural utterances (e.g., \"what is the newest published article?\"). Finally, our framework uses this data to train a semantic parser.Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014) . In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule.In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014) . Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain.Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny, as the structural mismatch between logic and language is the chief source of difficulty in semantic parsing (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014) .There are two important questions here. First, is it possible to design a simple grammar that simultaneously generates both logical forms and canonical utterances so that the utterances are understandable by a human? In Section 3, we show how to choose appropriate canonical utterances to maximize alignment with the logical forms.Second, our grammar can generate an infinite number of canonical utterances. How many do we need for adequate coverage? Certainly, single relations is insufficient: just knowing that \"publication date of X\" paraphrases to \"when X was published\" would offer insufficient information to generalize to \"articles that came after X\" mapping to \"article whose publication date is larger than publication date of X\". We call this phenomena sublexical compositionality-when a short lexical unit (\"came after\") maps onto a multi-predicate logical form. Our hypothesis is that the sublexical compositional units are small, so we only need to crowdsource a small number of canonical utterances to learn about most of the language variability in the given domain (Section 4).We applied our functionality-driven process to seven domains, which were chosen to explore particular types of phenomena, such as spatial language, temporal language, and high-arity relations. This resulted in seven new semantic parsing datasets, totaling 12.6K examples. Our approach, which was not tuned on any one domain, was able to obtain an average accuracy of 59% over all domains. On the day of this paper submission, we created an eighth domain and trained a semantic parser overnight.',\n",
       "    'cite_spans': [{'start': 1832,\n",
       "      'end': 1853,\n",
       "      'text': '(Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 2121,\n",
       "      'end': 2142,\n",
       "      'text': '(Rangel et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 2415,\n",
       "      'end': 2441,\n",
       "      'text': '(Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2442,\n",
       "      'end': 2465,\n",
       "      'text': 'Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 3174,\n",
       "      'end': 3194,\n",
       "      'text': '(Liang et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 3195,\n",
       "      'end': 3220,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 3221,\n",
       "      'end': 3244,\n",
       "      'text': 'Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'}],\n",
       "    'ref_spans': [{'start': 129,\n",
       "      'end': 137,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 755,\n",
       "      'end': 763,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In our functionality-driven process (Figure 1) , there are two parties: the builder, who provides domain-specific information, and the framework, which provides domain-general information. We assume that the builder has a fixed database w, represented as a set of triples (e 1 , p, e 2 ), where e 1 and e 2 are entities (e.g., article1, 2015) and p is a property (e.g., publicationDate). The database w can be queried using lambda DCS logical forms, described further in Section 2.1.The builder supplies a seed lexicon L, which contains for each database property p (e.g., publicationDate) a lexical entry of the form t → s[p] , where t is a natural language phrase (e.g., \"publication date\") and s is a syntactic cat-egory (e.g., RELNP). In addition, L contains two typical entities for each semantic type in the database (e.g., alice → NP[alice] for the type person). The purpose of L is to simply connect each predicate with some representation in natural language.The framework supplies a grammar G, which specifies the modes of composition, both on logical forms and canonical utterances. Formally, G is a set of rules of the form α 1 . . . α n → s[z] , where α 1 . . . α n is a sequence of tokens or categories, s is a syntactic category and z is the logical form constructed. For example, one rule in (r) .x] , which constructs z by reversing the binary predicate r and joining it with a the unary predicate x. We use the rules G ∪ L to generate a set of (z, c) pairs, where z is a logical form (e.g., R(publicationDate).article1), and c is the corresponding canonical utterance (e.g., \"publication date of article 1\"). The set of (z, c) is denoted by GEN(G ∪ L). See Section 3 for details.G is RELNP[r] of NP[x] → NP[RNext, the builder (backed by crowdsourcing) paraphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., \"when was article 1 published?\"). This defines a set of training examples D = {(x, c, z)}, for each (z, c) ∈ GEN(G ∪ L) and x ∈ P(c). The crowdsourcing setup is detailed in Section 5.Finally, the framework trains a semantic parser on D. Our semantic parser is a log-linear distribution p θ (z, c | x, w) over logical forms and canonical utterances specified by the grammar G. Note that the grammar G will in general not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014) .To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G ∪ L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(·) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser p θ (z, c | x, w).',\n",
       "    'cite_spans': [{'start': 327,\n",
       "      'end': 342,\n",
       "      'text': 'article1, 2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1308,\n",
       "      'end': 1311,\n",
       "      'text': '(r)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2374,\n",
       "      'end': 2397,\n",
       "      'text': 'Berant and Liang (2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'}],\n",
       "    'ref_spans': [{'start': 36,\n",
       "      'end': 46,\n",
       "      'text': '(Figure 1)',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details.Every logical form z in this paper is either a unary (denoting a set of entities) or a binary (denoting a set of entity-pairs). In the base case, each entity e (e.g., 2015) is a unary denoting the singleton set: e w = {e}; and each property p (e.g., publicationDate) is a binary denoting all entitypairs (e 1 , e 2 ) that satisfy the property p. Unaries and binaries can be composed: Given a binary b and unary u, the join b.u denotes all entities e 1 for which there exists an e 2 ∈ u w with (e 1 , e 2 ) ∈ b w . For example, publicationDate.2015 denote entities published in 2015.The intersection u 1 u 2 , union u 1 u 2 , complement ¬u denote the corresponding set operations on the denotations. We let R(b) denote the reversal of b: (e 1 , e 2 ) ∈ b w iff (e 2 , e 1 ) ∈ R(b) w . This allows us to define R(publicationDate).article1 as the publication date of article 1. We also include aggregation operations (count(u), sum(u) and average(u, b)), and superlatives (argmax(u, b)).Finally, we can construct binaries using lambda abstraction: λx.u denotes a set of (e 1 , e 2 ) where e 1 ∈ u[x/e 2 ] w and u[x/e 2 ] is the logical form where free occurrences of x are replaced with e 2 . For example, R(λx.count(R(cites).x)) denotes the set of entities (e 1 , e 2 ), where e 2 is the number of entities that e 1 cites.',\n",
       "    'cite_spans': [{'start': 167,\n",
       "      'end': 179,\n",
       "      'text': 'Liang (2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our functionality-driven process hinges on having a domain-general grammar that can connect logical forms with canonical utterances compositionally. The motivation is that while it is hard to write a grammar that parses all utterances, it is possible to write one that generates one canonical utterance for each logical form. To make this explicit: Assumption 1 (Canonical compositionality) Using a small grammar, all logical forms expressible in natural language can be realized compositionally based on the logical form.Grammar. We target database querying applications, where the parser needs to handle superlatives, comparatives, negation, and coordination. We define a simple grammar that captures these forms of compositionality using canonical utterances in a domain-general way. Figure 2 illustrates a derivation produced by the grammar. The seed lexicon specified by the builder contains canonical utterances for types, entities, and properties. All types (e.g., person) have the syntactic category TYPENP, and all entities (e.g., Figure 2: Deriving a logical form z (red) and a canonical utterance c (green) from the grammar G. Each node contains a syntactic category and a logical form, which is generated by applying a rule. Nodes with only leaves as children are produced using the seed lexicon; all other nodes are produced by rules in the domain-general grammar.alice) are ENTITYNP\\'s. Unary predicates are realized as verb phrases VP (e.g., \"has a private bath\"). The builder can choose to represent binaries as either relational noun phrases (RELNP) or generalized transitive verbs (VP/NP). RELNP\\'s are usually used to describe functional properties (e.g., \"publication date\"), especially numerical properties. VP/NP\\'s include transitive verbs (\"cites\") but also longer phrases with the same syntactic interface (\"is the president of\"). Table  1 shows the seed lexicon for the SOCIAL domain.From the seed lexicon, the domain-general grammar (Table 2 ) constructs noun phrases (NP), verbs phrases (VP), and complementizer phrase (CP), all of which denote unary logical forms. Broadly speaking, the rules (R1)-(R4), (C1)-(C4) take a binary and a noun phrase, and compose them (optionally via comparatives, counting, and negation) to produce a complementizer phrase CP representing a unary (e.g., \"that cites article 1\" or \"that cites more than three article\"). (G3) combines these CP\\'s with an NP (e.g., \"article\"). In addition, (S0)-(S4) handle superlatives (we include argmin in addition to argmax), which take an NP and return the extremum-attaining subset of its denotation. Finally, we support transformations such as join (T1) and disjunction (T4), as well as aggregation (A1)-(A2).Rendering utterances for multi-arity predicates was a major challenge.The predicate instances are typically reified in a graph database, akin to a neo-Davidsonian treatment of events: There is an abstract entity with binary predicates relating it to its arguments. For example, in the SOCIAL domain, Alice\\'s education can be represented in the database as five triples: All five properties here are represented as RELNP\\'s, with the first one designated as the subject (RELNP 0 ). We support two ways of querying multi-arity relations: \"student whose university is ucla\" (T2) and \"university of student Alice whose start date is 2005\" (T3).birthdate → RELNP[birthdate] person|university|field → TYPENP[person| · · · ] company|job title → TYPENP[company| · · · ] student|university|field of study → RELNP[student| · · · ] employee|employer|job title → RELNP[employee| · · · ] start date|end date → RELNP[startDate| · · · ] is friends with → VP/NP[friends| · · · ]Generating directly from the grammar in Table 2 would result in many uninterpretable canonical utterances. Thus, we perform type checking on the logical forms to rule out \"article that cites 2004\", and limit the amount of recursion, which keeps the canonical utterances understandable.Still, the utterances generated by our grammar are not perfectly grammatical; we do not use determiners and make all nouns singular. Nonetheless, AMT workers found most canonical utterances understandable (see Table 3 and Section 5 for details on crowdsourcing). One tip for the builder is to keep the RELNP\\'s and VP/NP\\'s as context-independent as possible; e.g., using \"publication date\" instead of \"date\". In cases where more context is required, we use parenthetical remarks (e.g., \"number of assists (over a season)\" → RELNP[...]) to pack more context into the confines of the part-of-speech.Limitations. While our domain-general grammar covers most of the common logical forms in a database querying application, there are several phenomena which are out of scope, notably nested quantification (e.g., \"show me each author\\'s most cited work\") and anaphora (e.g., \"author who cites herself at least twice\"). Handling these would require a more radical change to the grammar, but is still within scope. [glue] (G1) ENTITYNP[x] → NP[x] (G2) TYPENP[x] → NP[type.x] (G3) NP[x] CP[f ] (and CP[g])* → NP[x f g] [simple] (R0) that VP[x] → CP[x] (R1.z)))] [transformation] (T1) RELNP[r] of NP[y] → NP[R(r).y] (T2) RELNP 0 [h]CP[f ] (and CP[g])* → NP[R(h).(f g)] (T3) RELNP[r] of RELNP 0 [h] NP[x] CP[f ] (and CP[g])* → NP[R(r).(h.x f g)] (T4) NP[x] or NP[y] → NP[x y] [aggregation] (A1) number of NP[x] → NP[count(x)] (A2) total|average RELNP[r] of NP[x] → NP[sum|average(x, r)]',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 787,\n",
       "      'end': 795,\n",
       "      'text': 'Figure 2',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1853,\n",
       "      'end': 1861,\n",
       "      'text': 'Table  1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF2'},\n",
       "     {'start': 1957,\n",
       "      'end': 1965,\n",
       "      'text': '(Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF4'},\n",
       "     {'start': 4158,\n",
       "      'end': 4165,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'While the canonical utterance c is generated compositionally along with the logical form z, natural paraphrases x ∈ P(c) generally deviate from this compositional structure. For example, the canonical utterance \"NP[number of NP[article CP[whose publication date is larger than NP[publication date of article 1]]]]\" might get paraphrased to \"How many articles were published after article 1?\". Here, \"published after\" non-compositionally straddles the inner NP, intuitively responsible for both instances of \"publication date\". But how non-compositional can paraphrases be? Our framework rests on the assumption that the answer is \"not very\":Assumption 2 (Bounded non-compositionality) Natural utterances for expressing complex logical forms are compositional with respect to fragments of bounded size.In the above example, note that while \"published after\" is non-compositional with respect to the grammar, the rewriting of \"number of\" to \"how many\" is compositional. The upshot of Assumption 2 is that we only need to ask for paraphrases of canonical utterances generated by the grammar up to some small depth to learn about all the noncompositional uses of language, and still be able generalize (compositionally) beyond that. We now explore the nature of the possible paraphrases. Broadly speaking, most paraphrases involve some sort of compression, where the clunky but faithful canonical utterance is smoothed out into graceful prose.Alternations of single rules. The most basic paraphrase happens at the single phrase level with synonyms (\"block\" to \"brick\"), which preserve the part-of-speech. However, many of our properties are specified using relational noun phrases, which are more naturally realized using prepositions (\"meeting whose attendee is alice ⇒ meeting with alice\") or verbs (\"author of article 1 ⇒ who wrote article 1\"). If the RELNP is complex, then the argument can become embedded: \"player whose number of points is 15 ⇒ player who scored 15 points\". Superlative and comparative constructions reveal other RELNP-dependent words: \"article that has the largest publication date ⇒ newest article\". When the value of the relation has enough context, then the relation is elided completely: \"housing unit whose housing type is apartment ⇒ apartment\".Multi-arity predicates are compressed into a single frame: \"university of student alice whose field of study is music\" becomes \"At which university did Alice study music?\", where the semantic roles of the verb \"study\" carry the burden of expressing the multiple relations: student, university, and fieldOfStudy. With a different combination of arguments, the natural verb would change: \"Which university did Alice attend?\" Sublexical compositionality. The most interesting paraphrases occur across multiple rules, a phenomenon which we called sublexical compositionality. The idea is that common, multi-part concepts are compressed to single words or simpler constructions. The simplest compression is a lexical one: \"parent of alice whose gender is female ⇒ mother of alice\". Compression often occurs when we have the same predicate chained twice in a join: \"person that is author of paper whose author is X ⇒ co-author of X\" or \"person whose birthdate is birthdate of X ⇒ person born on the same day as X\". When two CP\\'s combined via coordination have some similarity, then the coordination can be pushed down (\"meeting whose start time is 3pm and whose end time is 5pm ⇒ meetings between 3pm and 5pm\") and sometimes even generalized (\"that allows cats and that allows dogs ⇒ that allows pets\"). Sometimes, compression happens due to metonymy, where people stand in for their papers: \"author of article that article whose author is X cites ⇒ who does X cite\".',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We tackled seven domains covering various linguistic phenomena. Table 3 lists the domains, their principal phenomena, statistics about their predicates and dataset, and an example from the dataset.We use Amazon Mechanical Turk (AMT) to paraphrase the canonical utterances generated by the domain-general grammar. In each AMT task, a worker is presented with four canonical utterances and is asked to reformulate them in natural language or state that they are incomprehensible. Each canonical utterance was presented to 10 workers. Over all domains, we collected 18,032 responses. The average time for paraphrasing one utterance was 28 seconds. Paraphrases that share the same canonical utterance are collapsed, while identical paraphrases that have distinct canonical utterances are deleted. This produced a total of 12,602 examples over all domains.To estimate the level of noise in the data, we manually judged the correctness of 20 examples in each domain, and found that 17% of the utterances were inaccurate. There are two main reasons: lexical ambiguity on our part (\"player that has the least number of team ⇒ player with the lowest jersey number\"), and failure on the worker\\'s part (\"restaurant whose star rating is 3 stars ⇒ hotel which has a 3 star rating\").',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 64,\n",
       "      'end': 71,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our semantic parsing model defines a distribution over logical forms given by the domaingeneral grammar G and additional rules triggered by the input utterance x. Specifically, given an utterance x, we detect numbers, dates, and perform string matching with database entities to recognize named entities. This results in a set of rules T(x). For example, if x is \"article published in 2015 that cites article 1\", then T(x) contains 2015 → NP [2015] andarticle 1 → NP[article1]. Let L x be the rules in the seed lexicon L where the entity rules (e.g., alice → NP[alice] ) are replaced by T(x). Our semantic parsing model defines a loglinear distribution over candidate pairs (z, c) ∈ GEN(G ∪ L x ):p θ (z, c | x, w) ∝ exp(φ(c, z, x, w) θ),(1)where φ(z, c, x, w) ∈ R d is a feature vector and θ ∈ R d is a parameter vector. To generate candidate logical forms, we use a simple beam search: For each search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance \"article\" would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015) .Training. We train our model by maximizing the regularized log-likelihood O(θ) = Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: \"Show me meetings after the weekly standup day\" c: \"meeting whose date is at least date of weekly standup\" z: type.meeting date. > R(date).weeklyStandup BLOCKS 19 1995 spatial language x: \"Select the brick that is to the furthest left.\" c: \"block that the most number of block is right of\" z: argmax(type.block, R(λx.count(R(right).x))) HOUSING 24 941 measurement units x: \"Housing that is 800 square feet or bigger?\" c: \"housing unit whose size is at least 800 square feet\" z: type.housingUnit area. > .800 RESTAURANTS 32 1657 long unary relations x: \"What restaurant can you eat lunch outside at?\" c: \"restaurant that has outdoor seating and that serves lunch\" z: type.restaurant hasOutdoorSeating serveslunch PUBLICATIONS 15 801 sublexical compositionality x: \"Who has co-authored articles with Efron?\" c: \"person that is author of article whose author is efron\" z: type.person R(author).(type.article author.efron) SOCIAL 45 4419 multi-arity relations x: \"When did alice start attending brown university?\" c: \"start date of student alice whose university is brown university\" z: R(date). Table 3 : We experimented on seven domains, covering a variety of phenomena. For each domain, we show the number of predicates, number of examples, and a (c, z) generated by our framework along with a paraphrased utterance x. Table 4 : Features for the paraphrasing model. pos(x i:i ) is the POS tag; type( z w ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c.',\n",
       "    'cite_spans': [{'start': 442,\n",
       "      'end': 448,\n",
       "      'text': '[2015]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1435,\n",
       "      'end': 1458,\n",
       "      'text': 'Berant and Liang (2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1494,\n",
       "      'end': 1518,\n",
       "      'text': 'Pasupat and Liang (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'}],\n",
       "    'ref_spans': [{'start': 2769,\n",
       "      'end': 2776,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2995,\n",
       "      'end': 3002,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': '(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 . To optimize, we use AdaGrad (Duchi et al., 2010) .Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) . We count the number of exact matches, PPDB matches, and unmatched words.To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4 ). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) . We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .',\n",
       "    'cite_spans': [{'start': 70,\n",
       "      'end': 90,\n",
       "      'text': '(Duchi et al., 2010)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 253,\n",
       "      'end': 280,\n",
       "      'text': '(Ganitkevitch et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 411,\n",
       "      'end': 431,\n",
       "      'text': '(Liang et al., 2006)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'},\n",
       "     {'start': 774,\n",
       "      'end': 793,\n",
       "      'text': '(Och and Ney, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'},\n",
       "     {'start': 1213,\n",
       "      'end': 1234,\n",
       "      'text': '(Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'}],\n",
       "    'ref_spans': [{'start': 101,\n",
       "      'end': 108,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 654,\n",
       "      'end': 661,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We evaluated our functionality-driven process on the seven domains described in Section 5 and one new domain we describe in Section 7.3. For each domain, we held out a random 20% of the examples as the test set, and performed development on the remaining 80%, further splitting it to a training and development set (80%/20%). We created a database for each domain by randomly generating facts using entities and properties in the domain (with type-checking). We evaluated using accuracy, which is the fraction of examples that yield the correct denotation.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our functionality-driven process is predicated on the fact that each domain exhibits domain-specific phenomena. To corroborate this, we compare our full system to NOLEX, a baseline that omits all lexical features (Table 4 ), but uses PPDB as a domain-general paraphrasing component. We perform the complementary experiment and compare to NOPPDB, a baseline that omits PPDB match features. We also run BASELINE, where we omit both lexical and PPDB features. Table 5 presents the results of this experiment. Overall, our framework obtains an average accuracy of 59% across all eight domains. The performance of NOLEX is dramatically lower than FULL, indicating that it is important to learn domain-specific paraphrases using lexical features. The accuracy of NOPPDB is only slightly lower than FULL, showing that most of the required paraphrases can be learned during training. As expected, removing both lexical and PPDB features results in poor performance (BASELINE).Analysis. We performed error analysis on 10 errors in each domain. Almost 70% of the errors are due to problems in the paraphrasing model, where the canonical utterance has extra material, is missing some content, or results in an incorrect paraphrase. For example, \"restaurants that have waiters and you can sit outside\" is paraphrased to \"restaurant that has waiter service and that takes reservations\". Another 12.5% result from reordering issues, e.g, we paraphrase \"What venue has fewer than two articles\" to \"article that has less than two venue\". Inaccurate paraphrases provided by AMT workers account for the rest of the errors.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 213,\n",
       "      'end': 221,\n",
       "      'text': '(Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 457,\n",
       "      'end': 464,\n",
       "      'text': 'Table 5',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF6'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We hypothesized that we need to obtain paraphrases of canonical utterances corresponding to logical forms of only small depth. We ran the following experiment in the CALENDAR domain to test this claim. First, we define by NP 0 , NP 1 , and NP 2 the set of utterances generated by an NP that has exactly zero, one, and two NPs embedded in it. We define the training scenario 0 → 1, where we train on examples from NP 0 and test on examples from NP 1 ; 0 ∪ 1 → 1, 0 ∪ 1 → 2, and 0 ∪ 1 ∪ 2 → 2 are defined analogously. Our Scenario Acc. Scenario Acc. 0 → 1 22.9 0 ∪ 1 → 2 28.1 0 ∪ 1 → 1 85.8 0 ∪ 1 ∪ 2 → 2 47.5 Table 6 : Test set results in the CALENDAR domain on bounded non-compositionality. hypothesis is that generalization on 0 ∪ 1 → 2 should be better than for 0 → 1, since NP 1 utterances have non-compositional paraphrases, but training on NP 0 does not expose them.The results in Table 6 verify this hypothesis. The accuracy of 0 → 1 is almost 65% lower than 0 ∪ 1 → 1. On the other hand, the accuracy of 0 ∪ 1 → 2 is only 19% lower than 0 ∪ 1 ∪ 2 → 2.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 608,\n",
       "      'end': 615,\n",
       "      'text': 'Table 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 886,\n",
       "      'end': 893,\n",
       "      'text': 'Table 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'To verify the title of this paper, we attempted to create a semantic parser for a new domain (RECIPES) exactly 24 hours before the submission deadline. Starting at midnight, we created a seed lexicon in less than 30 minutes. Then we generated canonical utterances and allowed AMT workers to provide paraphrases overnight. In the morning, we trained our parser and obtained an accuracy of 70.8% on the test set.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Geo880. To test how our parser generalizes to utterances independent of our framework, we created a semantic parser for the domain of US geography, and tested on the standard 280 test examples from GEO880 (Zelle and Mooney, 1996) . We did not use the standard 600 training examples. Our parser obtained 56.4% accuracy, which is substantially lower than state-of-the-art (∼ 90%).We performed error analysis on 100 random sentences from the development set where accuracy was 60%. We found that the parser learns from the training data to prefer shorter paraphrases, which accounts for 30% of the errors. In most of these cases, the correct logical form is ranked at the top-3 results (accuracy for the top-3 derivations is 73%). GEO880 contains highly compositional utterances, and in 25% of the errors the correct derivation tree exceeds the maximum depth used for our parser. Another 17.5% of the errors are caused by problems in the paraphrasing model. For example, in the utterance \"what is the size of california\", the model learns that \"size\" corresponds to \"population\" rather than \"area\". Errors related to reordering and the syntactic structure of the input utterance account for 7.5% of the errors. For example, the utterance \"what is the area of the largest state\" is paraphrased to \"state that has the largest area\".Calendar. In Section 7.1, we evaluated on utterances obtained by paraphrasing canonical utterances from the grammar. To examine the coverage of our parser on independently-produced utterances, we asked AMT workers to freely come up with queries. We collected 186 such queries; 5 were spam and discarded. We replaced all entities (people, dates, etc.) with entities from our seed lexicon to avoid focusing on entity detection.We were able to annotate 52% of the utterances with logical forms from our grammar. We could not annotate 20% of the utterances due to relative time references, such as \"What time is my next meeting?\". 14% of the utterances were not covered due to binary predicates not in the grammar (\"What is the agenda of the meeting?\") or missing entities (\"When is Dan\\'s birthday?\"). Another 2% required unsupported calculations (\"How much free time do I have tomorrow?\"), and the rest are out of scope for other reasons (\"When does my Verizon data plan start over?\").We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model.',\n",
       "    'cite_spans': [{'start': 205,\n",
       "      'end': 229,\n",
       "      'text': '(Zelle and Mooney, 1996)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF25'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013) . However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013) . Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal.Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014) . All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing.Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014) . We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010) . However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance.In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification.',\n",
       "    'cite_spans': [{'start': 101,\n",
       "      'end': 122,\n",
       "      'text': '(Cai and Yates, 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 123,\n",
       "      'end': 148,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 149,\n",
       "      'end': 169,\n",
       "      'text': 'Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 410,\n",
       "      'end': 438,\n",
       "      'text': '(Kushman and Barzilay, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 466,\n",
       "      'end': 489,\n",
       "      'text': '(Matuszek et al., 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'},\n",
       "     {'start': 490,\n",
       "      'end': 510,\n",
       "      'text': 'Tellex et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF21'},\n",
       "     {'start': 511,\n",
       "      'end': 542,\n",
       "      'text': 'Krishnamurthy and Kollar, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 756,\n",
       "      'end': 787,\n",
       "      'text': '(Zettlemoyer and Collins, 2005;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 788,\n",
       "      'end': 810,\n",
       "      'text': 'Wong and Mooney, 2007)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF23'},\n",
       "     {'start': 826,\n",
       "      'end': 847,\n",
       "      'text': '(Clarke et al., 2010;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 848,\n",
       "      'end': 867,\n",
       "      'text': 'Liang et al., 2011)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 888,\n",
       "      'end': 917,\n",
       "      'text': '(Artzi and Zettlemoyer, 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 918,\n",
       "      'end': 937,\n",
       "      'text': 'Reddy et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 1202,\n",
       "      'end': 1222,\n",
       "      'text': '(Fader et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 1244,\n",
       "      'end': 1268,\n",
       "      'text': '(Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1393,\n",
       "      'end': 1413,\n",
       "      'text': '(Woods et al., 1972;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF24'},\n",
       "     {'start': 1414,\n",
       "      'end': 1439,\n",
       "      'text': 'Warren and Pereira, 1982)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF22'},\n",
       "     {'start': 1472,\n",
       "      'end': 1489,\n",
       "      'text': '(Schwitter, 2010)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'}],\n",
       "    'ref_spans': [{'start': 1354,\n",
       "      'end': 1361,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF4'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {'FIGREF0': {'text': '(student.Alice university.Brown) BASKETBALL 24 1952 parentheticals x: \"How many fouls were played by Kobe Bryant in 2004?\" c: \"number of fouls (over a season) of player kobe bryant whose season is 2004\" z: count(R(fouls).(player.KobeBryant season.2004)',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF1': {'text': 'Basic #words and bigram matches in (x, c) #words and bigram PPDB matches in (x, c) #unmatched words in x #unmatched words in c size of denotation of z, (| z w |) pos(x0:0) conjoined with type( z w ) #nodes in tree generating z Lexical ∀(i, j) ∈ A. (xi:i, cj:j) ∀(i, j) ∈ A. (xi:i, cj:j+1) ∀(i, j) ∈ A. (xi:i, cj−1:j) ∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1, cj:j+1) all unaligned words in x and c (xi:j, c i :j ) if in phrase table',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'TABREF0': {'text': 'person that is author of the most number of article argmax(type.person, R(λx.count(type.article author.x))) ...what is the newest published article? who has published the most articles?',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF1': {'text': 'NP[type.article publicationDate.1950]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF2': {'text': 'The seed lexicon for the SOCIAL do- main, which specifies for each predicate (e.g., birthdate) a phrase (e.g., \"birthdate\") that real- izes that predicate and its syntactic category (e.g., RELNP).',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF3': {'text': ') whose RELNP[r] CMP[c] NP[y] → CP[r.c.y] is|is not|is smaller than|is larger than|is at least|is at most → CMP[= | = | < | > | ≤ | ≥]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF4': {'text': 'The domain-general grammar which is combined with the seed lexicon to generate logical forms and canonical utterances that cover the supported logical functionality.',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF6': {'text': 'Test set results on all domains and baselines.',\n",
       "    'latex': None,\n",
       "    'type': 'table'}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Bootstrapping semantic parsers from conversations',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '421--432',\n",
       "    'other_ids': {},\n",
       "    'links': '1140108'},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Weakly supervised learning of semantic parsers for mapping instructions to actions',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '1',\n",
       "    'issn': '',\n",
       "    'pages': '49--62',\n",
       "    'other_ids': {},\n",
       "    'links': '9963298'},\n",
       "   'BIBREF2': {'ref_id': 'b2',\n",
       "    'title': 'Semantic parsing via paraphrasing',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '1336493'},\n",
       "   'BIBREF3': {'ref_id': 'b3',\n",
       "    'title': 'Semantic parsing on Freebase from question-answer pairs',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': [], 'last': 'Chou', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Frostig', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6401679'},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Large-scale semantic parsing via schema matching and lexicon extension',\n",
       "    'authors': [{'first': 'Q', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': [], 'last': 'Yates', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '2265838'},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': \"Driving semantic parsing from the world's response\",\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Clarke', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Goldwasser', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Chang', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Roth', 'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'Computational Natural Language Learning (CoNLL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '18--27',\n",
       "    'other_ids': {},\n",
       "    'links': '5667590'},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'Adaptive subgradient methods for online learning and stochastic optimization',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Duchi', 'suffix': ''},\n",
       "     {'first': 'E', 'middle': [], 'last': 'Hazan', 'suffix': ''},\n",
       "     {'first': 'Y', 'middle': [], 'last': 'Singer', 'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'Conference on Learning Theory (COLT)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '538820'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': 'Paraphrase-driven learning for open question answering',\n",
       "    'authors': [{'first': 'A', 'middle': [], 'last': 'Fader', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''},\n",
       "     {'first': 'O', 'middle': [], 'last': 'Etzioni', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '8893912'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'PPDB: The paraphrase database',\n",
       "    'authors': [{'first': 'J',\n",
       "      'middle': [],\n",
       "      'last': 'Ganitkevitch',\n",
       "      'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['V'], 'last': 'Durme', 'suffix': ''},\n",
       "     {'first': 'C', 'middle': [], 'last': 'Callison-Burch', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '758--764',\n",
       "    'other_ids': {},\n",
       "    'links': '6067240'},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Jointly learning to parse and perceive: Connecting natural language to the physical world',\n",
       "    'authors': [{'first': 'J',\n",
       "      'middle': [],\n",
       "      'last': 'Krishnamurthy',\n",
       "      'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Kollar', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '1',\n",
       "    'issn': '',\n",
       "    'pages': '193--206',\n",
       "    'other_ids': {},\n",
       "    'links': '10250712'},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Using semantic unification to generate regular expressions from natural language',\n",
       "    'authors': [{'first': 'N', 'middle': [], 'last': 'Kushman', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Barzilay', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '826--836',\n",
       "    'other_ids': {},\n",
       "    'links': '6216733'},\n",
       "   'BIBREF11': {'ref_id': 'b11',\n",
       "    'title': 'Scaling semantic parsers with on-the-fly ontology matching',\n",
       "    'authors': [{'first': 'T',\n",
       "      'middle': [],\n",
       "      'last': 'Kwiatkowski',\n",
       "      'suffix': ''},\n",
       "     {'first': 'E', 'middle': [], 'last': 'Choi', 'suffix': ''},\n",
       "     {'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '14341841'},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Alignment by agreement',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': [], 'last': 'Taskar', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Klein', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': 'North American Association for Computational Linguistics (NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '104--111',\n",
       "    'other_ids': {},\n",
       "    'links': '618683'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'Learning dependency-based compositional semantics',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['I'], 'last': 'Jordan', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Klein', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '590--599',\n",
       "    'other_ids': {},\n",
       "    'links': '340852'},\n",
       "   'BIBREF14': {'ref_id': 'b14',\n",
       "    'title': 'Lambda dependency-based compositional semantics. arXiv',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'A joint model of language and perception for grounded attribute learning',\n",
       "    'authors': [{'first': 'C', 'middle': [], 'last': 'Matuszek', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Fitzgerald', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Bo', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Fox', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'International Conference on Machine Learning (ICML)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1671--1678',\n",
       "    'other_ids': {},\n",
       "    'links': '2408319'},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'The alignment template approach to statistical machine translation',\n",
       "    'authors': [{'first': 'F', 'middle': ['J'], 'last': 'Och', 'suffix': ''},\n",
       "     {'first': 'H', 'middle': [], 'last': 'Ney', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'Computational Linguistics',\n",
       "    'volume': '30',\n",
       "    'issn': '',\n",
       "    'pages': '417--449',\n",
       "    'other_ids': {},\n",
       "    'links': '1272090'},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Compositional semantic parsing on semi-structured tables',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Pasupat', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '9027681'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'Features and pitfalls that users should seek in natural language interfaces to databases',\n",
       "    'authors': [{'first': 'R',\n",
       "      'middle': ['A P'],\n",
       "      'last': 'Rangel',\n",
       "      'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['A'], 'last': 'Aguirre', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['J'], 'last': 'Gonzlez', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['M'], 'last': 'Carpio', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Recent Advances on Hybrid Approaches for Designing Intelligent Systems',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '617--630',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Largescale semantic parsing without question-answer pairs',\n",
       "    'authors': [{'first': 'S', 'middle': [], 'last': 'Reddy', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Lapata', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Steedman', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '2',\n",
       "    'issn': '10',\n",
       "    'pages': '377--392',\n",
       "    'other_ids': {},\n",
       "    'links': '15324422'},\n",
       "   'BIBREF20': {'ref_id': 'b20',\n",
       "    'title': 'Controlled natural languages for knowledge representation',\n",
       "    'authors': [{'first': 'R',\n",
       "      'middle': [],\n",
       "      'last': 'Schwitter',\n",
       "      'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'International Conference on Computational Linguistics (COLING)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1113--1121',\n",
       "    'other_ids': {},\n",
       "    'links': '10228634'},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'Understanding natural language commands for robotic navigation and mobile manipulation',\n",
       "    'authors': [{'first': 'S', 'middle': [], 'last': 'Tellex', 'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Kollar', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': [], 'last': 'Dickerson', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['R'], 'last': 'Walter', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': ['G'], 'last': 'Banerjee', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': ['J'], 'last': 'Teller', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Roy', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Association for the Advancement of Artificial Intelligence (AAAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '1078533'},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'An efficient easily adaptable system for interpreting natural language queries',\n",
       "    'authors': [{'first': 'D', 'middle': [], 'last': 'Warren', 'suffix': ''},\n",
       "     {'first': 'F', 'middle': [], 'last': 'Pereira', 'suffix': ''}],\n",
       "    'year': 1982,\n",
       "    'venue': 'Computational Linguistics',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '110--122',\n",
       "    'other_ids': {},\n",
       "    'links': '2498523'},\n",
       "   'BIBREF23': {'ref_id': 'b23',\n",
       "    'title': 'Learning synchronous grammars for semantic parsing with lambda calculus',\n",
       "    'authors': [{'first': 'Y', 'middle': ['W'], 'last': 'Wong', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['J'], 'last': 'Mooney', 'suffix': ''}],\n",
       "    'year': 2007,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '960--967',\n",
       "    'other_ids': {},\n",
       "    'links': '9337134'},\n",
       "   'BIBREF24': {'ref_id': 'b24',\n",
       "    'title': 'The lunar sciences natural language information system: Final report',\n",
       "    'authors': [{'first': 'W', 'middle': ['A'], 'last': 'Woods', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['M'], 'last': 'Kaplan', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['N'], 'last': 'Webber', 'suffix': ''}],\n",
       "    'year': 1972,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '62727207'},\n",
       "   'BIBREF25': {'ref_id': 'b25',\n",
       "    'title': 'Learning to parse database queries using inductive logic programming',\n",
       "    'authors': [{'first': 'M', 'middle': [], 'last': 'Zelle', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['J'], 'last': 'Mooney', 'suffix': ''}],\n",
       "    'year': 1996,\n",
       "    'venue': 'Association for the Advancement of Artificial Intelligence (AAAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1050--1055',\n",
       "    'other_ids': {},\n",
       "    'links': '263135'},\n",
       "   'BIBREF26': {'ref_id': 'b26',\n",
       "    'title': 'Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars',\n",
       "    'authors': [{'first': 'L',\n",
       "      'middle': ['S'],\n",
       "      'last': 'Zettlemoyer',\n",
       "      'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Collins', 'suffix': ''}],\n",
       "    'year': 2005,\n",
       "    'venue': 'Uncertainty in Artificial Intelligence (UAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '658--666',\n",
       "    'other_ids': {},\n",
       "    'links': '449252'}}},\n",
       " 'latex_parse': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "номер секции = 0 | кол-во ссылок 13\n",
      "0 {'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "1 {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "2 {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "3 {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "4 {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "5 {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "6 {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "7 {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "8 {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "9 {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "10 {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "11 {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "12 {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google's big developers' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "----\n",
      "номер секции = 1 | кол-во ссылок 1\n",
      "0 {'start': 451, 'end': 468, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.\n",
      "----\n",
      "номер секции = 2 | кол-во ссылок 6\n",
      "0 {'start': 32, 'end': 58, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "1 {'start': 59, 'end': 79, 'text': 'Rezende et al., 2014', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "2 {'start': 184, 'end': 200, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "3 {'start': 2384, 'end': 2401, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "4 {'start': 2433, 'end': 2456, 'text': '(Bahdanau et al., 2015;', 'latex': None, 'ref_id': 'BIBREF0'}\n",
      "5 {'start': 2457, 'end': 2476, 'text': 'Luong et al., 2015)', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.\n",
      "----\n",
      "номер секции = 3 | кол-во ссылок 3\n",
      "0 {'start': 86, 'end': 102, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "1 {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "2 {'start': 1133, 'end': 1158, 'text': '(Dantzig and Thapa, 2006)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .\n",
      "----\n",
      "номер секции = 8 | кол-во ссылок 1\n",
      "0 {'start': 107, 'end': 118, 'text': '(Lin, 2004)', 'latex': None, 'ref_id': 'BIBREF12'}\n",
      "The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n",
      "----\n",
      "номер секции = 9 | кол-во ссылок 4\n",
      "0 {'start': 351, 'end': 365, 'text': '(Wasson, 1998)', 'latex': None, 'ref_id': 'BIBREF20'}\n",
      "1 {'start': 492, 'end': 512, 'text': '(Radev et al., 2000)', 'latex': None, 'ref_id': 'BIBREF17'}\n",
      "2 {'start': 700, 'end': 723, 'text': '(Erkan and Radev, 2004)', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "3 {'start': 737, 'end': 763, 'text': '(Mihalcea and Tarau, 2004)', 'latex': None, 'ref_id': 'BIBREF14'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.\n",
      "----\n",
      "номер секции = 10 | кол-во ссылок 2\n",
      "0 {'start': 498, 'end': 518, 'text': '(Kingma and Ba, 2014', 'latex': None, 'ref_id': 'BIBREF8'}\n",
      "1 {'start': 652, 'end': 674, 'text': '(Bastien et al., 2012)', 'latex': None, 'ref_id': 'BIBREF1'}\n",
      "The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .\n",
      "----\n",
      "номер секции = 11 | кол-во ссылок 2\n",
      "0 {'start': 690, 'end': 707, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "1 {'start': 908, 'end': 925, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"'Bitcoin Mt. Gox Offlile\"' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.\n",
      "----\n",
      "номер секции = 13 | кол-во ссылок 1\n",
      "0 {'start': 517, 'end': 868, 'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world', 'latex': None, 'ref_id': None}\n",
      "We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world \n",
      "----\n",
      "====================\n",
      "номер секции = 0 | кол-во ссылок 6\n",
      "0 {'start': 173, 'end': 197, 'text': '(Zelle and Mooney, 1996;', 'latex': None, 'ref_id': 'BIBREF25'}\n",
      "1 {'start': 198, 'end': 228, 'text': 'Zettlemoyer and Collins, 2005;', 'latex': None, 'ref_id': 'BIBREF26'}\n",
      "2 {'start': 229, 'end': 248, 'text': 'Liang et al., 2011;', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "3 {'start': 249, 'end': 269, 'text': 'Berant et al., 2013;', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "4 {'start': 270, 'end': 295, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "5 {'start': 296, 'end': 323, 'text': 'Kushman and Barzilay, 2013)', 'latex': None, 'ref_id': 'BIBREF10'}\n",
      "By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Kushman and Barzilay, 2013) . Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain-for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start?In this paper, we advocate a functionalitydriven process for rapidly building a semantic * Both authors equally contributed to the paper. ...\n",
      "----\n",
      "номер секции = 1 | кол-во ссылок 7\n",
      "0 {'start': 1832, 'end': 1853, 'text': '(Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "1 {'start': 2121, 'end': 2142, 'text': '(Rangel et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "2 {'start': 2415, 'end': 2441, 'text': '(Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "3 {'start': 2442, 'end': 2465, 'text': 'Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "4 {'start': 3174, 'end': 3194, 'text': '(Liang et al., 2011;', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "5 {'start': 3195, 'end': 3220, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "6 {'start': 3221, 'end': 3244, 'text': 'Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "(1) by builder (∼30 minutes)(2) via domain-general grammar (3) via crowdsourcing (∼5 hours) (4) by training a paraphrasing model Figure 1 : Functionality-driven process for building semantic parsers. The two red boxes are the domain-specific parts provided by the builder of the semantic parser, and the other two are generated by the framework.parser in a new domain. At a high-level, we seek to minimize the amount of work needed for a new domain by factoring out the domaingeneral aspects (done by our framework) from the domain-specific ones (done by the builder of the semantic parser). We assume that the builder already has the desired functionality of the semantic parser in mind-e.g., the publications database is set up and the schema is fixed. Figure 1 depicts the functionality-driven process: First, the builder writes a seed lexicon specifying a canonical phrase (\"publication date\") for each predicate (publicationDate).Second, our framework uses a domain-general grammar, along with the seed lexicon and the database, to automatically generate a few hundred canonical utterances paired with their logical forms (e.g., \"article that has the largest publication date\" and arg max(type.article, publicationDate)). These utterances need not be the most elegant, but they should retain the semantics of the logical forms. Third, the builder leverages crowdsourcing to paraphrase each canonical utterance into a few natural utterances (e.g., \"what is the newest published article?\"). Finally, our framework uses this data to train a semantic parser.Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014) . In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule.In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014) . Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain.Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny, as the structural mismatch between logic and language is the chief source of difficulty in semantic parsing (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014) .There are two important questions here. First, is it possible to design a simple grammar that simultaneously generates both logical forms and canonical utterances so that the utterances are understandable by a human? In Section 3, we show how to choose appropriate canonical utterances to maximize alignment with the logical forms.Second, our grammar can generate an infinite number of canonical utterances. How many do we need for adequate coverage? Certainly, single relations is insufficient: just knowing that \"publication date of X\" paraphrases to \"when X was published\" would offer insufficient information to generalize to \"articles that came after X\" mapping to \"article whose publication date is larger than publication date of X\". We call this phenomena sublexical compositionality-when a short lexical unit (\"came after\") maps onto a multi-predicate logical form. Our hypothesis is that the sublexical compositional units are small, so we only need to crowdsource a small number of canonical utterances to learn about most of the language variability in the given domain (Section 4).We applied our functionality-driven process to seven domains, which were chosen to explore particular types of phenomena, such as spatial language, temporal language, and high-arity relations. This resulted in seven new semantic parsing datasets, totaling 12.6K examples. Our approach, which was not tuned on any one domain, was able to obtain an average accuracy of 59% over all domains. On the day of this paper submission, we created an eighth domain and trained a semantic parser overnight.\n",
      "----\n",
      "номер секции = 2 | кол-во ссылок 3\n",
      "0 {'start': 327, 'end': 342, 'text': 'article1, 2015)', 'latex': None, 'ref_id': None}\n",
      "1 {'start': 1308, 'end': 1311, 'text': '(r)', 'latex': None, 'ref_id': None}\n",
      "2 {'start': 2374, 'end': 2397, 'text': 'Berant and Liang (2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "In our functionality-driven process (Figure 1) , there are two parties: the builder, who provides domain-specific information, and the framework, which provides domain-general information. We assume that the builder has a fixed database w, represented as a set of triples (e 1 , p, e 2 ), where e 1 and e 2 are entities (e.g., article1, 2015) and p is a property (e.g., publicationDate). The database w can be queried using lambda DCS logical forms, described further in Section 2.1.The builder supplies a seed lexicon L, which contains for each database property p (e.g., publicationDate) a lexical entry of the form t → s[p] , where t is a natural language phrase (e.g., \"publication date\") and s is a syntactic cat-egory (e.g., RELNP). In addition, L contains two typical entities for each semantic type in the database (e.g., alice → NP[alice] for the type person). The purpose of L is to simply connect each predicate with some representation in natural language.The framework supplies a grammar G, which specifies the modes of composition, both on logical forms and canonical utterances. Formally, G is a set of rules of the form α 1 . . . α n → s[z] , where α 1 . . . α n is a sequence of tokens or categories, s is a syntactic category and z is the logical form constructed. For example, one rule in (r) .x] , which constructs z by reversing the binary predicate r and joining it with a the unary predicate x. We use the rules G ∪ L to generate a set of (z, c) pairs, where z is a logical form (e.g., R(publicationDate).article1), and c is the corresponding canonical utterance (e.g., \"publication date of article 1\"). The set of (z, c) is denoted by GEN(G ∪ L). See Section 3 for details.G is RELNP[r] of NP[x] → NP[RNext, the builder (backed by crowdsourcing) paraphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., \"when was article 1 published?\"). This defines a set of training examples D = {(x, c, z)}, for each (z, c) ∈ GEN(G ∪ L) and x ∈ P(c). The crowdsourcing setup is detailed in Section 5.Finally, the framework trains a semantic parser on D. Our semantic parser is a log-linear distribution p θ (z, c | x, w) over logical forms and canonical utterances specified by the grammar G. Note that the grammar G will in general not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014) .To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G ∪ L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(·) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser p θ (z, c | x, w).\n",
      "----\n",
      "номер секции = 3 | кол-во ссылок 1\n",
      "0 {'start': 167, 'end': 179, 'text': 'Liang (2013)', 'latex': None, 'ref_id': 'BIBREF14'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details.Every logical form z in this paper is either a unary (denoting a set of entities) or a binary (denoting a set of entity-pairs). In the base case, each entity e (e.g., 2015) is a unary denoting the singleton set: e w = {e}; and each property p (e.g., publicationDate) is a binary denoting all entitypairs (e 1 , e 2 ) that satisfy the property p. Unaries and binaries can be composed: Given a binary b and unary u, the join b.u denotes all entities e 1 for which there exists an e 2 ∈ u w with (e 1 , e 2 ) ∈ b w . For example, publicationDate.2015 denote entities published in 2015.The intersection u 1 u 2 , union u 1 u 2 , complement ¬u denote the corresponding set operations on the denotations. We let R(b) denote the reversal of b: (e 1 , e 2 ) ∈ b w iff (e 2 , e 1 ) ∈ R(b) w . This allows us to define R(publicationDate).article1 as the publication date of article 1. We also include aggregation operations (count(u), sum(u) and average(u, b)), and superlatives (argmax(u, b)).Finally, we can construct binaries using lambda abstraction: λx.u denotes a set of (e 1 , e 2 ) where e 1 ∈ u[x/e 2 ] w and u[x/e 2 ] is the logical form where free occurrences of x are replaced with e 2 . For example, R(λx.count(R(cites).x)) denotes the set of entities (e 1 , e 2 ), where e 2 is the number of entities that e 1 cites.\n",
      "----\n",
      "номер секции = 7 | кол-во ссылок 3\n",
      "0 {'start': 442, 'end': 448, 'text': '[2015]', 'latex': None, 'ref_id': None}\n",
      "1 {'start': 1435, 'end': 1458, 'text': 'Berant and Liang (2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "2 {'start': 1494, 'end': 1518, 'text': 'Pasupat and Liang (2015)', 'latex': None, 'ref_id': 'BIBREF17'}\n",
      "Our semantic parsing model defines a distribution over logical forms given by the domaingeneral grammar G and additional rules triggered by the input utterance x. Specifically, given an utterance x, we detect numbers, dates, and perform string matching with database entities to recognize named entities. This results in a set of rules T(x). For example, if x is \"article published in 2015 that cites article 1\", then T(x) contains 2015 → NP [2015] andarticle 1 → NP[article1]. Let L x be the rules in the seed lexicon L where the entity rules (e.g., alice → NP[alice] ) are replaced by T(x). Our semantic parsing model defines a loglinear distribution over candidate pairs (z, c) ∈ GEN(G ∪ L x ):p θ (z, c | x, w) ∝ exp(φ(c, z, x, w) θ),(1)where φ(z, c, x, w) ∈ R d is a feature vector and θ ∈ R d is a parameter vector. To generate candidate logical forms, we use a simple beam search: For each search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance \"article\" would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015) .Training. We train our model by maximizing the regularized log-likelihood O(θ) = Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: \"Show me meetings after the weekly standup day\" c: \"meeting whose date is at least date of weekly standup\" z: type.meeting date. > R(date).weeklyStandup BLOCKS 19 1995 spatial language x: \"Select the brick that is to the furthest left.\" c: \"block that the most number of block is right of\" z: argmax(type.block, R(λx.count(R(right).x))) HOUSING 24 941 measurement units x: \"Housing that is 800 square feet or bigger?\" c: \"housing unit whose size is at least 800 square feet\" z: type.housingUnit area. > .800 RESTAURANTS 32 1657 long unary relations x: \"What restaurant can you eat lunch outside at?\" c: \"restaurant that has outdoor seating and that serves lunch\" z: type.restaurant hasOutdoorSeating serveslunch PUBLICATIONS 15 801 sublexical compositionality x: \"Who has co-authored articles with Efron?\" c: \"person that is author of article whose author is efron\" z: type.person R(author).(type.article author.efron) SOCIAL 45 4419 multi-arity relations x: \"When did alice start attending brown university?\" c: \"start date of student alice whose university is brown university\" z: R(date). Table 3 : We experimented on seven domains, covering a variety of phenomena. For each domain, we show the number of predicates, number of examples, and a (c, z) generated by our framework along with a paraphrased utterance x. Table 4 : Features for the paraphrasing model. pos(x i:i ) is the POS tag; type( z w ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c.\n",
      "----\n",
      "номер секции = 8 | кол-во ссылок 5\n",
      "0 {'start': 70, 'end': 90, 'text': '(Duchi et al., 2010)', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "1 {'start': 253, 'end': 280, 'text': '(Ganitkevitch et al., 2013)', 'latex': None, 'ref_id': 'BIBREF8'}\n",
      "2 {'start': 411, 'end': 431, 'text': '(Liang et al., 2006)', 'latex': None, 'ref_id': 'BIBREF12'}\n",
      "3 {'start': 774, 'end': 793, 'text': '(Och and Ney, 2004)', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "4 {'start': 1213, 'end': 1234, 'text': '(Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 . To optimize, we use AdaGrad (Duchi et al., 2010) .Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) . We count the number of exact matches, PPDB matches, and unmatched words.To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4 ). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) . We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .\n",
      "----\n",
      "номер секции = 13 | кол-во ссылок 1\n",
      "0 {'start': 205, 'end': 229, 'text': '(Zelle and Mooney, 1996)', 'latex': None, 'ref_id': 'BIBREF25'}\n",
      "Geo880. To test how our parser generalizes to utterances independent of our framework, we created a semantic parser for the domain of US geography, and tested on the standard 280 test examples from GEO880 (Zelle and Mooney, 1996) . We did not use the standard 600 training examples. Our parser obtained 56.4% accuracy, which is substantially lower than state-of-the-art (∼ 90%).We performed error analysis on 100 random sentences from the development set where accuracy was 60%. We found that the parser learns from the training data to prefer shorter paraphrases, which accounts for 30% of the errors. In most of these cases, the correct logical form is ranked at the top-3 results (accuracy for the top-3 derivations is 73%). GEO880 contains highly compositional utterances, and in 25% of the errors the correct derivation tree exceeds the maximum depth used for our parser. Another 17.5% of the errors are caused by problems in the paraphrasing model. For example, in the utterance \"what is the size of california\", the model learns that \"size\" corresponds to \"population\" rather than \"area\". Errors related to reordering and the syntactic structure of the input utterance account for 7.5% of the errors. For example, the utterance \"what is the area of the largest state\" is paraphrased to \"state that has the largest area\".Calendar. In Section 7.1, we evaluated on utterances obtained by paraphrasing canonical utterances from the grammar. To examine the coverage of our parser on independently-produced utterances, we asked AMT workers to freely come up with queries. We collected 186 such queries; 5 were spam and discarded. We replaced all entities (people, dates, etc.) with entities from our seed lexicon to avoid focusing on entity detection.We were able to annotate 52% of the utterances with logical forms from our grammar. We could not annotate 20% of the utterances due to relative time references, such as \"What time is my next meeting?\". 14% of the utterances were not covered due to binary predicates not in the grammar (\"What is the agenda of the meeting?\") or missing entities (\"When is Dan's birthday?\"). Another 2% required unsupported calculations (\"How much free time do I have tomorrow?\"), and the rest are out of scope for other reasons (\"When does my Verizon data plan start over?\").We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model.\n",
      "----\n",
      "номер секции = 14 | кол-во ссылок 18\n",
      "0 {'start': 101, 'end': 122, 'text': '(Cai and Yates, 2013;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "1 {'start': 123, 'end': 148, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "2 {'start': 149, 'end': 169, 'text': 'Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "3 {'start': 410, 'end': 438, 'text': '(Kushman and Barzilay, 2013)', 'latex': None, 'ref_id': 'BIBREF10'}\n",
      "4 {'start': 466, 'end': 489, 'text': '(Matuszek et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "5 {'start': 490, 'end': 510, 'text': 'Tellex et al., 2011;', 'latex': None, 'ref_id': 'BIBREF21'}\n",
      "6 {'start': 511, 'end': 542, 'text': 'Krishnamurthy and Kollar, 2013)', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "7 {'start': 756, 'end': 787, 'text': '(Zettlemoyer and Collins, 2005;', 'latex': None, 'ref_id': 'BIBREF26'}\n",
      "8 {'start': 788, 'end': 810, 'text': 'Wong and Mooney, 2007)', 'latex': None, 'ref_id': 'BIBREF23'}\n",
      "9 {'start': 826, 'end': 847, 'text': '(Clarke et al., 2010;', 'latex': None, 'ref_id': 'BIBREF5'}\n",
      "10 {'start': 848, 'end': 867, 'text': 'Liang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "11 {'start': 888, 'end': 917, 'text': '(Artzi and Zettlemoyer, 2011;', 'latex': None, 'ref_id': 'BIBREF0'}\n",
      "12 {'start': 918, 'end': 937, 'text': 'Reddy et al., 2014)', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "13 {'start': 1202, 'end': 1222, 'text': '(Fader et al., 2013)', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "14 {'start': 1244, 'end': 1268, 'text': '(Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "15 {'start': 1393, 'end': 1413, 'text': '(Woods et al., 1972;', 'latex': None, 'ref_id': 'BIBREF24'}\n",
      "16 {'start': 1414, 'end': 1439, 'text': 'Warren and Pereira, 1982)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "17 {'start': 1472, 'end': 1489, 'text': '(Schwitter, 2010)', 'latex': None, 'ref_id': 'BIBREF20'}\n",
      "Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013) . However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013) . Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal.Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014) . All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing.Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014) . We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010) . However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance.In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification.\n",
      "----\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# article_with_sect = dict()\n",
    "for num,article in enumerate(all_articles):\n",
    "    if num == 2:\n",
    "        break\n",
    "    for cnt_sect,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "        if sections['cite_spans']:\n",
    "            print(f'номер секции = {cnt_sect} | кол-во ссылок',len(sections['cite_spans']))\n",
    "            for cnt_cite,cite in enumerate(sections['cite_spans']):\n",
    "                print(cnt_cite,cite)\n",
    "            print(sections['text'])\n",
    "            print('----')\n",
    "    print(10*'==')\n",
    "#             if article['paper_id'] in article_with_sect:\n",
    "#                 article_with_sect[article['paper_id']] +=1\n",
    "#             else:\n",
    "#                 article_with_sect[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_results[0] - Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset∗\n",
    "\n",
    "\n",
    "\n",
    "Result \n",
    "\n",
    "- 0 {'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
    "- 1 {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
    "- 2 {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
    "- 3 {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
    "- 4 {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
    "- 5 {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
    "\n",
    "True \n",
    "\n",
    "- 0 (Goldstein et al., 2000; \n",
    "- 1 Erkan and Radev,2004; \n",
    "- 2 Wan et al., 2007; \n",
    "- 3 Nenkova and McKeown, 2012; \n",
    "- 4 Min et al., 2012; \n",
    "- 5 Bing et al., 2015; \n",
    "- 6 Li et al.,2017)\n",
    "\n",
    "\n",
    "Result\n",
    "\n",
    "- {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
    "\n",
    "True\n",
    "\n",
    "- Woodsend and Lapata (2012), Bing et al. (2015), and Li et al. (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**При большом перечислении подряд ссылок, GROBID не выделяет предпоследнюю ссылку**\n",
    "\n",
    "**Также он не срабатывает на части ссылок**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### проверка наличия текста и названия секций во всех статьях в latex части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_articles if article['latex_parse'] and article['latex_parse']['body_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_body_text_tex = [article['paper_id'] for article in all_articles if not ( article['latex_parse'] and article['latex_parse']['body_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37621"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_ids_not_body_text_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n",
      "====================\n",
      "2 None\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for num,paper_id in enumerate(acl_ids_not_body_text_tex):\n",
    "    if num == 2:\n",
    "        break\n",
    "    id_lst = acl_paper_ids.index(paper_id)\n",
    "    print(id_lst,all_articles[id_lst]['latex_parse'])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "  'title': 'Multi-document summarization by sentence extraction',\n",
       "  'authors': [{'first': 'Jade',\n",
       "    'middle': [],\n",
       "    'last': 'Goldstein',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "   {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "   {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'NAACL-ANLPWorkshop',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '40--48',\n",
       "  'other_ids': {},\n",
       "  'links': '8294822'},\n",
       " 'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "  'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "  'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '4',\n",
       "  'issn': '',\n",
       "  'pages': '365--371',\n",
       "  'other_ids': {},\n",
       "  'links': '10418456'},\n",
       " 'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "  'title': 'Auto-encoding variational bayes',\n",
       "  'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "   {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '15789289'},\n",
       " 'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "  'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "  'authors': [{'first': 'Danilo',\n",
       "    'middle': [],\n",
       "    'last': 'Jimenez Rezende',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "   {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICML',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1278--1286',\n",
       "  'other_ids': {},\n",
       "  'links': '16895865'},\n",
       " 'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "  'title': 'Effective approaches to attention-based neural machine translation',\n",
       "  'authors': [{'first': 'Minh-Thang',\n",
       "    'middle': [],\n",
       "    'last': 'Luong',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "   {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1412--1421',\n",
       "  'other_ids': {},\n",
       "  'links': '1998416'},\n",
       " 'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "  'title': 'Multiple aspect summarization using integer linear programming',\n",
       "  'authors': [{'first': 'Kristian',\n",
       "    'middle': [],\n",
       "    'last': 'Woodsend',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'EMNLP-CNLL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '233--243',\n",
       "  'other_ids': {},\n",
       "  'links': '17497992'},\n",
       " 'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "  'title': 'Linear programming 1: introduction',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "   {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "  'year': 2006,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '53739754'},\n",
       " 'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "  'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "  'authors': [{'first': 'Mark', 'middle': [], 'last': 'Wasson', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1364--1368',\n",
       "  'other_ids': {},\n",
       "  'links': '12681629'},\n",
       " 'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "  'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "  'authors': [{'first': 'Hongyan',\n",
       "    'middle': [],\n",
       "    'last': 'Dragomir R Radev',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '21--30',\n",
       "  'other_ids': {},\n",
       "  'links': '1320'},\n",
       " 'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "  'title': 'Textrank: Bringing order into texts',\n",
       "  'authors': [{'first': 'Rada',\n",
       "    'middle': [],\n",
       "    'last': 'Mihalcea',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '577937'},\n",
       " 'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "  'title': 'Adam: A method for stochastic optimization',\n",
       "  'authors': [{'first': 'Diederik',\n",
       "    'middle': [],\n",
       "    'last': 'Kingma',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '6628106'},\n",
       " 'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "  'title': 'A survey of text summarization techniques',\n",
       "  'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "   {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Mining Text Data',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '43--76',\n",
       "  'other_ids': {},\n",
       "  'links': '556431'},\n",
       " 'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "  'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "  'authors': [{'first': 'Yen',\n",
       "    'middle': ['Kan'],\n",
       "    'last': 'Ziheng Lin Min',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'COLING',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '2093--2108',\n",
       "  'other_ids': {},\n",
       "  'links': '6317274'},\n",
       " 'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "  'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "  'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "   {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1587--1597',\n",
       "  'other_ids': {},\n",
       "  'links': '8377315'},\n",
       " 'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "  'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'AAAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '3497--3503',\n",
       "  'other_ids': {},\n",
       "  'links': '29562039'},\n",
       " 'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "  'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "  'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "   {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "   {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '291--298',\n",
       "  'other_ids': {},\n",
       "  'links': '13723748'},\n",
       " 'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "  'title': 'Social context summarization',\n",
       "  'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "   {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "   {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "   {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "   {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '255--264',\n",
       "  'other_ids': {},\n",
       "  'links': '704517'},\n",
       " 'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "  'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1270--1276',\n",
       "  'other_ids': {},\n",
       "  'links': '14777460'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: all_articles[0]['latex_parse']['bib_entries'][k] for k in sorted(all_articles[0]['latex_parse']['bib_entries'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_with_sect_latex = dict()\n",
    "for article in all_articles:\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']: \n",
    "        for sections in article['latex_parse']['body_text']:\n",
    "            if sections['section']:\n",
    "                if article['paper_id'] in article_with_sect_latex:\n",
    "                    article_with_sect_latex[article['paper_id']] +=1\n",
    "                else:\n",
    "                    article_with_sect_latex[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Количество статей, в которых есть названия секций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3657"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_with_sect_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка ситуации, когда есть **latex_parse**, но  нет **grobid_parse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_art ,article in enumerate(all_articles):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        if len(article['grobid_parse']['body_text'])==0:\n",
    "            print(num_art,articcle['paper_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не бывает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение обзорной части статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Самый простой принцип построения - по максимальному количеству ссылок в абзаце:\n",
    " - **Решение**:\n",
    "    - подсчитать количество ссылок в каждой секции\n",
    "    - выбрать секцию с максимальным количеством ссылок (возможно ещё оставить ещё 1 секцию, в которой количество ссылок было больше половины чем в максимальной)\n",
    "    - для latex статей надо объединить текст одинаковых секций в 1 абзац\n",
    " - **Критерий**:\n",
    "    -  в части latex публикаций есть названия секций => после выделения обзорных часте можно посмотреть какие секции выделились: какие топ-3, сделать просмотр глазами и после этого решать что делать дальше.\n",
    "    - возможно логично сохранять для 2 максимального текста название статей\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [{'start': 956, 'end': 964, 'text': 'Figure 1', 'latex': None, 'ref_id': 'FIGREF0'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 451, 'end': 468, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 12, 'end': 20, 'text': 'Figure 2', 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "6 {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [{'start': 32, 'end': 58, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 59, 'end': 79, 'text': 'Rezende et al., 2014', 'latex': None, 'ref_id': 'BIBREF18'}, {'start': 184, 'end': 200, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2384, 'end': 2401, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2433, 'end': 2456, 'text': '(Bahdanau et al., 2015;', 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 2457, 'end': 2476, 'text': 'Luong et al., 2015)', 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "3 {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .', 'cite_spans': [{'start': 86, 'end': 102, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 1133, 'end': 1158, 'text': '(Dantzig and Thapa, 2006)', 'latex': None, 'ref_id': 'BIBREF3'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 107, 'end': 118, 'text': '(Lin, 2004)', 'latex': None, 'ref_id': 'BIBREF12'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "4 {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [{'start': 351, 'end': 365, 'text': '(Wasson, 1998)', 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 492, 'end': 512, 'text': '(Radev et al., 2000)', 'latex': None, 'ref_id': 'BIBREF17'}, {'start': 700, 'end': 723, 'text': '(Erkan and Radev, 2004)', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 737, 'end': 763, 'text': '(Mihalcea and Tarau, 2004)', 'latex': None, 'ref_id': 'BIBREF14'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "2 {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .', 'cite_spans': [{'start': 498, 'end': 518, 'text': '(Kingma and Ba, 2014', 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 652, 'end': 674, 'text': '(Bastien et al., 2012)', 'latex': None, 'ref_id': 'BIBREF1'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "2 {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.', 'cite_spans': [{'start': 690, 'end': 707, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 908, 'end': 925, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 77, 'end': 84, 'text': 'Table 1', 'latex': None, 'ref_id': 'TABREF0'}, {'start': 746, 'end': 753, 'text': 'Table 2', 'latex': None, 'ref_id': 'TABREF1'}, {'start': 1184, 'end': 1191, 'text': 'Table 3', 'latex': None, 'ref_id': 'TABREF2'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".', 'cite_spans': [], 'ref_spans': [{'start': 315, 'end': 322, 'text': 'Table 4', 'latex': None, 'ref_id': 'TABREF3'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ', 'cite_spans': [{'start': 517, 'end': 868, 'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world', 'latex': None, 'ref_id': None}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "grobid_parse_overview = dict()\n",
    "for num_sec,sections in enumerate(all_articles[0]['grobid_parse']['body_text']):\n",
    "    grobid_parse_overview[num_sec] = sections\n",
    "    print(len(grobid_parse_overview[num_sec]['cite_spans']),sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "  'cite_spans': [{'start': 192,\n",
       "    'end': 216,\n",
       "    'text': '(Goldstein et al., 2000;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'},\n",
       "   {'start': 217,\n",
       "    'end': 239,\n",
       "    'text': 'Erkan and Radev, 2004;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 240,\n",
       "    'end': 257,\n",
       "    'text': 'Wan et al., 2007;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF19'},\n",
       "   {'start': 258,\n",
       "    'end': 284,\n",
       "    'text': 'Nenkova and McKeown, 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF16'},\n",
       "   {'start': 285,\n",
       "    'end': 302,\n",
       "    'text': 'Min et al., 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF15'},\n",
       "   {'start': 303,\n",
       "    'end': 319,\n",
       "    'text': 'Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 773,\n",
       "    'end': 797,\n",
       "    'text': '(Project Code: 14203414)',\n",
       "    'latex': None,\n",
       "    'ref_id': None},\n",
       "   {'start': 2288,\n",
       "    'end': 2305,\n",
       "    'text': '(Hu et al., 2008;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 2306,\n",
       "    'end': 2324,\n",
       "    'text': 'Yang et al., 2011)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF22'},\n",
       "   {'start': 2582,\n",
       "    'end': 2598,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 2911,\n",
       "    'end': 2927,\n",
       "    'text': 'Li et al. (2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 3069,\n",
       "    'end': 3095,\n",
       "    'text': '(Kingma and Welling, 2014;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'},\n",
       "   {'start': 3096,\n",
       "    'end': 3117,\n",
       "    'text': 'Rezende et al., 2014)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF18'}],\n",
       "  'ref_spans': [{'start': 956,\n",
       "    'end': 964,\n",
       "    'text': 'Figure 1',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF0'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 1: {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "  'cite_spans': [{'start': 451,\n",
       "    'end': 468,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'}],\n",
       "  'ref_spans': [{'start': 12,\n",
       "    'end': 20,\n",
       "    'text': 'Figure 2',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF2'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 2: {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "  'cite_spans': [{'start': 32,\n",
       "    'end': 58,\n",
       "    'text': '(Kingma and Welling, 2014;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'},\n",
       "   {'start': 59,\n",
       "    'end': 79,\n",
       "    'text': 'Rezende et al., 2014',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF18'},\n",
       "   {'start': 184,\n",
       "    'end': 200,\n",
       "    'text': 'Li et al. (2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 2384,\n",
       "    'end': 2401,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 2433,\n",
       "    'end': 2456,\n",
       "    'text': '(Bahdanau et al., 2015;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF0'},\n",
       "   {'start': 2457,\n",
       "    'end': 2476,\n",
       "    'text': 'Luong et al., 2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF13'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 3: {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "  'cite_spans': [{'start': 86,\n",
       "    'end': 102,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 971,\n",
       "    'end': 987,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 1133,\n",
       "    'end': 1158,\n",
       "    'text': '(Dantzig and Thapa, 2006)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF3'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 4: {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 5: {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 6: {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 7: {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 8: {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "  'cite_spans': [{'start': 107,\n",
       "    'end': 118,\n",
       "    'text': '(Lin, 2004)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF12'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 9: {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "  'cite_spans': [{'start': 351,\n",
       "    'end': 365,\n",
       "    'text': '(Wasson, 1998)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF20'},\n",
       "   {'start': 492,\n",
       "    'end': 512,\n",
       "    'text': '(Radev et al., 2000)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF17'},\n",
       "   {'start': 700,\n",
       "    'end': 723,\n",
       "    'text': '(Erkan and Radev, 2004)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 737,\n",
       "    'end': 763,\n",
       "    'text': '(Mihalcea and Tarau, 2004)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF14'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 10: {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "  'cite_spans': [{'start': 498,\n",
       "    'end': 518,\n",
       "    'text': '(Kingma and Ba, 2014',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF8'},\n",
       "   {'start': 652,\n",
       "    'end': 674,\n",
       "    'text': '(Bastien et al., 2012)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF1'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 11: {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "  'cite_spans': [{'start': 690,\n",
       "    'end': 707,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 908,\n",
       "    'end': 925,\n",
       "    'text': '(Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'}],\n",
       "  'ref_spans': [{'start': 77,\n",
       "    'end': 84,\n",
       "    'text': 'Table 1',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF0'},\n",
       "   {'start': 746,\n",
       "    'end': 753,\n",
       "    'text': 'Table 2',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF1'},\n",
       "   {'start': 1184,\n",
       "    'end': 1191,\n",
       "    'text': 'Table 3',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF2'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 12: {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 315,\n",
       "    'end': 322,\n",
       "    'text': 'Table 4',\n",
       "    'latex': None,\n",
       "    'ref_id': 'TABREF3'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 13: {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "  'cite_spans': [{'start': 517,\n",
       "    'end': 868,\n",
       "    'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "    'latex': None,\n",
       "    'ref_id': None}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None}}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_parse_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'cite_spans': [], 'ref_spans': [{'start': 118, 'end': 125, 'text': None, 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'cite_spans': [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Overview 1\n",
      "{'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 19, 'end': 26, 'text': None, 'latex': None, 'ref_id': 'FIGREF7'}], 'eq_spans': [{'start': 212, 'end': 223, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 739, 'end': 750, 'text': 'ρ i ', 'latex': '\\\\rho _i', 'ref_id': None}, {'start': 774, 'end': 785, 'text': '𝐱 c i ', 'latex': '\\\\mathbf {x}_c^i', 'ref_id': None}, {'start': 807, 'end': 818, 'text': 'ρ∈ℝ n c  ', 'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}], 'section': 'Overview'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "{'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'cite_spans': [{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [{'start': 472, 'end': 483, 'text': 'p θ (𝐳)=𝒩(0,𝐈)', 'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})', 'ref_id': None}, {'start': 488, 'end': 499, 'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)', 'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})', 'ref_id': None}, {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None}, {'start': 524, 'end': 535, 'text': 'σ', 'latex': '\\\\sigma ', 'ref_id': None}, {'start': 799, 'end': 811, 'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\", 'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n', 'ref_id': 'EQREF9'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [{'start': 50, 'end': 56, 'text': None, 'latex': None, 'ref_id': 'EQREF9'}], 'eq_spans': [{'start': 131, 'end': 142, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 145, 'end': 157, 'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\", 'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ', 'ref_id': 'EQREF10'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': '𝐱', 'latex': '\\\\mathbf {x}', 'ref_id': None}, {'start': 76, 'end': 87, 'text': '𝐱 d ', 'latex': '\\\\mathbf {x}_d', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐱 c ', 'latex': '\\\\mathbf {x}_c', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 364, 'end': 375, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 444, 'end': 456, 'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )', 'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n', 'ref_id': 'EQREF11'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The calculation of INLINEFORM0 will be discussed later.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 19, 'end': 30, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 113, 'end': 124, 'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }', 'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ', 'ref_id': None}, {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 219, 'end': 230, 'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }', 'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ', 'ref_id': None}, {'start': 320, 'end': 331, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 335, 'end': 346, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 402, 'end': 413, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 416, 'end': 428, 'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )', 'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n', 'ref_id': 'EQREF12'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'cite_spans': [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [{'start': 304, 'end': 315, 'text': 's h i ', 'latex': 's^i_{h}', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'h d j ', 'latex': 'h^j_{d}', 'ref_id': None}, {'start': 401, 'end': 412, 'text': 'a d ∈ℝ n d  ', 'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}', 'ref_id': None}, {'start': 472, 'end': 483, 'text': 'h c j ', 'latex': 'h^j_{c}', 'ref_id': None}, {'start': 507, 'end': 518, 'text': 'a c ∈ℝ n c  ', 'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}, {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 672, 'end': 684, 'text': 'a ˜ c =a c ×ρ', 'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n', 'ref_id': 'EQREF13'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 30, 'end': 41, 'text': 'c d i ', 'latex': 'c_d^i', 'ref_id': None}, {'start': 79, 'end': 90, 'text': 'c c i ', 'latex': 'c_c^i', 'ref_id': None}, {'start': 240, 'end': 252, 'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )', 'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n', 'ref_id': 'EQREF14'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 64, 'end': 75, 'text': 's ˜ h i ', 'latex': '{{\\\\tilde{s}}_h^i}', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 15, 'end': 26, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 33, 'end': 44, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 220, 'end': 231, 'text': '𝐀 d ∈ℝ n d ×m ', 'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}', 'ref_id': None}, {'start': 297, 'end': 308, 'text': '𝐀 c ∈ℝ n c ×m ', 'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}', 'ref_id': None}, {'start': 569, 'end': 581, 'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )', 'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n', 'ref_id': 'EQREF15'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 170, 'end': 182, 'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )', 'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n', 'ref_id': 'EQREF16'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'Θ', 'latex': '\\\\Theta ', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐀 d ', 'latex': '\\\\mathbf {A}_d', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 434, 'end': 445, 'text': 'R∈ℝ n d ×n c  ', 'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}', 'ref_id': None}, {'start': 450, 'end': 462, 'text': 'R=X d ×X c T ', 'latex': '\\nR = X_d\\\\times X_c^T\\n', 'ref_id': 'EQREF17'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 93, 'end': 105, 'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]', 'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n', 'ref_id': 'EQREF18'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': '(0,1)', 'latex': '(0,1)', 'ref_id': None}, {'start': 84, 'end': 96, 'text': 'ρ=sigmoid(𝐫)', 'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n', 'ref_id': 'EQREF19'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 332, 'end': 343, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 346, 'end': 358, 'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ', 'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n', 'ref_id': 'EQREF20'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'ρ z ', 'latex': '\\\\rho _z', 'ref_id': None}, {'start': 22, 'end': 33, 'text': 'ρ x ', 'latex': '\\\\rho _x', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Summary Construction 2\n",
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'cite_spans': [{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [{'start': 485, 'end': 497, 'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },', 'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n', 'ref_id': 'EQREF22'}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Summary Construction 4\n",
      "{'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.', 'cite_spans': [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'α i ', 'latex': '\\\\alpha _i', 'ref_id': None}, {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None}, {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 126, 'end': 137, 'text': 'α ij ', 'latex': '\\\\alpha _{ij}', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'R ij ', 'latex': 'R_{ij}', 'ref_id': None}, {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 234, 'end': 245, 'text': 'P j ', 'latex': 'P_j', 'ref_id': None}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Data Description 0\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Description'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Properties 0\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}], 'eq_spans': [], 'section': 'Data Properties'}\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "{'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}], 'ref_spans': [{'start': 58, 'end': 66, 'text': None, 'latex': None, 'ref_id': 'SECREF28'}], 'eq_spans': [], 'section': 'Dataset and Metrics'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'cite_spans': [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'cite_spans': [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 2\n",
      "{'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Experimental Settings 2\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.', 'cite_spans': [{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': '|V|', 'latex': '|V|', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None}, {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 360, 'end': 371, 'text': 'm=5', 'latex': 'm = 5', 'ref_id': None}, {'start': 431, 'end': 442, 'text': 'd h =500', 'latex': 'd_h = 500', 'ref_id': None}, {'start': 463, 'end': 474, 'text': 'K=100', 'latex': 'K = 100', 'ref_id': None}, {'start': 495, 'end': 506, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 538, 'end': 549, 'text': 'λ p =0.2', 'latex': '\\\\lambda _p=0.2', 'ref_id': None}], 'section': 'Experimental Settings'}\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.', 'cite_spans': [], 'ref_spans': [{'start': 83, 'end': 91, 'text': None, 'latex': None, 'ref_id': 'TABREF40'}], 'eq_spans': [{'start': 240, 'end': 251, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Results on Our Dataset'}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', 'cite_spans': [{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'TABREF42'}], 'eq_spans': [{'start': 380, 'end': 391, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\", 'cite_spans': [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 305, 'end': 313, 'text': None, 'latex': None, 'ref_id': 'TABREF43'}], 'eq_spans': [], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Case Study 0\n",
      "{'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.', 'cite_spans': [], 'ref_spans': [{'start': 250, 'end': 258, 'text': None, 'latex': None, 'ref_id': 'TABREF45'}], 'eq_spans': [], 'section': 'Case Study'}\n",
      "====================\n",
      "Conclusions 0\n",
      "{'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusions'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "latex_parse_overview = dict()\n",
    "for sections in all_articles[0]['latex_parse']['body_text']:\n",
    "    latex_parse_overview[sections['section']] = sections\n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.\n",
      "====================\n",
      "Introduction 0\n",
      "With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\n",
      "====================\n",
      "Introduction 3\n",
      "One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.\n",
      "====================\n",
      "Introduction 3\n",
      "Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.\n",
      "====================\n",
      "Introduction 0\n",
      "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.\n",
      "====================\n",
      "Introduction 0\n",
      "Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "====================\n",
      "Overview 1\n",
      "As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The calculation of INLINEFORM0 will be discussed later.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      " INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.\n",
      "====================\n",
      "Summary Construction 2\n",
      "In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 \n",
      "====================\n",
      "Summary Construction 4\n",
      "where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.\n",
      "====================\n",
      "Data Description 0\n",
      "In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.\n",
      "====================\n",
      "Background 0\n",
      "The definition of the terminology related to the dataset is given as follows.\n",
      "====================\n",
      "Background 0\n",
      "Topic: A topic refers to an event and it is composed of a set of news documents from different sources.\n",
      "====================\n",
      "Background 0\n",
      "Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.\n",
      "====================\n",
      "Background 0\n",
      "Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).\n",
      "====================\n",
      "Background 0\n",
      "Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.\n",
      "====================\n",
      "Background 0\n",
      "Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.\n",
      "====================\n",
      "Background 0\n",
      "Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.\n",
      "====================\n",
      "Data Collection 0\n",
      "The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .\n",
      "====================\n",
      "Data Collection 0\n",
      "For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.\n",
      "====================\n",
      "Data Collection 0\n",
      "After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.\n",
      "====================\n",
      "Data Properties 0\n",
      "The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n",
      "====================\n",
      "Comparative Methods 0\n",
      "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:\n",
      "====================\n",
      "Comparative Methods 1\n",
      "RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.\n",
      "====================\n",
      "Comparative Methods 2\n",
      "LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.\n",
      "====================\n",
      "Comparative Methods 0\n",
      "We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.\n",
      "====================\n",
      "Experimental Settings 2\n",
      "The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\n",
      "====================\n",
      "Case Study 0\n",
      "Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.\n",
      "====================\n",
      "Conclusions 0\n",
      "We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for article in all_articles[0]['latex_parse']['body_text']:\n",
    "    print(article['section'],len(article['cite_spans']))\n",
    "    print(article['text'])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'cite_spans': [], 'ref_spans': [{'start': 118, 'end': 125, 'text': None, 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'cite_spans': [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Overview 1\n",
      "{'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 19, 'end': 26, 'text': None, 'latex': None, 'ref_id': 'FIGREF7'}], 'eq_spans': [{'start': 212, 'end': 223, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 739, 'end': 750, 'text': 'ρ i ', 'latex': '\\\\rho _i', 'ref_id': None}, {'start': 774, 'end': 785, 'text': '𝐱 c i ', 'latex': '\\\\mathbf {x}_c^i', 'ref_id': None}, {'start': 807, 'end': 818, 'text': 'ρ∈ℝ n c  ', 'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}], 'section': 'Overview'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "{'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'cite_spans': [{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [{'start': 472, 'end': 483, 'text': 'p θ (𝐳)=𝒩(0,𝐈)', 'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})', 'ref_id': None}, {'start': 488, 'end': 499, 'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)', 'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})', 'ref_id': None}, {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None}, {'start': 524, 'end': 535, 'text': 'σ', 'latex': '\\\\sigma ', 'ref_id': None}, {'start': 799, 'end': 811, 'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\", 'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n', 'ref_id': 'EQREF9'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [{'start': 50, 'end': 56, 'text': None, 'latex': None, 'ref_id': 'EQREF9'}], 'eq_spans': [{'start': 131, 'end': 142, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 145, 'end': 157, 'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\", 'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ', 'ref_id': 'EQREF10'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': '𝐱', 'latex': '\\\\mathbf {x}', 'ref_id': None}, {'start': 76, 'end': 87, 'text': '𝐱 d ', 'latex': '\\\\mathbf {x}_d', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐱 c ', 'latex': '\\\\mathbf {x}_c', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 364, 'end': 375, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 444, 'end': 456, 'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )', 'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n', 'ref_id': 'EQREF11'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The calculation of INLINEFORM0 will be discussed later.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 19, 'end': 30, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 113, 'end': 124, 'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }', 'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ', 'ref_id': None}, {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 219, 'end': 230, 'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }', 'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ', 'ref_id': None}, {'start': 320, 'end': 331, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 335, 'end': 346, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 402, 'end': 413, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 416, 'end': 428, 'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )', 'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n', 'ref_id': 'EQREF12'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'cite_spans': [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [{'start': 304, 'end': 315, 'text': 's h i ', 'latex': 's^i_{h}', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'h d j ', 'latex': 'h^j_{d}', 'ref_id': None}, {'start': 401, 'end': 412, 'text': 'a d ∈ℝ n d  ', 'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}', 'ref_id': None}, {'start': 472, 'end': 483, 'text': 'h c j ', 'latex': 'h^j_{c}', 'ref_id': None}, {'start': 507, 'end': 518, 'text': 'a c ∈ℝ n c  ', 'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}, {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 672, 'end': 684, 'text': 'a ˜ c =a c ×ρ', 'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n', 'ref_id': 'EQREF13'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 30, 'end': 41, 'text': 'c d i ', 'latex': 'c_d^i', 'ref_id': None}, {'start': 79, 'end': 90, 'text': 'c c i ', 'latex': 'c_c^i', 'ref_id': None}, {'start': 240, 'end': 252, 'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )', 'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n', 'ref_id': 'EQREF14'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 64, 'end': 75, 'text': 's ˜ h i ', 'latex': '{{\\\\tilde{s}}_h^i}', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 15, 'end': 26, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 33, 'end': 44, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 220, 'end': 231, 'text': '𝐀 d ∈ℝ n d ×m ', 'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}', 'ref_id': None}, {'start': 297, 'end': 308, 'text': '𝐀 c ∈ℝ n c ×m ', 'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}', 'ref_id': None}, {'start': 569, 'end': 581, 'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )', 'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n', 'ref_id': 'EQREF15'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 170, 'end': 182, 'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )', 'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n', 'ref_id': 'EQREF16'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'Θ', 'latex': '\\\\Theta ', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐀 d ', 'latex': '\\\\mathbf {A}_d', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 434, 'end': 445, 'text': 'R∈ℝ n d ×n c  ', 'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}', 'ref_id': None}, {'start': 450, 'end': 462, 'text': 'R=X d ×X c T ', 'latex': '\\nR = X_d\\\\times X_c^T\\n', 'ref_id': 'EQREF17'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 93, 'end': 105, 'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]', 'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n', 'ref_id': 'EQREF18'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': '(0,1)', 'latex': '(0,1)', 'ref_id': None}, {'start': 84, 'end': 96, 'text': 'ρ=sigmoid(𝐫)', 'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n', 'ref_id': 'EQREF19'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 332, 'end': 343, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 346, 'end': 358, 'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ', 'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n', 'ref_id': 'EQREF20'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'ρ z ', 'latex': '\\\\rho _z', 'ref_id': None}, {'start': 22, 'end': 33, 'text': 'ρ x ', 'latex': '\\\\rho _x', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Summary Construction 2\n",
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'cite_spans': [{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [{'start': 485, 'end': 497, 'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },', 'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n', 'ref_id': 'EQREF22'}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Summary Construction 4\n",
      "{'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.', 'cite_spans': [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'α i ', 'latex': '\\\\alpha _i', 'ref_id': None}, {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None}, {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 126, 'end': 137, 'text': 'α ij ', 'latex': '\\\\alpha _{ij}', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'R ij ', 'latex': 'R_{ij}', 'ref_id': None}, {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 234, 'end': 245, 'text': 'P j ', 'latex': 'P_j', 'ref_id': None}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Data Description 0\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Description'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Properties 0\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}], 'eq_spans': [], 'section': 'Data Properties'}\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "{'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}], 'ref_spans': [{'start': 58, 'end': 66, 'text': None, 'latex': None, 'ref_id': 'SECREF28'}], 'eq_spans': [], 'section': 'Dataset and Metrics'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'cite_spans': [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'cite_spans': [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 2\n",
      "{'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Experimental Settings 2\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.', 'cite_spans': [{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': '|V|', 'latex': '|V|', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None}, {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 360, 'end': 371, 'text': 'm=5', 'latex': 'm = 5', 'ref_id': None}, {'start': 431, 'end': 442, 'text': 'd h =500', 'latex': 'd_h = 500', 'ref_id': None}, {'start': 463, 'end': 474, 'text': 'K=100', 'latex': 'K = 100', 'ref_id': None}, {'start': 495, 'end': 506, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 538, 'end': 549, 'text': 'λ p =0.2', 'latex': '\\\\lambda _p=0.2', 'ref_id': None}], 'section': 'Experimental Settings'}\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.', 'cite_spans': [], 'ref_spans': [{'start': 83, 'end': 91, 'text': None, 'latex': None, 'ref_id': 'TABREF40'}], 'eq_spans': [{'start': 240, 'end': 251, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Results on Our Dataset'}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', 'cite_spans': [{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'TABREF42'}], 'eq_spans': [{'start': 380, 'end': 391, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\", 'cite_spans': [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 305, 'end': 313, 'text': None, 'latex': None, 'ref_id': 'TABREF43'}], 'eq_spans': [], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Case Study 0\n",
      "{'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.', 'cite_spans': [], 'ref_spans': [{'start': 250, 'end': 258, 'text': None, 'latex': None, 'ref_id': 'TABREF45'}], 'eq_spans': [], 'section': 'Case Study'}\n",
      "====================\n",
      "Conclusions 0\n",
      "{'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusions'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "latex_parse_overview = dict()\n",
    "for sections in all_articles[0]['latex_parse']['body_text']:\n",
    "    if sections['section'] in latex_parse_overview:\n",
    "        # если есть дублирование, такое бывает у первых часте\n",
    "        if latex_parse_overview[sections['section']] == sections:\n",
    "            continue\n",
    "        else:\n",
    "            latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "            latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "            latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "            latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "#             latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "    else:\n",
    "        latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                      'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                      'section':[sections['section']], \n",
    "                                                      'bib_entries':all_articles[0]['latex_parse']}\n",
    "        \n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "  \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "  'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "  'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "  'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "  'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       " 'cite_spans': [[{'start': 193,\n",
       "    'end': 200,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF0'},\n",
       "   {'start': 203,\n",
       "    'end': 210,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF1'},\n",
       "   {'start': 213,\n",
       "    'end': 220,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 223,\n",
       "    'end': 230,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF3'},\n",
       "   {'start': 233,\n",
       "    'end': 240,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 243,\n",
       "    'end': 250,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF5'},\n",
       "   {'start': 253,\n",
       "    'end': 260,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'}],\n",
       "  [],\n",
       "  [{'start': 527,\n",
       "    'end': 534,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 537,\n",
       "    'end': 544,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF8'},\n",
       "   {'start': 802,\n",
       "    'end': 809,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'}],\n",
       "  [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "   {'start': 159,\n",
       "    'end': 167,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF10'},\n",
       "   {'start': 170,\n",
       "    'end': 178,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'}],\n",
       "  [],\n",
       "  []],\n",
       " 'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       " 'section': ['Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction'],\n",
       " 'bib_entries': {'abstract': [],\n",
       "  'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "    'cite_spans': [{'start': 193,\n",
       "      'end': 200,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 203,\n",
       "      'end': 210,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 213,\n",
       "      'end': 220,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 223,\n",
       "      'end': 230,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 233,\n",
       "      'end': 240,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 243,\n",
       "      'end': 250,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 253,\n",
       "      'end': 260,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 118,\n",
       "      'end': 125,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "    'cite_spans': [{'start': 527,\n",
       "      'end': 534,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 537,\n",
       "      'end': 544,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 802,\n",
       "      'end': 809,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 159,\n",
       "      'end': 167,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 170,\n",
       "      'end': 178,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "    'cite_spans': [{'start': 489,\n",
       "      'end': 496,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 19,\n",
       "      'end': 26,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF7'}],\n",
       "    'eq_spans': [{'start': 212,\n",
       "      'end': 223,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 228,\n",
       "      'end': 239,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 254,\n",
       "      'end': 265,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 285,\n",
       "      'end': 296,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 739,\n",
       "      'end': 750,\n",
       "      'text': 'ρ i ',\n",
       "      'latex': '\\\\rho _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 774,\n",
       "      'end': 785,\n",
       "      'text': '𝐱 c i ',\n",
       "      'latex': '\\\\mathbf {x}_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 807,\n",
       "      'end': 818,\n",
       "      'text': 'ρ∈ℝ n c  ',\n",
       "      'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Overview'},\n",
       "   {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 32,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 43,\n",
       "      'end': 51,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 154,\n",
       "      'end': 161,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "      'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 488,\n",
       "      'end': 499,\n",
       "      'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "      'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None},\n",
       "     {'start': 524,\n",
       "      'end': 535,\n",
       "      'text': 'σ',\n",
       "      'latex': '\\\\sigma ',\n",
       "      'ref_id': None},\n",
       "     {'start': 799,\n",
       "      'end': 811,\n",
       "      'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 50,\n",
       "      'end': 56,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'eq_spans': [{'start': 131,\n",
       "      'end': 142,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 145,\n",
       "      'end': 157,\n",
       "      'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "      'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "      'ref_id': 'EQREF10'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': '𝐱',\n",
       "      'latex': '\\\\mathbf {x}',\n",
       "      'ref_id': None},\n",
       "     {'start': 76,\n",
       "      'end': 87,\n",
       "      'text': '𝐱 d ',\n",
       "      'latex': '\\\\mathbf {x}_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐱 c ',\n",
       "      'latex': '\\\\mathbf {x}_c',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 364,\n",
       "      'end': 375,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 430,\n",
       "      'end': 441,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 444,\n",
       "      'end': 456,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "      'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "      'ref_id': 'EQREF11'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 19,\n",
       "      'end': 30,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 113,\n",
       "      'end': 124,\n",
       "      'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "      'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 219,\n",
       "      'end': 230,\n",
       "      'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "      'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 320,\n",
       "      'end': 331,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 335,\n",
       "      'end': 346,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 402,\n",
       "      'end': 413,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 416,\n",
       "      'end': 428,\n",
       "      'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "      'ref_id': 'EQREF12'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 7,\n",
       "      'end': 14,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 46,\n",
       "      'end': 54,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'},\n",
       "     {'start': 57,\n",
       "      'end': 65,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 304,\n",
       "      'end': 315,\n",
       "      'text': 's h i ',\n",
       "      'latex': 's^i_{h}',\n",
       "      'ref_id': None},\n",
       "     {'start': 366,\n",
       "      'end': 377,\n",
       "      'text': 'h d j ',\n",
       "      'latex': 'h^j_{d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 401,\n",
       "      'end': 412,\n",
       "      'text': 'a d ∈ℝ n d  ',\n",
       "      'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'h c j ',\n",
       "      'latex': 'h^j_{c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 507,\n",
       "      'end': 518,\n",
       "      'text': 'a c ∈ℝ n c  ',\n",
       "      'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 616,\n",
       "      'end': 627,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 672,\n",
       "      'end': 684,\n",
       "      'text': 'a ˜ c =a c ×ρ',\n",
       "      'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "      'ref_id': 'EQREF13'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 30,\n",
       "      'end': 41,\n",
       "      'text': 'c d i ',\n",
       "      'latex': 'c_d^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 79,\n",
       "      'end': 90,\n",
       "      'text': 'c c i ',\n",
       "      'latex': 'c_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 240,\n",
       "      'end': 252,\n",
       "      'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "      'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "      'ref_id': 'EQREF14'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 64,\n",
       "      'end': 75,\n",
       "      'text': 's ˜ h i ',\n",
       "      'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 1,\n",
       "      'end': 12,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 15,\n",
       "      'end': 26,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 33,\n",
       "      'end': 44,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "      'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 297,\n",
       "      'end': 308,\n",
       "      'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "      'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 569,\n",
       "      'end': 581,\n",
       "      'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "      'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "      'ref_id': 'EQREF15'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 170,\n",
       "      'end': 182,\n",
       "      'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "      'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "      'ref_id': 'EQREF16'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'Θ',\n",
       "      'latex': '\\\\Theta ',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐀 d ',\n",
       "      'latex': '\\\\mathbf {A}_d',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 199,\n",
       "      'end': 210,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 348,\n",
       "      'end': 359,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 390,\n",
       "      'end': 401,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 434,\n",
       "      'end': 445,\n",
       "      'text': 'R∈ℝ n d ×n c  ',\n",
       "      'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 450,\n",
       "      'end': 462,\n",
       "      'text': 'R=X d ×X c T ',\n",
       "      'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "      'ref_id': 'EQREF17'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 93,\n",
       "      'end': 105,\n",
       "      'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "      'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "      'ref_id': 'EQREF18'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': '(0,1)',\n",
       "      'latex': '(0,1)',\n",
       "      'ref_id': None},\n",
       "     {'start': 84,\n",
       "      'end': 96,\n",
       "      'text': 'ρ=sigmoid(𝐫)',\n",
       "      'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "      'ref_id': 'EQREF19'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 332,\n",
       "      'end': 343,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 346,\n",
       "      'end': 358,\n",
       "      'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "      'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "      'ref_id': 'EQREF20'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'ρ z ',\n",
       "      'latex': '\\\\rho _z',\n",
       "      'ref_id': None},\n",
       "     {'start': 22,\n",
       "      'end': 33,\n",
       "      'text': 'ρ x ',\n",
       "      'latex': '\\\\rho _x',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 82,\n",
       "      'end': 89,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 94,\n",
       "      'end': 101,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 485,\n",
       "      'end': 497,\n",
       "      'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "      'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "      'ref_id': 'EQREF22'}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "    'cite_spans': [{'start': 466,\n",
       "      'end': 474,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'},\n",
       "     {'start': 477,\n",
       "      'end': 484,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 491,\n",
       "      'end': 498,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 644,\n",
       "      'end': 652,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'α i ',\n",
       "      'latex': '\\\\alpha _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "     {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "     {'start': 112,\n",
       "      'end': 123,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 126,\n",
       "      'end': 137,\n",
       "      'text': 'α ij ',\n",
       "      'latex': '\\\\alpha _{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'R ij ',\n",
       "      'latex': 'R_{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 234,\n",
       "      'end': 245,\n",
       "      'text': 'P j ',\n",
       "      'latex': 'P_j',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Description'},\n",
       "   {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 222,\n",
       "      'end': 229,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF7'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Properties'},\n",
       "   {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "    'cite_spans': [{'start': 113,\n",
       "      'end': 121,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'}],\n",
       "    'ref_spans': [{'start': 58,\n",
       "      'end': 66,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF28'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Dataset and Metrics'},\n",
       "   {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "    'cite_spans': [{'start': 5,\n",
       "      'end': 13,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "    'cite_spans': [{'start': 9,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 29,\n",
       "      'end': 37,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "    'cite_spans': [{'start': 557,\n",
       "      'end': 565,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'},\n",
       "     {'start': 697,\n",
       "      'end': 705,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF21'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': '|V|',\n",
       "      'latex': '|V|',\n",
       "      'ref_id': None},\n",
       "     {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "     {'start': 194,\n",
       "      'end': 205,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 210,\n",
       "      'end': 221,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 360,\n",
       "      'end': 371,\n",
       "      'text': 'm=5',\n",
       "      'latex': 'm = 5',\n",
       "      'ref_id': None},\n",
       "     {'start': 431,\n",
       "      'end': 442,\n",
       "      'text': 'd h =500',\n",
       "      'latex': 'd_h = 500',\n",
       "      'ref_id': None},\n",
       "     {'start': 463,\n",
       "      'end': 474,\n",
       "      'text': 'K=100',\n",
       "      'latex': 'K = 100',\n",
       "      'ref_id': None},\n",
       "     {'start': 495,\n",
       "      'end': 506,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 538,\n",
       "      'end': 549,\n",
       "      'text': 'λ p =0.2',\n",
       "      'latex': '\\\\lambda _p=0.2',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Experimental Settings'},\n",
       "   {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 83,\n",
       "      'end': 91,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF40'}],\n",
       "    'eq_spans': [{'start': 240,\n",
       "      'end': 251,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Results on Our Dataset'},\n",
       "   {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "    'cite_spans': [{'start': 208,\n",
       "      'end': 215,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 260,\n",
       "      'end': 268,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF42'}],\n",
       "    'eq_spans': [{'start': 380,\n",
       "      'end': 391,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "    'cite_spans': [{'start': 33,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 305,\n",
       "      'end': 313,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF43'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 250,\n",
       "      'end': 258,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF45'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Case Study'},\n",
       "   {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Conclusions'}],\n",
       "  'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "    'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF9',\n",
       "    'type': 'equation'},\n",
       "   'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "    'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "    'ref_id': 'EQREF10',\n",
       "    'type': 'equation'},\n",
       "   'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "    'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "    'ref_id': 'EQREF11',\n",
       "    'type': 'equation'},\n",
       "   'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "    'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF12',\n",
       "    'type': 'equation'},\n",
       "   'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "    'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "    'ref_id': 'EQREF13',\n",
       "    'type': 'equation'},\n",
       "   'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "    'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "    'ref_id': 'EQREF14',\n",
       "    'type': 'equation'},\n",
       "   'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "    'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "    'ref_id': 'EQREF15',\n",
       "    'type': 'equation'},\n",
       "   'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "    'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "    'ref_id': 'EQREF16',\n",
       "    'type': 'equation'},\n",
       "   'EQREF17': {'text': 'R=X d ×X c T',\n",
       "    'latex': 'R = X_d\\\\times X_c^T',\n",
       "    'ref_id': 'EQREF17',\n",
       "    'type': 'equation'},\n",
       "   'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "    'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "    'ref_id': 'EQREF18',\n",
       "    'type': 'equation'},\n",
       "   'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "    'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "    'ref_id': 'EQREF19',\n",
       "    'type': 'equation'},\n",
       "   'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "    'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "    'ref_id': 'EQREF20',\n",
       "    'type': 'equation'},\n",
       "   'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "    'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "    'ref_id': 'EQREF22',\n",
       "    'type': 'equation'},\n",
       "   'FIGREF2': {'text': '1',\n",
       "    'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF2',\n",
       "    'type': 'figure'},\n",
       "   'FIGREF7': {'text': '2',\n",
       "    'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF7',\n",
       "    'type': 'figure'},\n",
       "   'TABREF40': {'text': '1',\n",
       "    'caption': 'Summarization performance.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF40',\n",
       "    'type': 'table'},\n",
       "   'TABREF42': {'text': '2',\n",
       "    'caption': 'Further investigation of RAVAESum.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF42',\n",
       "    'type': 'table'},\n",
       "   'TABREF43': {'text': '3',\n",
       "    'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF43',\n",
       "    'type': 'table'},\n",
       "   'TABREF45': {'text': '4',\n",
       "    'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF45',\n",
       "    'type': 'table'},\n",
       "   'TABREF46': {'text': '5',\n",
       "    'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF46',\n",
       "    'type': 'table'},\n",
       "   'SECREF1': {'text': 'Introduction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF1',\n",
       "    'type': 'section'},\n",
       "   'SECREF2': {'text': 'Framework',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF2',\n",
       "    'type': 'section'},\n",
       "   'SECREF6': {'text': 'Conclusions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF6',\n",
       "    'type': 'section'},\n",
       "   'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF8',\n",
       "    'type': 'section'},\n",
       "   'SECREF21': {'text': 'Summary Construction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF21',\n",
       "    'type': 'section'},\n",
       "   'SECREF3': {'text': 'Data Description',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF3',\n",
       "    'type': 'section'},\n",
       "   'SECREF24': {'text': 'Background',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF24',\n",
       "    'type': 'section'},\n",
       "   'SECREF26': {'text': 'Data Collection',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF26',\n",
       "    'type': 'section'},\n",
       "   'SECREF28': {'text': 'Data Properties',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF28',\n",
       "    'type': 'section'},\n",
       "   'SECREF4': {'text': 'Experimental Setup',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF4',\n",
       "    'type': 'section'},\n",
       "   'SECREF29': {'text': 'Dataset and Metrics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF29',\n",
       "    'type': 'section'},\n",
       "   'SECREF31': {'text': 'Comparative Methods',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF31',\n",
       "    'type': 'section'},\n",
       "   'SECREF37': {'text': 'Experimental Settings',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF37',\n",
       "    'type': 'section'},\n",
       "   'SECREF5': {'text': 'Results and Discussions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF5',\n",
       "    'type': 'section'},\n",
       "   'SECREF39': {'text': 'Results on Our Dataset',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF39',\n",
       "    'type': 'section'},\n",
       "   'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF41',\n",
       "    'type': 'section'},\n",
       "   'SECREF44': {'text': 'Case Study',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF44',\n",
       "    'type': 'section'},\n",
       "   'SECREF7': {'text': 'Topics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF7',\n",
       "    'type': 'section'}},\n",
       "  'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "    'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACL-ANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "    'title': 'Auto-encoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '15789289'},\n",
       "   'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "    'title': 'Effective approaches to attention-based neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "    'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_parse_overview['Introduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### мы должны выделить только обзорную часть из текста, а все остальноё сохранить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "overview_papers[all_articles[0]['paper_id']] = {\n",
    "    'paper_id':all_articles[0]['paper_id'],   'metadata':all_articles[0]['metadata'],\n",
    "    's2_pdf_hash':all_articles[0]['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers[all_articles[0]['paper_id']]['grobid_parse'] = {'abstract':all_articles[0]['grobid_parse']['abstract'],\n",
    "                                                                'overview_text':grobid_parse_overview,  \n",
    "                                                                'bib_entries':all_articles[0]['grobid_parse']['bib_entries']}\n",
    "overview_papers[all_articles[0]['paper_id']]['latex_parse'] =  {'abstract':all_articles[0]['latex_parse']['abstract'],\n",
    "                                                                'overview_text':latex_parse_overview,  \n",
    "                                                                'bib_entries':all_articles[0]['latex_parse']['bib_entries']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10164018': {'paper_id': '10164018',\n",
       "  'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "   'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "   'year': '2017',\n",
       "   'arxiv_id': '1708.01065',\n",
       "   'acl_id': 'W17-4512',\n",
       "   'pmc_id': None,\n",
       "   'pubmed_id': None,\n",
       "   'doi': '10.18653/v1/w17-4512',\n",
       "   'venue': 'ArXiv',\n",
       "   'journal': 'ArXiv'},\n",
       "  's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       "  'grobid_parse': {'abstract': [{'text': 'AbstractWe investigate the problem of readeraware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online 1 .',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Abstract'}],\n",
       "   'overview_text': {0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "     'cite_spans': [{'start': 192,\n",
       "       'end': 216,\n",
       "       'text': '(Goldstein et al., 2000;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 217,\n",
       "       'end': 239,\n",
       "       'text': 'Erkan and Radev, 2004;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 240,\n",
       "       'end': 257,\n",
       "       'text': 'Wan et al., 2007;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF19'},\n",
       "      {'start': 258,\n",
       "       'end': 284,\n",
       "       'text': 'Nenkova and McKeown, 2012;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF16'},\n",
       "      {'start': 285,\n",
       "       'end': 302,\n",
       "       'text': 'Min et al., 2012;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF15'},\n",
       "      {'start': 303,\n",
       "       'end': 319,\n",
       "       'text': 'Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 773,\n",
       "       'end': 797,\n",
       "       'text': '(Project Code: 14203414)',\n",
       "       'latex': None,\n",
       "       'ref_id': None},\n",
       "      {'start': 2288,\n",
       "       'end': 2305,\n",
       "       'text': '(Hu et al., 2008;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF7'},\n",
       "      {'start': 2306,\n",
       "       'end': 2324,\n",
       "       'text': 'Yang et al., 2011)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF22'},\n",
       "      {'start': 2582,\n",
       "       'end': 2598,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 2911,\n",
       "       'end': 2927,\n",
       "       'text': 'Li et al. (2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 3069,\n",
       "       'end': 3095,\n",
       "       'text': '(Kingma and Welling, 2014;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 3096,\n",
       "       'end': 3117,\n",
       "       'text': 'Rezende et al., 2014)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'}],\n",
       "     'ref_spans': [{'start': 956,\n",
       "       'end': 964,\n",
       "       'text': 'Figure 1',\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF0'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    1: {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "     'cite_spans': [{'start': 451,\n",
       "       'end': 468,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     'ref_spans': [{'start': 12,\n",
       "       'end': 20,\n",
       "       'text': 'Figure 2',\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF2'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    2: {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "     'cite_spans': [{'start': 32,\n",
       "       'end': 58,\n",
       "       'text': '(Kingma and Welling, 2014;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 59,\n",
       "       'end': 79,\n",
       "       'text': 'Rezende et al., 2014',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'},\n",
       "      {'start': 184,\n",
       "       'end': 200,\n",
       "       'text': 'Li et al. (2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 2384,\n",
       "       'end': 2401,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 2433,\n",
       "       'end': 2456,\n",
       "       'text': '(Bahdanau et al., 2015;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF0'},\n",
       "      {'start': 2457,\n",
       "       'end': 2476,\n",
       "       'text': 'Luong et al., 2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF13'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    3: {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "     'cite_spans': [{'start': 86,\n",
       "       'end': 102,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 971,\n",
       "       'end': 987,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 1133,\n",
       "       'end': 1158,\n",
       "       'text': '(Dantzig and Thapa, 2006)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF3'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    4: {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    5: {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    6: {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    7: {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    8: {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "     'cite_spans': [{'start': 107,\n",
       "       'end': 118,\n",
       "       'text': '(Lin, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF12'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    9: {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "     'cite_spans': [{'start': 351,\n",
       "       'end': 365,\n",
       "       'text': '(Wasson, 1998)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF20'},\n",
       "      {'start': 492,\n",
       "       'end': 512,\n",
       "       'text': '(Radev et al., 2000)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF17'},\n",
       "      {'start': 700,\n",
       "       'end': 723,\n",
       "       'text': '(Erkan and Radev, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 737,\n",
       "       'end': 763,\n",
       "       'text': '(Mihalcea and Tarau, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF14'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    10: {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "     'cite_spans': [{'start': 498,\n",
       "       'end': 518,\n",
       "       'text': '(Kingma and Ba, 2014',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF8'},\n",
       "      {'start': 652,\n",
       "       'end': 674,\n",
       "       'text': '(Bastien et al., 2012)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    11: {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "     'cite_spans': [{'start': 690,\n",
       "       'end': 707,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 908,\n",
       "       'end': 925,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     'ref_spans': [{'start': 77,\n",
       "       'end': 84,\n",
       "       'text': 'Table 1',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF0'},\n",
       "      {'start': 746,\n",
       "       'end': 753,\n",
       "       'text': 'Table 2',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF1'},\n",
       "      {'start': 1184,\n",
       "       'end': 1191,\n",
       "       'text': 'Table 3',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF2'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    12: {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 315,\n",
       "       'end': 322,\n",
       "       'text': 'Table 4',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF3'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    13: {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "     'cite_spans': [{'start': 517,\n",
       "       'end': 868,\n",
       "       'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "       'latex': None,\n",
       "       'ref_id': None}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None}},\n",
       "   'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "     'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "     'authors': [{'first': 'Dzmitry',\n",
       "       'middle': [],\n",
       "       'last': 'Bahdanau',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "      {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '11212020'},\n",
       "    'BIBREF1': {'ref_id': 'b1',\n",
       "     'title': 'Theano: new features and speed improvements',\n",
       "     'authors': [{'first': 'Frédéric',\n",
       "       'middle': [],\n",
       "       'last': 'Bastien',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "      {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "      {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "      {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "      {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "      {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "      {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "      {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "     'links': '8180128'},\n",
       "    'BIBREF2': {'ref_id': 'b2',\n",
       "     'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "     'authors': [{'first': 'Lidong',\n",
       "       'middle': [],\n",
       "       'last': 'Bing',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "      {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1587--1597',\n",
       "     'other_ids': {},\n",
       "     'links': '8377315'},\n",
       "    'BIBREF3': {'ref_id': 'b3',\n",
       "     'title': 'Linear programming 1: introduction',\n",
       "     'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "      {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "     'year': 2006,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '53739754'},\n",
       "    'BIBREF4': {'ref_id': 'b4',\n",
       "     'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "     'authors': [{'first': 'Günes',\n",
       "       'middle': [],\n",
       "       'last': 'Erkan',\n",
       "       'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '4',\n",
       "     'issn': '',\n",
       "     'pages': '365--371',\n",
       "     'other_ids': {},\n",
       "     'links': '10418456'},\n",
       "    'BIBREF5': {'ref_id': 'b5',\n",
       "     'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "     'authors': [{'first': 'Shuhei',\n",
       "       'middle': [],\n",
       "       'last': 'Yoshida',\n",
       "       'suffix': ''}],\n",
       "     'year': None,\n",
       "     'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': None},\n",
       "    'BIBREF6': {'ref_id': 'b6',\n",
       "     'title': 'Multi-document summarization by sentence extraction',\n",
       "     'authors': [{'first': 'Jade',\n",
       "       'middle': [],\n",
       "       'last': 'Goldstein',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "      {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "      {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'NAACLANLPWorkshop',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '40--48',\n",
       "     'other_ids': {},\n",
       "     'links': '8294822'},\n",
       "    'BIBREF7': {'ref_id': 'b7',\n",
       "     'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "     'authors': [{'first': 'Meishan',\n",
       "       'middle': [],\n",
       "       'last': 'Hu',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "      {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "     'year': 2008,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '291--298',\n",
       "     'other_ids': {},\n",
       "     'links': '13723748'},\n",
       "    'BIBREF8': {'ref_id': 'b8',\n",
       "     'title': 'Adam: A method for stochastic optimization',\n",
       "     'authors': [{'first': 'Diederik',\n",
       "       'middle': [],\n",
       "       'last': 'Kingma',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '6628106'},\n",
       "    'BIBREF9': {'ref_id': 'b9',\n",
       "     'title': 'Autoencoding variational bayes',\n",
       "     'authors': [{'first': 'P',\n",
       "       'middle': [],\n",
       "       'last': 'Diederik',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': None},\n",
       "    'BIBREF10': {'ref_id': 'b10',\n",
       "     'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1270--1276',\n",
       "     'other_ids': {},\n",
       "     'links': '14777460'},\n",
       "    'BIBREF11': {'ref_id': 'b11',\n",
       "     'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "     'year': 2017,\n",
       "     'venue': 'AAAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '3497--3503',\n",
       "     'other_ids': {},\n",
       "     'links': '29562039'},\n",
       "    'BIBREF12': {'ref_id': 'b12',\n",
       "     'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "     'authors': [{'first': 'Chin-Yew',\n",
       "       'middle': [],\n",
       "       'last': 'Lin',\n",
       "       'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "     'volume': '8',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '964287'},\n",
       "    'BIBREF13': {'ref_id': 'b13',\n",
       "     'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "     'authors': [{'first': 'Minh-Thang',\n",
       "       'middle': [],\n",
       "       'last': 'Luong',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "      {'first': 'Christopher D',\n",
       "       'middle': [],\n",
       "       'last': 'Manning',\n",
       "       'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1412--1421',\n",
       "     'other_ids': {},\n",
       "     'links': '1998416'},\n",
       "    'BIBREF14': {'ref_id': 'b14',\n",
       "     'title': 'Textrank: Bringing order into texts',\n",
       "     'authors': [{'first': 'Rada',\n",
       "       'middle': [],\n",
       "       'last': 'Mihalcea',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '577937'},\n",
       "    'BIBREF15': {'ref_id': 'b15',\n",
       "     'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "     'authors': [{'first': 'Yen',\n",
       "       'middle': ['Kan'],\n",
       "       'last': 'Ziheng Lin Min',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'COLING',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '2093--2108',\n",
       "     'other_ids': {},\n",
       "     'links': '6317274'},\n",
       "    'BIBREF16': {'ref_id': 'b16',\n",
       "     'title': 'A survey of text summarization techniques',\n",
       "     'authors': [{'first': 'Ani',\n",
       "       'middle': [],\n",
       "       'last': 'Nenkova',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'Mining Text Data',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '43--76',\n",
       "     'other_ids': {},\n",
       "     'links': '556431'},\n",
       "    'BIBREF17': {'ref_id': 'b17',\n",
       "     'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "     'authors': [{'first': 'Hongyan',\n",
       "       'middle': [],\n",
       "       'last': 'Dragomir R Radev',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '21--30',\n",
       "     'other_ids': {},\n",
       "     'links': '1320'},\n",
       "    'BIBREF18': {'ref_id': 'b18',\n",
       "     'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "     'authors': [{'first': 'Danilo',\n",
       "       'middle': [],\n",
       "       'last': 'Jimenez Rezende',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "      {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICML',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1278--1286',\n",
       "     'other_ids': {},\n",
       "     'links': '16895865'},\n",
       "    'BIBREF19': {'ref_id': 'b19',\n",
       "     'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "     'authors': [{'first': 'Xiaojun',\n",
       "       'middle': [],\n",
       "       'last': 'Wan',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "     'year': 2007,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '7',\n",
       "     'issn': '',\n",
       "     'pages': '2903--2908',\n",
       "     'other_ids': {},\n",
       "     'links': '532313'},\n",
       "    'BIBREF20': {'ref_id': 'b20',\n",
       "     'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "     'authors': [{'first': 'Mark',\n",
       "       'middle': [],\n",
       "       'last': 'Wasson',\n",
       "       'suffix': ''}],\n",
       "     'year': 1998,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1364--1368',\n",
       "     'other_ids': {},\n",
       "     'links': '12681629'},\n",
       "    'BIBREF21': {'ref_id': 'b21',\n",
       "     'title': 'Multiple aspect summarization using integer linear programming',\n",
       "     'authors': [{'first': 'Kristian',\n",
       "       'middle': [],\n",
       "       'last': 'Woodsend',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'EMNLP-CNLL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '233--243',\n",
       "     'other_ids': {},\n",
       "     'links': '17497992'},\n",
       "    'BIBREF22': {'ref_id': 'b22',\n",
       "     'title': 'Social context summarization',\n",
       "     'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "      {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "      {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "      {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "      {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "     'year': 2011,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '255--264',\n",
       "     'other_ids': {},\n",
       "     'links': '704517'}}},\n",
       "  'latex_parse': {'abstract': [],\n",
       "   'overview_text': {'Introduction': {'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "      \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "      'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "      'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "      'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "      'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       "     'cite_spans': [[{'start': 193,\n",
       "        'end': 200,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF0'},\n",
       "       {'start': 203,\n",
       "        'end': 210,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF1'},\n",
       "       {'start': 213,\n",
       "        'end': 220,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF2'},\n",
       "       {'start': 223,\n",
       "        'end': 230,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF3'},\n",
       "       {'start': 233,\n",
       "        'end': 240,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF4'},\n",
       "       {'start': 243,\n",
       "        'end': 250,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 253,\n",
       "        'end': 260,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [],\n",
       "      [{'start': 527,\n",
       "        'end': 534,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF7'},\n",
       "       {'start': 537,\n",
       "        'end': 544,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF8'},\n",
       "       {'start': 802,\n",
       "        'end': 809,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 10,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'},\n",
       "       {'start': 159,\n",
       "        'end': 167,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF10'},\n",
       "       {'start': 170,\n",
       "        'end': 178,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF11'}],\n",
       "      [],\n",
       "      []],\n",
       "     'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       "     'section': ['Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Overview': {'text': ['As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.'],\n",
       "     'cite_spans': [[{'start': 489,\n",
       "        'end': 496,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}]],\n",
       "     'cite_span_lens': [1],\n",
       "     'section': ['Overview'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Reader-Aware Salience Estimation': {'text': ['Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "      'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "      'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "      'The calculation of INLINEFORM0 will be discussed later.',\n",
       "      'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "      'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "      'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "      'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "      ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "      'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "      'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "      'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "      'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.'],\n",
       "     'cite_spans': [[{'start': 32,\n",
       "        'end': 40,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF10'},\n",
       "       {'start': 43,\n",
       "        'end': 51,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF11'},\n",
       "       {'start': 154,\n",
       "        'end': 161,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [{'start': 7,\n",
       "        'end': 14,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'},\n",
       "       {'start': 46,\n",
       "        'end': 54,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF12'},\n",
       "       {'start': 57,\n",
       "        'end': 65,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF13'}],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      []],\n",
       "     'cite_span_lens': [3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "     'section': ['Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Summary Construction': {'text': ['In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.'],\n",
       "     'cite_spans': [[{'start': 82,\n",
       "        'end': 89,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 94,\n",
       "        'end': 101,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 466,\n",
       "        'end': 474,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF14'},\n",
       "       {'start': 477,\n",
       "        'end': 484,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 491,\n",
       "        'end': 498,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'},\n",
       "       {'start': 644,\n",
       "        'end': 652,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF15'}]],\n",
       "     'cite_span_lens': [2, 4],\n",
       "     'section': ['Summary Construction', 'Summary Construction'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Description': {'text': ['In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Data Description'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Background': {'text': ['The definition of the terminology related to the dataset is given as follows.',\n",
       "      'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "      'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "      'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "      'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "      'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "      'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.'],\n",
       "     'cite_spans': [[], [], [], [], [], [], []],\n",
       "     'cite_span_lens': [0, 0, 0, 0, 0, 0, 0],\n",
       "     'section': ['Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Collection': {'text': ['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "      'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "      'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "      'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.'],\n",
       "     'cite_spans': [[], [], [], []],\n",
       "     'cite_span_lens': [0, 0, 0, 0],\n",
       "     'section': ['Data Collection',\n",
       "      'Data Collection',\n",
       "      'Data Collection',\n",
       "      'Data Collection'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Properties': {'text': ['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Data Properties'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Dataset and Metrics': {'text': ['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.'],\n",
       "     'cite_spans': [[{'start': 113,\n",
       "        'end': 121,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF16'}]],\n",
       "     'cite_span_lens': [1],\n",
       "     'section': ['Dataset and Metrics'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Comparative Methods': {'text': ['To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "      'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "      'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "      'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "      'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "      'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "      'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.'],\n",
       "     'cite_spans': [[],\n",
       "      [{'start': 10,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 5,\n",
       "        'end': 13,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF17'}],\n",
       "      [{'start': 9,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF18'}],\n",
       "      [{'start': 8,\n",
       "        'end': 15,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF1'},\n",
       "       {'start': 29,\n",
       "        'end': 37,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF19'}],\n",
       "      [{'start': 8,\n",
       "        'end': 15,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'}],\n",
       "      []],\n",
       "     'cite_span_lens': [0, 1, 1, 1, 2, 1, 0],\n",
       "     'section': ['Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Experimental Settings': {'text': ['The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.'],\n",
       "     'cite_spans': [[{'start': 557,\n",
       "        'end': 565,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF20'},\n",
       "       {'start': 697,\n",
       "        'end': 705,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF21'}]],\n",
       "     'cite_span_lens': [2],\n",
       "     'section': ['Experimental Settings'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Results on Our Dataset': {'text': ['The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Results on Our Dataset'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Further Investigation of Our Framework ': {'text': ['To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "      \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\"],\n",
       "     'cite_spans': [[{'start': 208,\n",
       "        'end': 215,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [{'start': 33,\n",
       "        'end': 40,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}]],\n",
       "     'cite_span_lens': [1, 1],\n",
       "     'section': ['Further Investigation of Our Framework ',\n",
       "      'Further Investigation of Our Framework '],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Case Study': {'text': ['Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Case Study'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Conclusions': {'text': ['We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Conclusions'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}}},\n",
       "   'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "     'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "     'authors': [{'first': 'Lidong',\n",
       "       'middle': [],\n",
       "       'last': 'Bing',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "      {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1587--1597',\n",
       "     'other_ids': {},\n",
       "     'links': '8377315'},\n",
       "    'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "     'title': 'Linear programming 1: introduction',\n",
       "     'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "      {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "     'year': 2006,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '53739754'},\n",
       "    'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "     'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "     'authors': [{'first': 'Günes',\n",
       "       'middle': [],\n",
       "       'last': 'Erkan',\n",
       "       'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '4',\n",
       "     'issn': '',\n",
       "     'pages': '365--371',\n",
       "     'other_ids': {},\n",
       "     'links': '10418456'},\n",
       "    'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "     'title': 'Multi-document summarization by sentence extraction',\n",
       "     'authors': [{'first': 'Jade',\n",
       "       'middle': [],\n",
       "       'last': 'Goldstein',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "      {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "      {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'NAACL-ANLPWorkshop',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '40--48',\n",
       "     'other_ids': {},\n",
       "     'links': '8294822'},\n",
       "    'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "     'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "     'authors': [{'first': 'Meishan',\n",
       "       'middle': [],\n",
       "       'last': 'Hu',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "      {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "     'year': 2008,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '291--298',\n",
       "     'other_ids': {},\n",
       "     'links': '13723748'},\n",
       "    'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "     'title': 'Adam: A method for stochastic optimization',\n",
       "     'authors': [{'first': 'Diederik',\n",
       "       'middle': [],\n",
       "       'last': 'Kingma',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '6628106'},\n",
       "    'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "     'title': 'Auto-encoding variational bayes',\n",
       "     'authors': [{'first': 'P',\n",
       "       'middle': [],\n",
       "       'last': 'Diederik',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '15789289'},\n",
       "    'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "     'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1270--1276',\n",
       "     'other_ids': {},\n",
       "     'links': '14777460'},\n",
       "    'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "     'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "     'year': 2017,\n",
       "     'venue': 'AAAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '3497--3503',\n",
       "     'other_ids': {},\n",
       "     'links': '29562039'},\n",
       "    'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "     'title': 'Effective approaches to attention-based neural machine translation',\n",
       "     'authors': [{'first': 'Minh-Thang',\n",
       "       'middle': [],\n",
       "       'last': 'Luong',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "      {'first': 'Christopher D',\n",
       "       'middle': [],\n",
       "       'last': 'Manning',\n",
       "       'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1412--1421',\n",
       "     'other_ids': {},\n",
       "     'links': '1998416'},\n",
       "    'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "     'title': 'Textrank: Bringing order into texts',\n",
       "     'authors': [{'first': 'Rada',\n",
       "       'middle': [],\n",
       "       'last': 'Mihalcea',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '577937'},\n",
       "    'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "     'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "     'authors': [{'first': 'Yen',\n",
       "       'middle': ['Kan'],\n",
       "       'last': 'Ziheng Lin Min',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'COLING',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '2093--2108',\n",
       "     'other_ids': {},\n",
       "     'links': '6317274'},\n",
       "    'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "     'title': 'A survey of text summarization techniques',\n",
       "     'authors': [{'first': 'Ani',\n",
       "       'middle': [],\n",
       "       'last': 'Nenkova',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'Mining Text Data',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '43--76',\n",
       "     'other_ids': {},\n",
       "     'links': '556431'},\n",
       "    'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "     'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "     'authors': [{'first': 'Hongyan',\n",
       "       'middle': [],\n",
       "       'last': 'Dragomir R Radev',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '21--30',\n",
       "     'other_ids': {},\n",
       "     'links': '1320'},\n",
       "    'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "     'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "     'authors': [{'first': 'Danilo',\n",
       "       'middle': [],\n",
       "       'last': 'Jimenez Rezende',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "      {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICML',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1278--1286',\n",
       "     'other_ids': {},\n",
       "     'links': '16895865'},\n",
       "    'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "     'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "     'authors': [{'first': 'Mark',\n",
       "       'middle': [],\n",
       "       'last': 'Wasson',\n",
       "       'suffix': ''}],\n",
       "     'year': 1998,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1364--1368',\n",
       "     'other_ids': {},\n",
       "     'links': '12681629'},\n",
       "    'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "     'title': 'Multiple aspect summarization using integer linear programming',\n",
       "     'authors': [{'first': 'Kristian',\n",
       "       'middle': [],\n",
       "       'last': 'Woodsend',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'EMNLP-CNLL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '233--243',\n",
       "     'other_ids': {},\n",
       "     'links': '17497992'},\n",
       "    'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "     'title': 'Social context summarization',\n",
       "     'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "      {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "      {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "      {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "      {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "     'year': 2011,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '255--264',\n",
       "     'other_ids': {},\n",
       "     'links': '704517'}}}}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения глазами ссылка на [статью](https://arxiv.org/pdf/1708.01065.pdf)\n",
    "\n",
    "Все супер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Introduction 13\n",
      "1 Overview 1\n",
      "2 Reader-Aware Salience Estimation 6\n",
      "3 Summary Construction 6\n",
      "4 Data Description 0\n",
      "5 Background 0\n",
      "6 Data Collection 0\n",
      "7 Data Properties 0\n",
      "8 Dataset and Metrics 1\n",
      "9 Comparative Methods 6\n",
      "10 Experimental Settings 2\n",
      "11 Results on Our Dataset 0\n",
      "12 Further Investigation of Our Framework  2\n",
      "13 Case Study 0\n",
      "14 Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for num_tex,(k,v) in enumerate(latex_parse_overview.items()):\n",
    "    print(num_tex,k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "1 1\n",
      "2 6\n",
      "3 3\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 1\n",
      "9 4\n",
      "10 2\n",
      "11 2\n",
      "12 0\n",
      "13 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in grobid_parse_overview.items():\n",
    "    print(k,len(v['cite_spans']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упорядочим по количеству ссылок и возьмём абзац с максимальным значением, а также со 2 максимальным значением если количество ссылок в нём больше половины от максимального кол-ва "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), \n",
    "                                                 key=lambda item: len(item[1]['cite_spans']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cite_span_sum = 0\n",
    "max_grobid_parse_overview = dict()\n",
    "for k,v in grobid_parse_overview.items():\n",
    "    if max_cite_span_sum < len(v['cite_spans']):\n",
    "        max_cite_span_sum = len(v['cite_spans'])\n",
    "        max_grobid_parse_overview[k] = v\n",
    "    elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "        max_grobid_parse_overview[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), \n",
    "                                                 key=lambda item: sum(item[1]['cite_span_lens']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cite_span_sum = 0\n",
    "max_latex_parse_overview = dict()\n",
    "for k,v in latex_parse_overview.items():\n",
    "    if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "        max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "        max_latex_parse_overview[k] = v\n",
    "    elif (max_cite_span_sum>7) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "        max_latex_parse_overview[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 13\n",
      "Reader-Aware Salience Estimation 6\n",
      "Summary Construction 6\n",
      "Comparative Methods 6\n",
      "Experimental Settings 2\n",
      "Further Investigation of Our Framework  2\n",
      "Overview 1\n",
      "Dataset and Metrics 1\n",
      "Data Description 0\n",
      "Background 0\n",
      "Data Collection 0\n",
      "Data Properties 0\n",
      "Results on Our Dataset 0\n",
      "Case Study 0\n",
      "Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for k,v in latex_parse_overview.items():\n",
    "    print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим для всех \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "\n",
    "with_2_max= False # будем сохранять 2 максимальное значение\n",
    "\n",
    "for num_artic,article in enumerate(all_articles):\n",
    "    # проверяем что у статьи есть grobid_parse и latex_parse и естб текст\n",
    "    if (article['grobid_parse'] and article['grobid_parse']['body_text']) or (article['latex_parse'] and article['latex_parse']['body_text']):\n",
    "        # задаем шаблон отражения статьи в укороченном формате (чтобы занимать меньше памяти)\n",
    "        overview_papers[article['paper_id']] = { 'paper_id':article['paper_id'],   'metadata':article['metadata'],\n",
    "                                                 's2_pdf_hash':article['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}\n",
    "        \n",
    "        grobid_parse_overview = None\n",
    "        # если у статьи есть article['grobid_parse']['body_text']\n",
    "        if article['grobid_parse'] and article['grobid_parse']['body_text']:\n",
    "            grobid_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "                grobid_parse_overview[num_sec] = sections\n",
    "            \n",
    "            # отсортируем по количеству цитат абзацы\n",
    "            grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), key=lambda item: len(item[1]['cite_spans']), reverse=True)}\n",
    "            \n",
    "            if with_2_max:\n",
    "                # найдем 1 и 2 максимум по обзорной части\n",
    "                max_cite_span_sum = 0\n",
    "                max_grobid_parse_overview = dict()\n",
    "                for k,v in grobid_parse_overview.items():\n",
    "                    if max_cite_span_sum < len(v['cite_spans']):\n",
    "                        max_cite_span_sum = len(v['cite_spans'])\n",
    "                        max_grobid_parse_overview[k] = v\n",
    "                    # записываем 2 максимум, если количество ссылок в егочасти больше половины от максимального \n",
    "                    elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "                        max_grobid_parse_overview[k] = v\n",
    "\n",
    "                grobid_parse_overview = max_grobid_parse_overview\n",
    "            \n",
    "        latex_parse_overview = None\n",
    "        # если у статьи есть article['latex_parse']['body_text']\n",
    "        if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "            latex_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            # в latex_parse \n",
    "            for sections in article['latex_parse']['body_text']:\n",
    "                if sections['section'] in latex_parse_overview:\n",
    "                    if latex_parse_overview[sections['section']] == sections:\n",
    "                        continue\n",
    "                    else:\n",
    "                        latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "                        latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "                        latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "                        latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "                else:\n",
    "                    latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                                  'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                                  'section':[sections['section']]}\n",
    "            latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), key=lambda item: item[1]['cite_span_lens'], reverse=True)}\n",
    "        \n",
    "            if with_2_max:\n",
    "                max_cite_span_sum = 0\n",
    "                max_latex_parse_overview = dict()\n",
    "                for k,v in latex_parse_overview.items():\n",
    "                    if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "                        max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "                        max_latex_parse_overview[k] = v\n",
    "                    elif (max_cite_span_sum>0) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "                        max_latex_parse_overview[k] = v\n",
    "\n",
    "                latex_parse_overview = max_latex_parse_overview\n",
    "        \n",
    "\n",
    "        if grobid_parse_overview:\n",
    "            overview_papers[article['paper_id']]['grobid_parse'] = {'abstract':None,\n",
    "                                                        'overview_text':grobid_parse_overview,  \n",
    "                                                        'bib_entries':None}\n",
    "            if article['grobid_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['abstract'] = article['grobid_parse']['abstract']\n",
    "            if article['grobid_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['bib_entries'] = article['grobid_parse']['bib_entries']\n",
    "            \n",
    "        if latex_parse_overview:            \n",
    "            overview_papers[article['paper_id']]['latex_parse'] = {'abstract':None,\n",
    "                                                                    'overview_text':latex_parse_overview,  \n",
    "                                                                    'bib_entries':None}\n",
    "            if article['latex_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['abstract'] = article['latex_parse']['abstract']\n",
    "            if article['latex_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['bib_entries'] = article['latex_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018',\n",
       " '14472576',\n",
       " '17302615',\n",
       " '3243536',\n",
       " '3248240',\n",
       " '2223737',\n",
       " '488',\n",
       " '14323173',\n",
       " '15251605',\n",
       " '8260435']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(overview_papers.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выделим стать, у которых есть latex_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers_w_latex = {k:v for k,v in overview_papers.items() if v['latex_parse']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overview_papers_w_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_in(list_toks,line):\n",
    "    flag = False\n",
    "    line = line.lower()\n",
    "    for tok in list_toks:\n",
    "        if tok.lower() in line:\n",
    "            flag = tok\n",
    "            break\n",
    "    return flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Расчет покрытия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Покрытие по всему тексту статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_in_paper.txt', encoding=\"utf8\") as fin:\n",
    "    content = fin.readlines()\n",
    "fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict_in_ids = [x.strip() for x in content]\n",
    "del content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5986243"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_in_ids_all = {val:1 for val in all_dict_in_ids}\n",
    "len(dict_in_ids_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'authors', 'abstract', 'year', 'arxiv_id', 'acl_id', 'pmc_id', 'pubmed_id', 'doi', 'venue', 'journal'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIBREF0\n",
      "Neural machine translation by jointly learning to align and translate\n",
      "11212020\n",
      "BIBREF1\n",
      "Theano: new features and speed improvements\n",
      "8180128\n",
      "BIBREF2\n",
      "Abstractive multidocument summarization via phrase selection and merging\n",
      "8377315\n",
      "BIBREF3\n",
      "Linear programming 1: introduction\n",
      "53739754\n",
      "BIBREF4\n",
      "Lexpagerank: Prestige in multi-document text summarization\n",
      "10418456\n",
      "BIBREF5\n",
      "A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\n",
      "None\n",
      "BIBREF6\n",
      "Multi-document summarization by sentence extraction\n",
      "8294822\n",
      "BIBREF7\n",
      "Comments-oriented document summarization: Understanding documents with readers' feedback\n",
      "13723748\n",
      "BIBREF8\n",
      "Adam: A method for stochastic optimization\n",
      "6628106\n",
      "BIBREF9\n",
      "Autoencoding variational bayes\n",
      "None\n",
      "BIBREF10\n",
      "Reader-aware multi-document summarization via sparse coding\n",
      "14777460\n",
      "BIBREF11\n",
      "Salience estimation via variational auto-encoders for multi-document summarization\n",
      "29562039\n",
      "BIBREF12\n",
      "Rouge: A package for automatic evaluation of summaries\n",
      "964287\n",
      "BIBREF13\n",
      "Effective approaches to attentionbased neural machine translation\n",
      "1998416\n",
      "BIBREF14\n",
      "Textrank: Bringing order into texts\n",
      "577937\n",
      "BIBREF15\n",
      "Exploiting category-specific information for multidocument summarization\n",
      "6317274\n",
      "BIBREF16\n",
      "A survey of text summarization techniques\n",
      "556431\n",
      "BIBREF17\n",
      "Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies\n",
      "1320\n",
      "BIBREF18\n",
      "Stochastic backpropagation and approximate inference in deep generative models\n",
      "16895865\n",
      "BIBREF19\n",
      "Manifold-ranking based topic-focused multidocument summarization\n",
      "532313\n",
      "BIBREF20\n",
      "Using leading text for news summaries: Evaluation results and implications for commercial summarization applications\n",
      "12681629\n",
      "BIBREF21\n",
      "Multiple aspect summarization using integer linear programming\n",
      "17497992\n",
      "BIBREF22\n",
      "Social context summarization\n",
      "704517\n"
     ]
    }
   ],
   "source": [
    "bib_entries_grobid = dict()\n",
    "for k,v in all_articles[0]['grobid_parse']['bib_entries'].items():\n",
    "    print(k)\n",
    "    print(v['title'])\n",
    "    print(v['links'])\n",
    "    bib_entries_grobid[v['title']] = v['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIBREF5\n",
      "Abstractive multi-document summarization via phrase selection and merging 8377315\n",
      "BIBREF15\n",
      "Linear programming 1: introduction 53739754\n",
      "BIBREF1\n",
      "Lexpagerank: Prestige in multi-document text summarization 10418456\n",
      "BIBREF0\n",
      "Multi-document summarization by sentence extraction 8294822\n",
      "BIBREF7\n",
      "Comments-oriented document summarization: Understanding documents with readers' feedback 13723748\n",
      "BIBREF20\n",
      "Adam: A method for stochastic optimization 6628106\n",
      "BIBREF10\n",
      "Auto-encoding variational bayes 15789289\n",
      "BIBREF9\n",
      "Reader-aware multi-document summarization via sparse coding 14777460\n",
      "BIBREF6\n",
      "Salience estimation via variational auto-encoders for multi-document summarization 29562039\n",
      "BIBREF13\n",
      "Effective approaches to attention-based neural machine translation 1998416\n",
      "BIBREF19\n",
      "Textrank: Bringing order into texts 577937\n",
      "BIBREF4\n",
      "Exploiting category-specific information for multi-document summarization 6317274\n",
      "BIBREF3\n",
      "A survey of text summarization techniques 556431\n",
      "BIBREF18\n",
      "Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies 1320\n",
      "BIBREF11\n",
      "Stochastic backpropagation and approximate inference in deep generative models 16895865\n",
      "BIBREF17\n",
      "Using leading text for news summaries: Evaluation results and implications for commercial summarization applications 12681629\n",
      "BIBREF14\n",
      "Multiple aspect summarization using integer linear programming 17497992\n",
      "BIBREF8\n",
      "Social context summarization 704517\n"
     ]
    }
   ],
   "source": [
    "bib_entries_latex = dict()\n",
    "for k,v in all_articles[0]['latex_parse']['bib_entries'].items():\n",
    "    print(k)\n",
    "    print(v['title'],v['links'])\n",
    "    bib_entries_latex[v['title']] = v['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_entries_grobid = {k: v for k, v in sorted(bib_entries_grobid.items(), key=lambda item: item[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "bib_entries_latex = {k: v for k, v in sorted(bib_entries_latex.items(), key=lambda item: item[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bib_entries = dict()\n",
    "if bib_entries_latex:\n",
    "    for k,v in bib_entries_latex.items():\n",
    "        all_bib_entries[k] = [v]\n",
    "if bib_entries_grobid:\n",
    "    for k,v in bib_entries_grobid.items():\n",
    "        if k in all_bib_entries:\n",
    "            if all_bib_entries[k][0] == v:\n",
    "                continue\n",
    "            else:\n",
    "                all_bib_entries[k].append(v)\n",
    "        else:\n",
    "            all_bib_entries[k] = [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 23, 18)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_bib_entries),len(bib_entries_grobid),len(bib_entries_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '14472576',\n",
       " 'metadata': {'title': 'Building a Semantic Parser Overnight',\n",
       "  'authors': [{'first': 'Yushi', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Jonathan', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "   {'first': 'Percy', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "  'abstract': 'How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.',\n",
       "  'year': '2015',\n",
       "  'arxiv_id': None,\n",
       "  'acl_id': 'P15-1129',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.3115/v1/P15-1129',\n",
       "  'venue': 'ACL',\n",
       "  'journal': 'Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)'},\n",
       " 's2_pdf_hash': 'f3de408be7d2e2720a61451bd196ac7e1ed9363a',\n",
       " 'grobid_parse': {'abstract': [{'text': 'AbstractHow do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Abstract'}],\n",
       "  'body_text': [{'text': 'By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Kushman and Barzilay, 2013) . Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain-for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start?In this paper, we advocate a functionalitydriven process for rapidly building a semantic * Both authors equally contributed to the paper. ...',\n",
       "    'cite_spans': [{'start': 173,\n",
       "      'end': 197,\n",
       "      'text': '(Zelle and Mooney, 1996;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF25'},\n",
       "     {'start': 198,\n",
       "      'end': 228,\n",
       "      'text': 'Zettlemoyer and Collins, 2005;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 229,\n",
       "      'end': 248,\n",
       "      'text': 'Liang et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 249,\n",
       "      'end': 269,\n",
       "      'text': 'Berant et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 270,\n",
       "      'end': 295,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 296,\n",
       "      'end': 323,\n",
       "      'text': 'Kushman and Barzilay, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': '(1) by builder (∼30 minutes)(2) via domain-general grammar (3) via crowdsourcing (∼5 hours) (4) by training a paraphrasing model Figure 1 : Functionality-driven process for building semantic parsers. The two red boxes are the domain-specific parts provided by the builder of the semantic parser, and the other two are generated by the framework.parser in a new domain. At a high-level, we seek to minimize the amount of work needed for a new domain by factoring out the domaingeneral aspects (done by our framework) from the domain-specific ones (done by the builder of the semantic parser). We assume that the builder already has the desired functionality of the semantic parser in mind-e.g., the publications database is set up and the schema is fixed. Figure 1 depicts the functionality-driven process: First, the builder writes a seed lexicon specifying a canonical phrase (\"publication date\") for each predicate (publicationDate).Second, our framework uses a domain-general grammar, along with the seed lexicon and the database, to automatically generate a few hundred canonical utterances paired with their logical forms (e.g., \"article that has the largest publication date\" and arg max(type.article, publicationDate)). These utterances need not be the most elegant, but they should retain the semantics of the logical forms. Third, the builder leverages crowdsourcing to paraphrase each canonical utterance into a few natural utterances (e.g., \"what is the newest published article?\"). Finally, our framework uses this data to train a semantic parser.Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014) . In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule.In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014) . Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain.Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny, as the structural mismatch between logic and language is the chief source of difficulty in semantic parsing (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014) .There are two important questions here. First, is it possible to design a simple grammar that simultaneously generates both logical forms and canonical utterances so that the utterances are understandable by a human? In Section 3, we show how to choose appropriate canonical utterances to maximize alignment with the logical forms.Second, our grammar can generate an infinite number of canonical utterances. How many do we need for adequate coverage? Certainly, single relations is insufficient: just knowing that \"publication date of X\" paraphrases to \"when X was published\" would offer insufficient information to generalize to \"articles that came after X\" mapping to \"article whose publication date is larger than publication date of X\". We call this phenomena sublexical compositionality-when a short lexical unit (\"came after\") maps onto a multi-predicate logical form. Our hypothesis is that the sublexical compositional units are small, so we only need to crowdsource a small number of canonical utterances to learn about most of the language variability in the given domain (Section 4).We applied our functionality-driven process to seven domains, which were chosen to explore particular types of phenomena, such as spatial language, temporal language, and high-arity relations. This resulted in seven new semantic parsing datasets, totaling 12.6K examples. Our approach, which was not tuned on any one domain, was able to obtain an average accuracy of 59% over all domains. On the day of this paper submission, we created an eighth domain and trained a semantic parser overnight.',\n",
       "    'cite_spans': [{'start': 1832,\n",
       "      'end': 1853,\n",
       "      'text': '(Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 2121,\n",
       "      'end': 2142,\n",
       "      'text': '(Rangel et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 2415,\n",
       "      'end': 2441,\n",
       "      'text': '(Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2442,\n",
       "      'end': 2465,\n",
       "      'text': 'Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 3174,\n",
       "      'end': 3194,\n",
       "      'text': '(Liang et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 3195,\n",
       "      'end': 3220,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 3221,\n",
       "      'end': 3244,\n",
       "      'text': 'Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'}],\n",
       "    'ref_spans': [{'start': 129,\n",
       "      'end': 137,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 755,\n",
       "      'end': 763,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In our functionality-driven process (Figure 1) , there are two parties: the builder, who provides domain-specific information, and the framework, which provides domain-general information. We assume that the builder has a fixed database w, represented as a set of triples (e 1 , p, e 2 ), where e 1 and e 2 are entities (e.g., article1, 2015) and p is a property (e.g., publicationDate). The database w can be queried using lambda DCS logical forms, described further in Section 2.1.The builder supplies a seed lexicon L, which contains for each database property p (e.g., publicationDate) a lexical entry of the form t → s[p] , where t is a natural language phrase (e.g., \"publication date\") and s is a syntactic cat-egory (e.g., RELNP). In addition, L contains two typical entities for each semantic type in the database (e.g., alice → NP[alice] for the type person). The purpose of L is to simply connect each predicate with some representation in natural language.The framework supplies a grammar G, which specifies the modes of composition, both on logical forms and canonical utterances. Formally, G is a set of rules of the form α 1 . . . α n → s[z] , where α 1 . . . α n is a sequence of tokens or categories, s is a syntactic category and z is the logical form constructed. For example, one rule in (r) .x] , which constructs z by reversing the binary predicate r and joining it with a the unary predicate x. We use the rules G ∪ L to generate a set of (z, c) pairs, where z is a logical form (e.g., R(publicationDate).article1), and c is the corresponding canonical utterance (e.g., \"publication date of article 1\"). The set of (z, c) is denoted by GEN(G ∪ L). See Section 3 for details.G is RELNP[r] of NP[x] → NP[RNext, the builder (backed by crowdsourcing) paraphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., \"when was article 1 published?\"). This defines a set of training examples D = {(x, c, z)}, for each (z, c) ∈ GEN(G ∪ L) and x ∈ P(c). The crowdsourcing setup is detailed in Section 5.Finally, the framework trains a semantic parser on D. Our semantic parser is a log-linear distribution p θ (z, c | x, w) over logical forms and canonical utterances specified by the grammar G. Note that the grammar G will in general not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014) .To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G ∪ L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(·) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser p θ (z, c | x, w).',\n",
       "    'cite_spans': [{'start': 327,\n",
       "      'end': 342,\n",
       "      'text': 'article1, 2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1308,\n",
       "      'end': 1311,\n",
       "      'text': '(r)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2374,\n",
       "      'end': 2397,\n",
       "      'text': 'Berant and Liang (2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'}],\n",
       "    'ref_spans': [{'start': 36,\n",
       "      'end': 46,\n",
       "      'text': '(Figure 1)',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details.Every logical form z in this paper is either a unary (denoting a set of entities) or a binary (denoting a set of entity-pairs). In the base case, each entity e (e.g., 2015) is a unary denoting the singleton set: e w = {e}; and each property p (e.g., publicationDate) is a binary denoting all entitypairs (e 1 , e 2 ) that satisfy the property p. Unaries and binaries can be composed: Given a binary b and unary u, the join b.u denotes all entities e 1 for which there exists an e 2 ∈ u w with (e 1 , e 2 ) ∈ b w . For example, publicationDate.2015 denote entities published in 2015.The intersection u 1 u 2 , union u 1 u 2 , complement ¬u denote the corresponding set operations on the denotations. We let R(b) denote the reversal of b: (e 1 , e 2 ) ∈ b w iff (e 2 , e 1 ) ∈ R(b) w . This allows us to define R(publicationDate).article1 as the publication date of article 1. We also include aggregation operations (count(u), sum(u) and average(u, b)), and superlatives (argmax(u, b)).Finally, we can construct binaries using lambda abstraction: λx.u denotes a set of (e 1 , e 2 ) where e 1 ∈ u[x/e 2 ] w and u[x/e 2 ] is the logical form where free occurrences of x are replaced with e 2 . For example, R(λx.count(R(cites).x)) denotes the set of entities (e 1 , e 2 ), where e 2 is the number of entities that e 1 cites.',\n",
       "    'cite_spans': [{'start': 167,\n",
       "      'end': 179,\n",
       "      'text': 'Liang (2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our functionality-driven process hinges on having a domain-general grammar that can connect logical forms with canonical utterances compositionally. The motivation is that while it is hard to write a grammar that parses all utterances, it is possible to write one that generates one canonical utterance for each logical form. To make this explicit: Assumption 1 (Canonical compositionality) Using a small grammar, all logical forms expressible in natural language can be realized compositionally based on the logical form.Grammar. We target database querying applications, where the parser needs to handle superlatives, comparatives, negation, and coordination. We define a simple grammar that captures these forms of compositionality using canonical utterances in a domain-general way. Figure 2 illustrates a derivation produced by the grammar. The seed lexicon specified by the builder contains canonical utterances for types, entities, and properties. All types (e.g., person) have the syntactic category TYPENP, and all entities (e.g., Figure 2: Deriving a logical form z (red) and a canonical utterance c (green) from the grammar G. Each node contains a syntactic category and a logical form, which is generated by applying a rule. Nodes with only leaves as children are produced using the seed lexicon; all other nodes are produced by rules in the domain-general grammar.alice) are ENTITYNP\\'s. Unary predicates are realized as verb phrases VP (e.g., \"has a private bath\"). The builder can choose to represent binaries as either relational noun phrases (RELNP) or generalized transitive verbs (VP/NP). RELNP\\'s are usually used to describe functional properties (e.g., \"publication date\"), especially numerical properties. VP/NP\\'s include transitive verbs (\"cites\") but also longer phrases with the same syntactic interface (\"is the president of\"). Table  1 shows the seed lexicon for the SOCIAL domain.From the seed lexicon, the domain-general grammar (Table 2 ) constructs noun phrases (NP), verbs phrases (VP), and complementizer phrase (CP), all of which denote unary logical forms. Broadly speaking, the rules (R1)-(R4), (C1)-(C4) take a binary and a noun phrase, and compose them (optionally via comparatives, counting, and negation) to produce a complementizer phrase CP representing a unary (e.g., \"that cites article 1\" or \"that cites more than three article\"). (G3) combines these CP\\'s with an NP (e.g., \"article\"). In addition, (S0)-(S4) handle superlatives (we include argmin in addition to argmax), which take an NP and return the extremum-attaining subset of its denotation. Finally, we support transformations such as join (T1) and disjunction (T4), as well as aggregation (A1)-(A2).Rendering utterances for multi-arity predicates was a major challenge.The predicate instances are typically reified in a graph database, akin to a neo-Davidsonian treatment of events: There is an abstract entity with binary predicates relating it to its arguments. For example, in the SOCIAL domain, Alice\\'s education can be represented in the database as five triples: All five properties here are represented as RELNP\\'s, with the first one designated as the subject (RELNP 0 ). We support two ways of querying multi-arity relations: \"student whose university is ucla\" (T2) and \"university of student Alice whose start date is 2005\" (T3).birthdate → RELNP[birthdate] person|university|field → TYPENP[person| · · · ] company|job title → TYPENP[company| · · · ] student|university|field of study → RELNP[student| · · · ] employee|employer|job title → RELNP[employee| · · · ] start date|end date → RELNP[startDate| · · · ] is friends with → VP/NP[friends| · · · ]Generating directly from the grammar in Table 2 would result in many uninterpretable canonical utterances. Thus, we perform type checking on the logical forms to rule out \"article that cites 2004\", and limit the amount of recursion, which keeps the canonical utterances understandable.Still, the utterances generated by our grammar are not perfectly grammatical; we do not use determiners and make all nouns singular. Nonetheless, AMT workers found most canonical utterances understandable (see Table 3 and Section 5 for details on crowdsourcing). One tip for the builder is to keep the RELNP\\'s and VP/NP\\'s as context-independent as possible; e.g., using \"publication date\" instead of \"date\". In cases where more context is required, we use parenthetical remarks (e.g., \"number of assists (over a season)\" → RELNP[...]) to pack more context into the confines of the part-of-speech.Limitations. While our domain-general grammar covers most of the common logical forms in a database querying application, there are several phenomena which are out of scope, notably nested quantification (e.g., \"show me each author\\'s most cited work\") and anaphora (e.g., \"author who cites herself at least twice\"). Handling these would require a more radical change to the grammar, but is still within scope. [glue] (G1) ENTITYNP[x] → NP[x] (G2) TYPENP[x] → NP[type.x] (G3) NP[x] CP[f ] (and CP[g])* → NP[x f g] [simple] (R0) that VP[x] → CP[x] (R1.z)))] [transformation] (T1) RELNP[r] of NP[y] → NP[R(r).y] (T2) RELNP 0 [h]CP[f ] (and CP[g])* → NP[R(h).(f g)] (T3) RELNP[r] of RELNP 0 [h] NP[x] CP[f ] (and CP[g])* → NP[R(r).(h.x f g)] (T4) NP[x] or NP[y] → NP[x y] [aggregation] (A1) number of NP[x] → NP[count(x)] (A2) total|average RELNP[r] of NP[x] → NP[sum|average(x, r)]',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 787,\n",
       "      'end': 795,\n",
       "      'text': 'Figure 2',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1853,\n",
       "      'end': 1861,\n",
       "      'text': 'Table  1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF2'},\n",
       "     {'start': 1957,\n",
       "      'end': 1965,\n",
       "      'text': '(Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF4'},\n",
       "     {'start': 4158,\n",
       "      'end': 4165,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'While the canonical utterance c is generated compositionally along with the logical form z, natural paraphrases x ∈ P(c) generally deviate from this compositional structure. For example, the canonical utterance \"NP[number of NP[article CP[whose publication date is larger than NP[publication date of article 1]]]]\" might get paraphrased to \"How many articles were published after article 1?\". Here, \"published after\" non-compositionally straddles the inner NP, intuitively responsible for both instances of \"publication date\". But how non-compositional can paraphrases be? Our framework rests on the assumption that the answer is \"not very\":Assumption 2 (Bounded non-compositionality) Natural utterances for expressing complex logical forms are compositional with respect to fragments of bounded size.In the above example, note that while \"published after\" is non-compositional with respect to the grammar, the rewriting of \"number of\" to \"how many\" is compositional. The upshot of Assumption 2 is that we only need to ask for paraphrases of canonical utterances generated by the grammar up to some small depth to learn about all the noncompositional uses of language, and still be able generalize (compositionally) beyond that. We now explore the nature of the possible paraphrases. Broadly speaking, most paraphrases involve some sort of compression, where the clunky but faithful canonical utterance is smoothed out into graceful prose.Alternations of single rules. The most basic paraphrase happens at the single phrase level with synonyms (\"block\" to \"brick\"), which preserve the part-of-speech. However, many of our properties are specified using relational noun phrases, which are more naturally realized using prepositions (\"meeting whose attendee is alice ⇒ meeting with alice\") or verbs (\"author of article 1 ⇒ who wrote article 1\"). If the RELNP is complex, then the argument can become embedded: \"player whose number of points is 15 ⇒ player who scored 15 points\". Superlative and comparative constructions reveal other RELNP-dependent words: \"article that has the largest publication date ⇒ newest article\". When the value of the relation has enough context, then the relation is elided completely: \"housing unit whose housing type is apartment ⇒ apartment\".Multi-arity predicates are compressed into a single frame: \"university of student alice whose field of study is music\" becomes \"At which university did Alice study music?\", where the semantic roles of the verb \"study\" carry the burden of expressing the multiple relations: student, university, and fieldOfStudy. With a different combination of arguments, the natural verb would change: \"Which university did Alice attend?\" Sublexical compositionality. The most interesting paraphrases occur across multiple rules, a phenomenon which we called sublexical compositionality. The idea is that common, multi-part concepts are compressed to single words or simpler constructions. The simplest compression is a lexical one: \"parent of alice whose gender is female ⇒ mother of alice\". Compression often occurs when we have the same predicate chained twice in a join: \"person that is author of paper whose author is X ⇒ co-author of X\" or \"person whose birthdate is birthdate of X ⇒ person born on the same day as X\". When two CP\\'s combined via coordination have some similarity, then the coordination can be pushed down (\"meeting whose start time is 3pm and whose end time is 5pm ⇒ meetings between 3pm and 5pm\") and sometimes even generalized (\"that allows cats and that allows dogs ⇒ that allows pets\"). Sometimes, compression happens due to metonymy, where people stand in for their papers: \"author of article that article whose author is X cites ⇒ who does X cite\".',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We tackled seven domains covering various linguistic phenomena. Table 3 lists the domains, their principal phenomena, statistics about their predicates and dataset, and an example from the dataset.We use Amazon Mechanical Turk (AMT) to paraphrase the canonical utterances generated by the domain-general grammar. In each AMT task, a worker is presented with four canonical utterances and is asked to reformulate them in natural language or state that they are incomprehensible. Each canonical utterance was presented to 10 workers. Over all domains, we collected 18,032 responses. The average time for paraphrasing one utterance was 28 seconds. Paraphrases that share the same canonical utterance are collapsed, while identical paraphrases that have distinct canonical utterances are deleted. This produced a total of 12,602 examples over all domains.To estimate the level of noise in the data, we manually judged the correctness of 20 examples in each domain, and found that 17% of the utterances were inaccurate. There are two main reasons: lexical ambiguity on our part (\"player that has the least number of team ⇒ player with the lowest jersey number\"), and failure on the worker\\'s part (\"restaurant whose star rating is 3 stars ⇒ hotel which has a 3 star rating\").',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 64,\n",
       "      'end': 71,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our semantic parsing model defines a distribution over logical forms given by the domaingeneral grammar G and additional rules triggered by the input utterance x. Specifically, given an utterance x, we detect numbers, dates, and perform string matching with database entities to recognize named entities. This results in a set of rules T(x). For example, if x is \"article published in 2015 that cites article 1\", then T(x) contains 2015 → NP [2015] andarticle 1 → NP[article1]. Let L x be the rules in the seed lexicon L where the entity rules (e.g., alice → NP[alice] ) are replaced by T(x). Our semantic parsing model defines a loglinear distribution over candidate pairs (z, c) ∈ GEN(G ∪ L x ):p θ (z, c | x, w) ∝ exp(φ(c, z, x, w) θ),(1)where φ(z, c, x, w) ∈ R d is a feature vector and θ ∈ R d is a parameter vector. To generate candidate logical forms, we use a simple beam search: For each search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance \"article\" would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015) .Training. We train our model by maximizing the regularized log-likelihood O(θ) = Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: \"Show me meetings after the weekly standup day\" c: \"meeting whose date is at least date of weekly standup\" z: type.meeting date. > R(date).weeklyStandup BLOCKS 19 1995 spatial language x: \"Select the brick that is to the furthest left.\" c: \"block that the most number of block is right of\" z: argmax(type.block, R(λx.count(R(right).x))) HOUSING 24 941 measurement units x: \"Housing that is 800 square feet or bigger?\" c: \"housing unit whose size is at least 800 square feet\" z: type.housingUnit area. > .800 RESTAURANTS 32 1657 long unary relations x: \"What restaurant can you eat lunch outside at?\" c: \"restaurant that has outdoor seating and that serves lunch\" z: type.restaurant hasOutdoorSeating serveslunch PUBLICATIONS 15 801 sublexical compositionality x: \"Who has co-authored articles with Efron?\" c: \"person that is author of article whose author is efron\" z: type.person R(author).(type.article author.efron) SOCIAL 45 4419 multi-arity relations x: \"When did alice start attending brown university?\" c: \"start date of student alice whose university is brown university\" z: R(date). Table 3 : We experimented on seven domains, covering a variety of phenomena. For each domain, we show the number of predicates, number of examples, and a (c, z) generated by our framework along with a paraphrased utterance x. Table 4 : Features for the paraphrasing model. pos(x i:i ) is the POS tag; type( z w ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c.',\n",
       "    'cite_spans': [{'start': 442,\n",
       "      'end': 448,\n",
       "      'text': '[2015]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1435,\n",
       "      'end': 1458,\n",
       "      'text': 'Berant and Liang (2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1494,\n",
       "      'end': 1518,\n",
       "      'text': 'Pasupat and Liang (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'}],\n",
       "    'ref_spans': [{'start': 2769,\n",
       "      'end': 2776,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2995,\n",
       "      'end': 3002,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': '(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 . To optimize, we use AdaGrad (Duchi et al., 2010) .Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) . We count the number of exact matches, PPDB matches, and unmatched words.To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4 ). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) . We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .',\n",
       "    'cite_spans': [{'start': 70,\n",
       "      'end': 90,\n",
       "      'text': '(Duchi et al., 2010)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 253,\n",
       "      'end': 280,\n",
       "      'text': '(Ganitkevitch et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 411,\n",
       "      'end': 431,\n",
       "      'text': '(Liang et al., 2006)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'},\n",
       "     {'start': 774,\n",
       "      'end': 793,\n",
       "      'text': '(Och and Ney, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'},\n",
       "     {'start': 1213,\n",
       "      'end': 1234,\n",
       "      'text': '(Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'}],\n",
       "    'ref_spans': [{'start': 101,\n",
       "      'end': 108,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 654,\n",
       "      'end': 661,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We evaluated our functionality-driven process on the seven domains described in Section 5 and one new domain we describe in Section 7.3. For each domain, we held out a random 20% of the examples as the test set, and performed development on the remaining 80%, further splitting it to a training and development set (80%/20%). We created a database for each domain by randomly generating facts using entities and properties in the domain (with type-checking). We evaluated using accuracy, which is the fraction of examples that yield the correct denotation.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our functionality-driven process is predicated on the fact that each domain exhibits domain-specific phenomena. To corroborate this, we compare our full system to NOLEX, a baseline that omits all lexical features (Table 4 ), but uses PPDB as a domain-general paraphrasing component. We perform the complementary experiment and compare to NOPPDB, a baseline that omits PPDB match features. We also run BASELINE, where we omit both lexical and PPDB features. Table 5 presents the results of this experiment. Overall, our framework obtains an average accuracy of 59% across all eight domains. The performance of NOLEX is dramatically lower than FULL, indicating that it is important to learn domain-specific paraphrases using lexical features. The accuracy of NOPPDB is only slightly lower than FULL, showing that most of the required paraphrases can be learned during training. As expected, removing both lexical and PPDB features results in poor performance (BASELINE).Analysis. We performed error analysis on 10 errors in each domain. Almost 70% of the errors are due to problems in the paraphrasing model, where the canonical utterance has extra material, is missing some content, or results in an incorrect paraphrase. For example, \"restaurants that have waiters and you can sit outside\" is paraphrased to \"restaurant that has waiter service and that takes reservations\". Another 12.5% result from reordering issues, e.g, we paraphrase \"What venue has fewer than two articles\" to \"article that has less than two venue\". Inaccurate paraphrases provided by AMT workers account for the rest of the errors.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 213,\n",
       "      'end': 221,\n",
       "      'text': '(Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 457,\n",
       "      'end': 464,\n",
       "      'text': 'Table 5',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF6'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We hypothesized that we need to obtain paraphrases of canonical utterances corresponding to logical forms of only small depth. We ran the following experiment in the CALENDAR domain to test this claim. First, we define by NP 0 , NP 1 , and NP 2 the set of utterances generated by an NP that has exactly zero, one, and two NPs embedded in it. We define the training scenario 0 → 1, where we train on examples from NP 0 and test on examples from NP 1 ; 0 ∪ 1 → 1, 0 ∪ 1 → 2, and 0 ∪ 1 ∪ 2 → 2 are defined analogously. Our Scenario Acc. Scenario Acc. 0 → 1 22.9 0 ∪ 1 → 2 28.1 0 ∪ 1 → 1 85.8 0 ∪ 1 ∪ 2 → 2 47.5 Table 6 : Test set results in the CALENDAR domain on bounded non-compositionality. hypothesis is that generalization on 0 ∪ 1 → 2 should be better than for 0 → 1, since NP 1 utterances have non-compositional paraphrases, but training on NP 0 does not expose them.The results in Table 6 verify this hypothesis. The accuracy of 0 → 1 is almost 65% lower than 0 ∪ 1 → 1. On the other hand, the accuracy of 0 ∪ 1 → 2 is only 19% lower than 0 ∪ 1 ∪ 2 → 2.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 608,\n",
       "      'end': 615,\n",
       "      'text': 'Table 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 886,\n",
       "      'end': 893,\n",
       "      'text': 'Table 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'To verify the title of this paper, we attempted to create a semantic parser for a new domain (RECIPES) exactly 24 hours before the submission deadline. Starting at midnight, we created a seed lexicon in less than 30 minutes. Then we generated canonical utterances and allowed AMT workers to provide paraphrases overnight. In the morning, we trained our parser and obtained an accuracy of 70.8% on the test set.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Geo880. To test how our parser generalizes to utterances independent of our framework, we created a semantic parser for the domain of US geography, and tested on the standard 280 test examples from GEO880 (Zelle and Mooney, 1996) . We did not use the standard 600 training examples. Our parser obtained 56.4% accuracy, which is substantially lower than state-of-the-art (∼ 90%).We performed error analysis on 100 random sentences from the development set where accuracy was 60%. We found that the parser learns from the training data to prefer shorter paraphrases, which accounts for 30% of the errors. In most of these cases, the correct logical form is ranked at the top-3 results (accuracy for the top-3 derivations is 73%). GEO880 contains highly compositional utterances, and in 25% of the errors the correct derivation tree exceeds the maximum depth used for our parser. Another 17.5% of the errors are caused by problems in the paraphrasing model. For example, in the utterance \"what is the size of california\", the model learns that \"size\" corresponds to \"population\" rather than \"area\". Errors related to reordering and the syntactic structure of the input utterance account for 7.5% of the errors. For example, the utterance \"what is the area of the largest state\" is paraphrased to \"state that has the largest area\".Calendar. In Section 7.1, we evaluated on utterances obtained by paraphrasing canonical utterances from the grammar. To examine the coverage of our parser on independently-produced utterances, we asked AMT workers to freely come up with queries. We collected 186 such queries; 5 were spam and discarded. We replaced all entities (people, dates, etc.) with entities from our seed lexicon to avoid focusing on entity detection.We were able to annotate 52% of the utterances with logical forms from our grammar. We could not annotate 20% of the utterances due to relative time references, such as \"What time is my next meeting?\". 14% of the utterances were not covered due to binary predicates not in the grammar (\"What is the agenda of the meeting?\") or missing entities (\"When is Dan\\'s birthday?\"). Another 2% required unsupported calculations (\"How much free time do I have tomorrow?\"), and the rest are out of scope for other reasons (\"When does my Verizon data plan start over?\").We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model.',\n",
       "    'cite_spans': [{'start': 205,\n",
       "      'end': 229,\n",
       "      'text': '(Zelle and Mooney, 1996)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF25'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013) . However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013) . Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal.Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014) . All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing.Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014) . We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010) . However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance.In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification.',\n",
       "    'cite_spans': [{'start': 101,\n",
       "      'end': 122,\n",
       "      'text': '(Cai and Yates, 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 123,\n",
       "      'end': 148,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 149,\n",
       "      'end': 169,\n",
       "      'text': 'Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 410,\n",
       "      'end': 438,\n",
       "      'text': '(Kushman and Barzilay, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 466,\n",
       "      'end': 489,\n",
       "      'text': '(Matuszek et al., 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'},\n",
       "     {'start': 490,\n",
       "      'end': 510,\n",
       "      'text': 'Tellex et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF21'},\n",
       "     {'start': 511,\n",
       "      'end': 542,\n",
       "      'text': 'Krishnamurthy and Kollar, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 756,\n",
       "      'end': 787,\n",
       "      'text': '(Zettlemoyer and Collins, 2005;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 788,\n",
       "      'end': 810,\n",
       "      'text': 'Wong and Mooney, 2007)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF23'},\n",
       "     {'start': 826,\n",
       "      'end': 847,\n",
       "      'text': '(Clarke et al., 2010;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 848,\n",
       "      'end': 867,\n",
       "      'text': 'Liang et al., 2011)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 888,\n",
       "      'end': 917,\n",
       "      'text': '(Artzi and Zettlemoyer, 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 918,\n",
       "      'end': 937,\n",
       "      'text': 'Reddy et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 1202,\n",
       "      'end': 1222,\n",
       "      'text': '(Fader et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 1244,\n",
       "      'end': 1268,\n",
       "      'text': '(Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1393,\n",
       "      'end': 1413,\n",
       "      'text': '(Woods et al., 1972;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF24'},\n",
       "     {'start': 1414,\n",
       "      'end': 1439,\n",
       "      'text': 'Warren and Pereira, 1982)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF22'},\n",
       "     {'start': 1472,\n",
       "      'end': 1489,\n",
       "      'text': '(Schwitter, 2010)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'}],\n",
       "    'ref_spans': [{'start': 1354,\n",
       "      'end': 1361,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF4'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {'FIGREF0': {'text': '(student.Alice university.Brown) BASKETBALL 24 1952 parentheticals x: \"How many fouls were played by Kobe Bryant in 2004?\" c: \"number of fouls (over a season) of player kobe bryant whose season is 2004\" z: count(R(fouls).(player.KobeBryant season.2004)',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF1': {'text': 'Basic #words and bigram matches in (x, c) #words and bigram PPDB matches in (x, c) #unmatched words in x #unmatched words in c size of denotation of z, (| z w |) pos(x0:0) conjoined with type( z w ) #nodes in tree generating z Lexical ∀(i, j) ∈ A. (xi:i, cj:j) ∀(i, j) ∈ A. (xi:i, cj:j+1) ∀(i, j) ∈ A. (xi:i, cj−1:j) ∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1, cj:j+1) all unaligned words in x and c (xi:j, c i :j ) if in phrase table',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'TABREF0': {'text': 'person that is author of the most number of article argmax(type.person, R(λx.count(type.article author.x))) ...what is the newest published article? who has published the most articles?',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF1': {'text': 'NP[type.article publicationDate.1950]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF2': {'text': 'The seed lexicon for the SOCIAL do- main, which specifies for each predicate (e.g., birthdate) a phrase (e.g., \"birthdate\") that real- izes that predicate and its syntactic category (e.g., RELNP).',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF3': {'text': ') whose RELNP[r] CMP[c] NP[y] → CP[r.c.y] is|is not|is smaller than|is larger than|is at least|is at most → CMP[= | = | < | > | ≤ | ≥]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF4': {'text': 'The domain-general grammar which is combined with the seed lexicon to generate logical forms and canonical utterances that cover the supported logical functionality.',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF6': {'text': 'Test set results on all domains and baselines.',\n",
       "    'latex': None,\n",
       "    'type': 'table'}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Bootstrapping semantic parsers from conversations',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '421--432',\n",
       "    'other_ids': {},\n",
       "    'links': '1140108'},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Weakly supervised learning of semantic parsers for mapping instructions to actions',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '1',\n",
       "    'issn': '',\n",
       "    'pages': '49--62',\n",
       "    'other_ids': {},\n",
       "    'links': '9963298'},\n",
       "   'BIBREF2': {'ref_id': 'b2',\n",
       "    'title': 'Semantic parsing via paraphrasing',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '1336493'},\n",
       "   'BIBREF3': {'ref_id': 'b3',\n",
       "    'title': 'Semantic parsing on Freebase from question-answer pairs',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': [], 'last': 'Chou', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Frostig', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6401679'},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Large-scale semantic parsing via schema matching and lexicon extension',\n",
       "    'authors': [{'first': 'Q', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': [], 'last': 'Yates', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '2265838'},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': \"Driving semantic parsing from the world's response\",\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Clarke', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Goldwasser', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Chang', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Roth', 'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'Computational Natural Language Learning (CoNLL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '18--27',\n",
       "    'other_ids': {},\n",
       "    'links': '5667590'},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'Adaptive subgradient methods for online learning and stochastic optimization',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Duchi', 'suffix': ''},\n",
       "     {'first': 'E', 'middle': [], 'last': 'Hazan', 'suffix': ''},\n",
       "     {'first': 'Y', 'middle': [], 'last': 'Singer', 'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'Conference on Learning Theory (COLT)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '538820'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': 'Paraphrase-driven learning for open question answering',\n",
       "    'authors': [{'first': 'A', 'middle': [], 'last': 'Fader', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''},\n",
       "     {'first': 'O', 'middle': [], 'last': 'Etzioni', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '8893912'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'PPDB: The paraphrase database',\n",
       "    'authors': [{'first': 'J',\n",
       "      'middle': [],\n",
       "      'last': 'Ganitkevitch',\n",
       "      'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['V'], 'last': 'Durme', 'suffix': ''},\n",
       "     {'first': 'C', 'middle': [], 'last': 'Callison-Burch', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '758--764',\n",
       "    'other_ids': {},\n",
       "    'links': '6067240'},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Jointly learning to parse and perceive: Connecting natural language to the physical world',\n",
       "    'authors': [{'first': 'J',\n",
       "      'middle': [],\n",
       "      'last': 'Krishnamurthy',\n",
       "      'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Kollar', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '1',\n",
       "    'issn': '',\n",
       "    'pages': '193--206',\n",
       "    'other_ids': {},\n",
       "    'links': '10250712'},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Using semantic unification to generate regular expressions from natural language',\n",
       "    'authors': [{'first': 'N', 'middle': [], 'last': 'Kushman', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Barzilay', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '826--836',\n",
       "    'other_ids': {},\n",
       "    'links': '6216733'},\n",
       "   'BIBREF11': {'ref_id': 'b11',\n",
       "    'title': 'Scaling semantic parsers with on-the-fly ontology matching',\n",
       "    'authors': [{'first': 'T',\n",
       "      'middle': [],\n",
       "      'last': 'Kwiatkowski',\n",
       "      'suffix': ''},\n",
       "     {'first': 'E', 'middle': [], 'last': 'Choi', 'suffix': ''},\n",
       "     {'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '14341841'},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Alignment by agreement',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': [], 'last': 'Taskar', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Klein', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': 'North American Association for Computational Linguistics (NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '104--111',\n",
       "    'other_ids': {},\n",
       "    'links': '618683'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'Learning dependency-based compositional semantics',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['I'], 'last': 'Jordan', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Klein', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '590--599',\n",
       "    'other_ids': {},\n",
       "    'links': '340852'},\n",
       "   'BIBREF14': {'ref_id': 'b14',\n",
       "    'title': 'Lambda dependency-based compositional semantics. arXiv',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'A joint model of language and perception for grounded attribute learning',\n",
       "    'authors': [{'first': 'C', 'middle': [], 'last': 'Matuszek', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Fitzgerald', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Bo', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Fox', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'International Conference on Machine Learning (ICML)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1671--1678',\n",
       "    'other_ids': {},\n",
       "    'links': '2408319'},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'The alignment template approach to statistical machine translation',\n",
       "    'authors': [{'first': 'F', 'middle': ['J'], 'last': 'Och', 'suffix': ''},\n",
       "     {'first': 'H', 'middle': [], 'last': 'Ney', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'Computational Linguistics',\n",
       "    'volume': '30',\n",
       "    'issn': '',\n",
       "    'pages': '417--449',\n",
       "    'other_ids': {},\n",
       "    'links': '1272090'},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Compositional semantic parsing on semi-structured tables',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Pasupat', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '9027681'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'Features and pitfalls that users should seek in natural language interfaces to databases',\n",
       "    'authors': [{'first': 'R',\n",
       "      'middle': ['A P'],\n",
       "      'last': 'Rangel',\n",
       "      'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['A'], 'last': 'Aguirre', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['J'], 'last': 'Gonzlez', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['M'], 'last': 'Carpio', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Recent Advances on Hybrid Approaches for Designing Intelligent Systems',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '617--630',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Largescale semantic parsing without question-answer pairs',\n",
       "    'authors': [{'first': 'S', 'middle': [], 'last': 'Reddy', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Lapata', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Steedman', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '2',\n",
       "    'issn': '10',\n",
       "    'pages': '377--392',\n",
       "    'other_ids': {},\n",
       "    'links': '15324422'},\n",
       "   'BIBREF20': {'ref_id': 'b20',\n",
       "    'title': 'Controlled natural languages for knowledge representation',\n",
       "    'authors': [{'first': 'R',\n",
       "      'middle': [],\n",
       "      'last': 'Schwitter',\n",
       "      'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'International Conference on Computational Linguistics (COLING)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1113--1121',\n",
       "    'other_ids': {},\n",
       "    'links': '10228634'},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'Understanding natural language commands for robotic navigation and mobile manipulation',\n",
       "    'authors': [{'first': 'S', 'middle': [], 'last': 'Tellex', 'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Kollar', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': [], 'last': 'Dickerson', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['R'], 'last': 'Walter', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': ['G'], 'last': 'Banerjee', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': ['J'], 'last': 'Teller', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Roy', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Association for the Advancement of Artificial Intelligence (AAAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '1078533'},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'An efficient easily adaptable system for interpreting natural language queries',\n",
       "    'authors': [{'first': 'D', 'middle': [], 'last': 'Warren', 'suffix': ''},\n",
       "     {'first': 'F', 'middle': [], 'last': 'Pereira', 'suffix': ''}],\n",
       "    'year': 1982,\n",
       "    'venue': 'Computational Linguistics',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '110--122',\n",
       "    'other_ids': {},\n",
       "    'links': '2498523'},\n",
       "   'BIBREF23': {'ref_id': 'b23',\n",
       "    'title': 'Learning synchronous grammars for semantic parsing with lambda calculus',\n",
       "    'authors': [{'first': 'Y', 'middle': ['W'], 'last': 'Wong', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['J'], 'last': 'Mooney', 'suffix': ''}],\n",
       "    'year': 2007,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '960--967',\n",
       "    'other_ids': {},\n",
       "    'links': '9337134'},\n",
       "   'BIBREF24': {'ref_id': 'b24',\n",
       "    'title': 'The lunar sciences natural language information system: Final report',\n",
       "    'authors': [{'first': 'W', 'middle': ['A'], 'last': 'Woods', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['M'], 'last': 'Kaplan', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['N'], 'last': 'Webber', 'suffix': ''}],\n",
       "    'year': 1972,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '62727207'},\n",
       "   'BIBREF25': {'ref_id': 'b25',\n",
       "    'title': 'Learning to parse database queries using inductive logic programming',\n",
       "    'authors': [{'first': 'M', 'middle': [], 'last': 'Zelle', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['J'], 'last': 'Mooney', 'suffix': ''}],\n",
       "    'year': 1996,\n",
       "    'venue': 'Association for the Advancement of Artificial Intelligence (AAAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1050--1055',\n",
       "    'other_ids': {},\n",
       "    'links': '263135'},\n",
       "   'BIBREF26': {'ref_id': 'b26',\n",
       "    'title': 'Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars',\n",
       "    'authors': [{'first': 'L',\n",
       "      'middle': ['S'],\n",
       "      'last': 'Zettlemoyer',\n",
       "      'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Collins', 'suffix': ''}],\n",
       "    'year': 2005,\n",
       "    'venue': 'Uncertainty in Artificial Intelligence (UAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '658--666',\n",
       "    'other_ids': {},\n",
       "    'links': '449252'}}},\n",
       " 'latex_parse': None}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A survey of text summarization techniques': ['556431'],\n",
       " 'Abstractive multi-document summarization via phrase selection and merging': ['8377315'],\n",
       " 'Adam: A method for stochastic optimization': ['6628106'],\n",
       " 'Auto-encoding variational bayes': ['15789289'],\n",
       " 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies': ['1320'],\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": ['13723748'],\n",
       " 'Effective approaches to attention-based neural machine translation': ['1998416'],\n",
       " 'Exploiting category-specific information for multi-document summarization': ['6317274'],\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': ['10418456'],\n",
       " 'Linear programming 1: introduction': ['53739754'],\n",
       " 'Multi-document summarization by sentence extraction': ['8294822'],\n",
       " 'Multiple aspect summarization using integer linear programming': ['17497992'],\n",
       " 'Reader-aware multi-document summarization via sparse coding': ['14777460'],\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': ['29562039'],\n",
       " 'Social context summarization': ['704517'],\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': ['16895865'],\n",
       " 'Textrank: Bringing order into texts': ['577937'],\n",
       " 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications': ['12681629'],\n",
       " \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\": [None],\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': ['8377315'],\n",
       " 'Autoencoding variational bayes': [None],\n",
       " 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies': ['1320'],\n",
       " 'Effective approaches to attentionbased neural machine translation': ['1998416'],\n",
       " 'Exploiting category-specific information for multidocument summarization': ['6317274'],\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': ['532313'],\n",
       " 'Neural machine translation by jointly learning to align and translate': ['11212020'],\n",
       " 'Rouge: A package for automatic evaluation of summaries': ['964287'],\n",
       " 'Theano: new features and speed improvements': ['8180128']}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bib_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bibs = {}\n",
    "for num_artic,article in enumerate(all_articles):\n",
    "#     if num_artic >2:\n",
    "#         break\n",
    "    \n",
    "    # проверка наличия определленных значений статьи\n",
    "    if not ((article['grobid_parse'] and article['grobid_parse']['bib_entries']) or (article['latex_parse'] and article['latex_parse']['body_text'])):\n",
    "#         article_bibs[article['paper_id']] = 'None'\n",
    "        continue\n",
    "        \n",
    "    bib_entries_grobid = None\n",
    "    if article['grobid_parse'] and article['grobid_parse']['bib_entries']:\n",
    "        bib_entries_grobid = dict()\n",
    "        \n",
    "        for k,v in article['grobid_parse']['bib_entries'].items():\n",
    "            bib_entries_grobid[v['title']] = v['links']\n",
    "            \n",
    "        bib_entries_grobid = {k: v for k, v in sorted(bib_entries_grobid.items(), key=lambda item: item[0])}\n",
    "        \n",
    "    bib_entries_latex = None\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        bib_entries_latex = dict()\n",
    "        for k,v in article['latex_parse']['bib_entries'].items():\n",
    "            bib_entries_latex[v['title']] = v['links']\n",
    "            \n",
    "        bib_entries_latex = {k: v for k, v in sorted(bib_entries_latex.items(), key=lambda item: item[0])}\n",
    "    \n",
    "    all_bib_entries = dict()\n",
    "    if bib_entries_latex:\n",
    "        for k,v in bib_entries_latex.items():\n",
    "            if k == '':\n",
    "                continue\n",
    "                \n",
    "            if v:\n",
    "                all_bib_entries[k] = [v]\n",
    "            else:\n",
    "                all_bib_entries[k] = []\n",
    "\n",
    "    if bib_entries_grobid:\n",
    "        for k,v in bib_entries_grobid.items():\n",
    "            if k == '':\n",
    "                continue\n",
    "                \n",
    "            if k in all_bib_entries:\n",
    "                if len(all_bib_entries[k]) == 0:\n",
    "                    if v:\n",
    "                        all_bib_entries[k] = [v]\n",
    "                    else:\n",
    "                        all_bib_entries[k] = []\n",
    "                else:\n",
    "                    if all_bib_entries[k][0] == v:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if v:\n",
    "                            all_bib_entries[k].append(v)\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                if v:\n",
    "                    all_bib_entries[k] = [v]\n",
    "                else:\n",
    "                    all_bib_entries[k] = []\n",
    "    article_bibs[article['paper_id']] = all_bib_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39856"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_bibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,\n",
       " {'A survey of text summarization techniques': ['556431'],\n",
       "  'Abstractive multi-document summarization via phrase selection and merging': ['8377315'],\n",
       "  'Adam: A method for stochastic optimization': ['6628106'],\n",
       "  'Auto-encoding variational bayes': ['15789289'],\n",
       "  'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies': ['1320'],\n",
       "  \"Comments-oriented document summarization: Understanding documents with readers' feedback\": ['13723748'],\n",
       "  'Effective approaches to attention-based neural machine translation': ['1998416'],\n",
       "  'Exploiting category-specific information for multi-document summarization': ['6317274'],\n",
       "  'Lexpagerank: Prestige in multi-document text summarization': ['10418456'],\n",
       "  'Linear programming 1: introduction': ['53739754'],\n",
       "  'Multi-document summarization by sentence extraction': ['8294822'],\n",
       "  'Multiple aspect summarization using integer linear programming': ['17497992'],\n",
       "  'Reader-aware multi-document summarization via sparse coding': ['14777460'],\n",
       "  'Salience estimation via variational auto-encoders for multi-document summarization': ['29562039'],\n",
       "  'Social context summarization': ['704517'],\n",
       "  'Stochastic backpropagation and approximate inference in deep generative models': ['16895865'],\n",
       "  'Textrank: Bringing order into texts': ['577937'],\n",
       "  'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications': ['12681629'],\n",
       "  \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\": [],\n",
       "  'Abstractive multidocument summarization via phrase selection and merging': ['8377315'],\n",
       "  'Autoencoding variational bayes': [],\n",
       "  'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies': ['1320'],\n",
       "  'Effective approaches to attentionbased neural machine translation': ['1998416'],\n",
       "  'Exploiting category-specific information for multidocument summarization': ['6317274'],\n",
       "  'Manifold-ranking based topic-focused multidocument summarization': ['532313'],\n",
       "  'Neural machine translation by jointly learning to align and translate': ['11212020'],\n",
       "  'Rouge: A package for automatic evaluation of summaries': ['964287'],\n",
       "  'Theano: new features and speed improvements': ['8180128']})"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_bibs['10164018']),article_bibs['10164018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27,\n",
       " {'A joint model of language and perception for grounded attribute learning': ['2408319'],\n",
       "  'Adaptive subgradient methods for online learning and stochastic optimization': ['538820'],\n",
       "  'Alignment by agreement': ['618683'],\n",
       "  'An efficient easily adaptable system for interpreting natural language queries': ['2498523'],\n",
       "  'Bootstrapping semantic parsers from conversations': ['1140108'],\n",
       "  'Compositional semantic parsing on semi-structured tables': ['9027681'],\n",
       "  'Controlled natural languages for knowledge representation': ['10228634'],\n",
       "  \"Driving semantic parsing from the world's response\": ['5667590'],\n",
       "  'Features and pitfalls that users should seek in natural language interfaces to databases': [],\n",
       "  'Jointly learning to parse and perceive: Connecting natural language to the physical world': ['10250712'],\n",
       "  'Lambda dependency-based compositional semantics. arXiv': [],\n",
       "  'Large-scale semantic parsing via schema matching and lexicon extension': ['2265838'],\n",
       "  'Largescale semantic parsing without question-answer pairs': ['15324422'],\n",
       "  'Learning dependency-based compositional semantics': ['340852'],\n",
       "  'Learning synchronous grammars for semantic parsing with lambda calculus': ['9337134'],\n",
       "  'Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars': ['449252'],\n",
       "  'Learning to parse database queries using inductive logic programming': ['263135'],\n",
       "  'PPDB: The paraphrase database': ['6067240'],\n",
       "  'Paraphrase-driven learning for open question answering': ['8893912'],\n",
       "  'Scaling semantic parsers with on-the-fly ontology matching': ['14341841'],\n",
       "  'Semantic parsing on Freebase from question-answer pairs': ['6401679'],\n",
       "  'Semantic parsing via paraphrasing': ['1336493'],\n",
       "  'The alignment template approach to statistical machine translation': ['1272090'],\n",
       "  'The lunar sciences natural language information system: Final report': ['62727207'],\n",
       "  'Understanding natural language commands for robotic navigation and mobile manipulation': ['1078533'],\n",
       "  'Using semantic unification to generate regular expressions from natural language': ['6216733'],\n",
       "  'Weakly supervised learning of semantic parsers for mapping instructions to actions': ['9963298']})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_bibs['14472576']),article_bibs['14472576']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "num_all_bibs = []\n",
    "num_all_bibs_with_links = []\n",
    "num_all_bibs_w_l_in_dict_in = []\n",
    "for num_artic,(key,val_dict) in enumerate(article_bibs.items()):\n",
    "    index.append(key)\n",
    "    num_bibs = 0\n",
    "    num_bibs_with_links = 0\n",
    "    num_bibs_w_l_in_dict_in = 0\n",
    "    try:\n",
    "        for k,val in val_dict.items():\n",
    "            num_bibs +=1\n",
    "            if len(val)>0:\n",
    "                num_bibs_with_links+=1\n",
    "            for link in val:\n",
    "                if link in dict_in_ids_all:\n",
    "                    num_bibs_w_l_in_dict_in+=1\n",
    "                    break\n",
    "    except:\n",
    "        print(num_artic)\n",
    "        print(key)\n",
    "        break\n",
    "    num_all_bibs.append(num_bibs)\n",
    "    num_all_bibs_with_links.append(num_bibs_with_links)\n",
    "    num_all_bibs_w_l_in_dict_in.append(num_bibs_w_l_in_dict_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'#bibs':num_all_bibs,'#bibs_with_links':num_all_bibs_with_links,'#bibs_with_links_in_dict':num_all_bibs_w_l_in_dict_in}\n",
    "df_covering =  pd.DataFrame(d)\n",
    "df_covering.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_covering['%_in_dict2bibs'] = df_covering['#bibs_with_links_in_dict']/df_covering['#bibs']\n",
    "df_covering['%_in_dict2bibs_w_l'] = df_covering['#bibs_with_links_in_dict']/df_covering['#bibs_with_links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пояснение к колонкам:\n",
    " - **#bibs** -  количество всех распознанных Reference статей\n",
    " - **#bibs_with_links** - количество всех распознанных Reference статей, которые присутствуют в датасете GORC\n",
    " - **#bibs_with_links_in_dict** - количество всех распознанных Reference статей, которые присутствуют в структуре данных ***in***,где есть статья и ссылающиеся на неё статьи\n",
    " - **%_in_dict2bibs** - отношение кол-ва ref **in** ко всему кол-ву ref статьи \n",
    " - **%_in_dict2bibs_w_l** - отношение кол-ва ref **in** ко всему кол-ву ref статьей, которые присутствуют в датасете GORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#bibs</th>\n",
       "      <th>#bibs_with_links</th>\n",
       "      <th>#bibs_with_links_in_dict</th>\n",
       "      <th>%_in_dict2bibs</th>\n",
       "      <th>%_in_dict2bibs_w_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17302615</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243536</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248240</th>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          #bibs  #bibs_with_links  #bibs_with_links_in_dict  %_in_dict2bibs  \\\n",
       "10164018     28                26                        20        0.714286   \n",
       "14472576     27                25                        21        0.777778   \n",
       "17302615      8                 5                         4        0.500000   \n",
       "3243536      10                 3                         2        0.200000   \n",
       "3248240      16                12                         8        0.500000   \n",
       "\n",
       "          %_in_dict2bibs_w_l  \n",
       "10164018            0.769231  \n",
       "14472576            0.840000  \n",
       "17302615            0.800000  \n",
       "3243536             0.666667  \n",
       "3248240             0.666667  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38197519738407737"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering['%_in_dict2bibs'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5416087540264509"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering['%_in_dict2bibs_w_l'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Покрытие по обзорным частям статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "for num_artic,article in enumerate(all_articles):\n",
    "    # проверяем что у статьи есть grobid_parse и latex_parse и естб текст\n",
    "    if (article['grobid_parse'] and article['grobid_parse']['body_text']) or (article['latex_parse'] and article['latex_parse']['body_text']):\n",
    "        # задаем шаблон отражения статьи в укороченном формате (чтобы занимать меньше памяти)\n",
    "        overview_papers[article['paper_id']] = { 'paper_id':article['paper_id'],   'metadata':article['metadata'],\n",
    "                                                 's2_pdf_hash':article['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}\n",
    "        \n",
    "        grobid_parse_overview = None\n",
    "        # если у статьи есть article['grobid_parse']['body_text']\n",
    "        if article['grobid_parse'] and article['grobid_parse']['body_text']:\n",
    "            grobid_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "                grobid_parse_overview[num_sec] = sections\n",
    "            \n",
    "            # отсортируем по количеству цитат абзацы\n",
    "            grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), key=lambda item: len(item[1]['cite_spans']), reverse=True)}\n",
    "            \n",
    "#             # найдем 1 , пока без 2 максимума по обзорной части\n",
    "            max_cite_span_sum = 0\n",
    "            max_grobid_parse_overview = dict()\n",
    "            for k,v in grobid_parse_overview.items():\n",
    "                if max_cite_span_sum < len(v['cite_spans']):\n",
    "                    max_cite_span_sum = len(v['cite_spans'])\n",
    "                    max_grobid_parse_overview[k] = v\n",
    "                    \n",
    "#                 # записываем 2 максимум, если количество ссылок в егочасти больше половины от максимального \n",
    "#                 elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "#                     max_grobid_parse_overview[k] = v\n",
    "                    \n",
    "            \n",
    "            grobid_parse_overview = max_grobid_parse_overview\n",
    "            \n",
    "        latex_parse_overview = None\n",
    "        # если у статьи есть article['latex_parse']['body_text']\n",
    "        if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "            latex_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            # в latex_parse \n",
    "            for sections in article['latex_parse']['body_text']:\n",
    "                if sections['section'] in latex_parse_overview:\n",
    "                    if latex_parse_overview[sections['section']] == sections:\n",
    "                        continue\n",
    "                    else:\n",
    "                        latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "                        latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "                        latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "                        latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "                else:\n",
    "                    latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                                  'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                                  'section':[sections['section']]}\n",
    "            latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), key=lambda item: item[1]['cite_span_lens'], reverse=True)}\n",
    "        \n",
    "        \n",
    "            max_cite_span_sum = 0\n",
    "            max_latex_parse_overview = dict()\n",
    "            for k,v in latex_parse_overview.items():\n",
    "                if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "                    max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "                    max_latex_parse_overview[k] = v\n",
    "#                 elif (max_cite_span_sum>0) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "#                     max_latex_parse_overview[k] = v\n",
    "            \n",
    "            latex_parse_overview = max_latex_parse_overview\n",
    "        \n",
    "\n",
    "        if grobid_parse_overview:\n",
    "            overview_papers[article['paper_id']]['grobid_parse'] = {'abstract':None,\n",
    "                                                        'overview_text':grobid_parse_overview,  \n",
    "                                                        'bib_entries':None}\n",
    "            if article['grobid_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['abstract'] = article['grobid_parse']['abstract']\n",
    "            if article['grobid_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['bib_entries'] = article['grobid_parse']['bib_entries']\n",
    "            \n",
    "        if latex_parse_overview:            \n",
    "            overview_papers[article['paper_id']]['latex_parse'] = {'abstract':None,\n",
    "                                                                    'overview_text':latex_parse_overview,  \n",
    "                                                                    'bib_entries':None}\n",
    "            if article['latex_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['abstract'] = article['latex_parse']['abstract']\n",
    "            if article['latex_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['bib_entries'] = article['latex_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018',\n",
       " '14472576',\n",
       " '17302615',\n",
       " '3243536',\n",
       " '3248240',\n",
       " '2223737',\n",
       " '488',\n",
       " '14323173',\n",
       " '15251605',\n",
       " '8260435']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(overview_papers)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])\n",
      "dict_keys(['abstract', 'overview_text', 'bib_entries'])\n",
      "dict_keys(['abstract', 'overview_text', 'bib_entries'])\n",
      "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])\n",
      "dict_keys(['text', 'cite_spans', 'cite_span_lens', 'section'])\n"
     ]
    }
   ],
   "source": [
    "print(overview_papers['10164018'].keys())\n",
    "print(overview_papers['10164018']['grobid_parse'].keys())\n",
    "print(overview_papers['10164018']['latex_parse'].keys())\n",
    "print(overview_papers['10164018']['grobid_parse']['overview_text'][0].keys())\n",
    "print(overview_papers['488']['latex_parse']['overview_text']['Introduction'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 99,\n",
       "  'end': 110,\n",
       "  'text': '(Nie, 2010)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF7'},\n",
       " {'start': 223,\n",
       "  'end': 245,\n",
       "  'text': '(Sokokov et al., 2013)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF9'},\n",
       " {'start': 513,\n",
       "  'end': 535,\n",
       "  'text': '(Sokolov et al., 2014)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF10'},\n",
       " {'start': 717,\n",
       "  'end': 738,\n",
       "  'text': '(Sokokov et al., 2013',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF9'},\n",
       " {'start': 1662,\n",
       "  'end': 1681,\n",
       "  'text': '(Zhou et al., 2012)',\n",
       "  'latex': None,\n",
       "  'ref_id': None},\n",
       " {'start': 1933,\n",
       "  'end': 1956,\n",
       "  'text': '(Bahdanau et al., 2015)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF0'},\n",
       " {'start': 2195,\n",
       "  'end': 2219,\n",
       "  'text': '(Zamani and Croft, 2017)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF13'},\n",
       " {'start': 2265,\n",
       "  'end': 2287,\n",
       "  'text': '(Mikolov et al., 2013)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF6'},\n",
       " {'start': 2491,\n",
       "  'end': 2511,\n",
       "  'text': '(Luong et al., 2015)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF5'}]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['189927790']['grobid_parse']['overview_text'][0]['cite_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'b0',\n",
       "  'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "  'authors': [{'first': 'Dzmitry',\n",
       "    'middle': [],\n",
       "    'last': 'Bahdanau',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "   {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'International Conference on Learning Representations (ICLR)',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '11212020'},\n",
       " 'BIBREF1': {'ref_id': 'b1',\n",
       "  'title': 'NLTK: The natural language toolkit',\n",
       "  'authors': [{'first': 'Steven', 'middle': [], 'last': 'Bird', 'suffix': ''},\n",
       "   {'first': 'Edward', 'middle': [], 'last': 'Loper', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'Proceedings of the ACL Interactive Poster and Demonstration Sessions',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '1438450'},\n",
       " 'BIBREF2': {'ref_id': 'b2',\n",
       "  'title': 'Training neural machine translation using word embedding-based loss',\n",
       "  'authors': [{'first': 'Satoshi',\n",
       "    'middle': [],\n",
       "    'last': 'Nakamura Katsuki Chousa',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Katsuhito', 'middle': [], 'last': 'Sudoh', 'suffix': ''}],\n",
       "  'year': 2018,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {'arXiv': ['arXiv:1807.11219']},\n",
       "  'links': '51882689'},\n",
       " 'BIBREF3': {'ref_id': 'b3',\n",
       "  'title': 'Europarl: A parallel corpus for statistical machine translation',\n",
       "  'authors': [{'first': 'Philipp',\n",
       "    'middle': [],\n",
       "    'last': 'Koehn',\n",
       "    'suffix': ''}],\n",
       "  'year': 2005,\n",
       "  'venue': 'MT summit',\n",
       "  'volume': '5',\n",
       "  'issn': '',\n",
       "  'pages': '79--86',\n",
       "  'other_ids': {},\n",
       "  'links': '38407095'},\n",
       " 'BIBREF4': {'ref_id': 'b4',\n",
       "  'title': 'Relevancebased language models',\n",
       "  'authors': [{'first': 'Victor',\n",
       "    'middle': [],\n",
       "    'last': 'Lavrenko',\n",
       "    'suffix': ''},\n",
       "   {'first': 'W. Bruce', 'middle': [], 'last': 'Croft', 'suffix': ''}],\n",
       "  'year': 2001,\n",
       "  'venue': 'SIGIR 2001: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '120--127',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF5': {'ref_id': 'b5',\n",
       "  'title': 'Effective approaches to attention-based neural machine translation',\n",
       "  'authors': [{'first': 'Thang', 'middle': [], 'last': 'Luong', 'suffix': ''},\n",
       "   {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "   {'first': 'Christopher', 'middle': ['D'], 'last': 'Manning', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1412--1421',\n",
       "  'other_ids': {},\n",
       "  'links': '1998416'},\n",
       " 'BIBREF6': {'ref_id': 'b6',\n",
       "  'title': 'Distributed representations of words and phrases and their compositionality',\n",
       "  'authors': [{'first': 'Tomas',\n",
       "    'middle': [],\n",
       "    'last': 'Mikolov',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Ilya', 'middle': [], 'last': 'Sutskever', 'suffix': ''},\n",
       "   {'first': 'Kai', 'middle': [], 'last': 'Chen', 'suffix': ''},\n",
       "   {'first': 'Greg', 'middle': ['S'], 'last': 'Corrado', 'suffix': ''},\n",
       "   {'first': 'Jeff', 'middle': [], 'last': 'Dean', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': 'Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems,NIPS',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '3111--3119',\n",
       "  'other_ids': {},\n",
       "  'links': '16447573'},\n",
       " 'BIBREF7': {'ref_id': 'b7',\n",
       "  'title': 'Cross-Language Information Retrieval. Synthesis Lectures on Human Language Technologies',\n",
       "  'authors': [{'first': 'Jian-Yun',\n",
       "    'middle': [],\n",
       "    'last': 'Nie',\n",
       "    'suffix': ''}],\n",
       "  'year': 2010,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF8': {'ref_id': 'b8',\n",
       "  'title': 'An overview of multi-task learning in deep neural networks',\n",
       "  'authors': [{'first': 'Sebastian',\n",
       "    'middle': [],\n",
       "    'last': 'Ruder',\n",
       "    'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {'arXiv': ['arXiv:1706.05098']},\n",
       "  'links': '10175374'},\n",
       " 'BIBREF9': {'ref_id': 'b9',\n",
       "  'title': 'Boosting cross-language retrieval by learning bilingual phrase associations from relevance rankings',\n",
       "  'authors': [{'first': 'Artem',\n",
       "    'middle': [],\n",
       "    'last': 'Sokokov',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Laura', 'middle': [], 'last': 'Jehl', 'suffix': ''},\n",
       "   {'first': 'Felix', 'middle': [], 'last': 'Hieber', 'suffix': ''},\n",
       "   {'first': 'Stefan', 'middle': [], 'last': 'Riezler', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': 'Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1688--1699',\n",
       "  'other_ids': {},\n",
       "  'links': '2790246'},\n",
       " 'BIBREF10': {'ref_id': 'b10',\n",
       "  'title': 'Learning to translate queries for CLIR',\n",
       "  'authors': [{'first': 'Artem',\n",
       "    'middle': [],\n",
       "    'last': 'Sokolov',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Felix', 'middle': [], 'last': 'Hieber', 'suffix': ''},\n",
       "   {'first': 'Stefan', 'middle': [], 'last': 'Riezler', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'The 37th International ACM SIGIR Conference on Research and Development in Information Retrieval',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1179--1182',\n",
       "  'other_ids': {},\n",
       "  'links': '46107'},\n",
       " 'BIBREF11': {'ref_id': 'b11',\n",
       "  'title': 'Attention is all you need',\n",
       "  'authors': [{'first': 'Ashish',\n",
       "    'middle': [],\n",
       "    'last': 'Vaswani',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Noam', 'middle': [], 'last': 'Shazeer', 'suffix': ''},\n",
       "   {'first': 'Niki', 'middle': [], 'last': 'Parmar', 'suffix': ''},\n",
       "   {'first': 'Jakob', 'middle': [], 'last': 'Uszkoreit', 'suffix': ''},\n",
       "   {'first': 'Llion', 'middle': [], 'last': 'Jones', 'suffix': ''},\n",
       "   {'first': 'Aidan', 'middle': ['N'], 'last': 'Gomez', 'suffix': ''},\n",
       "   {'first': 'Illia', 'middle': [], 'last': 'Kaiser', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Polosukhin', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'Advances in Neural Information Processing Systems',\n",
       "  'volume': '30',\n",
       "  'issn': '',\n",
       "  'pages': '5998--6008',\n",
       "  'other_ids': {},\n",
       "  'links': '13756489'},\n",
       " 'BIBREF12': {'ref_id': 'b12',\n",
       "  'title': 'Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings',\n",
       "  'authors': [{'first': 'Ivan', 'middle': [], 'last': 'Vulić', 'suffix': ''},\n",
       "   {'first': 'Marie-Francine', 'middle': [], 'last': 'Moens', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '363--372',\n",
       "  'other_ids': {},\n",
       "  'links': '2583305'},\n",
       " 'BIBREF13': {'ref_id': 'b13',\n",
       "  'title': 'Relevancebased word embedding',\n",
       "  'authors': [{'first': 'Hamed', 'middle': [], 'last': 'Zamani', 'suffix': ''},\n",
       "   {'first': 'W. Bruce', 'middle': [], 'last': 'Croft', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF14': {'ref_id': 'b14',\n",
       "  'title': 'A study of smoothing methods for language models applied to information retrieval',\n",
       "  'authors': [{'first': 'Chengxiang',\n",
       "    'middle': [],\n",
       "    'last': 'Zhai',\n",
       "    'suffix': ''},\n",
       "   {'first': 'John', 'middle': [], 'last': 'Lafferty', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'ACM Transactions on Information Systems (TOIS)',\n",
       "  'volume': '22',\n",
       "  'issn': '2',\n",
       "  'pages': '179--214',\n",
       "  'other_ids': {},\n",
       "  'links': '4106713'},\n",
       " 'BIBREF15': {'ref_id': 'b15',\n",
       "  'title': 'Vincent Wade, and Helen Ashman. 2012. Translation techniques in cross-language information retrieval',\n",
       "  'authors': [{'first': 'Dong', 'middle': [], 'last': 'Zhou', 'suffix': ''},\n",
       "   {'first': 'Mark', 'middle': [], 'last': 'Truran', 'suffix': ''},\n",
       "   {'first': 'Tim', 'middle': ['J'], 'last': 'Brailsford', 'suffix': ''}],\n",
       "  'year': None,\n",
       "  'venue': 'ACM Computing Surveys',\n",
       "  'volume': '45',\n",
       "  'issn': '1',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF16': {'ref_id': 'b16',\n",
       "  'title': 'A Loss Function and Validation Performance Analysis',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None}}"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['189927790']['grobid_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIBREF7 None Cross-Language Information Retrieval. Synthesis Lectures on Human Language Technologies\n",
      "BIBREF9 2790246 Boosting cross-language retrieval by learning bilingual phrase associations from relevance rankings\n",
      "BIBREF10 46107 Learning to translate queries for CLIR\n",
      "BIBREF9 2790246 Boosting cross-language retrieval by learning bilingual phrase associations from relevance rankings\n",
      "kek None None\n",
      "BIBREF0 11212020 Neural machine translation by jointly learning to align and translate\n",
      "BIBREF13 None Relevancebased word embedding\n",
      "BIBREF6 16447573 Distributed representations of words and phrases and their compositionality\n",
      "BIBREF5 1998416 Effective approaches to attention-based neural machine translation\n"
     ]
    }
   ],
   "source": [
    "bib_entries_grobid = dict()\n",
    "\n",
    "paper = overview_papers['189927790']\n",
    "key_grobid = list(paper['grobid_parse']['overview_text'].keys())[0]\n",
    "for cite_span in paper['grobid_parse']['overview_text'][key_grobid]['cite_spans']:\n",
    "    cite_ref = cite_span['ref_id']\n",
    "    cited_paper_id = None\n",
    "    if cite_ref in paper['grobid_parse']['bib_entries']:\n",
    "        cited_paper_id = paper['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "    if cited_paper_id:\n",
    "        print(cite_ref,cited_paper_id,paper['grobid_parse']['bib_entries'][cite_ref]['title'])\n",
    "        bib_entries_grobid[paper['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "    elif cite_ref  in paper['grobid_parse']['bib_entries']:\n",
    "        print(cite_ref,cited_paper_id,paper['grobid_parse']['bib_entries'][cite_ref]['title'])\n",
    "        bib_entries_grobid[paper['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "    else:\n",
    "        # неверно распознается, это редко статья\n",
    "        print('kek',cite_ref,cited_paper_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper['grobid_parse']['overview_text'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': '8294822',\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': '10418456',\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': '532313',\n",
       " 'A survey of text summarization techniques': '556431',\n",
       " 'Exploiting category-specific information for multidocument summarization': '6317274',\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': '29562039',\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": '13723748',\n",
       " 'Social context summarization': '704517',\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': '8377315',\n",
       " 'Autoencoding variational bayes': None,\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': '16895865'}"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_entries_grobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A survey of text summarization techniques': '556431',\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': '8377315',\n",
       " 'Autoencoding variational bayes': None,\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": '13723748',\n",
       " 'Exploiting category-specific information for multidocument summarization': '6317274',\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': '10418456',\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': '532313',\n",
       " 'Multi-document summarization by sentence extraction': '8294822',\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': '29562039',\n",
       " 'Social context summarization': '704517',\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': '16895865'}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bib_entries_grobid = {k: v for k, v in sorted(bib_entries_grobid.items(), key=lambda item: item[0])}\n",
    "bib_entries_grobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': None,\n",
       " 'overview_text': {'Introduction': {'text': ['CLIR systems retrieve documents written in a language that is different from search query language BIBREF0 . The primary objective of CLIR is to translate or project a query into the language of the document repository BIBREF1 , which we refer to as Retrieval Corpus (RC). To this end, common CLIR approaches translate search queries using a Machine Translation (MT) model and then use a monolingual IR system to retrieve from RC. In this process, a translation model is treated as a black box BIBREF2 , and it is usually trained on a sentence level parallel corpus, which we refer to as Translation Corpus (TC).',\n",
       "    'We address a pitfall of using existing MT models for query translation BIBREF1 . An MT model trained on TC does not have any knowledge of RC. In an extreme setting, where there are no common terms between the target side of TC and RC, a well trained and tested translation model would fail because of vocabulary mismatch between the translated query and documents of RC. Assuming a relaxed scenario where some commonality exists between two corpora, a translation model might still perform poorly, favoring terms that are more likely in TC but rare in RC. Our hypothesis is that a search query translation model would perform better if a translated query term is likely to appear in the both retrieval and translation corpora, a property we call balanced translation.',\n",
       "    'To achieve balanced translations, it is desired to construct an MT model that is aware of RC vocabulary. Different types of MT approaches have been adopted for CLIR task, such as dictionary-based MT, rule-based MT, statistical MT etc. BIBREF3 . However, to the best of our knowledge, a neural search query translation approach has yet to be taken by the community. NMT models with attention based encoder-decoder techniques have achieved state-of-the-art performance for several language pairs BIBREF4 . We propose a multi-task learning NMT architecture that takes RC vocabulary into account by learning Relevance-based Auxiliary Task (RAT). RAT is inspired from two word embedding learning approaches: Relevance-based Word Embedding (RWE) BIBREF5 and Continuous Bag of Words (CBOW) embedding BIBREF6 . We show that learning NMT with RAT enables it to generate balanced translation.',\n",
       "    'NMT models learn to encode the meaning of a source sentence and decode the meaning to generate words in a target language BIBREF7 . In the proposed multi-task learning model, RAT shares the decoder embedding and final representation layer with NMT. Our architecture answers the following question: In the decoding stage, can we restrict an NMT model so that it does not only generate terms that are highly likely in TC?. We show that training a strong baseline NMT with RAT roughly achieves 16% improvement over the baseline. Using a qualitative analysis, we further show that RAT works as a regularizer and prohibits NMT to overfit to TC vocabulary.'],\n",
       "   'cite_spans': [[{'start': 99,\n",
       "      'end': 106,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 219,\n",
       "      'end': 226,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 494,\n",
       "      'end': 501,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'}],\n",
       "    [{'start': 71,\n",
       "      'end': 78,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'}],\n",
       "    [{'start': 235,\n",
       "      'end': 242,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 494,\n",
       "      'end': 501,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 740,\n",
       "      'end': 747,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 793,\n",
       "      'end': 800,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    [{'start': 122,\n",
       "      'end': 129,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'}]],\n",
       "   'cite_span_lens': [3, 1, 4, 1],\n",
       "   'section': ['Introduction',\n",
       "    'Introduction',\n",
       "    'Introduction',\n",
       "    'Introduction']}},\n",
       " 'bib_entries': None}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['189927790']['latex_parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[37]['latex_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'start': 99, 'end': 106, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'},\n",
       "  {'start': 219, 'end': 226, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'},\n",
       "  {'start': 494,\n",
       "   'end': 501,\n",
       "   'text': None,\n",
       "   'latex': None,\n",
       "   'ref_id': 'BIBREF2'}],\n",
       " [{'start': 71, 'end': 78, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}],\n",
       " [{'start': 235, 'end': 242, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'},\n",
       "  {'start': 494, 'end': 501, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'},\n",
       "  {'start': 740, 'end': 747, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'},\n",
       "  {'start': 793,\n",
       "   'end': 800,\n",
       "   'text': None,\n",
       "   'latex': None,\n",
       "   'ref_id': 'BIBREF6'}],\n",
       " [{'start': 122,\n",
       "   'end': 129,\n",
       "   'text': None,\n",
       "   'latex': None,\n",
       "   'ref_id': 'BIBREF7'}]]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers['189927790']['latex_parse']['overview_text']['Introduction']['cite_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "  'title': 'Multi-document summarization by sentence extraction',\n",
       "  'authors': [{'first': 'Jade',\n",
       "    'middle': [],\n",
       "    'last': 'Goldstein',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "   {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "   {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'NAACL-ANLPWorkshop',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '40--48',\n",
       "  'other_ids': {},\n",
       "  'links': '8294822'},\n",
       " 'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "  'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "  'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '4',\n",
       "  'issn': '',\n",
       "  'pages': '365--371',\n",
       "  'other_ids': {},\n",
       "  'links': '10418456'},\n",
       " 'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "  'title': 'Auto-encoding variational bayes',\n",
       "  'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "   {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '15789289'},\n",
       " 'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "  'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "  'authors': [{'first': 'Danilo',\n",
       "    'middle': [],\n",
       "    'last': 'Jimenez Rezende',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "   {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICML',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1278--1286',\n",
       "  'other_ids': {},\n",
       "  'links': '16895865'},\n",
       " 'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "  'title': 'Effective approaches to attention-based neural machine translation',\n",
       "  'authors': [{'first': 'Minh-Thang',\n",
       "    'middle': [],\n",
       "    'last': 'Luong',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "   {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1412--1421',\n",
       "  'other_ids': {},\n",
       "  'links': '1998416'},\n",
       " 'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "  'title': 'Multiple aspect summarization using integer linear programming',\n",
       "  'authors': [{'first': 'Kristian',\n",
       "    'middle': [],\n",
       "    'last': 'Woodsend',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'EMNLP-CNLL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '233--243',\n",
       "  'other_ids': {},\n",
       "  'links': '17497992'},\n",
       " 'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "  'title': 'Linear programming 1: introduction',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "   {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "  'year': 2006,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '53739754'},\n",
       " 'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "  'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "  'authors': [{'first': 'Mark', 'middle': [], 'last': 'Wasson', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1364--1368',\n",
       "  'other_ids': {},\n",
       "  'links': '12681629'},\n",
       " 'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "  'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "  'authors': [{'first': 'Hongyan',\n",
       "    'middle': [],\n",
       "    'last': 'Dragomir R Radev',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '21--30',\n",
       "  'other_ids': {},\n",
       "  'links': '1320'},\n",
       " 'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "  'title': 'Textrank: Bringing order into texts',\n",
       "  'authors': [{'first': 'Rada',\n",
       "    'middle': [],\n",
       "    'last': 'Mihalcea',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '577937'},\n",
       " 'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "  'title': 'Adam: A method for stochastic optimization',\n",
       "  'authors': [{'first': 'Diederik',\n",
       "    'middle': [],\n",
       "    'last': 'Kingma',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '6628106'},\n",
       " 'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "  'title': 'A survey of text summarization techniques',\n",
       "  'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "   {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Mining Text Data',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '43--76',\n",
       "  'other_ids': {},\n",
       "  'links': '556431'},\n",
       " 'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "  'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "  'authors': [{'first': 'Yen',\n",
       "    'middle': ['Kan'],\n",
       "    'last': 'Ziheng Lin Min',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'COLING',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '2093--2108',\n",
       "  'other_ids': {},\n",
       "  'links': '6317274'},\n",
       " 'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "  'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "  'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "   {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1587--1597',\n",
       "  'other_ids': {},\n",
       "  'links': '8377315'},\n",
       " 'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "  'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'AAAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '3497--3503',\n",
       "  'other_ids': {},\n",
       "  'links': '29562039'},\n",
       " 'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "  'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "  'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "   {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "   {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '291--298',\n",
       "  'other_ids': {},\n",
       "  'links': '13723748'},\n",
       " 'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "  'title': 'Social context summarization',\n",
       "  'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "   {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "   {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "   {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "   {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '255--264',\n",
       "  'other_ids': {},\n",
       "  'links': '704517'},\n",
       " 'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "  'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1270--1276',\n",
       "  'other_ids': {},\n",
       "  'links': '14777460'}}"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in sorted(overview_papers['10164018']['latex_parse']['bib_entries'].items(), key=lambda item: item[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-329-75cd04051edb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mcite_ref\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcite_span\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ref_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mcited_article_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mcite_ref\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latex_parse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bib_entries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m                 \u001b[0mcited_article_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'latex_parse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bib_entries'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcite_ref\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'links'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "article = overview_papers['189927790']\n",
    "\n",
    "bib_entries_latex = dict()\n",
    "for cite_spans in article['latex_parse']['overview_text']['Introduction']['cite_spans']:\n",
    "    if len(cite_spans)>1:\n",
    "        for cite_span in cite_spans:\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_article_id = None\n",
    "            if cite_ref in article['latex_parse']['bib_entries']:\n",
    "                cited_article_id = article['latex_parse']['bib_entries'][cite_ref]['links']\n",
    "                \n",
    "            if cited_article_id:\n",
    "                print(cite_ref,cited_article_id,article['latex_parse']['bib_entries'][cite_ref]['title'])\n",
    "                bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "            elif cite_ref in article['latex_parse']['bib_entries']:\n",
    "                print(cite_ref,cited_article_id)\n",
    "                \n",
    "                bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "            else:\n",
    "                print('kek',cite_ref,cited_article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bib_entries = dict()\n",
    "if bib_entries_latex:\n",
    "    for k,v in bib_entries_latex.items():\n",
    "        all_bib_entries[k] = [v]\n",
    "if bib_entries_grobid:\n",
    "    for k,v in bib_entries_grobid.items():\n",
    "        if k in all_bib_entries:\n",
    "            if all_bib_entries[k][0] == v:\n",
    "                continue\n",
    "            else:\n",
    "                all_bib_entries[k].append(v)\n",
    "        else:\n",
    "            all_bib_entries[k] = [v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Multi-document summarization by sentence extraction': ['8294822'],\n",
       " 'Lexpagerank: Prestige in multi-document text summarization': ['10418456'],\n",
       " 'A survey of text summarization techniques': ['556431'],\n",
       " 'Exploiting category-specific information for multi-document summarization': ['6317274'],\n",
       " 'Abstractive multi-document summarization via phrase selection and merging': ['8377315'],\n",
       " 'Salience estimation via variational auto-encoders for multi-document summarization': ['29562039'],\n",
       " \"Comments-oriented document summarization: Understanding documents with readers' feedback\": ['13723748'],\n",
       " 'Social context summarization': ['704517'],\n",
       " 'Reader-aware multi-document summarization via sparse coding': ['14777460'],\n",
       " 'Auto-encoding variational bayes': ['15789289'],\n",
       " 'Stochastic backpropagation and approximate inference in deep generative models': ['16895865'],\n",
       " 'Manifold-ranking based topic-focused multidocument summarization': ['532313'],\n",
       " 'Exploiting category-specific information for multidocument summarization': ['6317274'],\n",
       " 'Abstractive multidocument summarization via phrase selection and merging': ['8377315'],\n",
       " 'Autoencoding variational bayes': [None]}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bib_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_bibs = {}\n",
    "for num_artic,article in enumerate(overview_papers):\n",
    "#     if num_artic >10:\n",
    "#         break\n",
    "#     print(num_artic,article)\n",
    "    article = overview_papers[article]\n",
    "    # проверка наличия определленных значений статьи\n",
    "    if not ((article['grobid_parse'] and article['grobid_parse']['bib_entries']) or (article['latex_parse'] and article['latex_parse']['bib_entries'])):\n",
    "#         article_bibs[article['paper_id']] = 'None'\n",
    "        continue\n",
    "        \n",
    "    bib_entries_grobid = None\n",
    "    if article['grobid_parse'] and article['grobid_parse']['bib_entries'] and article['grobid_parse']['overview_text']:\n",
    "        bib_entries_grobid = dict()\n",
    "        \n",
    "        key_grobid = list(article['grobid_parse']['overview_text'].keys())[0]\n",
    "        for cite_span in article['grobid_parse']['overview_text'][key_grobid]['cite_spans']:\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_paper_id = None\n",
    "            if cite_ref in article['grobid_parse']['bib_entries']:\n",
    "                cited_paper_id = article['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "            if cited_paper_id:\n",
    "                bib_entries_grobid[article['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "            elif cite_ref  in article['grobid_parse']['bib_entries']:\n",
    "                bib_entries_grobid[article['grobid_parse']['bib_entries'][cite_ref]['title']] = cited_paper_id\n",
    "        \n",
    "            \n",
    "        bib_entries_grobid = {k: v for k, v in sorted(bib_entries_grobid.items(), key=lambda item: item[0])}\n",
    "        \n",
    "    bib_entries_latex = None\n",
    "    if article['latex_parse'] and article['latex_parse']['overview_text'] and article['latex_parse']['bib_entries']:\n",
    "        bib_entries_latex = dict()\n",
    "        \n",
    "        key_tex = list(article['latex_parse']['overview_text'].keys())[0]\n",
    "        for cite_spans in article['latex_parse']['overview_text'][key_tex]['cite_spans']:\n",
    "            if len(cite_spans)>1:\n",
    "                for cite_span in cite_spans:\n",
    "                    cite_ref = cite_span['ref_id']\n",
    "                    cited_article_id = None\n",
    "                    if cite_ref in article['latex_parse']['bib_entries']:\n",
    "                        cited_article_id = article['latex_parse']['bib_entries'][cite_ref]['links']\n",
    "                    if cited_article_id:\n",
    "                        bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "                    elif cite_ref in article['latex_parse']['bib_entries']:\n",
    "                        bib_entries_latex[article['latex_parse']['bib_entries'][cite_ref]['title']] = cited_article_id\n",
    "    \n",
    "        bib_entries_latex = {k: v for k, v in sorted(bib_entries_latex.items(), key=lambda item: item[0])}\n",
    "    \n",
    "    all_bib_entries = dict()\n",
    "    if bib_entries_latex:\n",
    "        for k,v in bib_entries_latex.items():\n",
    "            if k == '':\n",
    "                continue\n",
    "                \n",
    "            if v:\n",
    "                all_bib_entries[k] = [v]\n",
    "            else:\n",
    "                all_bib_entries[k] = []\n",
    "\n",
    "    if bib_entries_grobid:\n",
    "        for k,v in bib_entries_grobid.items():\n",
    "            if k == '':\n",
    "                continue\n",
    "                \n",
    "            if k in all_bib_entries:\n",
    "                if len(all_bib_entries[k]) == 0:\n",
    "                    if v:\n",
    "                        all_bib_entries[k] = [v]\n",
    "                    else:\n",
    "                        all_bib_entries[k] = []\n",
    "                else:\n",
    "                    if all_bib_entries[k][0] == v:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if v:\n",
    "                            all_bib_entries[k].append(v)\n",
    "                        else:\n",
    "                            continue\n",
    "            else:\n",
    "                if v:\n",
    "                    all_bib_entries[k] = [v]\n",
    "                else:\n",
    "                    all_bib_entries[k] = []\n",
    "    article_bibs[article['paper_id']] = all_bib_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39428"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_bibs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = []\n",
    "num_all_bibs = []\n",
    "num_all_bibs_with_links = []\n",
    "num_all_bibs_w_l_in_dict_in = []\n",
    "for num_artic,(key,val_dict) in enumerate(article_bibs.items()):\n",
    "    index.append(key)\n",
    "    num_bibs = 0\n",
    "    num_bibs_with_links = 0\n",
    "    num_bibs_w_l_in_dict_in = 0\n",
    "    try:\n",
    "        for k,val in val_dict.items():\n",
    "            num_bibs +=1\n",
    "            if len(val)>0:\n",
    "                num_bibs_with_links+=1\n",
    "            for link in val:\n",
    "                if link in dict_in_ids_all:\n",
    "                    num_bibs_w_l_in_dict_in+=1\n",
    "                    break\n",
    "    except:\n",
    "        print(num_artic)\n",
    "        print(key)\n",
    "        break\n",
    "    num_all_bibs.append(num_bibs)\n",
    "    num_all_bibs_with_links.append(num_bibs_with_links)\n",
    "    num_all_bibs_w_l_in_dict_in.append(num_bibs_w_l_in_dict_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_overview = {'#bibs':num_all_bibs,'#bibs_with_links':num_all_bibs_with_links,'#bibs_with_links_in_dict':num_all_bibs_w_l_in_dict_in}\n",
    "df_covering_overview =  pd.DataFrame(d)\n",
    "df_covering_overview.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covering_overview['%_in_dict2bibs'] = df_covering_overview['#bibs_with_links_in_dict']/df_covering_overview['#bibs']\n",
    "df_covering_overview['%_in_dict2bibs_w_l'] = df_covering_overview['#bibs_with_links_in_dict']/df_covering_overview['#bibs_with_links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пояснение к колонкам:\n",
    " - **#bibs** -  количество всех распознанных Reference статей\n",
    " - **#bibs_with_links** - количество всех распознанных Reference статей, которые присутствуют в датасете GORC\n",
    " - **#bibs_with_links_in_dict** - количество всех распознанных Reference статей, которые присутствуют в структуре данных ***in***,где есть статья и ссылающиеся на неё статьи\n",
    " - **%_in_dict2bibs** - отношение кол-ва ref **in** ко всему кол-ву ref статьи \n",
    " - **%_in_dict2bibs_w_l** - отношение кол-ва ref **in** ко всему кол-ву ref статьей, которые присутствуют в датасете GORC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#bibs</th>\n",
       "      <th>#bibs_with_links</th>\n",
       "      <th>#bibs_with_links_in_dict</th>\n",
       "      <th>%_in_dict2bibs</th>\n",
       "      <th>%_in_dict2bibs_w_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10164018</th>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14472576</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17302615</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3243536</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248240</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          #bibs  #bibs_with_links  #bibs_with_links_in_dict  %_in_dict2bibs  \\\n",
       "10164018     15                14                         9        0.600000   \n",
       "14472576     18                18                        16        0.888889   \n",
       "17302615      3                 1                         0        0.000000   \n",
       "3243536       2                 1                         1        0.500000   \n",
       "3248240       4                 3                         3        0.750000   \n",
       "\n",
       "          %_in_dict2bibs_w_l  \n",
       "10164018            0.642857  \n",
       "14472576            0.888889  \n",
       "17302615            0.000000  \n",
       "3243536             1.000000  \n",
       "3248240             1.000000  "
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42435555470515196"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview['%_in_dict2bibs'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5696041037576695"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_covering_overview['%_in_dict2bibs_w_l'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мб добавить учет None и прогнать на реальном датасете\n",
    "\n",
    "\n",
    "либо подскачать данные"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
