{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def get_citation_contexts(paper: Dict, toks_in_context=10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve citation contexts from GORC paper\n",
    "    :param paper:\n",
    "    :param toks_in_context:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not paper:\n",
    "        return []\n",
    "\n",
    "    if not paper['grobid_parse']:\n",
    "        return []\n",
    "\n",
    "    if not paper['grobid_parse']['body_text']:\n",
    "        return []\n",
    "\n",
    "    contexts = []\n",
    "\n",
    "    for paragraph in paper['grobid_parse']['body_text']:\n",
    "        for cite_span in paragraph['cite_spans']:\n",
    "            # get cited paper id, skip if none\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_paper_id = None\n",
    "            if cite_ref in paper['grobid_parse']['bib_entries']:\n",
    "                cited_paper_id = paper['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "            if not cited_paper_id:\n",
    "                continue\n",
    "\n",
    "            # get pre and post tokens\n",
    "            pre_span_tokens = paragraph['text'][:cite_span['start']].split(' ')[-toks_in_context:]\n",
    "            post_span_tokens = paragraph['text'][cite_span['end']:].split(' ')[:toks_in_context]\n",
    "            pre_string = ' '.join(pre_span_tokens)\n",
    "            post_string = ' '.join(post_span_tokens)\n",
    "            full_context = pre_string + cite_span['text'] + post_string\n",
    "\n",
    "            contexts.append({\n",
    "                \"paper_id\": paper['paper_id'],\n",
    "                \"context_string\": full_context,\n",
    "                \"cite_start\": len(pre_string),\n",
    "                \"cite_end\": len(pre_string) + len(cite_span['text']),\n",
    "                \"cite_str\": cite_span['text'],\n",
    "                \"cited_paper_id\": cited_paper_id\n",
    "            })\n",
    "\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_DATA_FILE = 'data/example_papers.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_contexts = []\n",
    "all_papers = []\n",
    "context_dict = dict()\n",
    "with open(EXAMPLE_DATA_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        gorc_obj = json.loads(line)\n",
    "        all_papers.append(gorc_obj)\n",
    "        all_contexts += get_citation_contexts(gorc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "f_zips = []\n",
    "for (dirpath, dirnames, filenames) in walk('../../gorc/'):\n",
    "    f_zips.extend(filenames)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002,\n",
       " ['0.jsonl.gz',\n",
       "  '1.jsonl.gz',\n",
       "  '10.jsonl.gz',\n",
       "  '100.jsonl.gz',\n",
       "  '1000.jsonl.gz',\n",
       "  '1001.jsonl.gz',\n",
       "  '1002.jsonl.gz',\n",
       "  '1003.jsonl.gz',\n",
       "  '1004.jsonl.gz',\n",
       "  '1005.jsonl.gz',\n",
       "  '1006.jsonl.gz',\n",
       "  '1007.jsonl.gz',\n",
       "  '1008.jsonl.gz',\n",
       "  '1009.jsonl.gz',\n",
       "  '101.jsonl.gz',\n",
       "  '1010.jsonl.gz',\n",
       "  '1011.jsonl.gz',\n",
       "  '1012.jsonl.gz',\n",
       "  '1013.jsonl.gz',\n",
       "  '1014.jsonl.gz',\n",
       "  '1015.jsonl.gz',\n",
       "  '1016.jsonl.gz',\n",
       "  '1017.jsonl.gz',\n",
       "  '1018.jsonl.gz',\n",
       "  '1019.jsonl.gz',\n",
       "  '102.jsonl.gz',\n",
       "  '1020.jsonl.gz',\n",
       "  '1021.jsonl.gz',\n",
       "  '1022.jsonl.gz',\n",
       "  '1023.jsonl.gz',\n",
       "  '1024.jsonl.gz',\n",
       "  '1025.jsonl.gz',\n",
       "  '1026.jsonl.gz',\n",
       "  '1027.jsonl.gz',\n",
       "  '1028.jsonl.gz',\n",
       "  '1029.jsonl.gz',\n",
       "  '103.jsonl.gz',\n",
       "  '1030.jsonl.gz',\n",
       "  '1031.jsonl.gz',\n",
       "  '1032.jsonl.gz',\n",
       "  '1033.jsonl.gz',\n",
       "  '1034.jsonl.gz',\n",
       "  '1035.jsonl.gz',\n",
       "  '1036.jsonl.gz',\n",
       "  '1037.jsonl.gz',\n",
       "  '1038.jsonl.gz',\n",
       "  '1039.jsonl.gz',\n",
       "  '104.jsonl.gz',\n",
       "  '1040.jsonl.gz',\n",
       "  '1041.jsonl.gz',\n",
       "  '1042.jsonl.gz',\n",
       "  '1043.jsonl.gz',\n",
       "  '1044.jsonl.gz',\n",
       "  '1045.jsonl.gz',\n",
       "  '1046.jsonl.gz',\n",
       "  '1047.jsonl.gz',\n",
       "  '1048.jsonl.gz',\n",
       "  '1049.jsonl.gz',\n",
       "  '105.jsonl.gz',\n",
       "  '1050.jsonl.gz',\n",
       "  '1051.jsonl.gz',\n",
       "  '1052.jsonl.gz',\n",
       "  '1053.jsonl.gz',\n",
       "  '1054.jsonl.gz',\n",
       "  '1055.jsonl.gz',\n",
       "  '1056.jsonl.gz',\n",
       "  '1057.jsonl.gz',\n",
       "  '1058.jsonl.gz',\n",
       "  '1059.jsonl.gz',\n",
       "  '106.jsonl.gz',\n",
       "  '1060.jsonl.gz',\n",
       "  '1061.jsonl.gz',\n",
       "  '1062.jsonl.gz',\n",
       "  '1063.jsonl.gz',\n",
       "  '1064.jsonl.gz',\n",
       "  '1065.jsonl.gz',\n",
       "  '1066.jsonl.gz',\n",
       "  '1067.jsonl.gz',\n",
       "  '1068.jsonl.gz',\n",
       "  '1069.jsonl.gz',\n",
       "  '107.jsonl.gz',\n",
       "  '1070.jsonl.gz',\n",
       "  '1071.jsonl.gz',\n",
       "  '1072.jsonl.gz',\n",
       "  '1073.jsonl.gz',\n",
       "  '1074.jsonl.gz',\n",
       "  '1075.jsonl.gz',\n",
       "  '1076.jsonl.gz',\n",
       "  '1077.jsonl.gz',\n",
       "  '1078.jsonl.gz',\n",
       "  '1079.jsonl.gz',\n",
       "  '108.jsonl.gz',\n",
       "  '1080.jsonl.gz',\n",
       "  '1081.jsonl.gz',\n",
       "  '1082.jsonl.gz',\n",
       "  '1083.jsonl.gz',\n",
       "  '1084.jsonl.gz',\n",
       "  '1085.jsonl.gz',\n",
       "  '1086.jsonl.gz',\n",
       "  '1087.jsonl.gz',\n",
       "  '1088.jsonl.gz',\n",
       "  '1089.jsonl.gz',\n",
       "  '109.jsonl.gz',\n",
       "  '1090.jsonl.gz',\n",
       "  '1091.jsonl.gz',\n",
       "  '1092.jsonl.gz',\n",
       "  '1093.jsonl.gz',\n",
       "  '1094.jsonl.gz',\n",
       "  '1095.jsonl.gz',\n",
       "  '1096.jsonl.gz',\n",
       "  '1097.jsonl.gz',\n",
       "  '1098.jsonl.gz',\n",
       "  '1099.jsonl.gz',\n",
       "  '11.jsonl.gz',\n",
       "  '110.jsonl.gz',\n",
       "  '1100.jsonl.gz',\n",
       "  '1101.jsonl.gz',\n",
       "  '1102.jsonl.gz',\n",
       "  '1103.jsonl.gz',\n",
       "  '1104.jsonl.gz',\n",
       "  '1105.jsonl.gz',\n",
       "  '1106.jsonl.gz',\n",
       "  '1107.jsonl.gz',\n",
       "  '1108.jsonl.gz',\n",
       "  '1109.jsonl.gz',\n",
       "  '111.jsonl.gz',\n",
       "  '1110.jsonl.gz',\n",
       "  '1111.jsonl.gz',\n",
       "  '1112.jsonl.gz',\n",
       "  '1113.jsonl.gz',\n",
       "  '1114.jsonl.gz',\n",
       "  '1115.jsonl.gz',\n",
       "  '1116.jsonl.gz',\n",
       "  '1117.jsonl.gz',\n",
       "  '1118.jsonl.gz',\n",
       "  '1119.jsonl.gz',\n",
       "  '112.jsonl.gz',\n",
       "  '1120.jsonl.gz',\n",
       "  '1121.jsonl.gz',\n",
       "  '1122.jsonl.gz',\n",
       "  '1123.jsonl.gz',\n",
       "  '1124.jsonl.gz',\n",
       "  '1125.jsonl.gz',\n",
       "  '1126.jsonl.gz',\n",
       "  '1127.jsonl.gz',\n",
       "  '1128.jsonl.gz',\n",
       "  '1129.jsonl.gz',\n",
       "  '113.jsonl.gz',\n",
       "  '1130.jsonl.gz',\n",
       "  '1131.jsonl.gz',\n",
       "  '1132.jsonl.gz',\n",
       "  '1133.jsonl.gz',\n",
       "  '1134.jsonl.gz',\n",
       "  '1135.jsonl.gz',\n",
       "  '1136.jsonl.gz',\n",
       "  '1137.jsonl.gz',\n",
       "  '1138.jsonl.gz',\n",
       "  '1139.jsonl.gz',\n",
       "  '114.jsonl.gz',\n",
       "  '1140.jsonl.gz',\n",
       "  '1141.jsonl.gz',\n",
       "  '1142.jsonl.gz',\n",
       "  '1143.jsonl.gz',\n",
       "  '1144.jsonl.gz',\n",
       "  '1145.jsonl.gz',\n",
       "  '1146.jsonl.gz',\n",
       "  '1147.jsonl.gz',\n",
       "  '1148.jsonl.gz',\n",
       "  '1149.jsonl.gz',\n",
       "  '115.jsonl.gz',\n",
       "  '1150.jsonl.gz',\n",
       "  '1151.jsonl.gz',\n",
       "  '1152.jsonl.gz',\n",
       "  '1153.jsonl.gz',\n",
       "  '1154.jsonl.gz',\n",
       "  '1155.jsonl.gz',\n",
       "  '1156.jsonl.gz',\n",
       "  '1157.jsonl.gz',\n",
       "  '1158.jsonl.gz',\n",
       "  '1159.jsonl.gz',\n",
       "  '116.jsonl.gz',\n",
       "  '1160.jsonl.gz',\n",
       "  '1161.jsonl.gz',\n",
       "  '1162.jsonl.gz',\n",
       "  '1163.jsonl.gz',\n",
       "  '1164.jsonl.gz',\n",
       "  '1165.jsonl.gz',\n",
       "  '1166.jsonl.gz',\n",
       "  '1167.jsonl.gz',\n",
       "  '1168.jsonl.gz',\n",
       "  '1169.jsonl.gz',\n",
       "  '117.jsonl.gz',\n",
       "  '1170.jsonl.gz',\n",
       "  '1171.jsonl.gz',\n",
       "  '1172.jsonl.gz',\n",
       "  '1173.jsonl.gz',\n",
       "  '1174.jsonl.gz',\n",
       "  '1175.jsonl.gz',\n",
       "  '1176.jsonl.gz',\n",
       "  '1177.jsonl.gz',\n",
       "  '1178.jsonl.gz',\n",
       "  '1179.jsonl.gz',\n",
       "  '118.jsonl.gz',\n",
       "  '1180.jsonl.gz',\n",
       "  '1181.jsonl.gz',\n",
       "  '1182.jsonl.gz',\n",
       "  '1183.jsonl.gz',\n",
       "  '1184.jsonl.gz',\n",
       "  '1185.jsonl.gz',\n",
       "  '1186.jsonl.gz',\n",
       "  '1187.jsonl.gz',\n",
       "  '1188.jsonl.gz',\n",
       "  '1189.jsonl.gz',\n",
       "  '119.jsonl.gz',\n",
       "  '1190.jsonl.gz',\n",
       "  '1191.jsonl.gz',\n",
       "  '1192.jsonl.gz',\n",
       "  '1193.jsonl.gz',\n",
       "  '1194.jsonl.gz',\n",
       "  '1195.jsonl.gz',\n",
       "  '1196.jsonl.gz',\n",
       "  '1197.jsonl.gz',\n",
       "  '1198.jsonl.gz',\n",
       "  '1199.jsonl.gz',\n",
       "  '12.jsonl.gz',\n",
       "  '120.jsonl.gz',\n",
       "  '1200.jsonl.gz',\n",
       "  '1201.jsonl.gz',\n",
       "  '1202.jsonl.gz',\n",
       "  '1203.jsonl.gz',\n",
       "  '1204.jsonl.gz',\n",
       "  '1205.jsonl.gz',\n",
       "  '1206.jsonl.gz',\n",
       "  '1207.jsonl.gz',\n",
       "  '1208.jsonl.gz',\n",
       "  '1209.jsonl.gz',\n",
       "  '121.jsonl.gz',\n",
       "  '1210.jsonl.gz',\n",
       "  '1211.jsonl.gz',\n",
       "  '1212.jsonl.gz',\n",
       "  '1213.jsonl.gz',\n",
       "  '1214.jsonl.gz',\n",
       "  '1215.jsonl.gz',\n",
       "  '1216.jsonl.gz',\n",
       "  '1217.jsonl.gz',\n",
       "  '1218.jsonl.gz',\n",
       "  '1219.jsonl.gz',\n",
       "  '122.jsonl.gz',\n",
       "  '1220.jsonl.gz',\n",
       "  '1221.jsonl.gz',\n",
       "  '1222.jsonl.gz',\n",
       "  '1223.jsonl.gz',\n",
       "  '1224.jsonl.gz',\n",
       "  '1225.jsonl.gz',\n",
       "  '1226.jsonl.gz',\n",
       "  '1227.jsonl.gz',\n",
       "  '1228.jsonl.gz',\n",
       "  '1229.jsonl.gz',\n",
       "  '123.jsonl.gz',\n",
       "  '1230.jsonl.gz',\n",
       "  '1231.jsonl.gz',\n",
       "  '1232.jsonl.gz',\n",
       "  '1233.jsonl.gz',\n",
       "  '1234.jsonl.gz',\n",
       "  '1235.jsonl.gz',\n",
       "  '1236.jsonl.gz',\n",
       "  '1237.jsonl.gz',\n",
       "  '1238.jsonl.gz',\n",
       "  '1239.jsonl.gz',\n",
       "  '124.jsonl.gz',\n",
       "  '1240.jsonl.gz',\n",
       "  '1241.jsonl.gz',\n",
       "  '1242.jsonl.gz',\n",
       "  '1243.jsonl.gz',\n",
       "  '1244.jsonl.gz',\n",
       "  '1245.jsonl.gz',\n",
       "  '1246.jsonl.gz',\n",
       "  '1247.jsonl.gz',\n",
       "  '1248.jsonl.gz',\n",
       "  '1249.jsonl.gz',\n",
       "  '125.jsonl.gz',\n",
       "  '1250.jsonl.gz',\n",
       "  '1251.jsonl.gz',\n",
       "  '1252.jsonl.gz',\n",
       "  '1253.jsonl.gz',\n",
       "  '1254.jsonl.gz',\n",
       "  '1255.jsonl.gz',\n",
       "  '1256.jsonl.gz',\n",
       "  '1257.jsonl.gz',\n",
       "  '1258.jsonl.gz',\n",
       "  '1259.jsonl.gz',\n",
       "  '126.jsonl.gz',\n",
       "  '1260.jsonl.gz',\n",
       "  '1261.jsonl.gz',\n",
       "  '1262.jsonl.gz',\n",
       "  '1263.jsonl.gz',\n",
       "  '1264.jsonl.gz',\n",
       "  '1265.jsonl.gz',\n",
       "  '1266.jsonl.gz',\n",
       "  '1267.jsonl.gz',\n",
       "  '1268.jsonl.gz',\n",
       "  '1269.jsonl.gz',\n",
       "  '127.jsonl.gz',\n",
       "  '1270.jsonl.gz',\n",
       "  '1271.jsonl.gz',\n",
       "  '1272.jsonl.gz',\n",
       "  '1273.jsonl.gz',\n",
       "  '1274.jsonl.gz',\n",
       "  '1275.jsonl.gz',\n",
       "  '1276.jsonl.gz',\n",
       "  '1277.jsonl.gz',\n",
       "  '1278.jsonl.gz',\n",
       "  '1279.jsonl.gz',\n",
       "  '128.jsonl.gz',\n",
       "  '1280.jsonl.gz',\n",
       "  '1281.jsonl.gz',\n",
       "  '1282.jsonl.gz',\n",
       "  '1283.jsonl.gz',\n",
       "  '1284.jsonl.gz',\n",
       "  '1285.jsonl.gz',\n",
       "  '1286.jsonl.gz',\n",
       "  '1287.jsonl.gz',\n",
       "  '1288.jsonl.gz',\n",
       "  '1289.jsonl.gz',\n",
       "  '129.jsonl.gz',\n",
       "  '1290.jsonl.gz',\n",
       "  '1291.jsonl.gz',\n",
       "  '1292.jsonl.gz',\n",
       "  '1293.jsonl.gz',\n",
       "  '1294.jsonl.gz',\n",
       "  '1295.jsonl.gz',\n",
       "  '1296.jsonl.gz',\n",
       "  '1297.jsonl.gz',\n",
       "  '1298.jsonl.gz',\n",
       "  '1299.jsonl.gz',\n",
       "  '13.jsonl.gz',\n",
       "  '130.jsonl.gz',\n",
       "  '1300.jsonl.gz',\n",
       "  '1301.jsonl.gz',\n",
       "  '1302.jsonl.gz',\n",
       "  '1303.jsonl.gz',\n",
       "  '1304.jsonl.gz',\n",
       "  '1305.jsonl.gz',\n",
       "  '1306.jsonl.gz',\n",
       "  '1307.jsonl.gz',\n",
       "  '1308.jsonl.gz',\n",
       "  '1309.jsonl.gz',\n",
       "  '131.jsonl.gz',\n",
       "  '1310.jsonl.gz',\n",
       "  '1311.jsonl.gz',\n",
       "  '1312.jsonl.gz',\n",
       "  '1313.jsonl.gz',\n",
       "  '1314.jsonl.gz',\n",
       "  '1315.jsonl.gz',\n",
       "  '1316.jsonl.gz',\n",
       "  '1317.jsonl.gz',\n",
       "  '1318.jsonl.gz',\n",
       "  '1319.jsonl.gz',\n",
       "  '132.jsonl.gz',\n",
       "  '1320.jsonl.gz',\n",
       "  '1321.jsonl.gz',\n",
       "  '1322.jsonl.gz',\n",
       "  '1323.jsonl.gz',\n",
       "  '1324.jsonl.gz',\n",
       "  '1325.jsonl.gz',\n",
       "  '1326.jsonl.gz',\n",
       "  '1327.jsonl.gz',\n",
       "  '1328.jsonl.gz',\n",
       "  '1329.jsonl.gz',\n",
       "  '133.jsonl.gz',\n",
       "  '1330.jsonl.gz',\n",
       "  '1331.jsonl.gz',\n",
       "  '1332.jsonl.gz',\n",
       "  '1333.jsonl.gz',\n",
       "  '1334.jsonl.gz',\n",
       "  '1335.jsonl.gz',\n",
       "  '1336.jsonl.gz',\n",
       "  '1337.jsonl.gz',\n",
       "  '1338.jsonl.gz',\n",
       "  '1339.jsonl.gz',\n",
       "  '134.jsonl.gz',\n",
       "  '1340.jsonl.gz',\n",
       "  '1341.jsonl.gz',\n",
       "  '1342.jsonl.gz',\n",
       "  '1343.jsonl.gz',\n",
       "  '1344.jsonl.gz',\n",
       "  '1345.jsonl.gz',\n",
       "  '1346.jsonl.gz',\n",
       "  '1347.jsonl.gz',\n",
       "  '1348.jsonl.gz',\n",
       "  '1349.jsonl.gz',\n",
       "  '135.jsonl.gz',\n",
       "  '1350.jsonl.gz',\n",
       "  '1351.jsonl.gz',\n",
       "  '1352.jsonl.gz',\n",
       "  '1353.jsonl.gz',\n",
       "  '1354.jsonl.gz',\n",
       "  '1355.jsonl.gz',\n",
       "  '1356.jsonl.gz',\n",
       "  '1357.jsonl.gz',\n",
       "  '1358.jsonl.gz',\n",
       "  '1359.jsonl.gz',\n",
       "  '136.jsonl.gz',\n",
       "  '1360.jsonl.gz',\n",
       "  '1361.jsonl.gz',\n",
       "  '1362.jsonl.gz',\n",
       "  '1363.jsonl.gz',\n",
       "  '1364.jsonl.gz',\n",
       "  '1365.jsonl.gz',\n",
       "  '1366.jsonl.gz',\n",
       "  '1367.jsonl.gz',\n",
       "  '1368.jsonl.gz',\n",
       "  '1369.jsonl.gz',\n",
       "  '137.jsonl.gz',\n",
       "  '1370.jsonl.gz',\n",
       "  '1371.jsonl.gz',\n",
       "  '1372.jsonl.gz',\n",
       "  '1373.jsonl.gz',\n",
       "  '1374.jsonl.gz',\n",
       "  '1375.jsonl.gz',\n",
       "  '1376.jsonl.gz',\n",
       "  '1377.jsonl.gz',\n",
       "  '1378.jsonl.gz',\n",
       "  '1379.jsonl.gz',\n",
       "  '138.jsonl.gz',\n",
       "  '1380.jsonl.gz',\n",
       "  '1381.jsonl.gz',\n",
       "  '1382.jsonl.gz',\n",
       "  '1383.jsonl.gz',\n",
       "  '1384.jsonl.gz',\n",
       "  '1385.jsonl.gz',\n",
       "  '1386.jsonl.gz',\n",
       "  '1387.jsonl.gz',\n",
       "  '1388.jsonl.gz',\n",
       "  '1389.jsonl.gz',\n",
       "  '139.jsonl.gz',\n",
       "  '1390.jsonl.gz',\n",
       "  '1391.jsonl.gz',\n",
       "  '1392.jsonl.gz',\n",
       "  '1393.jsonl.gz',\n",
       "  '1394.jsonl.gz',\n",
       "  '1395.jsonl.gz',\n",
       "  '1396.jsonl.gz',\n",
       "  '1397.jsonl.gz',\n",
       "  '1398.jsonl.gz',\n",
       "  '1399.jsonl.gz',\n",
       "  '14.jsonl.gz',\n",
       "  '140.jsonl.gz',\n",
       "  '1400.jsonl.gz',\n",
       "  '1401.jsonl.gz',\n",
       "  '1402.jsonl.gz',\n",
       "  '1403.jsonl.gz',\n",
       "  '1404.jsonl.gz',\n",
       "  '1405.jsonl.gz',\n",
       "  '1406.jsonl.gz',\n",
       "  '1407.jsonl.gz',\n",
       "  '1408.jsonl.gz',\n",
       "  '1409.jsonl.gz',\n",
       "  '141.jsonl.gz',\n",
       "  '1410.jsonl.gz',\n",
       "  '1411.jsonl.gz',\n",
       "  '1412.jsonl.gz',\n",
       "  '1413.jsonl.gz',\n",
       "  '1414.jsonl.gz',\n",
       "  '1415.jsonl.gz',\n",
       "  '1416.jsonl.gz',\n",
       "  '1417.jsonl.gz',\n",
       "  '1418.jsonl.gz',\n",
       "  '1419.jsonl.gz',\n",
       "  '142.jsonl.gz',\n",
       "  '1420.jsonl.gz',\n",
       "  '1421.jsonl.gz',\n",
       "  '1422.jsonl.gz',\n",
       "  '1423.jsonl.gz',\n",
       "  '1424.jsonl.gz',\n",
       "  '1425.jsonl.gz',\n",
       "  '1426.jsonl.gz',\n",
       "  '1427.jsonl.gz',\n",
       "  '1428.jsonl.gz',\n",
       "  '1429.jsonl.gz',\n",
       "  '143.jsonl.gz',\n",
       "  '1430.jsonl.gz',\n",
       "  '1431.jsonl.gz',\n",
       "  '1432.jsonl.gz',\n",
       "  '1433.jsonl.gz',\n",
       "  '1434.jsonl.gz',\n",
       "  '1435.jsonl.gz',\n",
       "  '1436.jsonl.gz',\n",
       "  '1437.jsonl.gz',\n",
       "  '1438.jsonl.gz',\n",
       "  '1439.jsonl.gz',\n",
       "  '144.jsonl.gz',\n",
       "  '1440.jsonl.gz',\n",
       "  '1441.jsonl.gz',\n",
       "  '1442.jsonl.gz',\n",
       "  '1443.jsonl.gz',\n",
       "  '1444.jsonl.gz',\n",
       "  '1445.jsonl.gz',\n",
       "  '1446.jsonl.gz',\n",
       "  '1447.jsonl.gz',\n",
       "  '1448.jsonl.gz',\n",
       "  '1449.jsonl.gz',\n",
       "  '145.jsonl.gz',\n",
       "  '1450.jsonl.gz',\n",
       "  '1451.jsonl.gz',\n",
       "  '1452.jsonl.gz',\n",
       "  '1453.jsonl.gz',\n",
       "  '1454.jsonl.gz',\n",
       "  '1455.jsonl.gz',\n",
       "  '1456.jsonl.gz',\n",
       "  '1457.jsonl.gz',\n",
       "  '1458.jsonl.gz',\n",
       "  '1459.jsonl.gz',\n",
       "  '146.jsonl.gz',\n",
       "  '1460.jsonl.gz',\n",
       "  '1461.jsonl.gz',\n",
       "  '1462.jsonl.gz',\n",
       "  '1463.jsonl.gz',\n",
       "  '1464.jsonl.gz',\n",
       "  '1465.jsonl.gz',\n",
       "  '1466.jsonl.gz',\n",
       "  '1467.jsonl.gz',\n",
       "  '1468.jsonl.gz',\n",
       "  '1469.jsonl.gz',\n",
       "  '147.jsonl.gz',\n",
       "  '1470.jsonl.gz',\n",
       "  '1471.jsonl.gz',\n",
       "  '1472.jsonl.gz',\n",
       "  '1473.jsonl.gz',\n",
       "  '1474.jsonl.gz',\n",
       "  '1475.jsonl.gz',\n",
       "  '1476.jsonl.gz',\n",
       "  '1477.jsonl.gz',\n",
       "  '1478.jsonl.gz',\n",
       "  '1479.jsonl.gz',\n",
       "  '148.jsonl.gz',\n",
       "  '1480.jsonl.gz',\n",
       "  '1481.jsonl.gz',\n",
       "  '1482.jsonl.gz',\n",
       "  '1483.jsonl.gz',\n",
       "  '1484.jsonl.gz',\n",
       "  '1485.jsonl.gz',\n",
       "  '1486.jsonl.gz',\n",
       "  '1487.jsonl.gz',\n",
       "  '1488.jsonl.gz',\n",
       "  '1489.jsonl.gz',\n",
       "  '149.jsonl.gz',\n",
       "  '1490.jsonl.gz',\n",
       "  '1491.jsonl.gz',\n",
       "  '1492.jsonl.gz',\n",
       "  '1493.jsonl.gz',\n",
       "  '1494.jsonl.gz',\n",
       "  '1495.jsonl.gz',\n",
       "  '1496.jsonl.gz',\n",
       "  '1497.jsonl.gz',\n",
       "  '1498.jsonl.gz',\n",
       "  '1499.jsonl.gz',\n",
       "  '15.jsonl.gz',\n",
       "  '150.jsonl.gz',\n",
       "  '1500.jsonl.gz',\n",
       "  '1501.jsonl.gz',\n",
       "  '1502.jsonl.gz',\n",
       "  '1503.jsonl.gz',\n",
       "  '1504.jsonl.gz',\n",
       "  '1505.jsonl.gz',\n",
       "  '1506.jsonl.gz',\n",
       "  '1507.jsonl.gz',\n",
       "  '1508.jsonl.gz',\n",
       "  '1509.jsonl.gz',\n",
       "  '151.jsonl.gz',\n",
       "  '1510.jsonl.gz',\n",
       "  '1511.jsonl.gz',\n",
       "  '1512.jsonl.gz',\n",
       "  '1513.jsonl.gz',\n",
       "  '1514.jsonl.gz',\n",
       "  '1515.jsonl.gz',\n",
       "  '1516.jsonl.gz',\n",
       "  '1517.jsonl.gz',\n",
       "  '1518.jsonl.gz',\n",
       "  '1519.jsonl.gz',\n",
       "  '152.jsonl.gz',\n",
       "  '1520.jsonl.gz',\n",
       "  '1521.jsonl.gz',\n",
       "  '1522.jsonl.gz',\n",
       "  '1523.jsonl.gz',\n",
       "  '1524.jsonl.gz',\n",
       "  '1525.jsonl.gz',\n",
       "  '1526.jsonl.gz',\n",
       "  '1527.jsonl.gz',\n",
       "  '1528.jsonl.gz',\n",
       "  '1529.jsonl.gz',\n",
       "  '153.jsonl.gz',\n",
       "  '1530.jsonl.gz',\n",
       "  '1531.jsonl.gz',\n",
       "  '1532.jsonl.gz',\n",
       "  '1533.jsonl.gz',\n",
       "  '1534.jsonl.gz',\n",
       "  '1535.jsonl.gz',\n",
       "  '1536.jsonl.gz',\n",
       "  '1537.jsonl.gz',\n",
       "  '1538.jsonl.gz',\n",
       "  '1539.jsonl.gz',\n",
       "  '154.jsonl.gz',\n",
       "  '1540.jsonl.gz',\n",
       "  '1541.jsonl.gz',\n",
       "  '1542.jsonl.gz',\n",
       "  '1543.jsonl.gz',\n",
       "  '1544.jsonl.gz',\n",
       "  '1545.jsonl.gz',\n",
       "  '1546.jsonl.gz',\n",
       "  '1547.jsonl.gz',\n",
       "  '1548.jsonl.gz',\n",
       "  '1549.jsonl.gz',\n",
       "  '155.jsonl.gz',\n",
       "  '1550.jsonl.gz',\n",
       "  '1551.jsonl.gz',\n",
       "  '1552.jsonl.gz',\n",
       "  '1553.jsonl.gz',\n",
       "  '1554.jsonl.gz',\n",
       "  '1555.jsonl.gz',\n",
       "  '1556.jsonl.gz',\n",
       "  '1557.jsonl.gz',\n",
       "  '1558.jsonl.gz',\n",
       "  '1559.jsonl.gz',\n",
       "  '156.jsonl.gz',\n",
       "  '1560.jsonl.gz',\n",
       "  '1561.jsonl.gz',\n",
       "  '1562.jsonl.gz',\n",
       "  '1563.jsonl.gz',\n",
       "  '1564.jsonl.gz',\n",
       "  '1565.jsonl.gz',\n",
       "  '1566.jsonl.gz',\n",
       "  '1567.jsonl.gz',\n",
       "  '1568.jsonl.gz',\n",
       "  '1569.jsonl.gz',\n",
       "  '157.jsonl.gz',\n",
       "  '1570.jsonl.gz',\n",
       "  '1571.jsonl.gz',\n",
       "  '1572.jsonl.gz',\n",
       "  '1573.jsonl.gz',\n",
       "  '1574.jsonl.gz',\n",
       "  '1575.jsonl.gz',\n",
       "  '1576.jsonl.gz',\n",
       "  '1577.jsonl.gz',\n",
       "  '1578.jsonl.gz',\n",
       "  '1579.jsonl.gz',\n",
       "  '158.jsonl.gz',\n",
       "  '1580.jsonl.gz',\n",
       "  '1581.jsonl.gz',\n",
       "  '1582.jsonl.gz',\n",
       "  '1583.jsonl.gz',\n",
       "  '1584.jsonl.gz',\n",
       "  '1585.jsonl.gz',\n",
       "  '1586.jsonl.gz',\n",
       "  '1587.jsonl.gz',\n",
       "  '1588.jsonl.gz',\n",
       "  '1589.jsonl.gz',\n",
       "  '159.jsonl.gz',\n",
       "  '1590.jsonl.gz',\n",
       "  '1591.jsonl.gz',\n",
       "  '1592.jsonl.gz',\n",
       "  '1593.jsonl.gz',\n",
       "  '1594.jsonl.gz',\n",
       "  '1595.jsonl.gz',\n",
       "  '1596.jsonl.gz',\n",
       "  '1597.jsonl.gz',\n",
       "  '1598.jsonl.gz',\n",
       "  '1599.jsonl.gz',\n",
       "  '16.jsonl.gz',\n",
       "  '160.jsonl.gz',\n",
       "  '1600.jsonl.gz',\n",
       "  '1601.jsonl.gz',\n",
       "  '1602.jsonl.gz',\n",
       "  '1603.jsonl.gz',\n",
       "  '1604.jsonl.gz',\n",
       "  '1605.jsonl.gz',\n",
       "  '1606.jsonl.gz',\n",
       "  '1607.jsonl.gz',\n",
       "  '1608.jsonl.gz',\n",
       "  '1609.jsonl.gz',\n",
       "  '161.jsonl.gz',\n",
       "  '1610.jsonl.gz',\n",
       "  '1611.jsonl.gz',\n",
       "  '1612.jsonl.gz',\n",
       "  '1613.jsonl.gz',\n",
       "  '1614.jsonl.gz',\n",
       "  '1615.jsonl.gz',\n",
       "  '1616.jsonl.gz',\n",
       "  '1617.jsonl.gz',\n",
       "  '1618.jsonl.gz',\n",
       "  '1619.jsonl.gz',\n",
       "  '162.jsonl.gz',\n",
       "  '1620.jsonl.gz',\n",
       "  '1621.jsonl.gz',\n",
       "  '1622.jsonl.gz',\n",
       "  '1623.jsonl.gz',\n",
       "  '1624.jsonl.gz',\n",
       "  '1625.jsonl.gz',\n",
       "  '1626.jsonl.gz',\n",
       "  '1627.jsonl.gz',\n",
       "  '1628.jsonl.gz',\n",
       "  '1629.jsonl.gz',\n",
       "  '163.jsonl.gz',\n",
       "  '1630.jsonl.gz',\n",
       "  '1631.jsonl.gz',\n",
       "  '1632.jsonl.gz',\n",
       "  '1633.jsonl.gz',\n",
       "  '1634.jsonl.gz',\n",
       "  '1635.jsonl.gz',\n",
       "  '1636.jsonl.gz',\n",
       "  '1637.jsonl.gz',\n",
       "  '1638.jsonl.gz',\n",
       "  '1639.jsonl.gz',\n",
       "  '164.jsonl.gz',\n",
       "  '1640.jsonl.gz',\n",
       "  '1641.jsonl.gz',\n",
       "  '1642.jsonl.gz',\n",
       "  '1643.jsonl.gz',\n",
       "  '1644.jsonl.gz',\n",
       "  '1645.jsonl.gz',\n",
       "  '1646.jsonl.gz',\n",
       "  '1647.jsonl.gz',\n",
       "  '1648.jsonl.gz',\n",
       "  '1649.jsonl.gz',\n",
       "  '165.jsonl.gz',\n",
       "  '1650.jsonl.gz',\n",
       "  '1651.jsonl.gz',\n",
       "  '1652.jsonl.gz',\n",
       "  '1653.jsonl.gz',\n",
       "  '1654.jsonl.gz',\n",
       "  '1655.jsonl.gz',\n",
       "  '1656.jsonl.gz',\n",
       "  '1657.jsonl.gz',\n",
       "  '1658.jsonl.gz',\n",
       "  '1659.jsonl.gz',\n",
       "  '166.jsonl.gz',\n",
       "  '1660.jsonl.gz',\n",
       "  '1661.jsonl.gz',\n",
       "  '1662.jsonl.gz',\n",
       "  '1663.jsonl.gz',\n",
       "  '1664.jsonl.gz',\n",
       "  '1665.jsonl.gz',\n",
       "  '1666.jsonl.gz',\n",
       "  '1667.jsonl.gz',\n",
       "  '1668.jsonl.gz',\n",
       "  '1669.jsonl.gz',\n",
       "  '167.jsonl.gz',\n",
       "  '1670.jsonl.gz',\n",
       "  '1671.jsonl.gz',\n",
       "  '1672.jsonl.gz',\n",
       "  '1673.jsonl.gz',\n",
       "  '1674.jsonl.gz',\n",
       "  '1675.jsonl.gz',\n",
       "  '1676.jsonl.gz',\n",
       "  '1677.jsonl.gz',\n",
       "  '1678.jsonl.gz',\n",
       "  '1679.jsonl.gz',\n",
       "  '168.jsonl.gz',\n",
       "  '1680.jsonl.gz',\n",
       "  '1681.jsonl.gz',\n",
       "  '1682.jsonl.gz',\n",
       "  '1683.jsonl.gz',\n",
       "  '1684.jsonl.gz',\n",
       "  '1685.jsonl.gz',\n",
       "  '1686.jsonl.gz',\n",
       "  '1687.jsonl.gz',\n",
       "  '1688.jsonl.gz',\n",
       "  '1689.jsonl.gz',\n",
       "  '169.jsonl.gz',\n",
       "  '1690.jsonl.gz',\n",
       "  '1691.jsonl.gz',\n",
       "  '1692.jsonl.gz',\n",
       "  '1693.jsonl.gz',\n",
       "  '1694.jsonl.gz',\n",
       "  '1695.jsonl.gz',\n",
       "  '1696.jsonl.gz',\n",
       "  '1697.jsonl.gz',\n",
       "  '1698.jsonl.gz',\n",
       "  '1699.jsonl.gz',\n",
       "  '17.jsonl.gz',\n",
       "  '170.jsonl.gz',\n",
       "  '1700.jsonl.gz',\n",
       "  '1701.jsonl.gz',\n",
       "  '1702.jsonl.gz',\n",
       "  '1703.jsonl.gz',\n",
       "  '1704.jsonl.gz',\n",
       "  '1705.jsonl.gz',\n",
       "  '1706.jsonl.gz',\n",
       "  '1707.jsonl.gz',\n",
       "  '1708.jsonl.gz',\n",
       "  '1709.jsonl.gz',\n",
       "  '171.jsonl.gz',\n",
       "  '1710.jsonl.gz',\n",
       "  '1711.jsonl.gz',\n",
       "  '1712.jsonl.gz',\n",
       "  '1713.jsonl.gz',\n",
       "  '1714.jsonl.gz',\n",
       "  '1715.jsonl.gz',\n",
       "  '1716.jsonl.gz',\n",
       "  '1717.jsonl.gz',\n",
       "  '1718.jsonl.gz',\n",
       "  '1719.jsonl.gz',\n",
       "  '172.jsonl.gz',\n",
       "  '1720.jsonl.gz',\n",
       "  '1721.jsonl.gz',\n",
       "  '1722.jsonl.gz',\n",
       "  '1723.jsonl.gz',\n",
       "  '1724.jsonl.gz',\n",
       "  '1725.jsonl.gz',\n",
       "  '1726.jsonl.gz',\n",
       "  '1727.jsonl.gz',\n",
       "  '1728.jsonl.gz',\n",
       "  '1729.jsonl.gz',\n",
       "  '173.jsonl.gz',\n",
       "  '1730.jsonl.gz',\n",
       "  '1731.jsonl.gz',\n",
       "  '1732.jsonl.gz',\n",
       "  '1733.jsonl.gz',\n",
       "  '1734.jsonl.gz',\n",
       "  '1735.jsonl.gz',\n",
       "  '1736.jsonl.gz',\n",
       "  '1737.jsonl.gz',\n",
       "  '1738.jsonl.gz',\n",
       "  '1739.jsonl.gz',\n",
       "  '174.jsonl.gz',\n",
       "  '1740.jsonl.gz',\n",
       "  '1741.jsonl.gz',\n",
       "  '1742.jsonl.gz',\n",
       "  '1743.jsonl.gz',\n",
       "  '1744.jsonl.gz',\n",
       "  '1745.jsonl.gz',\n",
       "  '1746.jsonl.gz',\n",
       "  '1747.jsonl.gz',\n",
       "  '1748.jsonl.gz',\n",
       "  '1749.jsonl.gz',\n",
       "  '175.jsonl.gz',\n",
       "  '1750.jsonl.gz',\n",
       "  '1751.jsonl.gz',\n",
       "  '1752.jsonl.gz',\n",
       "  '1753.jsonl.gz',\n",
       "  '1754.jsonl.gz',\n",
       "  '1755.jsonl.gz',\n",
       "  '1756.jsonl.gz',\n",
       "  '1757.jsonl.gz',\n",
       "  '1758.jsonl.gz',\n",
       "  '1759.jsonl.gz',\n",
       "  '176.jsonl.gz',\n",
       "  '1760.jsonl.gz',\n",
       "  '1761.jsonl.gz',\n",
       "  '1762.jsonl.gz',\n",
       "  '1763.jsonl.gz',\n",
       "  '1764.jsonl.gz',\n",
       "  '1765.jsonl.gz',\n",
       "  '1766.jsonl.gz',\n",
       "  '1767.jsonl.gz',\n",
       "  '1768.jsonl.gz',\n",
       "  '1769.jsonl.gz',\n",
       "  '177.jsonl.gz',\n",
       "  '1770.jsonl.gz',\n",
       "  '1771.jsonl.gz',\n",
       "  '1772.jsonl.gz',\n",
       "  '1773.jsonl.gz',\n",
       "  '1774.jsonl.gz',\n",
       "  '1775.jsonl.gz',\n",
       "  '1776.jsonl.gz',\n",
       "  '1777.jsonl.gz',\n",
       "  '1778.jsonl.gz',\n",
       "  '1779.jsonl.gz',\n",
       "  '178.jsonl.gz',\n",
       "  '1780.jsonl.gz',\n",
       "  '1781.jsonl.gz',\n",
       "  '1782.jsonl.gz',\n",
       "  '1783.jsonl.gz',\n",
       "  '1784.jsonl.gz',\n",
       "  '1785.jsonl.gz',\n",
       "  '1786.jsonl.gz',\n",
       "  '1787.jsonl.gz',\n",
       "  '1788.jsonl.gz',\n",
       "  '1789.jsonl.gz',\n",
       "  '179.jsonl.gz',\n",
       "  '1790.jsonl.gz',\n",
       "  '1791.jsonl.gz',\n",
       "  '1792.jsonl.gz',\n",
       "  '1793.jsonl.gz',\n",
       "  '1794.jsonl.gz',\n",
       "  '1795.jsonl.gz',\n",
       "  '1796.jsonl.gz',\n",
       "  '1797.jsonl.gz',\n",
       "  '1798.jsonl.gz',\n",
       "  '1799.jsonl.gz',\n",
       "  '18.jsonl.gz',\n",
       "  '180.jsonl.gz',\n",
       "  '1800.jsonl.gz',\n",
       "  '1801.jsonl.gz',\n",
       "  '1802.jsonl.gz',\n",
       "  '1803.jsonl.gz',\n",
       "  '1804.jsonl.gz',\n",
       "  '1805.jsonl.gz',\n",
       "  '1806.jsonl.gz',\n",
       "  '1807.jsonl.gz',\n",
       "  '1808.jsonl.gz',\n",
       "  '1809.jsonl.gz',\n",
       "  '181.jsonl.gz',\n",
       "  '1810.jsonl.gz',\n",
       "  '1811.jsonl.gz',\n",
       "  '1812.jsonl.gz',\n",
       "  '1813.jsonl.gz',\n",
       "  '1814.jsonl.gz',\n",
       "  '1815.jsonl.gz',\n",
       "  '1816.jsonl.gz',\n",
       "  '1817.jsonl.gz',\n",
       "  '1818.jsonl.gz',\n",
       "  '1819.jsonl.gz',\n",
       "  '182.jsonl.gz',\n",
       "  '1820.jsonl.gz',\n",
       "  '1821.jsonl.gz',\n",
       "  '1822.jsonl.gz',\n",
       "  '1823.jsonl.gz',\n",
       "  '1824.jsonl.gz',\n",
       "  '1825.jsonl.gz',\n",
       "  '1826.jsonl.gz',\n",
       "  '1827.jsonl.gz',\n",
       "  '1828.jsonl.gz',\n",
       "  '1829.jsonl.gz',\n",
       "  '183.jsonl.gz',\n",
       "  '1830.jsonl.gz',\n",
       "  '1831.jsonl.gz',\n",
       "  '1832.jsonl.gz',\n",
       "  '1833.jsonl.gz',\n",
       "  '1834.jsonl.gz',\n",
       "  '1835.jsonl.gz',\n",
       "  '1836.jsonl.gz',\n",
       "  '1837.jsonl.gz',\n",
       "  '1838.jsonl.gz',\n",
       "  '1839.jsonl.gz',\n",
       "  '184.jsonl.gz',\n",
       "  '1840.jsonl.gz',\n",
       "  '1841.jsonl.gz',\n",
       "  '1842.jsonl.gz',\n",
       "  '1843.jsonl.gz',\n",
       "  '1844.jsonl.gz',\n",
       "  '1845.jsonl.gz',\n",
       "  '1846.jsonl.gz',\n",
       "  '1847.jsonl.gz',\n",
       "  '1848.jsonl.gz',\n",
       "  '1849.jsonl.gz',\n",
       "  '185.jsonl.gz',\n",
       "  '1850.jsonl.gz',\n",
       "  '1851.jsonl.gz',\n",
       "  '1852.jsonl.gz',\n",
       "  '1853.jsonl.gz',\n",
       "  '1854.jsonl.gz',\n",
       "  '1855.jsonl.gz',\n",
       "  '1856.jsonl.gz',\n",
       "  '1857.jsonl.gz',\n",
       "  '1858.jsonl.gz',\n",
       "  '1859.jsonl.gz',\n",
       "  '186.jsonl.gz',\n",
       "  '1860.jsonl.gz',\n",
       "  '1861.jsonl.gz',\n",
       "  '1862.jsonl.gz',\n",
       "  '1863.jsonl.gz',\n",
       "  '1864.jsonl.gz',\n",
       "  '1865.jsonl.gz',\n",
       "  '1866.jsonl.gz',\n",
       "  '1867.jsonl.gz',\n",
       "  '1868.jsonl.gz',\n",
       "  '1869.jsonl.gz',\n",
       "  '187.jsonl.gz',\n",
       "  '1870.jsonl.gz',\n",
       "  '1871.jsonl.gz',\n",
       "  '1872.jsonl.gz',\n",
       "  '1873.jsonl.gz',\n",
       "  '1874.jsonl.gz',\n",
       "  '1875.jsonl.gz',\n",
       "  '1876.jsonl.gz',\n",
       "  '1877.jsonl.gz',\n",
       "  '1878.jsonl.gz',\n",
       "  '1879.jsonl.gz',\n",
       "  '188.jsonl.gz',\n",
       "  '1880.jsonl.gz',\n",
       "  '1881.jsonl.gz',\n",
       "  '1882.jsonl.gz',\n",
       "  '1883.jsonl.gz',\n",
       "  '1884.jsonl.gz',\n",
       "  '1885.jsonl.gz',\n",
       "  '1886.jsonl.gz',\n",
       "  '1887.jsonl.gz',\n",
       "  '1888.jsonl.gz',\n",
       "  '1889.jsonl.gz',\n",
       "  '189.jsonl.gz',\n",
       "  '1890.jsonl.gz',\n",
       "  '1891.jsonl.gz',\n",
       "  '1892.jsonl.gz',\n",
       "  '1893.jsonl.gz',\n",
       "  '1894.jsonl.gz',\n",
       "  '1895.jsonl.gz',\n",
       "  '1896.jsonl.gz',\n",
       "  '1897.jsonl.gz',\n",
       "  '1898.jsonl.gz',\n",
       "  ...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_zips),f_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s2orc-master.zip']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in f_zips if '.gz' not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '104172',\n",
       " 'metadata': {'title': 'Nonlinear inversion of tilt-affected very long period records of explosive eruptions at Fuego volcano: INVERSION OF TILT-AFFECTED VLP EVENTS',\n",
       "  'authors': [{'first': 'Gregory',\n",
       "    'middle': ['P.'],\n",
       "    'last': 'Waite',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Federica', 'middle': [], 'last': 'Lanza', 'suffix': ''}],\n",
       "  'abstract': None,\n",
       "  'year': '2016',\n",
       "  'arxiv_id': None,\n",
       "  'acl_id': None,\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.1002/2016jb013287',\n",
       "  'venue': 'Journal of Geophysical Research: Solid Earth',\n",
       "  'journal': 'Journal of Geophysical Research'},\n",
       " 's2_pdf_hash': '73ed8076fc747e77c41845cb5f18b40ece350865',\n",
       " 'grobid_parse': {'abstract': [],\n",
       "  'body_text': [{'text': 'solution to this is to evaluate long wavelength, very-long-period (VLP) data that are relativelyFuego is a 3800 m stratovolcano that regularly produces Strombolian and weak 76Vulcanian explosions. The dynamics of these explosive events have been examined in the VLP 77 band [Lyons and Waite, 2011] and modeled together with infrasound and gas emission data. At 78 least three different styles of VLP event have been observed and attributed to eruptions from 79 either the summit vent or a flank vent [Waite et al., 2013] . The strongest recorded explosions 80 generated impulsive infrasound and seismic signals, ejected incandescent bombs and tephra, and 81were associated with repetitive VLP seismicity. The previous studies of Fuego VLP events 82 focused on periods from 30-10 seconds, where the influence of ground tilt is negligible given the 83 distances to the source and relatively short VLP wavelengths. Although the station geometry was 84 somewhat limited by the logistical and safety considerations, Lyons and Waite [2011] found that 85 the data best fit a source with a centroid 300 m below and 300 m to the west of the summit with a 86 moment tensor representative of primarily a dipping crack. The 30-10 second VLP captures the 87 inflation-deflation-reinflation cycle of this portion of the conduit as a small eruption occurs. 88 Lyons et al. [2012] examined the tilt signal associated with these same small explosions 89 at periods below the instrument corners. They found a significant tilt signal beginning up to 30 90 minutes prior to explosive eruptions. Forward modeling of the tilt from stations that were close 91 enough to record it suggested a shallow source midway between the VLP source centroid and the 92 summit. A full waveform inversion was not attempted. 93In this study, we perform full waveform inversions of stacks of events associated with 94 summit vent explosions in periods from 400 -10 seconds using a combined rotation-translation 95 approach similar to that of Maeda et al. [2011] . Inversions were performed in different bands to 96 explore the increasing influence of tilt with increasing period. While events with periods of 100s 97 of seconds are sometimes called Ultra-Long-Period events [e.g., Johnson et al., 2009], we simply 98 use the term VLP to cover the range of periods we investigate here. To improve the signal to 99 noise ratio, and ensure a representative dataset, inversions were performed on a set of phase-100 weighted, stacked seismograms from six explosions. The cleaner signals that resulted from 101 stacking also allowed for a larger number of seismic channels to be used than in previous studies. 102In order to constrain the uncertainty on the source type, we performed a nonlinear inversion for 103 moment tensor source type. This involves a grid search over all possible moment tensor types 104 and orientations at the best-fit centroid location, providing quantitative constraint on the source 105 type.',\n",
       "    'cite_spans': [{'start': 274,\n",
       "      'end': 297,\n",
       "      'text': '[Lyons and Waite, 2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 500,\n",
       "      'end': 520,\n",
       "      'text': '[Waite et al., 2013]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF27'},\n",
       "     {'start': 1027,\n",
       "      'end': 1033,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1342, 'end': 1344, 'text': '88', 'latex': None, 'ref_id': None},\n",
       "     {'start': 1358,\n",
       "      'end': 1364,\n",
       "      'text': '[2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2016,\n",
       "      'end': 2022,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In order to avoid problems associated with ground rotation contaminating the low 109 frequency seismic signal and, in fact, to take advantage of the tilt signal to investigate 110 frequencies below the typical VLP band, we invert jointly for translation and rotation. The 111 approach was first laid out by Maeda et al. [2011] and has been successfully applied at Kīlauea 112 [Chouet and Dawson, 2013] and Asama volcanoes ] . An important 113 aspect of this method is that the instrument responses are not deconvolved from the 114 seismograms. Instead, the responses to tilt and displacement are convolved with corresponding 115Green\\'s functions. Deconvolution effectively occurs during the inversion. For component n of 116 displacement at time t and receiver position € ! r , we can write the relationship between source and 117 receiver through rotation and translation Green\\'s functions in the frequency domain: 118 with respect to the q-coordinate. The translation response is derived from the poles and zeros in a 126 standard way, while the tilt response is defined as I rot = g I trans (iω) -2 , where 127 g is gravitational acceleration. We assume no tilt response on the vertical components. Although 128 it is not specified in equation (1), the G matrices naturally depend on the source position as well. 129u n seis ! r,ω ( ) = I n rot ω ( ) G np,q rot ! r,ω ( ) + I n trans ω ( ) G np,q trans ! r,ω ( ) ! \" # $ M pq ω ( ) ,(1)In practice, we do not invert directly for the source position, instead searching over a volume of 130 possible source locations and determining the source position from the inversion results. 131Equation 1 can be recast in matrix form as 132u ω ( ) = G ω ( ) s ω ( ) ,(2) 133where u is the R x 1 vector of Fourier-transformed ground displacement components, s is the 6 x 134 number of observed seismic traces. The matrix G is R x 6, composed from Fourier transform of 136 the terms inside the square brackets in equation 1. The equation is solved in a least-squares 137 inversion one frequency at a time, as described in Waite et al. [2008] . Initially, the inversion is 138 unconstrained, and the model moment tensor source time function is constructed from the 139 inverse Fourier transform of the inversion results. Although some studies have found that single 140 forces can be important in VLP source processes, especially when related to a reaction to vertical 141 mass ejection, previous work at Fuego [Lyons and Waite, 2011] showed they were not 142 significant. Single forces are unlikely to contribute to even lower frequency source models, 143 which are dominated by pre-explosion signal, so they were not considered in our study. 144',\n",
       "    'cite_spans': [{'start': 320,\n",
       "      'end': 326,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 376,\n",
       "      'end': 401,\n",
       "      'text': '[Chouet and Dawson, 2013]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 2074,\n",
       "      'end': 2080,\n",
       "      'text': '[2008]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2449,\n",
       "      'end': 2472,\n",
       "      'text': '[Lyons and Waite, 2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': \"The translation and rotation Green's functions were calculated using a finite-difference 146 method [Ohminato and Chouet, 1997 ] with a homogeneous model that includes the three-147 dimensional topography of Fuego volcano. Given the long wavelengths of the VLP signals, the 148 homogeneous structure is appropriate [see, e.g., Waite et al., 2008] . We used a compressional-149 wave velocity of 3.5 km/s, shear-wave velocity of 2 km/s, and density of 2650 kg/m 3 . In practice, 150 the error in the velocity model introduces little effect on the VLP inversion [Waite et al., 2008] . 151The model is centered on the summit of Fuego and extends 11.72 km east-west, 8.96 km north-152 south, and 6 km vertically with a 40 m grid spacing. Green's functions for synthetic sources over 153 a volume 680 m east-west, 480 m north-south, and 920 m vertically from the summit down. This 154 volume is centered west of the summit, because the VLP source found by Lyons and Waite 155[2011] was ~300 m west of the summit vent. In order to speed the calculations, we skip 156 alternating grid nodes, so the synthetic source volume has a spacing of 80 m. All other details 157 about the finite-difference modeling are as described in Lyons and Waite [2011] . The rotation 158Green's functions are computed from the curl of the displacement field (two times the tilt) during 159 the finite-difference simulations. 160\",\n",
       "    'cite_spans': [{'start': 100,\n",
       "      'end': 126,\n",
       "      'text': '[Ohminato and Chouet, 1997',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 327,\n",
       "      'end': 346,\n",
       "      'text': 'Waite et al., 2008]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 559,\n",
       "      'end': 579,\n",
       "      'text': '[Waite et al., 2008]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 1233,\n",
       "      'end': 1239,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The unconstrained inversion at hundreds of points inside a volume within the summit of 162Fuego volcano provides a spatial estimate of the location and uncertainty on the location (see 163 section below). Interpretation of the moment tensor source time function can be done by point-164by-point eigenvector decomposition, which provides the orientation and mechanism type in caseswhere the moment components are in phase throughout the source time function. We expand on 166 this approach to explore the uncertainty in the moment tensor type, following the nonlinear 167 inversion approach described below. 168Our approach involves a grid search over a total of five parameters for all possible 169 moment tensor types and orientations. For each model, we constrain the moment tensor using the 170 method of Lagrange multipliers and compute the data fit. First, we search over possible moment 171 tensor types using the fundamental lune source-type definition of Tape and Tape [2012] , which 172 involves two parameters that describe the ratios of the moment tensor eigenvalues (Figure 1) . 173The latitude parameter, δ, ranges from -90º to 90º and the longitude parameter, γ, ranges from -174 30º to 30º. Some example source types, the associated γ and δ, along with relative eigenvalues 175 (λ) are given in The search over the parameters γ and δ uses the surface spline method described by Tape  183 and Tape [2012] to evenly sample the moment tensor source type space. This method results in a 184 substantial computational savings over an evenly sampled grid [Wang and Dahlen, 1995] . We 185 evaluate γ from -30º to 30º, but because the lower half of the lune is simply the opposite sign of 186 the upper half (e.g., volume decrease versus volume increase), we evaluate δ from 0 to 90º. For each of the 1,300,536 trial moment tensor solutions, the six independent moment 195 tensor components of the trial tensor are used to constrain the inversion through a system of 196 equations, that fix the ratio of the moment tensor components [Menke, 1989] . For a given trial 197 tensor, H is a matrix that contains the ratios of matrix components to arbitrarily selected 198component M 11 (n) : 199 H = 1 − M 11 (n) M 22 (n) 0 0 0 0 1 0 − M 11 (n) M 33 (n) 0 0 0 1 0 0 − M 11 (n) M 12 (n) 0 0 1 0 0 0 − M 11 (n) M 23 (n) 0 1 0 0 0 0 − M 11 (n) M 13 (n) \" # $ $ $ $ $ $ $ $ % & \\' \\' \\' \\' \\' \\' \\' \\' 200where the notation M 11 (n) indicates the nth trial moment tensor. The constraint equations are 201 solved simultaneously in the inversion using least squares: 202G T G H T H Z ! \" # # $ % & & s l ! \" # $ % & = G T u h ! \" # # $ % & & 203where Z is a 5 x 5 matrix of zeros, h is a 5 x 1 vector of zeros, and s and u are the model and 204 data vectors as in equation 2. The vector l consists of the Lagrange multipliers. 205Given the large number of possible solutions, we perform the full grid search for only the 206 best-fit locations of each frequency band. Given that the source time function does not vary 207 rapidly spatially [Lyons and Waite, 2011] , this is a reasonable approach. 208',\n",
       "    'cite_spans': [{'start': 977,\n",
       "      'end': 983,\n",
       "      'text': '[2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1412,\n",
       "      'end': 1418,\n",
       "      'text': '[2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1564,\n",
       "      'end': 1587,\n",
       "      'text': '[Wang and Dahlen, 1995]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF28'},\n",
       "     {'start': 2040,\n",
       "      'end': 2053,\n",
       "      'text': '[Menke, 1989]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'},\n",
       "     {'start': 3028,\n",
       "      'end': 3051,\n",
       "      'text': '[Lyons and Waite, 2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [{'start': 1078,\n",
       "      'end': 1088,\n",
       "      'text': '(Figure 1)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1393,\n",
       "      'end': 1402,\n",
       "      'text': 'Tape  183',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': \"The nonlinear approaches to finding the best centroid location and moment tensor lend 210 themselves to quantitative error analysis. Misfit information is used to define uncertainty on thesource centroid, and in this work we extend this to examine uncertainty in the moment tensor 212 type. A similar approach was used by Ford et al. [2010] to examine non-double-couple 213 components of explosion and earthquake sources. In many applications of moment tensor 214 inversion at volcanoes, a weighted squared error is used, which is either normalized by station or 215 channel [Chouet et al., 2003; Ohminato et al., 1998 ]. As our approach follows this methodology, 216we adopt the squared error measure described as E 2 : 217E 2 = 1 N r u n 0 pΔt ( ) − u n s pΔt ( ) ( ) 2 p=1 N s ∑ 1 3 ∑ u n 0 pΔt ( ) ( ) 2 p=1 N s ∑ 1 3 ∑ $ % & & & & & ' ( ) ) ) ) ) n=1 N r ∑ (3) 218where is the pth sample of the nth data trace, is the pth sample of the nth 219 The threat of vandalism or theft prohibited deployment of solar panels at some sites, so 231 continuous operation of all 10 stations was restricted to 19 -21 January. Two of the stations are 232 very close to F9A, F9B, and F9C, and were not used in the study. Station F9SE, at 6 km from the 233 summit, did not record the VLP signals. In total, seven stations (21 channels) were used in the 234 inversions. More complete details of the experiment are given in Lyons and Waite [2011] . 235€ u n 0 pΔt ( ) € u n s pΔt ( )\",\n",
       "    'cite_spans': [{'start': 334,\n",
       "      'end': 340,\n",
       "      'text': '[2010]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 575,\n",
       "      'end': 596,\n",
       "      'text': '[Chouet et al., 2003;',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 597,\n",
       "      'end': 618,\n",
       "      'text': 'Ohminato et al., 1998',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 1425,\n",
       "      'end': 1431,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'During this deployment, all of the strong explosions issued from the summit vent and 237 produced clear VLP signals at the nearest stations. The closest station, which also happened to be 238 the most reliable, F900, recorded hundreds of similar explosions during the 19-day deployment. 239Lyons and Waite [2011] selected one of these events (19 January 2009 at 16:09:30) that was well 240 recorded on the network for their inversion. In order to improve the signal-to-noise ratio of the 241 data on some of the more distant channels, especially at periods below 30 seconds, we computed 242 phase-weighted stacks of similar events in multiple frequency bands for inversion. 243The 19 January event was used as a master event and compared against all the data at 244 station F900. With this procedure, we identified 209 events for which the combined three-245 channel cross-correlation coefficient was 2.0 or greater (roughly one event every 2 hours). Of 246 these, only 21 events were simultaneously recorded on all the stations from 19-21 January. The 247 waveform similarity generally degraded with increased distance from the source. While the 248 waveforms were generally consistent from event to event on a given channel in the 30-10 second 249 band, and produced a clean stack, the weaker signals and possibly greater noise at lower 250 frequencies led to poor correlations on many channels below 30 seconds. As an objective of this 251 study was to investigate frequencies below the instrument corner, we sought to use only events 252 that were similar. We visually inspected all the data channel-by-channel, and removed all data 253 from events where one or more channels was extremely noisy or had inverted polarity in the 400 254 to 60 or 60 to 10 seconds band. This left just six events to stack for the inversion. 255We followed the phase-weighted stacking approach of Schimmel and Paulssen [1997] 256 and found that it produced a vastly improved signal to noise ratio (Figure 3 ). This is particularly 257 important for the lower frequencies, which contain substantially more noise on a single trace. 258Even with just six events in the stack, the phase-weighted stacking procedure resulted in clean 259 waveforms at nearly every channel and every station. For comparison, we show the linear stack 260 along with the individual waveforms from all the events for the vertical component at three 261 stations in two non-overlapping frequency bands. In general, the phase-weighted stack is slightly 262 lower in amplitude but has much less pre-or post-event noise. In particular, single-event noise 263 that contributes substantially to the linear stack is absent in the phase-weighted stack. 264The gray bars in Figure 3 into one value using the square root of the sum of the squared standard deviations. Since Lyons 308and Waite [2011] found the 30-10 second period VLP source to be dominated by a dipping 309 crack, we used moment tensor eigenvalue ratios of 2:1:1. We also examined g for ratios of 3:1:1 310 and 1:1:1 and, although the values for g varied, we found no difference in which of the solutions 311 had the lowest g. Because it implies the most consistency throughout the source time function, 312 the solution within 5% of the minimum E 2 error that had the lowest g was chosen as the best 313 solution. 314',\n",
       "    'cite_spans': [{'start': 306,\n",
       "      'end': 312,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1903,\n",
       "      'end': 1909,\n",
       "      'text': '[1997]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2835,\n",
       "      'end': 2847,\n",
       "      'text': 'Waite [2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [{'start': 1981,\n",
       "      'end': 1990,\n",
       "      'text': '(Figure 3',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF3'},\n",
       "     {'start': 2723,\n",
       "      'end': 2731,\n",
       "      'text': 'Figure 3',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF3'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The best-fit centroid locations do not vary significantly with bandpass. The minimum E 2 316 solutions (E 2min ) are identical for bands from 60-10 s through 400-10 s, all very near the surface 317 and just west of the summit (see Figure 5 and Table 2 ). When the minimum g is also considered, 318 the best solutions are deeper, and the 60-10 s and 90-10 s solutions are 320 m east of the summit. 319The 120-10 s and 400-10 s solutions are about 80 m south of the E 2min solutions. The lowest 320 frequency (400-60 s) g min solution is at a shallower depth and location nearer the summit than the 321 E 2min solution. The best 30-10 s solutions are west of the summit when both E 2min and g min are 322considered; it only migrates south to north. The E 2min solution is within 200 m of the best fit 323 found by Lyons and Waite [2011] when inverting in the same 30-10 s band, although we use a 324 stack of 6 events that allows us to invert data from an additional station. We do not attempt tointerpret the fine details of the centroid locations, given the limitations in the station coverage, 326 but consider the locations to be geologically reasonable. 327Given the similarities in models for some of the intermediate bandpasses, we focus on 328 four representative bands: 30-10 s; 90-10 s; 120-10 s; and 400-60 s. The E 2min and g min centroid 329locations are shown in Figure 5 . In Table 2 , the E 2min and g min solutions for each bandpass are 330 shown, with the E 2min in the first row and g min in the second row. 331 Figure 6 shows data (black) and synthetic waveforms for each of the four bandpasses. 332The free inversion synthetics, shown in red, generally fit the data quite well. The error measure 333 we use weights stations, not channels, equally. The advantage to this is that stations that are 334 farther away or have lower amplitude because of the radiation pattern have equal importance. To 335give a sense of the fits on individual channels, we computed cross-correlation coefficients and 336 lag times between the data and free inversion synthetics. At F900, for example, the fits at all 337 channels in the 30-10 s bandpass are above 0.97 at lags of between -0.04 and 0.28 s. In contrast, 338 the north channel at F9A, which clearly fits less well, has a correlation coefficient of 0.83 at 0.02 339 s lag. The worst-fitting channel, the east channel of F9NW, has a correlation of 0.53 at 6.5 s lag. 340More than half of the correlation coefficients are above 0.9 in the 90-10 s band, and none of the 341 lags are more than 1 s. In each of the remaining two bands, at least 5 channels correlate above 342 0.9. The range of lag times increases slightly in the longer period bands but, even in the 400-60 s 343 band, well over half of the lags are below 5 sec. 344The synthetics from the fixed tensor solutions, described below, have slightly greater 345 misfits, as expected for models with fewer free parameters. The synthetics for the 90-10 s band 346 are notably much poorer for the fixed inversion. The correlation coefficients and lag times 347 between the data and both the free inversions and the fixed tensor inversions for each of the 348 channels are given in supplementary tables S1 and S2, respectively. 349The source time functions for these same four representative bandpasses are shown in 350 for 17 channels, compared with 19 channels used in this study, which led to much larger misfits. 359For the remaining rows, the first row associated with each band represents details of the E 2min 360 solution and the second row shows details of the g min solution. 361 362',\n",
       "    'cite_spans': [{'start': 828,\n",
       "      'end': 834,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [{'start': 231,\n",
       "      'end': 239,\n",
       "      'text': 'Figure 5',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 244,\n",
       "      'end': 251,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1375,\n",
       "      'end': 1383,\n",
       "      'text': 'Figure 5',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1389,\n",
       "      'end': 1396,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1529,\n",
       "      'end': 1537,\n",
       "      'text': 'Figure 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'For each of the representative bandpasses, the centroid location with the lowest g that 364 was also within 5% of E 2min was used to investigate the constraint on the source type. The 365 nonlinear analysis demonstrates that the range of models with similar misfits is fairly broad. 366 Figure 8 shows the misfits at each moment tensor type for the lowest misfit tensor orientation. 367The point-by-point source time function eigenvalue analysis for the best free inversion is shown 368 using red dots. These correspond to the γ and δ at each point in the source time function when 369 the amplitude was within 50% of the maximum. Points with negative δ were projected to the 370 upper portion of the plot. The misfits for the constrained inversion results are much larger than 371 those of the free inversions, because the moment tensor is fixed for the whole source time 372function. This means there is essentially one model parameter in the constrained inversions,compared to six independent model parameters for the free inversions. Each point in this lune 374 plot represents the lowest misfit value for all moment-tensor orientations with the γ -δ pair. 375Considering, first, the solution in the 30-10 s band, the minimum misfit in the constrained 376 inversions is 0.43, compared to 0.32 for the g min solution. The point-by-point γ−δ pairs are tightly 377 constrained, and the median ratio of eigenvalues is (1.0, 0.28, 0.12). But the nonlinear inversion 378for source type suggests a wider range of possible mechanism types fit the data. In Figure 8a , the 379 0.46 misfit contour encompasses the points in the free inversion and is about 5% above the 380 minimum, and so might be considered a reasonable level to explore the range of possible models. 381This includes sources from a tension crack to a DC. 382One interpretation of this is that the source represents a composite of two or more sources 383 that have different orientations. In fact, Lyons and Waite [2011] found that a combination of a 384 single dipping crack and nearly vertical pipe or crack provided the best-fitting model for the 30-385 10 s VLP. The two sources were out of phase and interpreted to represent the opposite sense of 386 volume change. That is, when the vertical dike or pipe was deflating, the dipping sill was 387 inflating. We computed the γ -δ pair for the combined crack-pipe tensor of Lyons and Waite 388 indicates multiple source types can fit the data well. 393As is clear from Figure 8b , the 90-10 s bandpass data do not constrain the source type. 394Clearly, even the g min solution is much less stable than the 30-10 s solution, and this is born out 395 in the nonlinear inversion for source type. Very little can be interpreted from this solution, but 396 note that the median eigenvalue ratios include a negative minimum and the point-by-point γ -397 δ pairs are all below the region in which all the components have the same sign. The poorly 398 constrained solution is probably mainly due to the complexity of the source. On some stations, 399 especially in the horizontal components, the spectra show two peaks between 90 and 10 s period, 400 which we might infer are tilt dominated, and translation dominated. These may reflect two 401 sources, distinct in space and with peak moment release occurring at distinct times. 402At the wider 120-10 s bandpass, the lower frequencies tend to dominate and the nonlinear 403 inversion for source type is somewhat better constrained. The free inversion is better constrainedthan the 90-10 s band, but the point-by-point γ -δ pairs still range widely. In this case, however, 405 the points are all in the region of a volumetric source. The median ratio of eigenvalues is (1.0, 406 0.73, 0.30) the nonlinear inversion (Figure 8c ) points to a source somewhat like a pipe. 407The final example we present is the 400-60 s band, below the corners of any of the 408 sensors. This inversion is much better constrained in both the free inversion and constrained 409 nonlinear inversion. The γ -δ pairs from the free g min inversion cluster near the point defined for 410 a pipe (1.5, 1.5, 1) and the median eigenvalues are (1.0, 0.78, 0.21). The range of misfit values is 411 much larger than in the previous inversions, and the region surrounding 5% above the minimum 412 occupies a relatively small area. In this case, the source type is well constrained to a volumetric 413 source, and eigenvalues suggest a mechanism like a pipe. Based on the eigenvectors a pipe 414would be dipping about 30º to the NNE, which is reasonable given the centroid location. 415',\n",
       "    'cite_spans': [{'start': 1976,\n",
       "      'end': 1982,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [{'start': 287,\n",
       "      'end': 295,\n",
       "      'text': 'Figure 8',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1552,\n",
       "      'end': 1561,\n",
       "      'text': 'Figure 8a',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2483,\n",
       "      'end': 2492,\n",
       "      'text': 'Figure 8b',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 3772,\n",
       "      'end': 3782,\n",
       "      'text': '(Figure 8c',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'These results do not necessarily simplify the interpretation of these events, but the 417 nonlinear inversion for source type provides a better means of assessing the reliability of the 418 models. In some cases, like the 400-60 s band result, the model is tightly constrained and can be 419 interpreted in terms of a portion of the conduit just below the vent that inflates and deflates 420 during each explosion. In other cases, such as the 30-10 s band, the nonlinear inversion results 421 suggest that a range of solutions from double-couple to a tension crack can explain the data 422 equally well. This can also be seen as evidence for a complex source that involves two or more 423 sources acting more-or-less coincidentally, but with opposite phase. Finally, the 90-10 s band is 424 an example that cannot be constrained to a model. In this case, we suggest that it is because it 425 includes both the crack-like source of the 30-10 s band west of the summit and the pipe-like 426 source of the 400-60 s band closer to the summit. 427Our interpretation is consistent with earlier work at Fuego, and we refer to Lyons and from four broadband stations to the north of the summit. In their study, they found that tilt 433 signals could be described by a spherical source just below the summit. While our inversionssuggest a source somewhat more like a dipping pipe than a pure sphere, the location and sense of 435 volume change are consistent. 436The tilt-dominated signal begins with inflation several minutes prior to the explosion 437 (Figure 7d ). Although we limited the band to 400 seconds, a much longer lead-up time can be 438 seen in the raw data when integrated [Lyons et al., 2012] . This is interpreted as pressurization 439 following sealing of the conduit in the summit area. The model for the 30-10 s VLP proposed by 440Lyons and Waite [2011] has pre-explosion inflation, mainly in a dipping sill, followed by 441 deflation of that portion of the conduit during the eruption. Our single model moment tensor 442 inversions are consistent with this and we favor this interpretation. 443',\n",
       "    'cite_spans': [{'start': 1678,\n",
       "      'end': 1698,\n",
       "      'text': '[Lyons et al., 2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 1857,\n",
       "      'end': 1863,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [{'start': 1544,\n",
       "      'end': 1554,\n",
       "      'text': '(Figure 7d',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF4'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Volcanic seismic events offer great complexity given the wide range of frequencies, 445 influence of near-field effects including ground rotation, and wide range of source types. We 446 adapt a moment-tensor inversion methodology for including tilt-affected seismograms so that we 447 can examine explosion signals over a range from 400-10 s period, well below the corner periods 448 of the instruments used. These data clearly include translation below the instrument corner, as 449 seen in the vertical components, but likely also include some tilt signal near the instrument 450 corner frequency. We show how a nonlinear inversion for source type can aid in interpretation of 451 the mechanism. In some of the intermediate ranges, the contributions from the longer period, 452 presumably tilt-affected signals and those of the translation dominated signals, mean that a single 453 moment-tensor source cannot fit the data. In other pass bands where either the translation or tilt-454 affected data dominate, the moment-tensor source types are better constrained. 455The broadening of the inversion to periods greater than 6 minutes allows for a more 456 complete description of the source. In this case, we find a result that is largely consistent with the 457 forward modeling done in prior work, but offers more detail about the mechanism type. Our 591 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 1354,\n",
       "      'end': 1476,\n",
       "      'text': '591 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF0'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'These supplementary tables include the scaled correlation coefficients and associated lag times for each data channel with the corresponding model synthetics. These tables can be compared with the waveform plots in Figure 6 . Although this information is not used in the inversion, it provides additional information about the model fit that may be compared to other waveform similarity studies. Note that the correlations are scaled so that absolute amplitude information is not preserved. Table S1 . The correlations between each data channel and synthetic waveform from the freeinversion models, and corresponding lag times, for each channel used. The maximum allowable lag time of +/-9.98 seconds was reached in some cases. These statistics were not computed for two channels that were not used in the inversions. Table S2 . The correlations between each data channel and synthetic waveform from the fixed-inversion models, and corresponding lag times, for each channel used. The maximum allowable lag time of +/-9.98 seconds was reached in some cases. These statistics were not computed for two channels that were not used in the inversions.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 215,\n",
       "      'end': 223,\n",
       "      'text': 'Figure 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 491,\n",
       "      'end': 499,\n",
       "      'text': 'Table S1',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 818,\n",
       "      'end': 826,\n",
       "      'text': 'Table S2',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {'FIGREF0': {'text': \") is the Fourier transform of the n component of the seismogram without 120 instrument correction, p and q are direction indices x, y, and z, M pq (ω) is the Fourier transform 121 of the time history of the pq-component of the moment tensor, I rot and I trans represent tilt and 122 translation instrument response functions, the matrices G rot and G trans are Green's functions that 123 relate the n-component of tilt or translation at the receiver position, € ! r , with the moment at the 124 source position, and ω is angular frequency. The notation p,q indicates spatial differentiation 125\",\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF1': {'text': \"To explore the full moment tensor space requires modeling the range of possible 188 orientations. We rotate the moment tensor at 10º intervals using a sequence of three rotations 189 about the initial coordinate system of the moment tensor. Full sampling of the symmetric tensor 190 involves a 360º range about the z axis, 180º of dip, and 90º of rotation about the new, rotated z' 191 axis [Goldstein et al., 2001]. Tests with finer intervals showed little difference in the pattern of 192 misfits on the lune and minor variation in the misfit values. This involves 5832 combinations of 193 rotation angles combined with 223 γ -δ pairs. 194\",\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF2': {'text': 'synthetic trace, N s is the number of samples in each trace, and N r is the number of three- 220 component receivers. Here the squared error is normalized by station, so that stations with 221 varying amplitude contribute equally to the error.data were recorded on a network of 10 three-component broadband 225 seismometers from 8-26 January 2009, 8 of which are shown in Figure 2. The network 226 configuration was limited by the steep topography, deep ravines, thick jungle, and safety 227 considerations due to the eruptive activity. Six of the sites were equipped with Güralp CMG 40T 228 sensors (0.02-30 s) and four sites featured Güralp 3 ESPC sensors (0.02-60 s). Data were 229 recorded on 10 Reftek 130 digitizers operating in continuous mode at 100 samples per second. 230',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF3': {'text': 'Figure 3 also highlights the difference between the vertical components in the low-270 frequency band. We expect no tilt on the vertical components, yet the events are evident in the 271 400-60 s band, especially on the 60 s stations (F9NW, F9B, F9SW, F9NE). This suggests that 272 the events produce translational motion at frequencies below the instrument corner. Figure 4, 273 which shows the spectra for each of the bands analyzed at three stations, further highlights the 274 vertical component signal well below the instrument corner. The spectra also demonstrate the 275 large amplitude of the horizontal components at the lowest frequencies. Given the known 276 influence of tilt on the horizontal components at those low frequencies, a joint inversion 277 approach is required to investigate frequencies below the corner of the sensors. Only station 278 F9SE, which was ~6 km from the vent, did not record signal in the VLP band, so it was omitted 279 from further analysis. 280',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF4': {'text': 'Figure 7. They share some common features, with the dipole components in phase, and apart 351 from the 90-10 s band, dominating the source time function. There are differences in the 352 eigenvectors and eigenvalues, although the major difference is between the 90-10 s band and the 353 others. The median ratios of minimum and intermediate eigenvalues to the largest are shown in 354',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF5': {'text': ', which is dominated by the crack, and plotted it in Figure 8a. This combined mechanism 389 is actually closer to a single crack than the free inversion solution, with γ =-22 and δ=58. The 390 two-crack model of Lyons and Waite [2011] is in a similar location (γ =-20 and δ=63). The 391 relatively small range of E 2min values, compared to Figure 8c and d, throughout the lune space 392',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF6': {'text': '[2011] for detailed discussion of the source dynamics of the VLP events. Apart from the 429 quantitative description of the source-type uncertainty, the key new results involve the formal 430 moment-tensor solutions for periods below 30 s. The results of inversion for tilt-affected and tilt- 431 dominated signals are consistent with forward modeling by Lyons et al. [2012] of tilt derived 432',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF7': {'text': 'Figure 2. Locations of stations used in this study (circles) plotted on a shaded relief map of 545 Fuego volcano (a). Contour interval is 200 m. The red dots represent stations with 30 s sensors; 546 the blue dots represent 60 s sensors. The orange star marks the approximate location of the 547 summit vent. Part (b) shows the location of Fuego (arrow) in the chain of active Guatemalan 548 volcanoes. Station F9SE was not used in the inversions because the VLP signal was not observed 549 at that distance from the source. 550 551',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF8': {'text': \"Locations of best-fit source centroids cluster near the summit. Both E 2min and g min 619 solutions are plotted by bandpass. Uncertainties on centroid locations, not shown for simplicity, 620 are 200-350 m for solutions within 5% of the minimum. Contour interval is 200 m; the 621 topography model plotted in map view and in the cross sections is the same as that used to 622 compute the Green's functions. Seismic stations are shown with dots, with red dots representing 623 stations with 30 s sensors; blue dots represent 60 s sensors as in Figure 2. The hexagon shows the 624 best-fit location from Lyons and Waite [2011]. 625 626\",\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'TABREF0': {'text': 'and in Figure 1. We refer readers to Tape and Tape [2012] for a 176 thorough description of the lune parameterization. 177Relationship between moment tensor eigenvalues, lune latitude and lune longitude 178Examples for two different Poisson ratios are shown for the crack and pipe models. 181',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF1': {'text': 'The source types are described in the next section. 355Best solution information as a function of bandpass 357Lyons and Waite [2011] is shown for comparison. In that study, error was computed 358',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF2': {'text': 'band channel corr. coef. lag [s] channel corr. coef. lag [s] channel corr. coef. lag [s]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF3': {'text': 'band channel corr. coef. lag [s] channel corr. coef. lag [s] channel corr. coef. lag [s]',\n",
       "    'latex': None,\n",
       "    'type': 'table'}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Tilt change recorded by broadband seismometer prior to 476 small phreatic explosion of Meakan-dake volcano',\n",
       "    'authors': [{'first': 'H', 'middle': [], 'last': 'Aoyama', 'suffix': ''},\n",
       "     {'first': 'H', 'middle': [], 'last': 'Oshima', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'Geophys. Res. Lett',\n",
       "    'volume': '477',\n",
       "    'issn': '6',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Very long period conduit oscillations induced by rockfalls at 479',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Source mechanisms of explosions at Stromboli Volcano',\n",
       "    'authors': [{'first': 'G', 'middle': [], 'last': 'Milana', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Scarpa', 'suffix': ''}],\n",
       "    'year': 2003,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': 'determined from moment-tensor inversions of very-long-period data',\n",
       "    'authors': [{'first': '', 'middle': [], 'last': 'Italy', 'suffix': ''}],\n",
       "    'year': None,\n",
       "    'venue': 'J. Geophys. 483 Res',\n",
       "    'volume': '108',\n",
       "    'issn': 'B1',\n",
       "    'pages': '',\n",
       "    'other_ids': {'doi': ['10.1029/2002JB001919']},\n",
       "    'links': None},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'A multi-decadal view of seismic methods for detecting 485 precursors of magma movement and eruption',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['S'], 'last': 'Matoza', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'J. Volcanol. Geotherm. Res',\n",
       "    'volume': '252',\n",
       "    'issn': '',\n",
       "    'pages': '108--175',\n",
       "    'other_ids': {'doi': ['10.1016/j.jvolgeores.2012.11.013']},\n",
       "    'links': '129636416'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': 'Network Sensitivity Solutions for Regional 488 Moment-Tensor Inversions',\n",
       "    'authors': [{'first': 'S', 'middle': ['R'], 'last': 'Ford', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': ['S'], 'last': 'Dreger', 'suffix': ''},\n",
       "     {'first': 'W', 'middle': ['R'], 'last': 'Walter', 'suffix': ''}],\n",
       "    'year': 1962,\n",
       "    'venue': 'Bull. Seism. Soc. Am',\n",
       "    'volume': '100',\n",
       "    'issn': '5A',\n",
       "    'pages': '',\n",
       "    'other_ids': {'doi': ['10.1785/0120090140']},\n",
       "    'links': '73647128'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'Classical Mechanics',\n",
       "    'authors': [{'first': 'H',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'C', 'middle': ['P J'], 'last': 'Poole', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': ['L'], 'last': 'Safko', 'suffix': ''}],\n",
       "    'year': 2001,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Dynamics of explosive volcanism at Fuego volcano imaged 492 with very long period seismicity',\n",
       "    'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "     {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '116',\n",
       "    'issn': 'B9',\n",
       "    'pages': '',\n",
       "    'other_ids': {'doi': ['10.1029/2011jb008521']},\n",
       "    'links': '129058472'},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Tilt prior to explosions and the 494 effect of topography on ultra-long-period seismic records at Fuego volcano',\n",
       "    'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "     {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Ichihara', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['M'], 'last': 'Lees', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Very-long-period pulses at Asama volcano, central Japan, 497 inferred from dense seismic observations',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '185',\n",
       "    'issn': '1',\n",
       "    'pages': '265--282',\n",
       "    'other_ids': {'doi': ['10.1111/j.1365-246X.2011.04938.x']},\n",
       "    'links': '131264104'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'A waveform inversion including tilt: method and 500 simple tests',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '184',\n",
       "    'issn': '2',\n",
       "    'pages': '907--918',\n",
       "    'other_ids': {'doi': ['10.1111/j.1365-246X.2010.04892.x']},\n",
       "    'links': None},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'Source mechanism of small long-period events at 503',\n",
       "    'authors': [{'first': 'T', 'middle': ['D'], 'last': 'Moran', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Mikesell', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'Helens in July 2005 using template matching, phase-weighted stacking, and 504 full-waveform inversion',\n",
       "    'authors': [{'first': 'Mount', 'middle': [], 'last': 'St', 'suffix': ''}],\n",
       "    'year': None,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '120',\n",
       "    'issn': '9',\n",
       "    'pages': '6351--6364',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Geophysical Data Analysis: Discrete Inverse Theory',\n",
       "    'authors': [{'first': 'W', 'middle': [], 'last': 'Menke', 'suffix': ''}],\n",
       "    'year': 1989,\n",
       "    'venue': '',\n",
       "    'volume': '507',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '117957106'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'A free-surface boundary condition for including 3D 509 topography in the finite-difference method',\n",
       "    'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''}],\n",
       "    'year': 1997,\n",
       "    'venue': 'Bull. Seism. Soc. Am',\n",
       "    'volume': '87',\n",
       "    'issn': '2',\n",
       "    'pages': '494--515',\n",
       "    'other_ids': {},\n",
       "    'links': '129694378'},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Waveform inversion of very long 511 period impulsive signals associated with magmatic injection beneath Kilauea Volcano',\n",
       "    'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': [], 'last': 'Kedar', 'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'The response of the horizontal pendulum seismometer to Rayleigh and 514 Love waves, tilt, and free oscillations of the Earth',\n",
       "    'authors': [{'first': 'P',\n",
       "      'middle': ['W'],\n",
       "      'last': 'Rogers',\n",
       "      'suffix': ''}],\n",
       "    'year': 1968,\n",
       "    'venue': 'Bull. Seism. Soc. Am',\n",
       "    'volume': '58',\n",
       "    'issn': '5',\n",
       "    'pages': '1384--515',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'Noise reduction and detection of weak, coherent signals 517 through phase-weighted stacks',\n",
       "    'authors': [{'first': 'M', 'middle': [], 'last': 'Schimmel', 'suffix': ''},\n",
       "     {'first': 'H', 'middle': [], 'last': 'Paulssen', 'suffix': ''}],\n",
       "    'year': 1997,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '130',\n",
       "    'issn': '',\n",
       "    'pages': '497--505',\n",
       "    'other_ids': {},\n",
       "    'links': '41492712'},\n",
       "   'BIBREF23': {'ref_id': 'b23',\n",
       "    'title': 'A geometric setting for moment tensors',\n",
       "    'authors': [{'first': 'W', 'middle': [], 'last': 'Tape', 'suffix': ''},\n",
       "     {'first': 'C', 'middle': [], 'last': 'Tape', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '190',\n",
       "    'issn': '',\n",
       "    'pages': '476--498',\n",
       "    'other_ids': {'doi': ['10.1111/j.1365-246X.2012.05491.x']},\n",
       "    'links': '122638175'},\n",
       "   'BIBREF24': {'ref_id': 'b24',\n",
       "    'title': 'Very-Long-Period Seismicity at Active Volcanoes: Source Mechanisms',\n",
       "    'authors': [{'first': 'G',\n",
       "      'middle': ['P'],\n",
       "      'last': 'Waite',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': '522 Encyclopedia of Earthquake Engineering',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '135418931'},\n",
       "   'BIBREF26': {'ref_id': 'b26',\n",
       "    'title': 'Eruption dynamics at Mount St. Helens 526 imaged from broadband seismic waveforms: Interaction of the shallow magmatic and 527 hydrothermal systems',\n",
       "    'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': ['B'], 'last': 'Dawson', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '113',\n",
       "    'issn': 'B2',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '14550325'},\n",
       "   'BIBREF27': {'ref_id': 'b27',\n",
       "    'title': 'Variability in eruption style and associated 529 very-long-period earthquakes at Fuego volcano, Guatemala',\n",
       "    'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': ['A'], 'last': 'Nadeau', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '118',\n",
       "    'issn': '4',\n",
       "    'pages': '1526--1533',\n",
       "    'other_ids': {'doi': ['10.1002/jgrb.50075']},\n",
       "    'links': None},\n",
       "   'BIBREF28': {'ref_id': 'b28',\n",
       "    'title': 'Spherical-spline parameterization of three-dimensional Earth 532 models',\n",
       "    'authors': [{'first': 'Z', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'F', 'middle': ['A'], 'last': 'Dahlen', 'suffix': ''}],\n",
       "    'year': 1995,\n",
       "    'venue': 'Geophys. Res. Lett',\n",
       "    'volume': '22',\n",
       "    'issn': '22',\n",
       "    'pages': '3099--3102',\n",
       "    'other_ids': {'doi': ['10.1029/95GL03080']},\n",
       "    'links': '129880072'},\n",
       "   'BIBREF29': {'ref_id': 'b29',\n",
       "    'title': 'Data (black), free-inversion synthetics (red), and best-fitting constrained inversion 651 synthetics (blue) for each of the four bandpasses: a) 30-10 s; b) 90-10 s; c) 120-10 s',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '6',\n",
       "    'issn': '',\n",
       "    'pages': '400--652',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF30': {'ref_id': 'b30',\n",
       "    'title': 'The free-inversion results are generally better than the constrained inversion, as expected',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF31': {'ref_id': 'b31',\n",
       "    'title': 'Source time functions from free inversions in the a) 30-10, b) 90-10, c) 120-10',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF32': {'ref_id': 'b32',\n",
       "    'title': '400-60 s pass bands are largely consistent, although the 90-10 s band has notable differences',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF33': {'ref_id': 'b33',\n",
       "    'title': 'Note the difference in amplitude and time scales for each of the four plots',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF34': {'ref_id': 'b34',\n",
       "    'title': 'Error (E 2 ) by fixed moment-tensor solution plotted using the indicated color scale for 710 each of the bandpasses in Figure 7: a) 30-10, b) 90-10, c) 120-10, and d) 400-60 s. The γ−δ pairs 711 computed from the point-by-point eigenvalue analysis for the best free inversion for each of four 712 bandpasses are plotted in red. The lack of consistency in the free inversion for the 90-10 s band 713 is reflected in the lack of a resolved moment-tensor type',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF35': {'ref_id': 'b35',\n",
       "    'title': 'The combined crack-pipe tensor of Lyons and Waite',\n",
       "    'authors': [],\n",
       "    'year': 2011,\n",
       "    'venue': '',\n",
       "    'volume': '715',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None}}},\n",
       " 'latex_parse': None}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Nonlinear inversion of tilt-affected very long period records of explosive eruptions at Fuego volcano: INVERSION OF TILT-AFFECTED VLP EVENTS',\n",
       " 'authors': [{'first': 'Gregory',\n",
       "   'middle': ['P.'],\n",
       "   'last': 'Waite',\n",
       "   'suffix': ''},\n",
       "  {'first': 'Federica', 'middle': [], 'last': 'Lanza', 'suffix': ''}],\n",
       " 'abstract': None,\n",
       " 'year': '2016',\n",
       " 'arxiv_id': None,\n",
       " 'acl_id': None,\n",
       " 'pmc_id': None,\n",
       " 'pubmed_id': None,\n",
       " 'doi': '10.1002/2016jb013287',\n",
       " 'venue': 'Journal of Geophysical Research: Solid Earth',\n",
       " 'journal': 'Journal of Geophysical Research'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]['metadata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'b0',\n",
       "  'title': 'Tilt change recorded by broadband seismometer prior to 476 small phreatic explosion of Meakan-dake volcano',\n",
       "  'authors': [{'first': 'H', 'middle': [], 'last': 'Aoyama', 'suffix': ''},\n",
       "   {'first': 'H', 'middle': [], 'last': 'Oshima', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'Geophys. Res. Lett',\n",
       "  'volume': '477',\n",
       "  'issn': '6',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF1': {'ref_id': 'b1',\n",
       "  'title': 'Very long period conduit oscillations induced by rockfalls at 479',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF4': {'ref_id': 'b4',\n",
       "  'title': 'Source mechanisms of explosions at Stromboli Volcano',\n",
       "  'authors': [{'first': 'G', 'middle': [], 'last': 'Milana', 'suffix': ''},\n",
       "   {'first': 'R', 'middle': [], 'last': 'Scarpa', 'suffix': ''}],\n",
       "  'year': 2003,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF5': {'ref_id': 'b5',\n",
       "  'title': 'determined from moment-tensor inversions of very-long-period data',\n",
       "  'authors': [{'first': '', 'middle': [], 'last': 'Italy', 'suffix': ''}],\n",
       "  'year': None,\n",
       "  'venue': 'J. Geophys. 483 Res',\n",
       "  'volume': '108',\n",
       "  'issn': 'B1',\n",
       "  'pages': '',\n",
       "  'other_ids': {'doi': ['10.1029/2002JB001919']},\n",
       "  'links': None},\n",
       " 'BIBREF6': {'ref_id': 'b6',\n",
       "  'title': 'A multi-decadal view of seismic methods for detecting 485 precursors of magma movement and eruption',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'R', 'middle': ['S'], 'last': 'Matoza', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': 'J. Volcanol. Geotherm. Res',\n",
       "  'volume': '252',\n",
       "  'issn': '',\n",
       "  'pages': '108--175',\n",
       "  'other_ids': {'doi': ['10.1016/j.jvolgeores.2012.11.013']},\n",
       "  'links': '129636416'},\n",
       " 'BIBREF7': {'ref_id': 'b7',\n",
       "  'title': 'Network Sensitivity Solutions for Regional 488 Moment-Tensor Inversions',\n",
       "  'authors': [{'first': 'S', 'middle': ['R'], 'last': 'Ford', 'suffix': ''},\n",
       "   {'first': 'D', 'middle': ['S'], 'last': 'Dreger', 'suffix': ''},\n",
       "   {'first': 'W', 'middle': ['R'], 'last': 'Walter', 'suffix': ''}],\n",
       "  'year': 1962,\n",
       "  'venue': 'Bull. Seism. Soc. Am',\n",
       "  'volume': '100',\n",
       "  'issn': '5A',\n",
       "  'pages': '',\n",
       "  'other_ids': {'doi': ['10.1785/0120090140']},\n",
       "  'links': '73647128'},\n",
       " 'BIBREF8': {'ref_id': 'b8',\n",
       "  'title': 'Classical Mechanics',\n",
       "  'authors': [{'first': 'H', 'middle': [], 'last': 'Goldstein', 'suffix': ''},\n",
       "   {'first': 'C', 'middle': ['P J'], 'last': 'Poole', 'suffix': ''},\n",
       "   {'first': 'L', 'middle': ['L'], 'last': 'Safko', 'suffix': ''}],\n",
       "  'year': 2001,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF9': {'ref_id': 'b9',\n",
       "  'title': 'Dynamics of explosive volcanism at Fuego volcano imaged 492 with very long period seismicity',\n",
       "  'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "   {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '116',\n",
       "  'issn': 'B9',\n",
       "  'pages': '',\n",
       "  'other_ids': {'doi': ['10.1029/2011jb008521']},\n",
       "  'links': '129058472'},\n",
       " 'BIBREF10': {'ref_id': 'b10',\n",
       "  'title': 'Tilt prior to explosions and the 494 effect of topography on ultra-long-period seismic records at Fuego volcano',\n",
       "  'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "   {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "   {'first': 'M', 'middle': [], 'last': 'Ichihara', 'suffix': ''},\n",
       "   {'first': 'J', 'middle': ['M'], 'last': 'Lees', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF12': {'ref_id': 'b12',\n",
       "  'title': 'Very-long-period pulses at Asama volcano, central Japan, 497 inferred from dense seismic observations',\n",
       "  'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "   {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '185',\n",
       "  'issn': '1',\n",
       "  'pages': '265--282',\n",
       "  'other_ids': {'doi': ['10.1111/j.1365-246X.2011.04938.x']},\n",
       "  'links': '131264104'},\n",
       " 'BIBREF13': {'ref_id': 'b13',\n",
       "  'title': 'A waveform inversion including tilt: method and 500 simple tests',\n",
       "  'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "   {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''},\n",
       "   {'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '184',\n",
       "  'issn': '2',\n",
       "  'pages': '907--918',\n",
       "  'other_ids': {'doi': ['10.1111/j.1365-246X.2010.04892.x']},\n",
       "  'links': None},\n",
       " 'BIBREF15': {'ref_id': 'b15',\n",
       "  'title': 'Source mechanism of small long-period events at 503',\n",
       "  'authors': [{'first': 'T', 'middle': ['D'], 'last': 'Moran', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Mikesell', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF16': {'ref_id': 'b16',\n",
       "  'title': 'Helens in July 2005 using template matching, phase-weighted stacking, and 504 full-waveform inversion',\n",
       "  'authors': [{'first': 'Mount', 'middle': [], 'last': 'St', 'suffix': ''}],\n",
       "  'year': None,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '120',\n",
       "  'issn': '9',\n",
       "  'pages': '6351--6364',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF17': {'ref_id': 'b17',\n",
       "  'title': 'Geophysical Data Analysis: Discrete Inverse Theory',\n",
       "  'authors': [{'first': 'W', 'middle': [], 'last': 'Menke', 'suffix': ''}],\n",
       "  'year': 1989,\n",
       "  'venue': '',\n",
       "  'volume': '507',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '117957106'},\n",
       " 'BIBREF18': {'ref_id': 'b18',\n",
       "  'title': 'A free-surface boundary condition for including 3D 509 topography in the finite-difference method',\n",
       "  'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "   {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''}],\n",
       "  'year': 1997,\n",
       "  'venue': 'Bull. Seism. Soc. Am',\n",
       "  'volume': '87',\n",
       "  'issn': '2',\n",
       "  'pages': '494--515',\n",
       "  'other_ids': {},\n",
       "  'links': '129694378'},\n",
       " 'BIBREF19': {'ref_id': 'b19',\n",
       "  'title': 'Waveform inversion of very long 511 period impulsive signals associated with magmatic injection beneath Kilauea Volcano',\n",
       "  'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "   {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''},\n",
       "   {'first': 'S', 'middle': [], 'last': 'Kedar', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF21': {'ref_id': 'b21',\n",
       "  'title': 'The response of the horizontal pendulum seismometer to Rayleigh and 514 Love waves, tilt, and free oscillations of the Earth',\n",
       "  'authors': [{'first': 'P', 'middle': ['W'], 'last': 'Rogers', 'suffix': ''}],\n",
       "  'year': 1968,\n",
       "  'venue': 'Bull. Seism. Soc. Am',\n",
       "  'volume': '58',\n",
       "  'issn': '5',\n",
       "  'pages': '1384--515',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF22': {'ref_id': 'b22',\n",
       "  'title': 'Noise reduction and detection of weak, coherent signals 517 through phase-weighted stacks',\n",
       "  'authors': [{'first': 'M', 'middle': [], 'last': 'Schimmel', 'suffix': ''},\n",
       "   {'first': 'H', 'middle': [], 'last': 'Paulssen', 'suffix': ''}],\n",
       "  'year': 1997,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '130',\n",
       "  'issn': '',\n",
       "  'pages': '497--505',\n",
       "  'other_ids': {},\n",
       "  'links': '41492712'},\n",
       " 'BIBREF23': {'ref_id': 'b23',\n",
       "  'title': 'A geometric setting for moment tensors',\n",
       "  'authors': [{'first': 'W', 'middle': [], 'last': 'Tape', 'suffix': ''},\n",
       "   {'first': 'C', 'middle': [], 'last': 'Tape', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Geophys. J. Int',\n",
       "  'volume': '190',\n",
       "  'issn': '',\n",
       "  'pages': '476--498',\n",
       "  'other_ids': {'doi': ['10.1111/j.1365-246X.2012.05491.x']},\n",
       "  'links': '122638175'},\n",
       " 'BIBREF24': {'ref_id': 'b24',\n",
       "  'title': 'Very-Long-Period Seismicity at Active Volcanoes: Source Mechanisms',\n",
       "  'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': '522 Encyclopedia of Earthquake Engineering',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '135418931'},\n",
       " 'BIBREF26': {'ref_id': 'b26',\n",
       "  'title': 'Eruption dynamics at Mount St. Helens 526 imaged from broadband seismic waveforms: Interaction of the shallow magmatic and 527 hydrothermal systems',\n",
       "  'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "   {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': ['B'], 'last': 'Dawson', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '113',\n",
       "  'issn': 'B2',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '14550325'},\n",
       " 'BIBREF27': {'ref_id': 'b27',\n",
       "  'title': 'Variability in eruption style and associated 529 very-long-period earthquakes at Fuego volcano, Guatemala',\n",
       "  'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "   {'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "   {'first': 'P', 'middle': ['A'], 'last': 'Nadeau', 'suffix': ''}],\n",
       "  'year': 2013,\n",
       "  'venue': 'J. Geophys. Res',\n",
       "  'volume': '118',\n",
       "  'issn': '4',\n",
       "  'pages': '1526--1533',\n",
       "  'other_ids': {'doi': ['10.1002/jgrb.50075']},\n",
       "  'links': None},\n",
       " 'BIBREF28': {'ref_id': 'b28',\n",
       "  'title': 'Spherical-spline parameterization of three-dimensional Earth 532 models',\n",
       "  'authors': [{'first': 'Z', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'F', 'middle': ['A'], 'last': 'Dahlen', 'suffix': ''}],\n",
       "  'year': 1995,\n",
       "  'venue': 'Geophys. Res. Lett',\n",
       "  'volume': '22',\n",
       "  'issn': '22',\n",
       "  'pages': '3099--3102',\n",
       "  'other_ids': {'doi': ['10.1029/95GL03080']},\n",
       "  'links': '129880072'},\n",
       " 'BIBREF29': {'ref_id': 'b29',\n",
       "  'title': 'Data (black), free-inversion synthetics (red), and best-fitting constrained inversion 651 synthetics (blue) for each of the four bandpasses: a) 30-10 s; b) 90-10 s; c) 120-10 s',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '6',\n",
       "  'issn': '',\n",
       "  'pages': '400--652',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF30': {'ref_id': 'b30',\n",
       "  'title': 'The free-inversion results are generally better than the constrained inversion, as expected',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF31': {'ref_id': 'b31',\n",
       "  'title': 'Source time functions from free inversions in the a) 30-10, b) 90-10, c) 120-10',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF32': {'ref_id': 'b32',\n",
       "  'title': '400-60 s pass bands are largely consistent, although the 90-10 s band has notable differences',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF33': {'ref_id': 'b33',\n",
       "  'title': 'Note the difference in amplitude and time scales for each of the four plots',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF34': {'ref_id': 'b34',\n",
       "  'title': 'Error (E 2 ) by fixed moment-tensor solution plotted using the indicated color scale for 710 each of the bandpasses in Figure 7: a) 30-10, b) 90-10, c) 120-10, and d) 400-60 s. The γ−δ pairs 711 computed from the point-by-point eigenvalue analysis for the best free inversion for each of four 712 bandpasses are plotted in red. The lack of consistency in the free inversion for the 90-10 s band 713 is reflected in the lack of a resolved moment-tensor type',\n",
       "  'authors': [],\n",
       "  'year': None,\n",
       "  'venue': '',\n",
       "  'volume': '8',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None},\n",
       " 'BIBREF35': {'ref_id': 'b35',\n",
       "  'title': 'The combined crack-pipe tensor of Lyons and Waite',\n",
       "  'authors': [],\n",
       "  'year': 2011,\n",
       "  'venue': '',\n",
       "  'volume': '715',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': None}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]['grobid_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]['grobid_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = all_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for paper in all_papers if paper['metadata']['acl_id']!=None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41660\n"
     ]
    }
   ],
   "source": [
    "with open(\"acl_only_json_list_10000.json\", \"r\") as read_file:\n",
    "    all_articles = json.load(read_file)\n",
    "print(len(all_articles))\n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ подборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### проверка наличия названия секции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_paper_ids = [article['paper_id'] for article in all_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'authors', 'abstract', 'year', 'arxiv_id', 'acl_id', 'pmc_id', 'pubmed_id', 'doi', 'venue', 'journal'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['metadata'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### проверка наличия текста и названия секций во всех статьях в grobid части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41439"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_articles if article['grobid_parse']['body_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_bofy_text = [article['paper_id'] for article in all_articles if not article['grobid_parse']['body_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 {'paper_id': '60131735', 'metadata': {'title': 'Breadth and D e p t h of Semant ic Lexicons Proceedings of a Workshop Sponsored by the Special Interest Group on the Lexicon of the Associat ion for Computat ional Linguistics', 'authors': [{'first': 'Evelyne', 'middle': [], 'last': 'Viegas', 'suffix': ''}], 'abstract': 'Preface. Contributors. Introduction: E. Viegas. I. Lexical Rules and Underspecification. II. Breadth of Semantic Lexicons. III. Depth of Semantic Lexicons. IV. Lexical Semantics and Pragmatics. Subject Index. Author Index.', 'year': '1999', 'arxiv_id': None, 'acl_id': 'W96-0300', 'pmc_id': None, 'pubmed_id': None, 'doi': '10.1007/978-94-017-0952-1', 'venue': 'Text, Speech and Language Technology', 'journal': 'Text, Speech and Language Technology'}, 's2_pdf_hash': 'fef5115fca1124e5f994cd49414c743ca005b853', 'grobid_parse': {'abstract': [], 'body_text': [], 'ref_entries': {'TABREF0': {'text': 'Introduction', 'latex': None, 'type': 'table'}}, 'bib_entries': {}}, 'latex_parse': None}\n",
      "====================\n",
      "186 {'paper_id': '7473534', 'metadata': {'title': 'Interfacing Ontologies and Lexical Resources', 'authors': [{'first': 'Laurent', 'middle': [], 'last': 'Prévot', 'suffix': ''}, {'first': 'Stefano', 'middle': [], 'last': 'Borgo', 'suffix': ''}, {'first': 'Alessandro', 'middle': [], 'last': 'Oltramari', 'suffix': ''}], 'abstract': 'During the last few years, a number of works aiming at interfacing ontologies and lexical resources have been initiated. This paper aims at clarifying the current picture of this domain. It compares ontologies built following different methodologies and analyses their combination with lexical resources. A point defended in the paper is that different methodologies lead to very different characteristics for the resulting resources. We classify these methodologies show how actual projects fit into this classification.', 'year': '2005', 'arxiv_id': None, 'acl_id': 'I05-7013', 'pmc_id': None, 'pubmed_id': None, 'doi': '10.1017/CBO9780511676536.011', 'venue': 'Proceedings of OntoLex 2005 - Ontologies and Lexical Resources', 'journal': 'Ontology and the Lexicon'}, 's2_pdf_hash': '01c53413fa384b32929e074fb60d2dfa0318d7b9', 'grobid_parse': {'abstract': [], 'body_text': [], 'ref_entries': {}, 'bib_entries': {}}, 'latex_parse': None}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for num,paper_id in enumerate(acl_ids_not_bofy_text):\n",
    "    if num == 2:\n",
    "        break\n",
    "    id_lst = acl_paper_ids.index(paper_id)\n",
    "    print(id_lst,all_articles[id_lst])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '7473534',\n",
       " 'metadata': {'title': 'Interfacing Ontologies and Lexical Resources',\n",
       "  'authors': [{'first': 'Laurent',\n",
       "    'middle': [],\n",
       "    'last': 'Prévot',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Stefano', 'middle': [], 'last': 'Borgo', 'suffix': ''},\n",
       "   {'first': 'Alessandro', 'middle': [], 'last': 'Oltramari', 'suffix': ''}],\n",
       "  'abstract': 'During the last few years, a number of works aiming at interfacing ontologies and lexical resources have been initiated. This paper aims at clarifying the current picture of this domain. It compares ontologies built following different methodologies and analyses their combination with lexical resources. A point defended in the paper is that different methodologies lead to very different characteristics for the resulting resources. We classify these methodologies show how actual projects fit into this classification.',\n",
       "  'year': '2005',\n",
       "  'arxiv_id': None,\n",
       "  'acl_id': 'I05-7013',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.1017/CBO9780511676536.011',\n",
       "  'venue': 'Proceedings of OntoLex 2005 - Ontologies and Lexical Resources',\n",
       "  'journal': 'Ontology and the Lexicon'},\n",
       " 's2_pdf_hash': '01c53413fa384b32929e074fb60d2dfa0318d7b9',\n",
       " 'grobid_parse': {'abstract': [],\n",
       "  'body_text': [],\n",
       "  'ref_entries': {},\n",
       "  'bib_entries': {}},\n",
       " 'latex_parse': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[186]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles[0]['grobid_parse']['body_text'][0]['section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_with_sect = dict()\n",
    "for article in all_articles:\n",
    "    for sections in article['grobid_parse']['body_text']:\n",
    "        if sections['section']:\n",
    "            if article['paper_id'] in article_with_sect:\n",
    "                article_with_sect[article['paper_id']] +=1\n",
    "            else:\n",
    "                article_with_sect[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_with_sect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '14472576',\n",
       " 'metadata': {'title': 'Building a Semantic Parser Overnight',\n",
       "  'authors': [{'first': 'Yushi', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Jonathan', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "   {'first': 'Percy', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "  'abstract': 'How do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.',\n",
       "  'year': '2015',\n",
       "  'arxiv_id': None,\n",
       "  'acl_id': 'P15-1129',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.3115/v1/P15-1129',\n",
       "  'venue': 'ACL',\n",
       "  'journal': 'Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)'},\n",
       " 's2_pdf_hash': 'f3de408be7d2e2720a61451bd196ac7e1ed9363a',\n",
       " 'grobid_parse': {'abstract': [{'text': 'AbstractHow do we build a semantic parser in a new domain starting with zero training examples? We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances. The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily). We then use crowdsourcing to paraphrase these canonical utterances into natural utterances. The resulting data is used to train the semantic parser. We further study the role of compositionality in the resulting paraphrases. Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Abstract'}],\n",
       "  'body_text': [{'text': 'By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Kushman and Barzilay, 2013) . Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain-for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start?In this paper, we advocate a functionalitydriven process for rapidly building a semantic * Both authors equally contributed to the paper. ...',\n",
       "    'cite_spans': [{'start': 173,\n",
       "      'end': 197,\n",
       "      'text': '(Zelle and Mooney, 1996;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF25'},\n",
       "     {'start': 198,\n",
       "      'end': 228,\n",
       "      'text': 'Zettlemoyer and Collins, 2005;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 229,\n",
       "      'end': 248,\n",
       "      'text': 'Liang et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 249,\n",
       "      'end': 269,\n",
       "      'text': 'Berant et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 270,\n",
       "      'end': 295,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 296,\n",
       "      'end': 323,\n",
       "      'text': 'Kushman and Barzilay, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': '(1) by builder (∼30 minutes)(2) via domain-general grammar (3) via crowdsourcing (∼5 hours) (4) by training a paraphrasing model Figure 1 : Functionality-driven process for building semantic parsers. The two red boxes are the domain-specific parts provided by the builder of the semantic parser, and the other two are generated by the framework.parser in a new domain. At a high-level, we seek to minimize the amount of work needed for a new domain by factoring out the domaingeneral aspects (done by our framework) from the domain-specific ones (done by the builder of the semantic parser). We assume that the builder already has the desired functionality of the semantic parser in mind-e.g., the publications database is set up and the schema is fixed. Figure 1 depicts the functionality-driven process: First, the builder writes a seed lexicon specifying a canonical phrase (\"publication date\") for each predicate (publicationDate).Second, our framework uses a domain-general grammar, along with the seed lexicon and the database, to automatically generate a few hundred canonical utterances paired with their logical forms (e.g., \"article that has the largest publication date\" and arg max(type.article, publicationDate)). These utterances need not be the most elegant, but they should retain the semantics of the logical forms. Third, the builder leverages crowdsourcing to paraphrase each canonical utterance into a few natural utterances (e.g., \"what is the newest published article?\"). Finally, our framework uses this data to train a semantic parser.Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014) . In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule.In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014) . Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain.Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny, as the structural mismatch between logic and language is the chief source of difficulty in semantic parsing (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014) .There are two important questions here. First, is it possible to design a simple grammar that simultaneously generates both logical forms and canonical utterances so that the utterances are understandable by a human? In Section 3, we show how to choose appropriate canonical utterances to maximize alignment with the logical forms.Second, our grammar can generate an infinite number of canonical utterances. How many do we need for adequate coverage? Certainly, single relations is insufficient: just knowing that \"publication date of X\" paraphrases to \"when X was published\" would offer insufficient information to generalize to \"articles that came after X\" mapping to \"article whose publication date is larger than publication date of X\". We call this phenomena sublexical compositionality-when a short lexical unit (\"came after\") maps onto a multi-predicate logical form. Our hypothesis is that the sublexical compositional units are small, so we only need to crowdsource a small number of canonical utterances to learn about most of the language variability in the given domain (Section 4).We applied our functionality-driven process to seven domains, which were chosen to explore particular types of phenomena, such as spatial language, temporal language, and high-arity relations. This resulted in seven new semantic parsing datasets, totaling 12.6K examples. Our approach, which was not tuned on any one domain, was able to obtain an average accuracy of 59% over all domains. On the day of this paper submission, we created an eighth domain and trained a semantic parser overnight.',\n",
       "    'cite_spans': [{'start': 1832,\n",
       "      'end': 1853,\n",
       "      'text': '(Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 2121,\n",
       "      'end': 2142,\n",
       "      'text': '(Rangel et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 2415,\n",
       "      'end': 2441,\n",
       "      'text': '(Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2442,\n",
       "      'end': 2465,\n",
       "      'text': 'Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 3174,\n",
       "      'end': 3194,\n",
       "      'text': '(Liang et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 3195,\n",
       "      'end': 3220,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 3221,\n",
       "      'end': 3244,\n",
       "      'text': 'Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'}],\n",
       "    'ref_spans': [{'start': 129,\n",
       "      'end': 137,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 755,\n",
       "      'end': 763,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In our functionality-driven process (Figure 1) , there are two parties: the builder, who provides domain-specific information, and the framework, which provides domain-general information. We assume that the builder has a fixed database w, represented as a set of triples (e 1 , p, e 2 ), where e 1 and e 2 are entities (e.g., article1, 2015) and p is a property (e.g., publicationDate). The database w can be queried using lambda DCS logical forms, described further in Section 2.1.The builder supplies a seed lexicon L, which contains for each database property p (e.g., publicationDate) a lexical entry of the form t → s[p] , where t is a natural language phrase (e.g., \"publication date\") and s is a syntactic cat-egory (e.g., RELNP). In addition, L contains two typical entities for each semantic type in the database (e.g., alice → NP[alice] for the type person). The purpose of L is to simply connect each predicate with some representation in natural language.The framework supplies a grammar G, which specifies the modes of composition, both on logical forms and canonical utterances. Formally, G is a set of rules of the form α 1 . . . α n → s[z] , where α 1 . . . α n is a sequence of tokens or categories, s is a syntactic category and z is the logical form constructed. For example, one rule in (r) .x] , which constructs z by reversing the binary predicate r and joining it with a the unary predicate x. We use the rules G ∪ L to generate a set of (z, c) pairs, where z is a logical form (e.g., R(publicationDate).article1), and c is the corresponding canonical utterance (e.g., \"publication date of article 1\"). The set of (z, c) is denoted by GEN(G ∪ L). See Section 3 for details.G is RELNP[r] of NP[x] → NP[RNext, the builder (backed by crowdsourcing) paraphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., \"when was article 1 published?\"). This defines a set of training examples D = {(x, c, z)}, for each (z, c) ∈ GEN(G ∪ L) and x ∈ P(c). The crowdsourcing setup is detailed in Section 5.Finally, the framework trains a semantic parser on D. Our semantic parser is a log-linear distribution p θ (z, c | x, w) over logical forms and canonical utterances specified by the grammar G. Note that the grammar G will in general not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014) .To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G ∪ L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(·) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser p θ (z, c | x, w).',\n",
       "    'cite_spans': [{'start': 327,\n",
       "      'end': 342,\n",
       "      'text': 'article1, 2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1308,\n",
       "      'end': 1311,\n",
       "      'text': '(r)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2374,\n",
       "      'end': 2397,\n",
       "      'text': 'Berant and Liang (2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'}],\n",
       "    'ref_spans': [{'start': 36,\n",
       "      'end': 46,\n",
       "      'text': '(Figure 1)',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details.Every logical form z in this paper is either a unary (denoting a set of entities) or a binary (denoting a set of entity-pairs). In the base case, each entity e (e.g., 2015) is a unary denoting the singleton set: e w = {e}; and each property p (e.g., publicationDate) is a binary denoting all entitypairs (e 1 , e 2 ) that satisfy the property p. Unaries and binaries can be composed: Given a binary b and unary u, the join b.u denotes all entities e 1 for which there exists an e 2 ∈ u w with (e 1 , e 2 ) ∈ b w . For example, publicationDate.2015 denote entities published in 2015.The intersection u 1 u 2 , union u 1 u 2 , complement ¬u denote the corresponding set operations on the denotations. We let R(b) denote the reversal of b: (e 1 , e 2 ) ∈ b w iff (e 2 , e 1 ) ∈ R(b) w . This allows us to define R(publicationDate).article1 as the publication date of article 1. We also include aggregation operations (count(u), sum(u) and average(u, b)), and superlatives (argmax(u, b)).Finally, we can construct binaries using lambda abstraction: λx.u denotes a set of (e 1 , e 2 ) where e 1 ∈ u[x/e 2 ] w and u[x/e 2 ] is the logical form where free occurrences of x are replaced with e 2 . For example, R(λx.count(R(cites).x)) denotes the set of entities (e 1 , e 2 ), where e 2 is the number of entities that e 1 cites.',\n",
       "    'cite_spans': [{'start': 167,\n",
       "      'end': 179,\n",
       "      'text': 'Liang (2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our functionality-driven process hinges on having a domain-general grammar that can connect logical forms with canonical utterances compositionally. The motivation is that while it is hard to write a grammar that parses all utterances, it is possible to write one that generates one canonical utterance for each logical form. To make this explicit: Assumption 1 (Canonical compositionality) Using a small grammar, all logical forms expressible in natural language can be realized compositionally based on the logical form.Grammar. We target database querying applications, where the parser needs to handle superlatives, comparatives, negation, and coordination. We define a simple grammar that captures these forms of compositionality using canonical utterances in a domain-general way. Figure 2 illustrates a derivation produced by the grammar. The seed lexicon specified by the builder contains canonical utterances for types, entities, and properties. All types (e.g., person) have the syntactic category TYPENP, and all entities (e.g., Figure 2: Deriving a logical form z (red) and a canonical utterance c (green) from the grammar G. Each node contains a syntactic category and a logical form, which is generated by applying a rule. Nodes with only leaves as children are produced using the seed lexicon; all other nodes are produced by rules in the domain-general grammar.alice) are ENTITYNP\\'s. Unary predicates are realized as verb phrases VP (e.g., \"has a private bath\"). The builder can choose to represent binaries as either relational noun phrases (RELNP) or generalized transitive verbs (VP/NP). RELNP\\'s are usually used to describe functional properties (e.g., \"publication date\"), especially numerical properties. VP/NP\\'s include transitive verbs (\"cites\") but also longer phrases with the same syntactic interface (\"is the president of\"). Table  1 shows the seed lexicon for the SOCIAL domain.From the seed lexicon, the domain-general grammar (Table 2 ) constructs noun phrases (NP), verbs phrases (VP), and complementizer phrase (CP), all of which denote unary logical forms. Broadly speaking, the rules (R1)-(R4), (C1)-(C4) take a binary and a noun phrase, and compose them (optionally via comparatives, counting, and negation) to produce a complementizer phrase CP representing a unary (e.g., \"that cites article 1\" or \"that cites more than three article\"). (G3) combines these CP\\'s with an NP (e.g., \"article\"). In addition, (S0)-(S4) handle superlatives (we include argmin in addition to argmax), which take an NP and return the extremum-attaining subset of its denotation. Finally, we support transformations such as join (T1) and disjunction (T4), as well as aggregation (A1)-(A2).Rendering utterances for multi-arity predicates was a major challenge.The predicate instances are typically reified in a graph database, akin to a neo-Davidsonian treatment of events: There is an abstract entity with binary predicates relating it to its arguments. For example, in the SOCIAL domain, Alice\\'s education can be represented in the database as five triples: All five properties here are represented as RELNP\\'s, with the first one designated as the subject (RELNP 0 ). We support two ways of querying multi-arity relations: \"student whose university is ucla\" (T2) and \"university of student Alice whose start date is 2005\" (T3).birthdate → RELNP[birthdate] person|university|field → TYPENP[person| · · · ] company|job title → TYPENP[company| · · · ] student|university|field of study → RELNP[student| · · · ] employee|employer|job title → RELNP[employee| · · · ] start date|end date → RELNP[startDate| · · · ] is friends with → VP/NP[friends| · · · ]Generating directly from the grammar in Table 2 would result in many uninterpretable canonical utterances. Thus, we perform type checking on the logical forms to rule out \"article that cites 2004\", and limit the amount of recursion, which keeps the canonical utterances understandable.Still, the utterances generated by our grammar are not perfectly grammatical; we do not use determiners and make all nouns singular. Nonetheless, AMT workers found most canonical utterances understandable (see Table 3 and Section 5 for details on crowdsourcing). One tip for the builder is to keep the RELNP\\'s and VP/NP\\'s as context-independent as possible; e.g., using \"publication date\" instead of \"date\". In cases where more context is required, we use parenthetical remarks (e.g., \"number of assists (over a season)\" → RELNP[...]) to pack more context into the confines of the part-of-speech.Limitations. While our domain-general grammar covers most of the common logical forms in a database querying application, there are several phenomena which are out of scope, notably nested quantification (e.g., \"show me each author\\'s most cited work\") and anaphora (e.g., \"author who cites herself at least twice\"). Handling these would require a more radical change to the grammar, but is still within scope. [glue] (G1) ENTITYNP[x] → NP[x] (G2) TYPENP[x] → NP[type.x] (G3) NP[x] CP[f ] (and CP[g])* → NP[x f g] [simple] (R0) that VP[x] → CP[x] (R1.z)))] [transformation] (T1) RELNP[r] of NP[y] → NP[R(r).y] (T2) RELNP 0 [h]CP[f ] (and CP[g])* → NP[R(h).(f g)] (T3) RELNP[r] of RELNP 0 [h] NP[x] CP[f ] (and CP[g])* → NP[R(r).(h.x f g)] (T4) NP[x] or NP[y] → NP[x y] [aggregation] (A1) number of NP[x] → NP[count(x)] (A2) total|average RELNP[r] of NP[x] → NP[sum|average(x, r)]',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 787,\n",
       "      'end': 795,\n",
       "      'text': 'Figure 2',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1853,\n",
       "      'end': 1861,\n",
       "      'text': 'Table  1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF2'},\n",
       "     {'start': 1957,\n",
       "      'end': 1965,\n",
       "      'text': '(Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF4'},\n",
       "     {'start': 4158,\n",
       "      'end': 4165,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'While the canonical utterance c is generated compositionally along with the logical form z, natural paraphrases x ∈ P(c) generally deviate from this compositional structure. For example, the canonical utterance \"NP[number of NP[article CP[whose publication date is larger than NP[publication date of article 1]]]]\" might get paraphrased to \"How many articles were published after article 1?\". Here, \"published after\" non-compositionally straddles the inner NP, intuitively responsible for both instances of \"publication date\". But how non-compositional can paraphrases be? Our framework rests on the assumption that the answer is \"not very\":Assumption 2 (Bounded non-compositionality) Natural utterances for expressing complex logical forms are compositional with respect to fragments of bounded size.In the above example, note that while \"published after\" is non-compositional with respect to the grammar, the rewriting of \"number of\" to \"how many\" is compositional. The upshot of Assumption 2 is that we only need to ask for paraphrases of canonical utterances generated by the grammar up to some small depth to learn about all the noncompositional uses of language, and still be able generalize (compositionally) beyond that. We now explore the nature of the possible paraphrases. Broadly speaking, most paraphrases involve some sort of compression, where the clunky but faithful canonical utterance is smoothed out into graceful prose.Alternations of single rules. The most basic paraphrase happens at the single phrase level with synonyms (\"block\" to \"brick\"), which preserve the part-of-speech. However, many of our properties are specified using relational noun phrases, which are more naturally realized using prepositions (\"meeting whose attendee is alice ⇒ meeting with alice\") or verbs (\"author of article 1 ⇒ who wrote article 1\"). If the RELNP is complex, then the argument can become embedded: \"player whose number of points is 15 ⇒ player who scored 15 points\". Superlative and comparative constructions reveal other RELNP-dependent words: \"article that has the largest publication date ⇒ newest article\". When the value of the relation has enough context, then the relation is elided completely: \"housing unit whose housing type is apartment ⇒ apartment\".Multi-arity predicates are compressed into a single frame: \"university of student alice whose field of study is music\" becomes \"At which university did Alice study music?\", where the semantic roles of the verb \"study\" carry the burden of expressing the multiple relations: student, university, and fieldOfStudy. With a different combination of arguments, the natural verb would change: \"Which university did Alice attend?\" Sublexical compositionality. The most interesting paraphrases occur across multiple rules, a phenomenon which we called sublexical compositionality. The idea is that common, multi-part concepts are compressed to single words or simpler constructions. The simplest compression is a lexical one: \"parent of alice whose gender is female ⇒ mother of alice\". Compression often occurs when we have the same predicate chained twice in a join: \"person that is author of paper whose author is X ⇒ co-author of X\" or \"person whose birthdate is birthdate of X ⇒ person born on the same day as X\". When two CP\\'s combined via coordination have some similarity, then the coordination can be pushed down (\"meeting whose start time is 3pm and whose end time is 5pm ⇒ meetings between 3pm and 5pm\") and sometimes even generalized (\"that allows cats and that allows dogs ⇒ that allows pets\"). Sometimes, compression happens due to metonymy, where people stand in for their papers: \"author of article that article whose author is X cites ⇒ who does X cite\".',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We tackled seven domains covering various linguistic phenomena. Table 3 lists the domains, their principal phenomena, statistics about their predicates and dataset, and an example from the dataset.We use Amazon Mechanical Turk (AMT) to paraphrase the canonical utterances generated by the domain-general grammar. In each AMT task, a worker is presented with four canonical utterances and is asked to reformulate them in natural language or state that they are incomprehensible. Each canonical utterance was presented to 10 workers. Over all domains, we collected 18,032 responses. The average time for paraphrasing one utterance was 28 seconds. Paraphrases that share the same canonical utterance are collapsed, while identical paraphrases that have distinct canonical utterances are deleted. This produced a total of 12,602 examples over all domains.To estimate the level of noise in the data, we manually judged the correctness of 20 examples in each domain, and found that 17% of the utterances were inaccurate. There are two main reasons: lexical ambiguity on our part (\"player that has the least number of team ⇒ player with the lowest jersey number\"), and failure on the worker\\'s part (\"restaurant whose star rating is 3 stars ⇒ hotel which has a 3 star rating\").',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 64,\n",
       "      'end': 71,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our semantic parsing model defines a distribution over logical forms given by the domaingeneral grammar G and additional rules triggered by the input utterance x. Specifically, given an utterance x, we detect numbers, dates, and perform string matching with database entities to recognize named entities. This results in a set of rules T(x). For example, if x is \"article published in 2015 that cites article 1\", then T(x) contains 2015 → NP [2015] andarticle 1 → NP[article1]. Let L x be the rules in the seed lexicon L where the entity rules (e.g., alice → NP[alice] ) are replaced by T(x). Our semantic parsing model defines a loglinear distribution over candidate pairs (z, c) ∈ GEN(G ∪ L x ):p θ (z, c | x, w) ∝ exp(φ(c, z, x, w) θ),(1)where φ(z, c, x, w) ∈ R d is a feature vector and θ ∈ R d is a parameter vector. To generate candidate logical forms, we use a simple beam search: For each search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance \"article\" would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015) .Training. We train our model by maximizing the regularized log-likelihood O(θ) = Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: \"Show me meetings after the weekly standup day\" c: \"meeting whose date is at least date of weekly standup\" z: type.meeting date. > R(date).weeklyStandup BLOCKS 19 1995 spatial language x: \"Select the brick that is to the furthest left.\" c: \"block that the most number of block is right of\" z: argmax(type.block, R(λx.count(R(right).x))) HOUSING 24 941 measurement units x: \"Housing that is 800 square feet or bigger?\" c: \"housing unit whose size is at least 800 square feet\" z: type.housingUnit area. > .800 RESTAURANTS 32 1657 long unary relations x: \"What restaurant can you eat lunch outside at?\" c: \"restaurant that has outdoor seating and that serves lunch\" z: type.restaurant hasOutdoorSeating serveslunch PUBLICATIONS 15 801 sublexical compositionality x: \"Who has co-authored articles with Efron?\" c: \"person that is author of article whose author is efron\" z: type.person R(author).(type.article author.efron) SOCIAL 45 4419 multi-arity relations x: \"When did alice start attending brown university?\" c: \"start date of student alice whose university is brown university\" z: R(date). Table 3 : We experimented on seven domains, covering a variety of phenomena. For each domain, we show the number of predicates, number of examples, and a (c, z) generated by our framework along with a paraphrased utterance x. Table 4 : Features for the paraphrasing model. pos(x i:i ) is the POS tag; type( z w ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c.',\n",
       "    'cite_spans': [{'start': 442,\n",
       "      'end': 448,\n",
       "      'text': '[2015]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1435,\n",
       "      'end': 1458,\n",
       "      'text': 'Berant and Liang (2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1494,\n",
       "      'end': 1518,\n",
       "      'text': 'Pasupat and Liang (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'}],\n",
       "    'ref_spans': [{'start': 2769,\n",
       "      'end': 2776,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2995,\n",
       "      'end': 3002,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': '(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 . To optimize, we use AdaGrad (Duchi et al., 2010) .Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) . We count the number of exact matches, PPDB matches, and unmatched words.To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4 ). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) . We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .',\n",
       "    'cite_spans': [{'start': 70,\n",
       "      'end': 90,\n",
       "      'text': '(Duchi et al., 2010)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 253,\n",
       "      'end': 280,\n",
       "      'text': '(Ganitkevitch et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 411,\n",
       "      'end': 431,\n",
       "      'text': '(Liang et al., 2006)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'},\n",
       "     {'start': 774,\n",
       "      'end': 793,\n",
       "      'text': '(Och and Ney, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'},\n",
       "     {'start': 1213,\n",
       "      'end': 1234,\n",
       "      'text': '(Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'}],\n",
       "    'ref_spans': [{'start': 101,\n",
       "      'end': 108,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 654,\n",
       "      'end': 661,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We evaluated our functionality-driven process on the seven domains described in Section 5 and one new domain we describe in Section 7.3. For each domain, we held out a random 20% of the examples as the test set, and performed development on the remaining 80%, further splitting it to a training and development set (80%/20%). We created a database for each domain by randomly generating facts using entities and properties in the domain (with type-checking). We evaluated using accuracy, which is the fraction of examples that yield the correct denotation.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Our functionality-driven process is predicated on the fact that each domain exhibits domain-specific phenomena. To corroborate this, we compare our full system to NOLEX, a baseline that omits all lexical features (Table 4 ), but uses PPDB as a domain-general paraphrasing component. We perform the complementary experiment and compare to NOPPDB, a baseline that omits PPDB match features. We also run BASELINE, where we omit both lexical and PPDB features. Table 5 presents the results of this experiment. Overall, our framework obtains an average accuracy of 59% across all eight domains. The performance of NOLEX is dramatically lower than FULL, indicating that it is important to learn domain-specific paraphrases using lexical features. The accuracy of NOPPDB is only slightly lower than FULL, showing that most of the required paraphrases can be learned during training. As expected, removing both lexical and PPDB features results in poor performance (BASELINE).Analysis. We performed error analysis on 10 errors in each domain. Almost 70% of the errors are due to problems in the paraphrasing model, where the canonical utterance has extra material, is missing some content, or results in an incorrect paraphrase. For example, \"restaurants that have waiters and you can sit outside\" is paraphrased to \"restaurant that has waiter service and that takes reservations\". Another 12.5% result from reordering issues, e.g, we paraphrase \"What venue has fewer than two articles\" to \"article that has less than two venue\". Inaccurate paraphrases provided by AMT workers account for the rest of the errors.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 213,\n",
       "      'end': 221,\n",
       "      'text': '(Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 457,\n",
       "      'end': 464,\n",
       "      'text': 'Table 5',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF6'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We hypothesized that we need to obtain paraphrases of canonical utterances corresponding to logical forms of only small depth. We ran the following experiment in the CALENDAR domain to test this claim. First, we define by NP 0 , NP 1 , and NP 2 the set of utterances generated by an NP that has exactly zero, one, and two NPs embedded in it. We define the training scenario 0 → 1, where we train on examples from NP 0 and test on examples from NP 1 ; 0 ∪ 1 → 1, 0 ∪ 1 → 2, and 0 ∪ 1 ∪ 2 → 2 are defined analogously. Our Scenario Acc. Scenario Acc. 0 → 1 22.9 0 ∪ 1 → 2 28.1 0 ∪ 1 → 1 85.8 0 ∪ 1 ∪ 2 → 2 47.5 Table 6 : Test set results in the CALENDAR domain on bounded non-compositionality. hypothesis is that generalization on 0 ∪ 1 → 2 should be better than for 0 → 1, since NP 1 utterances have non-compositional paraphrases, but training on NP 0 does not expose them.The results in Table 6 verify this hypothesis. The accuracy of 0 → 1 is almost 65% lower than 0 ∪ 1 → 1. On the other hand, the accuracy of 0 ∪ 1 → 2 is only 19% lower than 0 ∪ 1 ∪ 2 → 2.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 608,\n",
       "      'end': 615,\n",
       "      'text': 'Table 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 886,\n",
       "      'end': 893,\n",
       "      'text': 'Table 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'To verify the title of this paper, we attempted to create a semantic parser for a new domain (RECIPES) exactly 24 hours before the submission deadline. Starting at midnight, we created a seed lexicon in less than 30 minutes. Then we generated canonical utterances and allowed AMT workers to provide paraphrases overnight. In the morning, we trained our parser and obtained an accuracy of 70.8% on the test set.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Geo880. To test how our parser generalizes to utterances independent of our framework, we created a semantic parser for the domain of US geography, and tested on the standard 280 test examples from GEO880 (Zelle and Mooney, 1996) . We did not use the standard 600 training examples. Our parser obtained 56.4% accuracy, which is substantially lower than state-of-the-art (∼ 90%).We performed error analysis on 100 random sentences from the development set where accuracy was 60%. We found that the parser learns from the training data to prefer shorter paraphrases, which accounts for 30% of the errors. In most of these cases, the correct logical form is ranked at the top-3 results (accuracy for the top-3 derivations is 73%). GEO880 contains highly compositional utterances, and in 25% of the errors the correct derivation tree exceeds the maximum depth used for our parser. Another 17.5% of the errors are caused by problems in the paraphrasing model. For example, in the utterance \"what is the size of california\", the model learns that \"size\" corresponds to \"population\" rather than \"area\". Errors related to reordering and the syntactic structure of the input utterance account for 7.5% of the errors. For example, the utterance \"what is the area of the largest state\" is paraphrased to \"state that has the largest area\".Calendar. In Section 7.1, we evaluated on utterances obtained by paraphrasing canonical utterances from the grammar. To examine the coverage of our parser on independently-produced utterances, we asked AMT workers to freely come up with queries. We collected 186 such queries; 5 were spam and discarded. We replaced all entities (people, dates, etc.) with entities from our seed lexicon to avoid focusing on entity detection.We were able to annotate 52% of the utterances with logical forms from our grammar. We could not annotate 20% of the utterances due to relative time references, such as \"What time is my next meeting?\". 14% of the utterances were not covered due to binary predicates not in the grammar (\"What is the agenda of the meeting?\") or missing entities (\"When is Dan\\'s birthday?\"). Another 2% required unsupported calculations (\"How much free time do I have tomorrow?\"), and the rest are out of scope for other reasons (\"When does my Verizon data plan start over?\").We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model.',\n",
       "    'cite_spans': [{'start': 205,\n",
       "      'end': 229,\n",
       "      'text': '(Zelle and Mooney, 1996)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF25'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013) . However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013) . Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal.Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014) . All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing.Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014) . We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010) . However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance.In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification.',\n",
       "    'cite_spans': [{'start': 101,\n",
       "      'end': 122,\n",
       "      'text': '(Cai and Yates, 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 123,\n",
       "      'end': 148,\n",
       "      'text': 'Kwiatkowski et al., 2013;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 149,\n",
       "      'end': 169,\n",
       "      'text': 'Berant et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 410,\n",
       "      'end': 438,\n",
       "      'text': '(Kushman and Barzilay, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 466,\n",
       "      'end': 489,\n",
       "      'text': '(Matuszek et al., 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'},\n",
       "     {'start': 490,\n",
       "      'end': 510,\n",
       "      'text': 'Tellex et al., 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF21'},\n",
       "     {'start': 511,\n",
       "      'end': 542,\n",
       "      'text': 'Krishnamurthy and Kollar, 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 756,\n",
       "      'end': 787,\n",
       "      'text': '(Zettlemoyer and Collins, 2005;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 788,\n",
       "      'end': 810,\n",
       "      'text': 'Wong and Mooney, 2007)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF23'},\n",
       "     {'start': 826,\n",
       "      'end': 847,\n",
       "      'text': '(Clarke et al., 2010;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 848,\n",
       "      'end': 867,\n",
       "      'text': 'Liang et al., 2011)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'},\n",
       "     {'start': 888,\n",
       "      'end': 917,\n",
       "      'text': '(Artzi and Zettlemoyer, 2011;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 918,\n",
       "      'end': 937,\n",
       "      'text': 'Reddy et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 1202,\n",
       "      'end': 1222,\n",
       "      'text': '(Fader et al., 2013)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 1244,\n",
       "      'end': 1268,\n",
       "      'text': '(Berant and Liang, 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1393,\n",
       "      'end': 1413,\n",
       "      'text': '(Woods et al., 1972;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF24'},\n",
       "     {'start': 1414,\n",
       "      'end': 1439,\n",
       "      'text': 'Warren and Pereira, 1982)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF22'},\n",
       "     {'start': 1472,\n",
       "      'end': 1489,\n",
       "      'text': '(Schwitter, 2010)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'}],\n",
       "    'ref_spans': [{'start': 1354,\n",
       "      'end': 1361,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF4'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {'FIGREF0': {'text': '(student.Alice university.Brown) BASKETBALL 24 1952 parentheticals x: \"How many fouls were played by Kobe Bryant in 2004?\" c: \"number of fouls (over a season) of player kobe bryant whose season is 2004\" z: count(R(fouls).(player.KobeBryant season.2004)',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF1': {'text': 'Basic #words and bigram matches in (x, c) #words and bigram PPDB matches in (x, c) #unmatched words in x #unmatched words in c size of denotation of z, (| z w |) pos(x0:0) conjoined with type( z w ) #nodes in tree generating z Lexical ∀(i, j) ∈ A. (xi:i, cj:j) ∀(i, j) ∈ A. (xi:i, cj:j+1) ∀(i, j) ∈ A. (xi:i, cj−1:j) ∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1, cj:j+1) all unaligned words in x and c (xi:j, c i :j ) if in phrase table',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'TABREF0': {'text': 'person that is author of the most number of article argmax(type.person, R(λx.count(type.article author.x))) ...what is the newest published article? who has published the most articles?',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF1': {'text': 'NP[type.article publicationDate.1950]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF2': {'text': 'The seed lexicon for the SOCIAL do- main, which specifies for each predicate (e.g., birthdate) a phrase (e.g., \"birthdate\") that real- izes that predicate and its syntactic category (e.g., RELNP).',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF3': {'text': ') whose RELNP[r] CMP[c] NP[y] → CP[r.c.y] is|is not|is smaller than|is larger than|is at least|is at most → CMP[= | = | < | > | ≤ | ≥]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF4': {'text': 'The domain-general grammar which is combined with the seed lexicon to generate logical forms and canonical utterances that cover the supported logical functionality.',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF6': {'text': 'Test set results on all domains and baselines.',\n",
       "    'latex': None,\n",
       "    'type': 'table'}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Bootstrapping semantic parsers from conversations',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '421--432',\n",
       "    'other_ids': {},\n",
       "    'links': '1140108'},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Weakly supervised learning of semantic parsers for mapping instructions to actions',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '1',\n",
       "    'issn': '',\n",
       "    'pages': '49--62',\n",
       "    'other_ids': {},\n",
       "    'links': '9963298'},\n",
       "   'BIBREF2': {'ref_id': 'b2',\n",
       "    'title': 'Semantic parsing via paraphrasing',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '1336493'},\n",
       "   'BIBREF3': {'ref_id': 'b3',\n",
       "    'title': 'Semantic parsing on Freebase from question-answer pairs',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Berant', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': [], 'last': 'Chou', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Frostig', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6401679'},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Large-scale semantic parsing via schema matching and lexicon extension',\n",
       "    'authors': [{'first': 'Q', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': [], 'last': 'Yates', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '2265838'},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': \"Driving semantic parsing from the world's response\",\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Clarke', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Goldwasser', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Chang', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Roth', 'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'Computational Natural Language Learning (CoNLL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '18--27',\n",
       "    'other_ids': {},\n",
       "    'links': '5667590'},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'Adaptive subgradient methods for online learning and stochastic optimization',\n",
       "    'authors': [{'first': 'J', 'middle': [], 'last': 'Duchi', 'suffix': ''},\n",
       "     {'first': 'E', 'middle': [], 'last': 'Hazan', 'suffix': ''},\n",
       "     {'first': 'Y', 'middle': [], 'last': 'Singer', 'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'Conference on Learning Theory (COLT)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '538820'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': 'Paraphrase-driven learning for open question answering',\n",
       "    'authors': [{'first': 'A', 'middle': [], 'last': 'Fader', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''},\n",
       "     {'first': 'O', 'middle': [], 'last': 'Etzioni', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '8893912'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'PPDB: The paraphrase database',\n",
       "    'authors': [{'first': 'J',\n",
       "      'middle': [],\n",
       "      'last': 'Ganitkevitch',\n",
       "      'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['V'], 'last': 'Durme', 'suffix': ''},\n",
       "     {'first': 'C', 'middle': [], 'last': 'Callison-Burch', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '758--764',\n",
       "    'other_ids': {},\n",
       "    'links': '6067240'},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Jointly learning to parse and perceive: Connecting natural language to the physical world',\n",
       "    'authors': [{'first': 'J',\n",
       "      'middle': [],\n",
       "      'last': 'Krishnamurthy',\n",
       "      'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Kollar', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '1',\n",
       "    'issn': '',\n",
       "    'pages': '193--206',\n",
       "    'other_ids': {},\n",
       "    'links': '10250712'},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Using semantic unification to generate regular expressions from natural language',\n",
       "    'authors': [{'first': 'N', 'middle': [], 'last': 'Kushman', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Barzilay', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '826--836',\n",
       "    'other_ids': {},\n",
       "    'links': '6216733'},\n",
       "   'BIBREF11': {'ref_id': 'b11',\n",
       "    'title': 'Scaling semantic parsers with on-the-fly ontology matching',\n",
       "    'authors': [{'first': 'T',\n",
       "      'middle': [],\n",
       "      'last': 'Kwiatkowski',\n",
       "      'suffix': ''},\n",
       "     {'first': 'E', 'middle': [], 'last': 'Choi', 'suffix': ''},\n",
       "     {'first': 'Y', 'middle': [], 'last': 'Artzi', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'Empirical Methods in Natural Language Processing (EMNLP)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '14341841'},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Alignment by agreement',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': [], 'last': 'Taskar', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Klein', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': 'North American Association for Computational Linguistics (NAACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '104--111',\n",
       "    'other_ids': {},\n",
       "    'links': '618683'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'Learning dependency-based compositional semantics',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['I'], 'last': 'Jordan', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Klein', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '590--599',\n",
       "    'other_ids': {},\n",
       "    'links': '340852'},\n",
       "   'BIBREF14': {'ref_id': 'b14',\n",
       "    'title': 'Lambda dependency-based compositional semantics. arXiv',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'A joint model of language and perception for grounded attribute learning',\n",
       "    'authors': [{'first': 'C', 'middle': [], 'last': 'Matuszek', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Fitzgerald', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Zettlemoyer', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': [], 'last': 'Bo', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': [], 'last': 'Fox', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'International Conference on Machine Learning (ICML)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1671--1678',\n",
       "    'other_ids': {},\n",
       "    'links': '2408319'},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'The alignment template approach to statistical machine translation',\n",
       "    'authors': [{'first': 'F', 'middle': ['J'], 'last': 'Och', 'suffix': ''},\n",
       "     {'first': 'H', 'middle': [], 'last': 'Ney', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'Computational Linguistics',\n",
       "    'volume': '30',\n",
       "    'issn': '',\n",
       "    'pages': '417--449',\n",
       "    'other_ids': {},\n",
       "    'links': '1272090'},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Compositional semantic parsing on semi-structured tables',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Pasupat', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Liang', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '9027681'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'Features and pitfalls that users should seek in natural language interfaces to databases',\n",
       "    'authors': [{'first': 'R',\n",
       "      'middle': ['A P'],\n",
       "      'last': 'Rangel',\n",
       "      'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['A'], 'last': 'Aguirre', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['J'], 'last': 'Gonzlez', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['M'], 'last': 'Carpio', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Recent Advances on Hybrid Approaches for Designing Intelligent Systems',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '617--630',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Largescale semantic parsing without question-answer pairs',\n",
       "    'authors': [{'first': 'S', 'middle': [], 'last': 'Reddy', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Lapata', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Steedman', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'Transactions of the Association for Computational Linguistics (TACL)',\n",
       "    'volume': '2',\n",
       "    'issn': '10',\n",
       "    'pages': '377--392',\n",
       "    'other_ids': {},\n",
       "    'links': '15324422'},\n",
       "   'BIBREF20': {'ref_id': 'b20',\n",
       "    'title': 'Controlled natural languages for knowledge representation',\n",
       "    'authors': [{'first': 'R',\n",
       "      'middle': [],\n",
       "      'last': 'Schwitter',\n",
       "      'suffix': ''}],\n",
       "    'year': 2010,\n",
       "    'venue': 'International Conference on Computational Linguistics (COLING)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1113--1121',\n",
       "    'other_ids': {},\n",
       "    'links': '10228634'},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'Understanding natural language commands for robotic navigation and mobile manipulation',\n",
       "    'authors': [{'first': 'S', 'middle': [], 'last': 'Tellex', 'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Kollar', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': [], 'last': 'Dickerson', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': ['R'], 'last': 'Walter', 'suffix': ''},\n",
       "     {'first': 'A', 'middle': ['G'], 'last': 'Banerjee', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': ['J'], 'last': 'Teller', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Roy', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Association for the Advancement of Artificial Intelligence (AAAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '1078533'},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'An efficient easily adaptable system for interpreting natural language queries',\n",
       "    'authors': [{'first': 'D', 'middle': [], 'last': 'Warren', 'suffix': ''},\n",
       "     {'first': 'F', 'middle': [], 'last': 'Pereira', 'suffix': ''}],\n",
       "    'year': 1982,\n",
       "    'venue': 'Computational Linguistics',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '110--122',\n",
       "    'other_ids': {},\n",
       "    'links': '2498523'},\n",
       "   'BIBREF23': {'ref_id': 'b23',\n",
       "    'title': 'Learning synchronous grammars for semantic parsing with lambda calculus',\n",
       "    'authors': [{'first': 'Y', 'middle': ['W'], 'last': 'Wong', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['J'], 'last': 'Mooney', 'suffix': ''}],\n",
       "    'year': 2007,\n",
       "    'venue': 'Association for Computational Linguistics (ACL)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '960--967',\n",
       "    'other_ids': {},\n",
       "    'links': '9337134'},\n",
       "   'BIBREF24': {'ref_id': 'b24',\n",
       "    'title': 'The lunar sciences natural language information system: Final report',\n",
       "    'authors': [{'first': 'W', 'middle': ['A'], 'last': 'Woods', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['M'], 'last': 'Kaplan', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['N'], 'last': 'Webber', 'suffix': ''}],\n",
       "    'year': 1972,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '62727207'},\n",
       "   'BIBREF25': {'ref_id': 'b25',\n",
       "    'title': 'Learning to parse database queries using inductive logic programming',\n",
       "    'authors': [{'first': 'M', 'middle': [], 'last': 'Zelle', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['J'], 'last': 'Mooney', 'suffix': ''}],\n",
       "    'year': 1996,\n",
       "    'venue': 'Association for the Advancement of Artificial Intelligence (AAAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1050--1055',\n",
       "    'other_ids': {},\n",
       "    'links': '263135'},\n",
       "   'BIBREF26': {'ref_id': 'b26',\n",
       "    'title': 'Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars',\n",
       "    'authors': [{'first': 'L',\n",
       "      'middle': ['S'],\n",
       "      'last': 'Zettlemoyer',\n",
       "      'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Collins', 'suffix': ''}],\n",
       "    'year': 2005,\n",
       "    'venue': 'Uncertainty in Artificial Intelligence (UAI)',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '658--666',\n",
       "    'other_ids': {},\n",
       "    'links': '449252'}}},\n",
       " 'latex_parse': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "0 {'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "1 {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "2 {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "3 {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "4 {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "5 {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "6 {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "7 {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "8 {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "9 {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "10 {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "11 {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "12 {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google's big developers' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "----\n",
      "1 1\n",
      "0 {'start': 451, 'end': 468, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.\n",
      "----\n",
      "2 6\n",
      "0 {'start': 32, 'end': 58, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "1 {'start': 59, 'end': 79, 'text': 'Rezende et al., 2014', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "2 {'start': 184, 'end': 200, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "3 {'start': 2384, 'end': 2401, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "4 {'start': 2433, 'end': 2456, 'text': '(Bahdanau et al., 2015;', 'latex': None, 'ref_id': 'BIBREF0'}\n",
      "5 {'start': 2457, 'end': 2476, 'text': 'Luong et al., 2015)', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.\n",
      "----\n",
      "3 3\n",
      "0 {'start': 86, 'end': 102, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "1 {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "2 {'start': 1133, 'end': 1158, 'text': '(Dantzig and Thapa, 2006)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .\n",
      "----\n",
      "8 1\n",
      "0 {'start': 107, 'end': 118, 'text': '(Lin, 2004)', 'latex': None, 'ref_id': 'BIBREF12'}\n",
      "The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n",
      "----\n",
      "9 4\n",
      "0 {'start': 351, 'end': 365, 'text': '(Wasson, 1998)', 'latex': None, 'ref_id': 'BIBREF20'}\n",
      "1 {'start': 492, 'end': 512, 'text': '(Radev et al., 2000)', 'latex': None, 'ref_id': 'BIBREF17'}\n",
      "2 {'start': 700, 'end': 723, 'text': '(Erkan and Radev, 2004)', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "3 {'start': 737, 'end': 763, 'text': '(Mihalcea and Tarau, 2004)', 'latex': None, 'ref_id': 'BIBREF14'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.\n",
      "----\n",
      "10 2\n",
      "0 {'start': 498, 'end': 518, 'text': '(Kingma and Ba, 2014', 'latex': None, 'ref_id': 'BIBREF8'}\n",
      "1 {'start': 652, 'end': 674, 'text': '(Bastien et al., 2012)', 'latex': None, 'ref_id': 'BIBREF1'}\n",
      "The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .\n",
      "----\n",
      "11 2\n",
      "0 {'start': 690, 'end': 707, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "1 {'start': 908, 'end': 925, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"'Bitcoin Mt. Gox Offlile\"' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.\n",
      "----\n",
      "13 1\n",
      "0 {'start': 517, 'end': 868, 'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world', 'latex': None, 'ref_id': None}\n",
      "We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world \n",
      "----\n",
      "====================\n",
      "0 6\n",
      "0 {'start': 173, 'end': 197, 'text': '(Zelle and Mooney, 1996;', 'latex': None, 'ref_id': 'BIBREF25'}\n",
      "1 {'start': 198, 'end': 228, 'text': 'Zettlemoyer and Collins, 2005;', 'latex': None, 'ref_id': 'BIBREF26'}\n",
      "2 {'start': 229, 'end': 248, 'text': 'Liang et al., 2011;', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "3 {'start': 249, 'end': 269, 'text': 'Berant et al., 2013;', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "4 {'start': 270, 'end': 295, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "5 {'start': 296, 'end': 323, 'text': 'Kushman and Barzilay, 2013)', 'latex': None, 'ref_id': 'BIBREF10'}\n",
      "By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Kushman and Barzilay, 2013) . Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain-for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start?In this paper, we advocate a functionalitydriven process for rapidly building a semantic * Both authors equally contributed to the paper. ...\n",
      "----\n",
      "1 7\n",
      "0 {'start': 1832, 'end': 1853, 'text': '(Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "1 {'start': 2121, 'end': 2142, 'text': '(Rangel et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "2 {'start': 2415, 'end': 2441, 'text': '(Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "3 {'start': 2442, 'end': 2465, 'text': 'Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "4 {'start': 3174, 'end': 3194, 'text': '(Liang et al., 2011;', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "5 {'start': 3195, 'end': 3220, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "6 {'start': 3221, 'end': 3244, 'text': 'Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "(1) by builder (∼30 minutes)(2) via domain-general grammar (3) via crowdsourcing (∼5 hours) (4) by training a paraphrasing model Figure 1 : Functionality-driven process for building semantic parsers. The two red boxes are the domain-specific parts provided by the builder of the semantic parser, and the other two are generated by the framework.parser in a new domain. At a high-level, we seek to minimize the amount of work needed for a new domain by factoring out the domaingeneral aspects (done by our framework) from the domain-specific ones (done by the builder of the semantic parser). We assume that the builder already has the desired functionality of the semantic parser in mind-e.g., the publications database is set up and the schema is fixed. Figure 1 depicts the functionality-driven process: First, the builder writes a seed lexicon specifying a canonical phrase (\"publication date\") for each predicate (publicationDate).Second, our framework uses a domain-general grammar, along with the seed lexicon and the database, to automatically generate a few hundred canonical utterances paired with their logical forms (e.g., \"article that has the largest publication date\" and arg max(type.article, publicationDate)). These utterances need not be the most elegant, but they should retain the semantics of the logical forms. Third, the builder leverages crowdsourcing to paraphrase each canonical utterance into a few natural utterances (e.g., \"what is the newest published article?\"). Finally, our framework uses this data to train a semantic parser.Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014) . In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule.In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014) . Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain.Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny, as the structural mismatch between logic and language is the chief source of difficulty in semantic parsing (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014) .There are two important questions here. First, is it possible to design a simple grammar that simultaneously generates both logical forms and canonical utterances so that the utterances are understandable by a human? In Section 3, we show how to choose appropriate canonical utterances to maximize alignment with the logical forms.Second, our grammar can generate an infinite number of canonical utterances. How many do we need for adequate coverage? Certainly, single relations is insufficient: just knowing that \"publication date of X\" paraphrases to \"when X was published\" would offer insufficient information to generalize to \"articles that came after X\" mapping to \"article whose publication date is larger than publication date of X\". We call this phenomena sublexical compositionality-when a short lexical unit (\"came after\") maps onto a multi-predicate logical form. Our hypothesis is that the sublexical compositional units are small, so we only need to crowdsource a small number of canonical utterances to learn about most of the language variability in the given domain (Section 4).We applied our functionality-driven process to seven domains, which were chosen to explore particular types of phenomena, such as spatial language, temporal language, and high-arity relations. This resulted in seven new semantic parsing datasets, totaling 12.6K examples. Our approach, which was not tuned on any one domain, was able to obtain an average accuracy of 59% over all domains. On the day of this paper submission, we created an eighth domain and trained a semantic parser overnight.\n",
      "----\n",
      "2 3\n",
      "0 {'start': 327, 'end': 342, 'text': 'article1, 2015)', 'latex': None, 'ref_id': None}\n",
      "1 {'start': 1308, 'end': 1311, 'text': '(r)', 'latex': None, 'ref_id': None}\n",
      "2 {'start': 2374, 'end': 2397, 'text': 'Berant and Liang (2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "In our functionality-driven process (Figure 1) , there are two parties: the builder, who provides domain-specific information, and the framework, which provides domain-general information. We assume that the builder has a fixed database w, represented as a set of triples (e 1 , p, e 2 ), where e 1 and e 2 are entities (e.g., article1, 2015) and p is a property (e.g., publicationDate). The database w can be queried using lambda DCS logical forms, described further in Section 2.1.The builder supplies a seed lexicon L, which contains for each database property p (e.g., publicationDate) a lexical entry of the form t → s[p] , where t is a natural language phrase (e.g., \"publication date\") and s is a syntactic cat-egory (e.g., RELNP). In addition, L contains two typical entities for each semantic type in the database (e.g., alice → NP[alice] for the type person). The purpose of L is to simply connect each predicate with some representation in natural language.The framework supplies a grammar G, which specifies the modes of composition, both on logical forms and canonical utterances. Formally, G is a set of rules of the form α 1 . . . α n → s[z] , where α 1 . . . α n is a sequence of tokens or categories, s is a syntactic category and z is the logical form constructed. For example, one rule in (r) .x] , which constructs z by reversing the binary predicate r and joining it with a the unary predicate x. We use the rules G ∪ L to generate a set of (z, c) pairs, where z is a logical form (e.g., R(publicationDate).article1), and c is the corresponding canonical utterance (e.g., \"publication date of article 1\"). The set of (z, c) is denoted by GEN(G ∪ L). See Section 3 for details.G is RELNP[r] of NP[x] → NP[RNext, the builder (backed by crowdsourcing) paraphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., \"when was article 1 published?\"). This defines a set of training examples D = {(x, c, z)}, for each (z, c) ∈ GEN(G ∪ L) and x ∈ P(c). The crowdsourcing setup is detailed in Section 5.Finally, the framework trains a semantic parser on D. Our semantic parser is a log-linear distribution p θ (z, c | x, w) over logical forms and canonical utterances specified by the grammar G. Note that the grammar G will in general not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014) .To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G ∪ L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(·) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser p θ (z, c | x, w).\n",
      "----\n",
      "3 1\n",
      "0 {'start': 167, 'end': 179, 'text': 'Liang (2013)', 'latex': None, 'ref_id': 'BIBREF14'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details.Every logical form z in this paper is either a unary (denoting a set of entities) or a binary (denoting a set of entity-pairs). In the base case, each entity e (e.g., 2015) is a unary denoting the singleton set: e w = {e}; and each property p (e.g., publicationDate) is a binary denoting all entitypairs (e 1 , e 2 ) that satisfy the property p. Unaries and binaries can be composed: Given a binary b and unary u, the join b.u denotes all entities e 1 for which there exists an e 2 ∈ u w with (e 1 , e 2 ) ∈ b w . For example, publicationDate.2015 denote entities published in 2015.The intersection u 1 u 2 , union u 1 u 2 , complement ¬u denote the corresponding set operations on the denotations. We let R(b) denote the reversal of b: (e 1 , e 2 ) ∈ b w iff (e 2 , e 1 ) ∈ R(b) w . This allows us to define R(publicationDate).article1 as the publication date of article 1. We also include aggregation operations (count(u), sum(u) and average(u, b)), and superlatives (argmax(u, b)).Finally, we can construct binaries using lambda abstraction: λx.u denotes a set of (e 1 , e 2 ) where e 1 ∈ u[x/e 2 ] w and u[x/e 2 ] is the logical form where free occurrences of x are replaced with e 2 . For example, R(λx.count(R(cites).x)) denotes the set of entities (e 1 , e 2 ), where e 2 is the number of entities that e 1 cites.\n",
      "----\n",
      "7 3\n",
      "0 {'start': 442, 'end': 448, 'text': '[2015]', 'latex': None, 'ref_id': None}\n",
      "1 {'start': 1435, 'end': 1458, 'text': 'Berant and Liang (2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "2 {'start': 1494, 'end': 1518, 'text': 'Pasupat and Liang (2015)', 'latex': None, 'ref_id': 'BIBREF17'}\n",
      "Our semantic parsing model defines a distribution over logical forms given by the domaingeneral grammar G and additional rules triggered by the input utterance x. Specifically, given an utterance x, we detect numbers, dates, and perform string matching with database entities to recognize named entities. This results in a set of rules T(x). For example, if x is \"article published in 2015 that cites article 1\", then T(x) contains 2015 → NP [2015] andarticle 1 → NP[article1]. Let L x be the rules in the seed lexicon L where the entity rules (e.g., alice → NP[alice] ) are replaced by T(x). Our semantic parsing model defines a loglinear distribution over candidate pairs (z, c) ∈ GEN(G ∪ L x ):p θ (z, c | x, w) ∝ exp(φ(c, z, x, w) θ),(1)where φ(z, c, x, w) ∈ R d is a feature vector and θ ∈ R d is a parameter vector. To generate candidate logical forms, we use a simple beam search: For each search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance \"article\" would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015) .Training. We train our model by maximizing the regularized log-likelihood O(θ) = Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: \"Show me meetings after the weekly standup day\" c: \"meeting whose date is at least date of weekly standup\" z: type.meeting date. > R(date).weeklyStandup BLOCKS 19 1995 spatial language x: \"Select the brick that is to the furthest left.\" c: \"block that the most number of block is right of\" z: argmax(type.block, R(λx.count(R(right).x))) HOUSING 24 941 measurement units x: \"Housing that is 800 square feet or bigger?\" c: \"housing unit whose size is at least 800 square feet\" z: type.housingUnit area. > .800 RESTAURANTS 32 1657 long unary relations x: \"What restaurant can you eat lunch outside at?\" c: \"restaurant that has outdoor seating and that serves lunch\" z: type.restaurant hasOutdoorSeating serveslunch PUBLICATIONS 15 801 sublexical compositionality x: \"Who has co-authored articles with Efron?\" c: \"person that is author of article whose author is efron\" z: type.person R(author).(type.article author.efron) SOCIAL 45 4419 multi-arity relations x: \"When did alice start attending brown university?\" c: \"start date of student alice whose university is brown university\" z: R(date). Table 3 : We experimented on seven domains, covering a variety of phenomena. For each domain, we show the number of predicates, number of examples, and a (c, z) generated by our framework along with a paraphrased utterance x. Table 4 : Features for the paraphrasing model. pos(x i:i ) is the POS tag; type( z w ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c.\n",
      "----\n",
      "8 5\n",
      "0 {'start': 70, 'end': 90, 'text': '(Duchi et al., 2010)', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "1 {'start': 253, 'end': 280, 'text': '(Ganitkevitch et al., 2013)', 'latex': None, 'ref_id': 'BIBREF8'}\n",
      "2 {'start': 411, 'end': 431, 'text': '(Liang et al., 2006)', 'latex': None, 'ref_id': 'BIBREF12'}\n",
      "3 {'start': 774, 'end': 793, 'text': '(Och and Ney, 2004)', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "4 {'start': 1213, 'end': 1234, 'text': '(Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 . To optimize, we use AdaGrad (Duchi et al., 2010) .Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) . We count the number of exact matches, PPDB matches, and unmatched words.To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4 ). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) . We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .\n",
      "----\n",
      "13 1\n",
      "0 {'start': 205, 'end': 229, 'text': '(Zelle and Mooney, 1996)', 'latex': None, 'ref_id': 'BIBREF25'}\n",
      "Geo880. To test how our parser generalizes to utterances independent of our framework, we created a semantic parser for the domain of US geography, and tested on the standard 280 test examples from GEO880 (Zelle and Mooney, 1996) . We did not use the standard 600 training examples. Our parser obtained 56.4% accuracy, which is substantially lower than state-of-the-art (∼ 90%).We performed error analysis on 100 random sentences from the development set where accuracy was 60%. We found that the parser learns from the training data to prefer shorter paraphrases, which accounts for 30% of the errors. In most of these cases, the correct logical form is ranked at the top-3 results (accuracy for the top-3 derivations is 73%). GEO880 contains highly compositional utterances, and in 25% of the errors the correct derivation tree exceeds the maximum depth used for our parser. Another 17.5% of the errors are caused by problems in the paraphrasing model. For example, in the utterance \"what is the size of california\", the model learns that \"size\" corresponds to \"population\" rather than \"area\". Errors related to reordering and the syntactic structure of the input utterance account for 7.5% of the errors. For example, the utterance \"what is the area of the largest state\" is paraphrased to \"state that has the largest area\".Calendar. In Section 7.1, we evaluated on utterances obtained by paraphrasing canonical utterances from the grammar. To examine the coverage of our parser on independently-produced utterances, we asked AMT workers to freely come up with queries. We collected 186 such queries; 5 were spam and discarded. We replaced all entities (people, dates, etc.) with entities from our seed lexicon to avoid focusing on entity detection.We were able to annotate 52% of the utterances with logical forms from our grammar. We could not annotate 20% of the utterances due to relative time references, such as \"What time is my next meeting?\". 14% of the utterances were not covered due to binary predicates not in the grammar (\"What is the agenda of the meeting?\") or missing entities (\"When is Dan's birthday?\"). Another 2% required unsupported calculations (\"How much free time do I have tomorrow?\"), and the rest are out of scope for other reasons (\"When does my Verizon data plan start over?\").We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model.\n",
      "----\n",
      "14 18\n",
      "0 {'start': 101, 'end': 122, 'text': '(Cai and Yates, 2013;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "1 {'start': 123, 'end': 148, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "2 {'start': 149, 'end': 169, 'text': 'Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "3 {'start': 410, 'end': 438, 'text': '(Kushman and Barzilay, 2013)', 'latex': None, 'ref_id': 'BIBREF10'}\n",
      "4 {'start': 466, 'end': 489, 'text': '(Matuszek et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "5 {'start': 490, 'end': 510, 'text': 'Tellex et al., 2011;', 'latex': None, 'ref_id': 'BIBREF21'}\n",
      "6 {'start': 511, 'end': 542, 'text': 'Krishnamurthy and Kollar, 2013)', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "7 {'start': 756, 'end': 787, 'text': '(Zettlemoyer and Collins, 2005;', 'latex': None, 'ref_id': 'BIBREF26'}\n",
      "8 {'start': 788, 'end': 810, 'text': 'Wong and Mooney, 2007)', 'latex': None, 'ref_id': 'BIBREF23'}\n",
      "9 {'start': 826, 'end': 847, 'text': '(Clarke et al., 2010;', 'latex': None, 'ref_id': 'BIBREF5'}\n",
      "10 {'start': 848, 'end': 867, 'text': 'Liang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "11 {'start': 888, 'end': 917, 'text': '(Artzi and Zettlemoyer, 2011;', 'latex': None, 'ref_id': 'BIBREF0'}\n",
      "12 {'start': 918, 'end': 937, 'text': 'Reddy et al., 2014)', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "13 {'start': 1202, 'end': 1222, 'text': '(Fader et al., 2013)', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "14 {'start': 1244, 'end': 1268, 'text': '(Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "15 {'start': 1393, 'end': 1413, 'text': '(Woods et al., 1972;', 'latex': None, 'ref_id': 'BIBREF24'}\n",
      "16 {'start': 1414, 'end': 1439, 'text': 'Warren and Pereira, 1982)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "17 {'start': 1472, 'end': 1489, 'text': '(Schwitter, 2010)', 'latex': None, 'ref_id': 'BIBREF20'}\n",
      "Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013) . However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013) . Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal.Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014) . All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing.Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014) . We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010) . However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance.In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification.\n",
      "----\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "article_with_sect = dict()\n",
    "for num,article in enumerate(all_articles):\n",
    "    if num == 2:\n",
    "        break\n",
    "    for cnt_sect,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "        if sections['cite_spans']:\n",
    "            print(cnt_sect,len(sections['cite_spans']))\n",
    "            for cnt_cite,cite in enumerate(sections['cite_spans']):\n",
    "                print(cnt_cite,cite)\n",
    "            print(sections['text'])\n",
    "            print('----')\n",
    "    print(10*'==')\n",
    "#             if article['paper_id'] in article_with_sect:\n",
    "#                 article_with_sect[article['paper_id']] +=1\n",
    "#             else:\n",
    "#                 article_with_sect[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_results[0] - Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset∗\n",
    "\n",
    "\n",
    "\n",
    "Result \n",
    "\n",
    "0 {'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
    "1 {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
    "2 {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
    "3 {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
    "4 {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
    "5 {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
    "\n",
    "True \n",
    "\n",
    "0 (Goldstein et al., 2000; \n",
    "1 Erkan and Radev,2004; \n",
    "2 Wan et al., 2007; \n",
    "3 Nenkova and McKeown, 2012; \n",
    "4 Min et al., 2012; \n",
    "5 Bing et al., 2015; \n",
    "6 Li et al.,2017)\n",
    "\n",
    "\n",
    "Result\n",
    "\n",
    "{'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
    "\n",
    "True\n",
    "\n",
    "Woodsend and Lapata (2012), Bing et al. (2015), and Li et al. (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**При большом перечислении подряд ссылок, GROBID не выделяет предпоследнюю ссылку**\n",
    "\n",
    "**Также он не срабатывает на части ссылок**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### проверка наличия текста и названия секций во всех статьях в latex части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_articles if article['latex_parse'] and article['latex_parse']['body_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_body_text_tex = [article['paper_id'] for article in all_articles if not ( article['latex_parse'] and article['latex_parse']['body_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37621"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_ids_not_body_text_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n",
      "====================\n",
      "2 None\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for num,paper_id in enumerate(acl_ids_not_body_text_tex):\n",
    "    if num == 2:\n",
    "        break\n",
    "    id_lst = acl_paper_ids.index(paper_id)\n",
    "    print(id_lst,all_articles[id_lst]['latex_parse'])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '10164018',\n",
       " 'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "  'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "  'year': '2017',\n",
       "  'arxiv_id': '1708.01065',\n",
       "  'acl_id': 'W17-4512',\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.18653/v1/w17-4512',\n",
       "  'venue': 'ArXiv',\n",
       "  'journal': 'ArXiv'},\n",
       " 's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       " 'grobid_parse': {'abstract': [{'text': 'AbstractWe investigate the problem of readeraware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online 1 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Abstract'}],\n",
       "  'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [{'start': 192,\n",
       "      'end': 216,\n",
       "      'text': '(Goldstein et al., 2000;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 217,\n",
       "      'end': 239,\n",
       "      'text': 'Erkan and Radev, 2004;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 240,\n",
       "      'end': 257,\n",
       "      'text': 'Wan et al., 2007;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 258,\n",
       "      'end': 284,\n",
       "      'text': 'Nenkova and McKeown, 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'},\n",
       "     {'start': 285,\n",
       "      'end': 302,\n",
       "      'text': 'Min et al., 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'},\n",
       "     {'start': 303,\n",
       "      'end': 319,\n",
       "      'text': 'Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 773,\n",
       "      'end': 797,\n",
       "      'text': '(Project Code: 14203414)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2288,\n",
       "      'end': 2305,\n",
       "      'text': '(Hu et al., 2008;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 2306,\n",
       "      'end': 2324,\n",
       "      'text': 'Yang et al., 2011)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF22'},\n",
       "     {'start': 2582,\n",
       "      'end': 2598,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 2911,\n",
       "      'end': 2927,\n",
       "      'text': 'Li et al. (2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 3069,\n",
       "      'end': 3095,\n",
       "      'text': '(Kingma and Welling, 2014;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 3096,\n",
       "      'end': 3117,\n",
       "      'text': 'Rezende et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [{'start': 956,\n",
       "      'end': 964,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF0'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "    'cite_spans': [{'start': 451,\n",
       "      'end': 468,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [{'start': 12,\n",
       "      'end': 20,\n",
       "      'text': 'Figure 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "    'cite_spans': [{'start': 32,\n",
       "      'end': 58,\n",
       "      'text': '(Kingma and Welling, 2014;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 59,\n",
       "      'end': 79,\n",
       "      'text': 'Rezende et al., 2014',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 184,\n",
       "      'end': 200,\n",
       "      'text': 'Li et al. (2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2384,\n",
       "      'end': 2401,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2433,\n",
       "      'end': 2456,\n",
       "      'text': '(Bahdanau et al., 2015;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 2457,\n",
       "      'end': 2476,\n",
       "      'text': 'Luong et al., 2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "    'cite_spans': [{'start': 86,\n",
       "      'end': 102,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 971,\n",
       "      'end': 987,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1133,\n",
       "      'end': 1158,\n",
       "      'text': '(Dantzig and Thapa, 2006)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "    'cite_spans': [{'start': 107,\n",
       "      'end': 118,\n",
       "      'text': '(Lin, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "    'cite_spans': [{'start': 351,\n",
       "      'end': 365,\n",
       "      'text': '(Wasson, 1998)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'},\n",
       "     {'start': 492,\n",
       "      'end': 512,\n",
       "      'text': '(Radev et al., 2000)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'},\n",
       "     {'start': 700,\n",
       "      'end': 723,\n",
       "      'text': '(Erkan and Radev, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 737,\n",
       "      'end': 763,\n",
       "      'text': '(Mihalcea and Tarau, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "    'cite_spans': [{'start': 498,\n",
       "      'end': 518,\n",
       "      'text': '(Kingma and Ba, 2014',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 652,\n",
       "      'end': 674,\n",
       "      'text': '(Bastien et al., 2012)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "    'cite_spans': [{'start': 690,\n",
       "      'end': 707,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 908,\n",
       "      'end': 925,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [{'start': 77,\n",
       "      'end': 84,\n",
       "      'text': 'Table 1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF0'},\n",
       "     {'start': 746,\n",
       "      'end': 753,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF1'},\n",
       "     {'start': 1184,\n",
       "      'end': 1191,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 315,\n",
       "      'end': 322,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF3'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "    'cite_spans': [{'start': 517,\n",
       "      'end': 868,\n",
       "      'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {'FIGREF0': {'text': 'Figure 1: Reader comments of the news \"The most important announcements from Google\\'s big developers\\' conference (May, 2017)\".',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF2': {'text': 'Figure 2: Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence x d and comment sentence x c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. A d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'TABREF0': {'text': 'Summarization performance.',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF1': {'text': 'Further investigation of RAVAESum.',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF2': {'text': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF3': {'text': 'Generated summaries for the topic \"Sony Virtual Reality PS4\".A virtual reality headset that\\'s coming to the PlayStation 4. Today announced the development of \"Project Mor- pheus\" (Morpheus) \"a virtual reality (VR) system that takes the PlayStation4 (PS4)\". Shuhei Yoshida, presi- dent of Sony Computer Entertainment, revealed a proto- type of Morpheus at the Game Developers Conference in San Francisco on Tuesday. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. The camera on the Playstation 4 using sensors that track the player\\'s head movements.',\n",
       "    'latex': None,\n",
       "    'type': 'table'}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "    'authors': [{'first': 'Dzmitry',\n",
       "      'middle': [],\n",
       "      'last': 'Bahdanau',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "     {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '11212020'},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Theano: new features and speed improvements',\n",
       "    'authors': [{'first': 'Frédéric',\n",
       "      'middle': [],\n",
       "      'last': 'Bastien',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "     {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "     {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "     {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "     {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "     {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "     {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "     {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "    'links': '8180128'},\n",
       "   'BIBREF2': {'ref_id': 'b2',\n",
       "    'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF3': {'ref_id': 'b3',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "    'authors': [{'first': 'Shuhei',\n",
       "      'middle': [],\n",
       "      'last': 'Yoshida',\n",
       "      'suffix': ''}],\n",
       "    'year': None,\n",
       "    'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACLANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Autoencoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF11': {'ref_id': 'b11',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "    'authors': [{'first': 'Chin-Yew',\n",
       "      'middle': [],\n",
       "      'last': 'Lin',\n",
       "      'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '964287'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF14': {'ref_id': 'b14',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "    'authors': [{'first': 'Xiaojun',\n",
       "      'middle': [],\n",
       "      'last': 'Wan',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "    'year': 2007,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '7',\n",
       "    'issn': '',\n",
       "    'pages': '2903--2908',\n",
       "    'other_ids': {},\n",
       "    'links': '532313'},\n",
       "   'BIBREF20': {'ref_id': 'b20',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}},\n",
       " 'latex_parse': {'abstract': [],\n",
       "  'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "    'cite_spans': [{'start': 193,\n",
       "      'end': 200,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 203,\n",
       "      'end': 210,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 213,\n",
       "      'end': 220,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 223,\n",
       "      'end': 230,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'},\n",
       "     {'start': 233,\n",
       "      'end': 240,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 243,\n",
       "      'end': 250,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 253,\n",
       "      'end': 260,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 118,\n",
       "      'end': 125,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "    'cite_spans': [{'start': 527,\n",
       "      'end': 534,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 537,\n",
       "      'end': 544,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 802,\n",
       "      'end': 809,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 159,\n",
       "      'end': 167,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 170,\n",
       "      'end': 178,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Introduction'},\n",
       "   {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "    'cite_spans': [{'start': 489,\n",
       "      'end': 496,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 19,\n",
       "      'end': 26,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF7'}],\n",
       "    'eq_spans': [{'start': 212,\n",
       "      'end': 223,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 228,\n",
       "      'end': 239,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 254,\n",
       "      'end': 265,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 285,\n",
       "      'end': 296,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 739,\n",
       "      'end': 750,\n",
       "      'text': 'ρ i ',\n",
       "      'latex': '\\\\rho _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 774,\n",
       "      'end': 785,\n",
       "      'text': '𝐱 c i ',\n",
       "      'latex': '\\\\mathbf {x}_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 807,\n",
       "      'end': 818,\n",
       "      'text': 'ρ∈ℝ n c  ',\n",
       "      'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Overview'},\n",
       "   {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 32,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 43,\n",
       "      'end': 51,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 154,\n",
       "      'end': 161,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "      'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 488,\n",
       "      'end': 499,\n",
       "      'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "      'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "      'ref_id': None},\n",
       "     {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None},\n",
       "     {'start': 524,\n",
       "      'end': 535,\n",
       "      'text': 'σ',\n",
       "      'latex': '\\\\sigma ',\n",
       "      'ref_id': None},\n",
       "     {'start': 799,\n",
       "      'end': 811,\n",
       "      'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 50,\n",
       "      'end': 56,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'EQREF9'}],\n",
       "    'eq_spans': [{'start': 131,\n",
       "      'end': 142,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 145,\n",
       "      'end': 157,\n",
       "      'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "      'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "      'ref_id': 'EQREF10'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': '𝐱',\n",
       "      'latex': '\\\\mathbf {x}',\n",
       "      'ref_id': None},\n",
       "     {'start': 76,\n",
       "      'end': 87,\n",
       "      'text': '𝐱 d ',\n",
       "      'latex': '\\\\mathbf {x}_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐱 c ',\n",
       "      'latex': '\\\\mathbf {x}_c',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 364,\n",
       "      'end': 375,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 430,\n",
       "      'end': 441,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 444,\n",
       "      'end': 456,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "      'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "      'ref_id': 'EQREF11'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 19,\n",
       "      'end': 30,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 113,\n",
       "      'end': 124,\n",
       "      'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "      'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 219,\n",
       "      'end': 230,\n",
       "      'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "      'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "      'ref_id': None},\n",
       "     {'start': 320,\n",
       "      'end': 331,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 335,\n",
       "      'end': 346,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "     {'start': 402,\n",
       "      'end': 413,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 416,\n",
       "      'end': 428,\n",
       "      'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "      'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "      'ref_id': 'EQREF12'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 7,\n",
       "      'end': 14,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 46,\n",
       "      'end': 54,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'},\n",
       "     {'start': 57,\n",
       "      'end': 65,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 304,\n",
       "      'end': 315,\n",
       "      'text': 's h i ',\n",
       "      'latex': 's^i_{h}',\n",
       "      'ref_id': None},\n",
       "     {'start': 366,\n",
       "      'end': 377,\n",
       "      'text': 'h d j ',\n",
       "      'latex': 'h^j_{d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 401,\n",
       "      'end': 412,\n",
       "      'text': 'a d ∈ℝ n d  ',\n",
       "      'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "      'ref_id': None},\n",
       "     {'start': 472,\n",
       "      'end': 483,\n",
       "      'text': 'h c j ',\n",
       "      'latex': 'h^j_{c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 507,\n",
       "      'end': 518,\n",
       "      'text': 'a c ∈ℝ n c  ',\n",
       "      'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 616,\n",
       "      'end': 627,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 672,\n",
       "      'end': 684,\n",
       "      'text': 'a ˜ c =a c ×ρ',\n",
       "      'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "      'ref_id': 'EQREF13'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 30,\n",
       "      'end': 41,\n",
       "      'text': 'c d i ',\n",
       "      'latex': 'c_d^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 79,\n",
       "      'end': 90,\n",
       "      'text': 'c c i ',\n",
       "      'latex': 'c_c^i',\n",
       "      'ref_id': None},\n",
       "     {'start': 240,\n",
       "      'end': 252,\n",
       "      'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "      'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "      'ref_id': 'EQREF14'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 64,\n",
       "      'end': 75,\n",
       "      'text': 's ˜ h i ',\n",
       "      'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 1,\n",
       "      'end': 12,\n",
       "      'text': '𝐒 z ',\n",
       "      'latex': '\\\\mathbf {S}_z',\n",
       "      'ref_id': None},\n",
       "     {'start': 15,\n",
       "      'end': 26,\n",
       "      'text': '𝐒 h ',\n",
       "      'latex': '\\\\mathbf {S}_h',\n",
       "      'ref_id': None},\n",
       "     {'start': 33,\n",
       "      'end': 44,\n",
       "      'text': '𝐒 x ',\n",
       "      'latex': '\\\\mathbf {S}_x',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "      'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 297,\n",
       "      'end': 308,\n",
       "      'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "      'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "      'ref_id': None},\n",
       "     {'start': 569,\n",
       "      'end': 581,\n",
       "      'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "      'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "      'ref_id': 'EQREF15'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "      'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "      'ref_id': None},\n",
       "     {'start': 170,\n",
       "      'end': 182,\n",
       "      'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "      'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "      'ref_id': 'EQREF16'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'Θ',\n",
       "      'latex': '\\\\Theta ',\n",
       "      'ref_id': None},\n",
       "     {'start': 110,\n",
       "      'end': 121,\n",
       "      'text': '𝐀 d ',\n",
       "      'latex': '\\\\mathbf {A}_d',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 199,\n",
       "      'end': 210,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None},\n",
       "     {'start': 348,\n",
       "      'end': 359,\n",
       "      'text': 'X d ',\n",
       "      'latex': 'X_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 390,\n",
       "      'end': 401,\n",
       "      'text': 'X c ',\n",
       "      'latex': 'X_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 434,\n",
       "      'end': 445,\n",
       "      'text': 'R∈ℝ n d ×n c  ',\n",
       "      'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "      'ref_id': None},\n",
       "     {'start': 450,\n",
       "      'end': 462,\n",
       "      'text': 'R=X d ×X c T ',\n",
       "      'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "      'ref_id': 'EQREF17'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 93,\n",
       "      'end': 105,\n",
       "      'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "      'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "      'ref_id': 'EQREF18'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 70,\n",
       "      'end': 81,\n",
       "      'text': '(0,1)',\n",
       "      'latex': '(0,1)',\n",
       "      'ref_id': None},\n",
       "     {'start': 84,\n",
       "      'end': 96,\n",
       "      'text': 'ρ=sigmoid(𝐫)',\n",
       "      'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "      'ref_id': 'EQREF19'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 332,\n",
       "      'end': 343,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 346,\n",
       "      'end': 358,\n",
       "      'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "      'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "      'ref_id': 'EQREF20'}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'ρ z ',\n",
       "      'latex': '\\\\rho _z',\n",
       "      'ref_id': None},\n",
       "     {'start': 22,\n",
       "      'end': 33,\n",
       "      'text': 'ρ x ',\n",
       "      'latex': '\\\\rho _x',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'ρ',\n",
       "      'latex': '\\\\rho ',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Reader-Aware Salience Estimation'},\n",
       "   {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "    'cite_spans': [{'start': 82,\n",
       "      'end': 89,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 94,\n",
       "      'end': 101,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 485,\n",
       "      'end': 497,\n",
       "      'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "      'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "      'ref_id': 'EQREF22'}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "    'cite_spans': [{'start': 466,\n",
       "      'end': 474,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'},\n",
       "     {'start': 477,\n",
       "      'end': 484,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'},\n",
       "     {'start': 491,\n",
       "      'end': 498,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 644,\n",
       "      'end': 652,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 6,\n",
       "      'end': 17,\n",
       "      'text': 'α i ',\n",
       "      'latex': '\\\\alpha _i',\n",
       "      'ref_id': None},\n",
       "     {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "     {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "     {'start': 112,\n",
       "      'end': 123,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 126,\n",
       "      'end': 137,\n",
       "      'text': 'α ij ',\n",
       "      'latex': '\\\\alpha _{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 142,\n",
       "      'end': 153,\n",
       "      'text': 'R ij ',\n",
       "      'latex': 'R_{ij}',\n",
       "      'ref_id': None},\n",
       "     {'start': 220,\n",
       "      'end': 231,\n",
       "      'text': 'P i ',\n",
       "      'latex': 'P_i',\n",
       "      'ref_id': None},\n",
       "     {'start': 234,\n",
       "      'end': 245,\n",
       "      'text': 'P j ',\n",
       "      'latex': 'P_j',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Summary Construction'},\n",
       "   {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Description'},\n",
       "   {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Background'},\n",
       "   {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Collection'},\n",
       "   {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 222,\n",
       "      'end': 229,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF7'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Data Properties'},\n",
       "   {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "    'cite_spans': [{'start': 113,\n",
       "      'end': 121,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'}],\n",
       "    'ref_spans': [{'start': 58,\n",
       "      'end': 66,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'SECREF28'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Dataset and Metrics'},\n",
       "   {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "    'cite_spans': [{'start': 10,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "    'cite_spans': [{'start': 5,\n",
       "      'end': 13,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "    'cite_spans': [{'start': 9,\n",
       "      'end': 17,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 29,\n",
       "      'end': 37,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "    'cite_spans': [{'start': 8,\n",
       "      'end': 15,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF5'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Comparative Methods'},\n",
       "   {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "    'cite_spans': [{'start': 557,\n",
       "      'end': 565,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'},\n",
       "     {'start': 697,\n",
       "      'end': 705,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF21'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 94,\n",
       "      'end': 105,\n",
       "      'text': '|V|',\n",
       "      'latex': '|V|',\n",
       "      'ref_id': None},\n",
       "     {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "     {'start': 194,\n",
       "      'end': 205,\n",
       "      'text': 'n d ',\n",
       "      'latex': 'n_d',\n",
       "      'ref_id': None},\n",
       "     {'start': 210,\n",
       "      'end': 221,\n",
       "      'text': 'n c ',\n",
       "      'latex': 'n_c',\n",
       "      'ref_id': None},\n",
       "     {'start': 360,\n",
       "      'end': 371,\n",
       "      'text': 'm=5',\n",
       "      'latex': 'm = 5',\n",
       "      'ref_id': None},\n",
       "     {'start': 431,\n",
       "      'end': 442,\n",
       "      'text': 'd h =500',\n",
       "      'latex': 'd_h = 500',\n",
       "      'ref_id': None},\n",
       "     {'start': 463,\n",
       "      'end': 474,\n",
       "      'text': 'K=100',\n",
       "      'latex': 'K = 100',\n",
       "      'ref_id': None},\n",
       "     {'start': 495,\n",
       "      'end': 506,\n",
       "      'text': 'λ p ',\n",
       "      'latex': '\\\\lambda _p',\n",
       "      'ref_id': None},\n",
       "     {'start': 538,\n",
       "      'end': 549,\n",
       "      'text': 'λ p =0.2',\n",
       "      'latex': '\\\\lambda _p=0.2',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Experimental Settings'},\n",
       "   {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 83,\n",
       "      'end': 91,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF40'}],\n",
       "    'eq_spans': [{'start': 240,\n",
       "      'end': 251,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Results on Our Dataset'},\n",
       "   {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "    'cite_spans': [{'start': 208,\n",
       "      'end': 215,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 260,\n",
       "      'end': 268,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF42'}],\n",
       "    'eq_spans': [{'start': 380,\n",
       "      'end': 391,\n",
       "      'text': 'p<0.05',\n",
       "      'latex': 'p<0.05',\n",
       "      'ref_id': None}],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "    'cite_spans': [{'start': 33,\n",
       "      'end': 40,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'}],\n",
       "    'ref_spans': [{'start': 305,\n",
       "      'end': 313,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF43'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Further Investigation of Our Framework '},\n",
       "   {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 250,\n",
       "      'end': 258,\n",
       "      'text': None,\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF45'}],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Case Study'},\n",
       "   {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': 'Conclusions'}],\n",
       "  'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "    'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF9',\n",
       "    'type': 'equation'},\n",
       "   'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "    'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "    'ref_id': 'EQREF10',\n",
       "    'type': 'equation'},\n",
       "   'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "    'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "    'ref_id': 'EQREF11',\n",
       "    'type': 'equation'},\n",
       "   'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "    'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "    'ref_id': 'EQREF12',\n",
       "    'type': 'equation'},\n",
       "   'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "    'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "    'ref_id': 'EQREF13',\n",
       "    'type': 'equation'},\n",
       "   'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "    'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "    'ref_id': 'EQREF14',\n",
       "    'type': 'equation'},\n",
       "   'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "    'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "    'ref_id': 'EQREF15',\n",
       "    'type': 'equation'},\n",
       "   'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "    'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "    'ref_id': 'EQREF16',\n",
       "    'type': 'equation'},\n",
       "   'EQREF17': {'text': 'R=X d ×X c T',\n",
       "    'latex': 'R = X_d\\\\times X_c^T',\n",
       "    'ref_id': 'EQREF17',\n",
       "    'type': 'equation'},\n",
       "   'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "    'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "    'ref_id': 'EQREF18',\n",
       "    'type': 'equation'},\n",
       "   'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "    'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "    'ref_id': 'EQREF19',\n",
       "    'type': 'equation'},\n",
       "   'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "    'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "    'ref_id': 'EQREF20',\n",
       "    'type': 'equation'},\n",
       "   'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "    'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "    'ref_id': 'EQREF22',\n",
       "    'type': 'equation'},\n",
       "   'FIGREF2': {'text': '1',\n",
       "    'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF2',\n",
       "    'type': 'figure'},\n",
       "   'FIGREF7': {'text': '2',\n",
       "    'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF7',\n",
       "    'type': 'figure'},\n",
       "   'TABREF40': {'text': '1',\n",
       "    'caption': 'Summarization performance.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF40',\n",
       "    'type': 'table'},\n",
       "   'TABREF42': {'text': '2',\n",
       "    'caption': 'Further investigation of RAVAESum.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF42',\n",
       "    'type': 'table'},\n",
       "   'TABREF43': {'text': '3',\n",
       "    'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF43',\n",
       "    'type': 'table'},\n",
       "   'TABREF45': {'text': '4',\n",
       "    'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF45',\n",
       "    'type': 'table'},\n",
       "   'TABREF46': {'text': '5',\n",
       "    'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "    'latex': [],\n",
       "    'ref_id': 'TABREF46',\n",
       "    'type': 'table'},\n",
       "   'SECREF1': {'text': 'Introduction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF1',\n",
       "    'type': 'section'},\n",
       "   'SECREF2': {'text': 'Framework',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF2',\n",
       "    'type': 'section'},\n",
       "   'SECREF6': {'text': 'Conclusions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF6',\n",
       "    'type': 'section'},\n",
       "   'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF8',\n",
       "    'type': 'section'},\n",
       "   'SECREF21': {'text': 'Summary Construction',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF21',\n",
       "    'type': 'section'},\n",
       "   'SECREF3': {'text': 'Data Description',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF3',\n",
       "    'type': 'section'},\n",
       "   'SECREF24': {'text': 'Background',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF24',\n",
       "    'type': 'section'},\n",
       "   'SECREF26': {'text': 'Data Collection',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF26',\n",
       "    'type': 'section'},\n",
       "   'SECREF28': {'text': 'Data Properties',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF28',\n",
       "    'type': 'section'},\n",
       "   'SECREF4': {'text': 'Experimental Setup',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF4',\n",
       "    'type': 'section'},\n",
       "   'SECREF29': {'text': 'Dataset and Metrics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF29',\n",
       "    'type': 'section'},\n",
       "   'SECREF31': {'text': 'Comparative Methods',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF31',\n",
       "    'type': 'section'},\n",
       "   'SECREF37': {'text': 'Experimental Settings',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF37',\n",
       "    'type': 'section'},\n",
       "   'SECREF5': {'text': 'Results and Discussions',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF5',\n",
       "    'type': 'section'},\n",
       "   'SECREF39': {'text': 'Results on Our Dataset',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF39',\n",
       "    'type': 'section'},\n",
       "   'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF41',\n",
       "    'type': 'section'},\n",
       "   'SECREF44': {'text': 'Case Study',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF44',\n",
       "    'type': 'section'},\n",
       "   'SECREF7': {'text': 'Topics',\n",
       "    'latex': None,\n",
       "    'ref_id': 'SECREF7',\n",
       "    'type': 'section'}},\n",
       "  'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "    'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "    'authors': [{'first': 'Lidong',\n",
       "      'middle': [],\n",
       "      'last': 'Bing',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1587--1597',\n",
       "    'other_ids': {},\n",
       "    'links': '8377315'},\n",
       "   'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "    'title': 'Linear programming 1: introduction',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "     {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "    'year': 2006,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '53739754'},\n",
       "   'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "    'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "    'authors': [{'first': 'Günes',\n",
       "      'middle': [],\n",
       "      'last': 'Erkan',\n",
       "      'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '4',\n",
       "    'issn': '',\n",
       "    'pages': '365--371',\n",
       "    'other_ids': {},\n",
       "    'links': '10418456'},\n",
       "   'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "    'title': 'Multi-document summarization by sentence extraction',\n",
       "    'authors': [{'first': 'Jade',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "     {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'NAACL-ANLPWorkshop',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '40--48',\n",
       "    'other_ids': {},\n",
       "    'links': '8294822'},\n",
       "   'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "    'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "    'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "     {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "     {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '291--298',\n",
       "    'other_ids': {},\n",
       "    'links': '13723748'},\n",
       "   'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "    'title': 'Adam: A method for stochastic optimization',\n",
       "    'authors': [{'first': 'Diederik',\n",
       "      'middle': [],\n",
       "      'last': 'Kingma',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '6628106'},\n",
       "   'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "    'title': 'Auto-encoding variational bayes',\n",
       "    'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "     {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICLR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '15789289'},\n",
       "   'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "    'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1270--1276',\n",
       "    'other_ids': {},\n",
       "    'links': '14777460'},\n",
       "   'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "    'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "    'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "     {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "     {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "     {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3497--3503',\n",
       "    'other_ids': {},\n",
       "    'links': '29562039'},\n",
       "   'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "    'title': 'Effective approaches to attention-based neural machine translation',\n",
       "    'authors': [{'first': 'Minh-Thang',\n",
       "      'middle': [],\n",
       "      'last': 'Luong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "     {'first': 'Christopher D',\n",
       "      'middle': [],\n",
       "      'last': 'Manning',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'EMNLP',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1412--1421',\n",
       "    'other_ids': {},\n",
       "    'links': '1998416'},\n",
       "   'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "    'title': 'Textrank: Bringing order into texts',\n",
       "    'authors': [{'first': 'Rada',\n",
       "      'middle': [],\n",
       "      'last': 'Mihalcea',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "    'year': 2004,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '577937'},\n",
       "   'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "    'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "    'authors': [{'first': 'Yen',\n",
       "      'middle': ['Kan'],\n",
       "      'last': 'Ziheng Lin Min',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'COLING',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2093--2108',\n",
       "    'other_ids': {},\n",
       "    'links': '6317274'},\n",
       "   'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "    'title': 'A survey of text summarization techniques',\n",
       "    'authors': [{'first': 'Ani',\n",
       "      'middle': [],\n",
       "      'last': 'Nenkova',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Mining Text Data',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '43--76',\n",
       "    'other_ids': {},\n",
       "    'links': '556431'},\n",
       "   'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "    'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "    'authors': [{'first': 'Hongyan',\n",
       "      'middle': [],\n",
       "      'last': 'Dragomir R Radev',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "    'year': 2000,\n",
       "    'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '21--30',\n",
       "    'other_ids': {},\n",
       "    'links': '1320'},\n",
       "   'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "    'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "    'authors': [{'first': 'Danilo',\n",
       "      'middle': [],\n",
       "      'last': 'Jimenez Rezende',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "     {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': 'ICML',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1278--1286',\n",
       "    'other_ids': {},\n",
       "    'links': '16895865'},\n",
       "   'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "    'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "    'authors': [{'first': 'Mark',\n",
       "      'middle': [],\n",
       "      'last': 'Wasson',\n",
       "      'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': 'ACL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1364--1368',\n",
       "    'other_ids': {},\n",
       "    'links': '12681629'},\n",
       "   'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "    'title': 'Multiple aspect summarization using integer linear programming',\n",
       "    'authors': [{'first': 'Kristian',\n",
       "      'middle': [],\n",
       "      'last': 'Woodsend',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'EMNLP-CNLL',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '233--243',\n",
       "    'other_ids': {},\n",
       "    'links': '17497992'},\n",
       "   'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "    'title': 'Social context summarization',\n",
       "    'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "     {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "     {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "     {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "     {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "     {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'SIGIR',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '255--264',\n",
       "    'other_ids': {},\n",
       "    'links': '704517'}}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "  'title': 'Multi-document summarization by sentence extraction',\n",
       "  'authors': [{'first': 'Jade',\n",
       "    'middle': [],\n",
       "    'last': 'Goldstein',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "   {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "   {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'NAACL-ANLPWorkshop',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '40--48',\n",
       "  'other_ids': {},\n",
       "  'links': '8294822'},\n",
       " 'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "  'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "  'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '4',\n",
       "  'issn': '',\n",
       "  'pages': '365--371',\n",
       "  'other_ids': {},\n",
       "  'links': '10418456'},\n",
       " 'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "  'title': 'Auto-encoding variational bayes',\n",
       "  'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "   {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '15789289'},\n",
       " 'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "  'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "  'authors': [{'first': 'Danilo',\n",
       "    'middle': [],\n",
       "    'last': 'Jimenez Rezende',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "   {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICML',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1278--1286',\n",
       "  'other_ids': {},\n",
       "  'links': '16895865'},\n",
       " 'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "  'title': 'Effective approaches to attention-based neural machine translation',\n",
       "  'authors': [{'first': 'Minh-Thang',\n",
       "    'middle': [],\n",
       "    'last': 'Luong',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "   {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1412--1421',\n",
       "  'other_ids': {},\n",
       "  'links': '1998416'},\n",
       " 'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "  'title': 'Multiple aspect summarization using integer linear programming',\n",
       "  'authors': [{'first': 'Kristian',\n",
       "    'middle': [],\n",
       "    'last': 'Woodsend',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'EMNLP-CNLL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '233--243',\n",
       "  'other_ids': {},\n",
       "  'links': '17497992'},\n",
       " 'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "  'title': 'Linear programming 1: introduction',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "   {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "  'year': 2006,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '53739754'},\n",
       " 'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "  'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "  'authors': [{'first': 'Mark', 'middle': [], 'last': 'Wasson', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1364--1368',\n",
       "  'other_ids': {},\n",
       "  'links': '12681629'},\n",
       " 'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "  'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "  'authors': [{'first': 'Hongyan',\n",
       "    'middle': [],\n",
       "    'last': 'Dragomir R Radev',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '21--30',\n",
       "  'other_ids': {},\n",
       "  'links': '1320'},\n",
       " 'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "  'title': 'Textrank: Bringing order into texts',\n",
       "  'authors': [{'first': 'Rada',\n",
       "    'middle': [],\n",
       "    'last': 'Mihalcea',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '577937'},\n",
       " 'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "  'title': 'Adam: A method for stochastic optimization',\n",
       "  'authors': [{'first': 'Diederik',\n",
       "    'middle': [],\n",
       "    'last': 'Kingma',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '6628106'},\n",
       " 'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "  'title': 'A survey of text summarization techniques',\n",
       "  'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "   {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Mining Text Data',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '43--76',\n",
       "  'other_ids': {},\n",
       "  'links': '556431'},\n",
       " 'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "  'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "  'authors': [{'first': 'Yen',\n",
       "    'middle': ['Kan'],\n",
       "    'last': 'Ziheng Lin Min',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'COLING',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '2093--2108',\n",
       "  'other_ids': {},\n",
       "  'links': '6317274'},\n",
       " 'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "  'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "  'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "   {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1587--1597',\n",
       "  'other_ids': {},\n",
       "  'links': '8377315'},\n",
       " 'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "  'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'AAAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '3497--3503',\n",
       "  'other_ids': {},\n",
       "  'links': '29562039'},\n",
       " 'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "  'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "  'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "   {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "   {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '291--298',\n",
       "  'other_ids': {},\n",
       "  'links': '13723748'},\n",
       " 'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "  'title': 'Social context summarization',\n",
       "  'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "   {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "   {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "   {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "   {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '255--264',\n",
       "  'other_ids': {},\n",
       "  'links': '704517'},\n",
       " 'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "  'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1270--1276',\n",
       "  'other_ids': {},\n",
       "  'links': '14777460'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: all_articles[0]['latex_parse']['bib_entries'][k] for k in sorted(all_articles[0]['latex_parse']['bib_entries'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': [],\n",
       " 'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "   'cite_spans': [{'start': 193,\n",
       "     'end': 200,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF0'},\n",
       "    {'start': 203,\n",
       "     'end': 210,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 213,\n",
       "     'end': 220,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF2'},\n",
       "    {'start': 223,\n",
       "     'end': 230,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF3'},\n",
       "    {'start': 233,\n",
       "     'end': 240,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF4'},\n",
       "    {'start': 243,\n",
       "     'end': 250,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 253,\n",
       "     'end': 260,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 118,\n",
       "     'end': 125,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF2'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "   'cite_spans': [{'start': 527,\n",
       "     'end': 534,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF7'},\n",
       "    {'start': 537,\n",
       "     'end': 544,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF8'},\n",
       "    {'start': 802,\n",
       "     'end': 809,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "   'cite_spans': [{'start': 10,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'},\n",
       "    {'start': 159,\n",
       "     'end': 167,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 170,\n",
       "     'end': 178,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "   'cite_spans': [{'start': 489,\n",
       "     'end': 496,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 19,\n",
       "     'end': 26,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF7'}],\n",
       "   'eq_spans': [{'start': 212,\n",
       "     'end': 223,\n",
       "     'text': 'X d ',\n",
       "     'latex': 'X_d',\n",
       "     'ref_id': None},\n",
       "    {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None},\n",
       "    {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None},\n",
       "    {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None},\n",
       "    {'start': 739,\n",
       "     'end': 750,\n",
       "     'text': 'ρ i ',\n",
       "     'latex': '\\\\rho _i',\n",
       "     'ref_id': None},\n",
       "    {'start': 774,\n",
       "     'end': 785,\n",
       "     'text': '𝐱 c i ',\n",
       "     'latex': '\\\\mathbf {x}_c^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 807,\n",
       "     'end': 818,\n",
       "     'text': 'ρ∈ℝ n c  ',\n",
       "     'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Overview'},\n",
       "  {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 32,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 43,\n",
       "     'end': 51,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'},\n",
       "    {'start': 154,\n",
       "     'end': 161,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 472,\n",
       "     'end': 483,\n",
       "     'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "     'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "     'ref_id': None},\n",
       "    {'start': 488,\n",
       "     'end': 499,\n",
       "     'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "     'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "     'ref_id': None},\n",
       "    {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None},\n",
       "    {'start': 524,\n",
       "     'end': 535,\n",
       "     'text': 'σ',\n",
       "     'latex': '\\\\sigma ',\n",
       "     'ref_id': None},\n",
       "    {'start': 799,\n",
       "     'end': 811,\n",
       "     'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "     'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "     'ref_id': 'EQREF9'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 50,\n",
       "     'end': 56,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'EQREF9'}],\n",
       "   'eq_spans': [{'start': 131,\n",
       "     'end': 142,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 145,\n",
       "     'end': 157,\n",
       "     'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "     'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "     'ref_id': 'EQREF10'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': '𝐱',\n",
       "     'latex': '\\\\mathbf {x}',\n",
       "     'ref_id': None},\n",
       "    {'start': 76,\n",
       "     'end': 87,\n",
       "     'text': '𝐱 d ',\n",
       "     'latex': '\\\\mathbf {x}_d',\n",
       "     'ref_id': None},\n",
       "    {'start': 110,\n",
       "     'end': 121,\n",
       "     'text': '𝐱 c ',\n",
       "     'latex': '\\\\mathbf {x}_c',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 364,\n",
       "     'end': 375,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 444,\n",
       "     'end': 456,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "     'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "     'ref_id': 'EQREF11'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 19,\n",
       "     'end': 30,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 113,\n",
       "     'end': 124,\n",
       "     'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "     'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "    {'start': 219,\n",
       "     'end': 230,\n",
       "     'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "     'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 320,\n",
       "     'end': 331,\n",
       "     'text': '𝐒 z ',\n",
       "     'latex': '\\\\mathbf {S}_z',\n",
       "     'ref_id': None},\n",
       "    {'start': 335,\n",
       "     'end': 346,\n",
       "     'text': '𝐒 h ',\n",
       "     'latex': '\\\\mathbf {S}_h',\n",
       "     'ref_id': None},\n",
       "    {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "    {'start': 402,\n",
       "     'end': 413,\n",
       "     'text': '𝐒 x ',\n",
       "     'latex': '\\\\mathbf {S}_x',\n",
       "     'ref_id': None},\n",
       "    {'start': 416,\n",
       "     'end': 428,\n",
       "     'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "     'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "     'ref_id': 'EQREF12'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 7,\n",
       "     'end': 14,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'},\n",
       "    {'start': 46,\n",
       "     'end': 54,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF12'},\n",
       "    {'start': 57,\n",
       "     'end': 65,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF13'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 304,\n",
       "     'end': 315,\n",
       "     'text': 's h i ',\n",
       "     'latex': 's^i_{h}',\n",
       "     'ref_id': None},\n",
       "    {'start': 366,\n",
       "     'end': 377,\n",
       "     'text': 'h d j ',\n",
       "     'latex': 'h^j_{d}',\n",
       "     'ref_id': None},\n",
       "    {'start': 401,\n",
       "     'end': 412,\n",
       "     'text': 'a d ∈ℝ n d  ',\n",
       "     'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "     'ref_id': None},\n",
       "    {'start': 472,\n",
       "     'end': 483,\n",
       "     'text': 'h c j ',\n",
       "     'latex': 'h^j_{c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 507,\n",
       "     'end': 518,\n",
       "     'text': 'a c ∈ℝ n c  ',\n",
       "     'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 672,\n",
       "     'end': 684,\n",
       "     'text': 'a ˜ c =a c ×ρ',\n",
       "     'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "     'ref_id': 'EQREF13'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 30,\n",
       "     'end': 41,\n",
       "     'text': 'c d i ',\n",
       "     'latex': 'c_d^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 79,\n",
       "     'end': 90,\n",
       "     'text': 'c c i ',\n",
       "     'latex': 'c_c^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 240,\n",
       "     'end': 252,\n",
       "     'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "     'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "     'ref_id': 'EQREF14'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 64,\n",
       "     'end': 75,\n",
       "     'text': 's ˜ h i ',\n",
       "     'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': '𝐒 z ',\n",
       "     'latex': '\\\\mathbf {S}_z',\n",
       "     'ref_id': None},\n",
       "    {'start': 15,\n",
       "     'end': 26,\n",
       "     'text': '𝐒 h ',\n",
       "     'latex': '\\\\mathbf {S}_h',\n",
       "     'ref_id': None},\n",
       "    {'start': 33,\n",
       "     'end': 44,\n",
       "     'text': '𝐒 x ',\n",
       "     'latex': '\\\\mathbf {S}_x',\n",
       "     'ref_id': None},\n",
       "    {'start': 220,\n",
       "     'end': 231,\n",
       "     'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "     'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "     'ref_id': None},\n",
       "    {'start': 297,\n",
       "     'end': 308,\n",
       "     'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "     'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "     'ref_id': None},\n",
       "    {'start': 569,\n",
       "     'end': 581,\n",
       "     'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "     'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "     'ref_id': 'EQREF15'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 70,\n",
       "     'end': 81,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 170,\n",
       "     'end': 182,\n",
       "     'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "     'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "     'ref_id': 'EQREF16'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'Θ',\n",
       "     'latex': '\\\\Theta ',\n",
       "     'ref_id': None},\n",
       "    {'start': 110,\n",
       "     'end': 121,\n",
       "     'text': '𝐀 d ',\n",
       "     'latex': '\\\\mathbf {A}_d',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 94,\n",
       "     'end': 105,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None},\n",
       "    {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None},\n",
       "    {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None},\n",
       "    {'start': 434,\n",
       "     'end': 445,\n",
       "     'text': 'R∈ℝ n d ×n c  ',\n",
       "     'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 450,\n",
       "     'end': 462,\n",
       "     'text': 'R=X d ×X c T ',\n",
       "     'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "     'ref_id': 'EQREF17'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 93,\n",
       "     'end': 105,\n",
       "     'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "     'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "     'ref_id': 'EQREF18'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 70,\n",
       "     'end': 81,\n",
       "     'text': '(0,1)',\n",
       "     'latex': '(0,1)',\n",
       "     'ref_id': None},\n",
       "    {'start': 84,\n",
       "     'end': 96,\n",
       "     'text': 'ρ=sigmoid(𝐫)',\n",
       "     'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "     'ref_id': 'EQREF19'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 332,\n",
       "     'end': 343,\n",
       "     'text': 'λ p ',\n",
       "     'latex': '\\\\lambda _p',\n",
       "     'ref_id': None},\n",
       "    {'start': 346,\n",
       "     'end': 358,\n",
       "     'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "     'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "     'ref_id': 'EQREF20'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'ρ z ',\n",
       "     'latex': '\\\\rho _z',\n",
       "     'ref_id': None},\n",
       "    {'start': 22,\n",
       "     'end': 33,\n",
       "     'text': 'ρ x ',\n",
       "     'latex': '\\\\rho _x',\n",
       "     'ref_id': None},\n",
       "    {'start': 142,\n",
       "     'end': 153,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 82,\n",
       "     'end': 89,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 94,\n",
       "     'end': 101,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 485,\n",
       "     'end': 497,\n",
       "     'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "     'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "     'ref_id': 'EQREF22'}],\n",
       "   'section': 'Summary Construction'},\n",
       "  {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "   'cite_spans': [{'start': 466,\n",
       "     'end': 474,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF14'},\n",
       "    {'start': 477,\n",
       "     'end': 484,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 491,\n",
       "     'end': 498,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'},\n",
       "    {'start': 644,\n",
       "     'end': 652,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF15'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'α i ',\n",
       "     'latex': '\\\\alpha _i',\n",
       "     'ref_id': None},\n",
       "    {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "    {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 126,\n",
       "     'end': 137,\n",
       "     'text': 'α ij ',\n",
       "     'latex': '\\\\alpha _{ij}',\n",
       "     'ref_id': None},\n",
       "    {'start': 142,\n",
       "     'end': 153,\n",
       "     'text': 'R ij ',\n",
       "     'latex': 'R_{ij}',\n",
       "     'ref_id': None},\n",
       "    {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 234,\n",
       "     'end': 245,\n",
       "     'text': 'P j ',\n",
       "     'latex': 'P_j',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Summary Construction'},\n",
       "  {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Description'},\n",
       "  {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 222,\n",
       "     'end': 229,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF7'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Properties'},\n",
       "  {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "   'cite_spans': [{'start': 113,\n",
       "     'end': 121,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF16'}],\n",
       "   'ref_spans': [{'start': 58,\n",
       "     'end': 66,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF28'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Dataset and Metrics'},\n",
       "  {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "   'cite_spans': [{'start': 10,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "   'cite_spans': [{'start': 5,\n",
       "     'end': 13,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF17'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "   'cite_spans': [{'start': 9,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF18'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "   'cite_spans': [{'start': 8,\n",
       "     'end': 15,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 29,\n",
       "     'end': 37,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF19'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "   'cite_spans': [{'start': 8,\n",
       "     'end': 15,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "   'cite_spans': [{'start': 557,\n",
       "     'end': 565,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF20'},\n",
       "    {'start': 697,\n",
       "     'end': 705,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF21'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 94,\n",
       "     'end': 105,\n",
       "     'text': '|V|',\n",
       "     'latex': '|V|',\n",
       "     'ref_id': None},\n",
       "    {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "    {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None},\n",
       "    {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None},\n",
       "    {'start': 360,\n",
       "     'end': 371,\n",
       "     'text': 'm=5',\n",
       "     'latex': 'm = 5',\n",
       "     'ref_id': None},\n",
       "    {'start': 431,\n",
       "     'end': 442,\n",
       "     'text': 'd h =500',\n",
       "     'latex': 'd_h = 500',\n",
       "     'ref_id': None},\n",
       "    {'start': 463,\n",
       "     'end': 474,\n",
       "     'text': 'K=100',\n",
       "     'latex': 'K = 100',\n",
       "     'ref_id': None},\n",
       "    {'start': 495,\n",
       "     'end': 506,\n",
       "     'text': 'λ p ',\n",
       "     'latex': '\\\\lambda _p',\n",
       "     'ref_id': None},\n",
       "    {'start': 538,\n",
       "     'end': 549,\n",
       "     'text': 'λ p =0.2',\n",
       "     'latex': '\\\\lambda _p=0.2',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Experimental Settings'},\n",
       "  {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 83,\n",
       "     'end': 91,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF40'}],\n",
       "   'eq_spans': [{'start': 240,\n",
       "     'end': 251,\n",
       "     'text': 'p<0.05',\n",
       "     'latex': 'p<0.05',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Results on Our Dataset'},\n",
       "  {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "   'cite_spans': [{'start': 208,\n",
       "     'end': 215,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 260,\n",
       "     'end': 268,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF42'}],\n",
       "   'eq_spans': [{'start': 380,\n",
       "     'end': 391,\n",
       "     'text': 'p<0.05',\n",
       "     'latex': 'p<0.05',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Further Investigation of Our Framework '},\n",
       "  {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "   'cite_spans': [{'start': 33,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 305,\n",
       "     'end': 313,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF43'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Further Investigation of Our Framework '},\n",
       "  {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 250,\n",
       "     'end': 258,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF45'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Case Study'},\n",
       "  {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Conclusions'}],\n",
       " 'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "   'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "   'ref_id': 'EQREF9',\n",
       "   'type': 'equation'},\n",
       "  'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "   'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "   'ref_id': 'EQREF10',\n",
       "   'type': 'equation'},\n",
       "  'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "   'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "   'ref_id': 'EQREF11',\n",
       "   'type': 'equation'},\n",
       "  'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "   'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "   'ref_id': 'EQREF12',\n",
       "   'type': 'equation'},\n",
       "  'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "   'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "   'ref_id': 'EQREF13',\n",
       "   'type': 'equation'},\n",
       "  'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "   'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "   'ref_id': 'EQREF14',\n",
       "   'type': 'equation'},\n",
       "  'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "   'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "   'ref_id': 'EQREF15',\n",
       "   'type': 'equation'},\n",
       "  'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "   'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "   'ref_id': 'EQREF16',\n",
       "   'type': 'equation'},\n",
       "  'EQREF17': {'text': 'R=X d ×X c T',\n",
       "   'latex': 'R = X_d\\\\times X_c^T',\n",
       "   'ref_id': 'EQREF17',\n",
       "   'type': 'equation'},\n",
       "  'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "   'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "   'ref_id': 'EQREF18',\n",
       "   'type': 'equation'},\n",
       "  'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "   'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "   'ref_id': 'EQREF19',\n",
       "   'type': 'equation'},\n",
       "  'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "   'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "   'ref_id': 'EQREF20',\n",
       "   'type': 'equation'},\n",
       "  'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "   'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "   'ref_id': 'EQREF22',\n",
       "   'type': 'equation'},\n",
       "  'FIGREF2': {'text': '1',\n",
       "   'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "   'latex': None,\n",
       "   'ref_id': 'FIGREF2',\n",
       "   'type': 'figure'},\n",
       "  'FIGREF7': {'text': '2',\n",
       "   'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "   'latex': None,\n",
       "   'ref_id': 'FIGREF7',\n",
       "   'type': 'figure'},\n",
       "  'TABREF40': {'text': '1',\n",
       "   'caption': 'Summarization performance.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF40',\n",
       "   'type': 'table'},\n",
       "  'TABREF42': {'text': '2',\n",
       "   'caption': 'Further investigation of RAVAESum.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF42',\n",
       "   'type': 'table'},\n",
       "  'TABREF43': {'text': '3',\n",
       "   'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF43',\n",
       "   'type': 'table'},\n",
       "  'TABREF45': {'text': '4',\n",
       "   'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF45',\n",
       "   'type': 'table'},\n",
       "  'TABREF46': {'text': '5',\n",
       "   'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF46',\n",
       "   'type': 'table'},\n",
       "  'SECREF1': {'text': 'Introduction',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF1',\n",
       "   'type': 'section'},\n",
       "  'SECREF2': {'text': 'Framework',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF2',\n",
       "   'type': 'section'},\n",
       "  'SECREF6': {'text': 'Conclusions',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF6',\n",
       "   'type': 'section'},\n",
       "  'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF8',\n",
       "   'type': 'section'},\n",
       "  'SECREF21': {'text': 'Summary Construction',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF21',\n",
       "   'type': 'section'},\n",
       "  'SECREF3': {'text': 'Data Description',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF3',\n",
       "   'type': 'section'},\n",
       "  'SECREF24': {'text': 'Background',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF24',\n",
       "   'type': 'section'},\n",
       "  'SECREF26': {'text': 'Data Collection',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF26',\n",
       "   'type': 'section'},\n",
       "  'SECREF28': {'text': 'Data Properties',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF28',\n",
       "   'type': 'section'},\n",
       "  'SECREF4': {'text': 'Experimental Setup',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF4',\n",
       "   'type': 'section'},\n",
       "  'SECREF29': {'text': 'Dataset and Metrics',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF29',\n",
       "   'type': 'section'},\n",
       "  'SECREF31': {'text': 'Comparative Methods',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF31',\n",
       "   'type': 'section'},\n",
       "  'SECREF37': {'text': 'Experimental Settings',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF37',\n",
       "   'type': 'section'},\n",
       "  'SECREF5': {'text': 'Results and Discussions',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF5',\n",
       "   'type': 'section'},\n",
       "  'SECREF39': {'text': 'Results on Our Dataset',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF39',\n",
       "   'type': 'section'},\n",
       "  'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF41',\n",
       "   'type': 'section'},\n",
       "  'SECREF44': {'text': 'Case Study',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF44',\n",
       "   'type': 'section'},\n",
       "  'SECREF7': {'text': 'Topics',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF7',\n",
       "   'type': 'section'}},\n",
       " 'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "   'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "   'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "    {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'ACL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1587--1597',\n",
       "   'other_ids': {},\n",
       "   'links': '8377315'},\n",
       "  'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "   'title': 'Linear programming 1: introduction',\n",
       "   'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "    {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "   'year': 2006,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '53739754'},\n",
       "  'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "   'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "   'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "   'year': 2004,\n",
       "   'venue': 'EMNLP',\n",
       "   'volume': '4',\n",
       "   'issn': '',\n",
       "   'pages': '365--371',\n",
       "   'other_ids': {},\n",
       "   'links': '10418456'},\n",
       "  'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "   'title': 'Multi-document summarization by sentence extraction',\n",
       "   'authors': [{'first': 'Jade',\n",
       "     'middle': [],\n",
       "     'last': 'Goldstein',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "    {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "    {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "   'year': 2000,\n",
       "   'venue': 'NAACL-ANLPWorkshop',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '40--48',\n",
       "   'other_ids': {},\n",
       "   'links': '8294822'},\n",
       "  'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "   'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "   'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "    {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "    {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "   'year': 2008,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '291--298',\n",
       "   'other_ids': {},\n",
       "   'links': '13723748'},\n",
       "  'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "   'title': 'Adam: A method for stochastic optimization',\n",
       "   'authors': [{'first': 'Diederik',\n",
       "     'middle': [],\n",
       "     'last': 'Kingma',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '6628106'},\n",
       "  'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "   'title': 'Auto-encoding variational bayes',\n",
       "   'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "    {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '15789289'},\n",
       "  'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "   'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'IJCAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1270--1276',\n",
       "   'other_ids': {},\n",
       "   'links': '14777460'},\n",
       "  'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "   'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'AAAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '3497--3503',\n",
       "   'other_ids': {},\n",
       "   'links': '29562039'},\n",
       "  'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "   'title': 'Effective approaches to attention-based neural machine translation',\n",
       "   'authors': [{'first': 'Minh-Thang',\n",
       "     'middle': [],\n",
       "     'last': 'Luong',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "    {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'EMNLP',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1412--1421',\n",
       "   'other_ids': {},\n",
       "   'links': '1998416'},\n",
       "  'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "   'title': 'Textrank: Bringing order into texts',\n",
       "   'authors': [{'first': 'Rada',\n",
       "     'middle': [],\n",
       "     'last': 'Mihalcea',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "   'year': 2004,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '577937'},\n",
       "  'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "   'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "   'authors': [{'first': 'Yen',\n",
       "     'middle': ['Kan'],\n",
       "     'last': 'Ziheng Lin Min',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'COLING',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '2093--2108',\n",
       "   'other_ids': {},\n",
       "   'links': '6317274'},\n",
       "  'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "   'title': 'A survey of text summarization techniques',\n",
       "   'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "    {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'Mining Text Data',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '43--76',\n",
       "   'other_ids': {},\n",
       "   'links': '556431'},\n",
       "  'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "   'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "   'authors': [{'first': 'Hongyan',\n",
       "     'middle': [],\n",
       "     'last': 'Dragomir R Radev',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "   'year': 2000,\n",
       "   'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '21--30',\n",
       "   'other_ids': {},\n",
       "   'links': '1320'},\n",
       "  'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "   'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "   'authors': [{'first': 'Danilo',\n",
       "     'middle': [],\n",
       "     'last': 'Jimenez Rezende',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "    {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1278--1286',\n",
       "   'other_ids': {},\n",
       "   'links': '16895865'},\n",
       "  'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "   'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "   'authors': [{'first': 'Mark',\n",
       "     'middle': [],\n",
       "     'last': 'Wasson',\n",
       "     'suffix': ''}],\n",
       "   'year': 1998,\n",
       "   'venue': 'ACL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1364--1368',\n",
       "   'other_ids': {},\n",
       "   'links': '12681629'},\n",
       "  'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "   'title': 'Multiple aspect summarization using integer linear programming',\n",
       "   'authors': [{'first': 'Kristian',\n",
       "     'middle': [],\n",
       "     'last': 'Woodsend',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'EMNLP-CNLL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '233--243',\n",
       "   'other_ids': {},\n",
       "   'links': '17497992'},\n",
       "  'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "   'title': 'Social context summarization',\n",
       "   'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "    {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "    {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "    {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "    {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '255--264',\n",
       "   'other_ids': {},\n",
       "   'links': '704517'}}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_with_sect_latex = dict()\n",
    "for article in all_articles:\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']: \n",
    "        for sections in article['latex_parse']['body_text']:\n",
    "            if sections['section']:\n",
    "                if article['paper_id'] in article_with_sect_latex:\n",
    "                    article_with_sect_latex[article['paper_id']] +=1\n",
    "                else:\n",
    "                    article_with_sect_latex[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Количество статей, в которых есть названия секций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3657"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_with_sect_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение количества выделенных ссылок в grobid_parse & latex_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 16050464\n",
      "101 173990592\n",
      "286 29245285\n",
      "320 100300\n",
      "366 2558\n",
      "530 86813509\n",
      "579 1703535\n",
      "634 14170854\n",
      "687 49358911\n",
      "721 371926\n",
      "736 682772\n",
      "740 2840197\n",
      "779 2411\n",
      "817 16273304\n",
      "874 17511008\n",
      "935 3101294\n",
      "936 3161327\n",
      "942 5740960\n",
      "972 2145766\n",
      "1023 52073201\n",
      "1127 5079594\n",
      "1206 298504\n",
      "1212 870921\n",
      "1257 53217693\n",
      "1380 1438450\n",
      "1401 309476\n",
      "1402 44278\n",
      "1414 7669927\n",
      "1571 711424\n",
      "1604 1423962\n",
      "1801 370914\n",
      "1833 27246259\n",
      "1846 15600925\n",
      "1864 184488087\n",
      "1919 10086161\n",
      "1976 11492268\n",
      "2153 534431\n",
      "2190 15986631\n",
      "2238 189998202\n",
      "2255 21665312\n",
      "2299 9371149\n",
      "2302 311594\n",
      "2305 6256345\n",
      "2412 14974\n",
      "2428 3933075\n",
      "2483 1571038\n",
      "2489 2862211\n",
      "2495 6210126\n",
      "2629 44123113\n",
      "2705 15881253\n",
      "2784 5201435\n",
      "2823 5054582\n",
      "2919 10324034\n",
      "2998 7021843\n",
      "3198 3152424\n",
      "3274 139106285\n",
      "3291 14922772\n",
      "3292 2652169\n",
      "3318 3204831\n",
      "3383 12245103\n",
      "3422 85543217\n",
      "3434 17297069\n",
      "3485 621025\n",
      "3538 85556928\n",
      "3571 13661068\n",
      "3572 1765384\n",
      "3589 3025759\n",
      "3768 1871596\n",
      "3815 27914547\n",
      "3841 53082704\n",
      "3852 118680003\n",
      "3857 11451871\n",
      "4013 3132651\n",
      "4039 20995314\n",
      "4042 85529973\n",
      "4043 14401063\n",
      "4050 2213896\n",
      "4078 3204935\n",
      "4110 537\n",
      "4230 1031444\n",
      "4249 5112203\n",
      "4256 465\n",
      "4277 174798366\n",
      "4328 52123817\n",
      "4381 102353614\n",
      "4406 4943905\n",
      "4529 3262717\n",
      "4641 2373337\n",
      "4682 195658138\n",
      "4768 84842\n",
      "4891 5848469\n",
      "4895 91183987\n",
      "5024 969555\n",
      "5069 1707814\n",
      "5130 3543642\n",
      "5200 2315102\n",
      "5321 10636826\n",
      "5324 6444309\n",
      "5327 14330405\n",
      "5397 52112703\n",
      "5399 30617432\n",
      "5497 14817153\n",
      "5500 174802484\n",
      "5633 6985080\n",
      "5641 6160155\n",
      "5670 72940921\n",
      "5692 15360530\n",
      "5731 6519154\n",
      "5750 15314162\n",
      "5760 102351489\n",
      "5822 174802490\n",
      "5933 119190241\n",
      "6076 53080846\n",
      "6084 6508854\n",
      "6209 174798343\n",
      "6219 5062433\n",
      "6230 4328410\n",
      "6445 3115914\n",
      "6457 5536079\n",
      "6494 173188095\n",
      "6498 85518248\n",
      "6511 11805625\n",
      "6518 3725815\n",
      "6561 661\n",
      "6592 12051021\n",
      "6593 102353391\n",
      "6604 85496823\n",
      "6611 21678954\n",
      "6706 49865798\n",
      "6724 52009840\n",
      "6727 6497091\n",
      "6788 10694414\n",
      "6821 155091369\n",
      "6871 6972699\n",
      "6926 13696025\n",
      "7054 195218358\n",
      "7089 1175070\n",
      "7131 29622196\n",
      "7138 6278197\n",
      "7158 2162648\n",
      "7185 10182773\n",
      "7261 1138221\n",
      "7265 2367456\n",
      "7349 384520\n",
      "7453 2476229\n",
      "7554 160010264\n",
      "7571 8703520\n",
      "7595 5666926\n",
      "7624 278288\n",
      "7670 11503583\n",
      "7692 1477127\n",
      "7766 13533556\n",
      "7810 174797774\n",
      "7824 1662415\n",
      "7886 29154296\n",
      "7928 53083054\n",
      "7954 102351751\n",
      "8039 8821211\n",
      "8208 13949438\n",
      "8224 7105713\n",
      "8322 2782776\n",
      "8387 2612150\n",
      "8462 11022639\n",
      "8684 1076\n",
      "8714 6188348\n",
      "8723 1461182\n",
      "8745 18733074\n",
      "8855 57759363\n",
      "8905 8314118\n",
      "8995 52126537\n",
      "9056 14036650\n",
      "9134 1696837\n",
      "9170 9884935\n",
      "9180 152282449\n",
      "9322 8932111\n",
      "9325 1931472\n",
      "9328 3626819\n",
      "9337 6981674\n",
      "9452 13699041\n",
      "9458 10159938\n",
      "9494 1618800\n",
      "9539 1572802\n",
      "9571 7250091\n",
      "9590 184487983\n",
      "9616 2783746\n",
      "9640 1065088\n",
      "9843 159041465\n",
      "9860 51979567\n",
      "9924 102354684\n",
      "9938 56657817\n",
      "9943 15412473\n",
      "9950 1356419\n",
      "10075 5357119\n",
      "10191 51874724\n",
      "10192 6612964\n",
      "10229 119284742\n",
      "10351 44083182\n",
      "10486 1171264\n",
      "10497 153311735\n",
      "10665 18176945\n",
      "10727 60121\n",
      "10766 1057469\n",
      "10813 1148525\n",
      "10822 5370615\n",
      "10983 67856712\n",
      "10990 13550959\n",
      "11032 9889475\n",
      "11192 13742419\n",
      "11321 11113728\n",
      "11324 7156590\n",
      "11329 13530374\n",
      "11383 52281331\n",
      "11493 2468783\n",
      "11500 118716003\n",
      "11564 8451212\n",
      "11586 17646396\n",
      "11642 2061521\n",
      "11853 3157058\n",
      "11882 52811641\n",
      "11904 2556\n",
      "11940 8495258\n",
      "11945 6462501\n",
      "11952 172552\n",
      "11980 3263890\n",
      "11990 2360770\n",
      "12014 14656950\n",
      "12044 397556\n",
      "12045 182953113\n",
      "12069 46938852\n",
      "12097 3266250\n",
      "12098 5247929\n",
      "12263 2328808\n",
      "12407 5574231\n",
      "12466 52878910\n",
      "12582 2414511\n",
      "12597 4723212\n",
      "12684 9345583\n",
      "12713 7940109\n",
      "12737 10458880\n",
      "12773 5039697\n",
      "12851 195750470\n",
      "12957 12245213\n",
      "13004 67856532\n",
      "13020 398440\n",
      "13067 10128510\n",
      "13075 189898046\n",
      "13103 386957\n",
      "13141 5430734\n",
      "13161 52153793\n",
      "13290 14043792\n",
      "13292 3265566\n",
      "13378 67855494\n",
      "13384 3264822\n",
      "13388 46935811\n",
      "13400 102350941\n",
      "13445 7130747\n",
      "13502 10017527\n",
      "13515 989572\n",
      "13672 44084020\n",
      "13687 6665511\n",
      "13759 118674834\n",
      "13819 2868247\n",
      "13825 15690223\n",
      "13870 6478896\n",
      "13952 44105751\n",
      "13983 211597\n",
      "14026 59910\n",
      "14062 195873924\n",
      "14095 53295957\n",
      "14239 19662556\n",
      "14307 195345703\n",
      "14354 14081838\n",
      "14377 2574224\n",
      "14445 14151217\n",
      "14511 14434979\n",
      "14526 52074232\n",
      "14542 15124020\n",
      "14573 67855882\n",
      "14601 6821575\n",
      "14615 162183964\n",
      "14627 1353980\n",
      "14643 12147910\n",
      "14701 46939562\n",
      "14743 15065468\n",
      "14746 4993665\n",
      "14793 5761781\n",
      "14829 311942\n",
      "14874 22615716\n",
      "14875 52112576\n",
      "14938 2395426\n",
      "14968 9752897\n",
      "15108 12075649\n",
      "15152 2704974\n",
      "15200 52171279\n",
      "15246 174797999\n",
      "15299 102350691\n",
      "15304 665441\n",
      "15318 58116\n",
      "15373 6506243\n",
      "15403 2734446\n",
      "15417 7343584\n",
      "15425 2300310\n",
      "15493 3266340\n",
      "15496 5484698\n",
      "15497 6670611\n",
      "15579 2680887\n",
      "15657 119298411\n",
      "15667 186206974\n",
      "15738 11904633\n",
      "15819 12857568\n",
      "15858 13375869\n",
      "15884 8555434\n",
      "15896 9134916\n",
      "15900 1196960\n",
      "15946 52098907\n",
      "16002 39487\n",
      "16160 12615602\n",
      "16169 184488346\n",
      "16188 9763372\n",
      "16194 4528610\n",
      "16292 2957\n",
      "16370 2717698\n",
      "16387 6509354\n",
      "16391 182952524\n",
      "16491 1440229\n",
      "16547 195886443\n",
      "16608 44009215\n",
      "16622 207754\n",
      "16636 29170917\n",
      "16648 15201331\n",
      "16703 3263993\n",
      "16725 6694311\n",
      "16770 14008742\n",
      "16831 83458688\n",
      "16938 8749656\n",
      "16992 1282\n",
      "17053 52158174\n",
      "17059 381439\n",
      "17151 195584301\n",
      "17373 3546207\n",
      "17448 5586\n",
      "17532 776002\n",
      "17542 15326443\n",
      "17578 14909391\n",
      "17598 49430466\n",
      "17724 16538528\n",
      "17833 52160496\n",
      "17862 1918127\n",
      "17871 5034059\n",
      "17951 14874026\n",
      "18092 3549266\n",
      "18251 8074746\n",
      "18294 182952390\n",
      "18325 76666127\n",
      "18391 979\n",
      "18423 4438009\n",
      "18456 14321437\n",
      "18512 53645946\n",
      "18548 3199424\n",
      "18555 53025882\n",
      "18604 7212475\n",
      "18651 13214003\n",
      "18796 1488466\n",
      "18803 198980526\n",
      "18844 11157751\n",
      "18893 598478\n",
      "19226 2687883\n",
      "19258 5983351\n",
      "19333 12744871\n",
      "19343 7807216\n",
      "19359 594\n",
      "19439 1964946\n",
      "19494 13691704\n",
      "19540 12851711\n",
      "19636 184486914\n",
      "19658 195069392\n",
      "19659 52058047\n",
      "19693 3576631\n",
      "19856 1040\n",
      "19985 67856188\n",
      "20055 6246996\n",
      "20241 5905515\n",
      "20315 4950709\n",
      "20333 195750836\n",
      "20370 8688235\n",
      "20571 174801519\n",
      "20583 168170148\n",
      "20662 10549256\n",
      "20734 52962051\n",
      "20806 3008394\n",
      "20875 46985924\n",
      "20876 926149\n",
      "20933 645515\n",
      "21002 9150889\n",
      "21062 7092886\n",
      "21074 1210515\n",
      "21119 1733985\n",
      "21130 894481\n",
      "21203 1321\n",
      "21229 155089628\n",
      "21238 8395799\n",
      "21284 21717477\n",
      "21288 9206785\n",
      "21322 13753765\n",
      "21324 174798168\n",
      "21376 5590763\n",
      "21392 434304\n",
      "21445 5068596\n",
      "21536 21700944\n",
      "21576 102350747\n",
      "21741 195791725\n",
      "21904 48358519\n",
      "22000 53081529\n",
      "22021 21669304\n",
      "22090 51877905\n",
      "22110 14544878\n",
      "22171 14527730\n",
      "22352 174798022\n",
      "22373 3231502\n",
      "22396 17844260\n",
      "22534 1277731\n",
      "22734 12317426\n",
      "22800 2014049\n",
      "22817 195886070\n",
      "22865 77529\n",
      "22877 19603477\n",
      "22924 6278207\n",
      "22940 11054466\n",
      "22998 14651385\n",
      "23100 29051190\n",
      "23150 52281487\n",
      "23168 8940645\n",
      "23367 57721278\n",
      "23453 13444726\n",
      "23459 153313061\n",
      "23546 51985622\n",
      "23587 18193214\n",
      "23599 174797779\n",
      "23627 21731209\n",
      "23671 174798018\n",
      "23754 44127108\n",
      "23785 10479248\n",
      "23797 8894136\n",
      "23846 198229871\n",
      "23871 173188049\n",
      "23909 102352781\n",
      "23995 195345550\n",
      "24023 1880070\n",
      "24076 8409243\n",
      "24155 186206883\n",
      "24284 14275144\n",
      "24302 1198964\n",
      "24324 2382442\n",
      "24372 49525534\n",
      "24404 85518425\n",
      "24460 6946103\n",
      "24498 59599752\n",
      "24499 10553280\n",
      "24503 6667804\n",
      "24513 14131077\n",
      "24524 6618571\n",
      "24531 182952931\n",
      "24692 998001\n",
      "24727 5527031\n",
      "24729 14908221\n",
      "24766 21697648\n",
      "24767 2329174\n",
      "24798 5591459\n",
      "24853 44019606\n",
      "24860 88524743\n",
      "24965 989721\n",
      "25019 1100249\n",
      "25092 196170479\n",
      "25131 15980568\n",
      "25237 7480074\n",
      "25280 6880969\n",
      "25354 195700014\n",
      "25382 9164227\n",
      "25414 2098562\n",
      "25505 195064954\n",
      "25646 174801080\n",
      "25653 13747175\n",
      "25755 15870937\n",
      "25800 7682221\n",
      "25866 9716222\n",
      "25915 9683221\n",
      "25917 10565222\n",
      "25926 9777884\n",
      "25962 7833469\n",
      "25982 8699915\n",
      "26006 3860960\n",
      "26050 67856193\n",
      "26086 13624075\n",
      "26096 102350997\n",
      "26129 129945536\n",
      "26138 6315454\n",
      "26154 102485992\n",
      "26164 2012188\n",
      "26239 3206604\n",
      "26281 16173223\n",
      "26313 4941839\n",
      "26360 3074496\n",
      "26366 12829285\n",
      "26429 156053191\n",
      "26615 7586460\n",
      "26637 202660972\n",
      "26665 53034346\n",
      "26688 6157443\n",
      "26784 15827373\n",
      "26810 52138366\n",
      "26814 2131122\n",
      "26834 2202801\n",
      "26867 7421176\n",
      "26939 3264135\n",
      "27198 2075553\n",
      "27234 51868339\n",
      "27328 27191574\n",
      "27332 1918428\n",
      "27348 53181926\n",
      "27416 174803587\n",
      "27463 170078852\n",
      "27464 18706304\n",
      "27493 5029790\n",
      "27531 2209093\n",
      "27559 5332396\n",
      "27570 402805\n",
      "27637 174799410\n",
      "27645 12304778\n",
      "27767 11642690\n",
      "27835 6116433\n",
      "27888 46918664\n",
      "27940 10460485\n",
      "28012 15818617\n",
      "28038 52943910\n",
      "28055 6217983\n",
      "28077 13115581\n",
      "28164 8661576\n",
      "28179 1241421\n",
      "28186 195776229\n",
      "28203 3882054\n",
      "28251 652921\n",
      "28293 195069169\n",
      "28364 13313668\n",
      "28383 12921197\n",
      "28476 12742243\n",
      "28497 1045\n",
      "28536 159041722\n",
      "28540 9938621\n",
      "28616 13750296\n",
      "28686 6987092\n",
      "28699 13694466\n",
      "28740 2499521\n",
      "28743 1253333\n",
      "28813 1689426\n",
      "28842 6528378\n",
      "28844 119117163\n",
      "29001 1768042\n",
      "29162 52171904\n",
      "29168 174799480\n",
      "29201 1173840\n",
      "29298 189999604\n",
      "29337 9204815\n",
      "29395 3718988\n",
      "29440 6359641\n",
      "29459 1516923\n",
      "29500 14451951\n",
      "29532 85517978\n",
      "29614 201632901\n",
      "29634 371\n",
      "29666 61629\n",
      "29675 16484912\n",
      "29719 3266594\n",
      "29769 8506049\n",
      "29844 51875355\n",
      "29856 13951114\n",
      "30009 1747018\n",
      "30022 52802182\n",
      "30045 3264213\n",
      "30059 67855733\n",
      "30112 155100086\n",
      "30156 52136564\n",
      "30231 2075057\n",
      "30261 12203802\n",
      "30273 4449680\n",
      "30326 58302\n",
      "30350 139105363\n",
      "30461 63346\n",
      "30514 4792796\n",
      "30524 3328394\n",
      "30572 1144073\n",
      "30581 498\n",
      "30595 9564958\n",
      "30657 31924\n",
      "30759 52124023\n",
      "30828 189927857\n",
      "30869 2616110\n",
      "30878 196181734\n",
      "30920 1929239\n",
      "31043 2104869\n",
      "31047 1437\n",
      "31114 3068831\n",
      "31117 14039866\n",
      "31122 102350748\n",
      "31158 159041758\n",
      "31161 48353722\n",
      "31167 13508500\n",
      "31178 3202289\n",
      "31185 14727144\n",
      "31422 29162884\n",
      "31447 51838647\n",
      "31452 8443958\n",
      "31489 13888490\n",
      "31512 9471964\n",
      "31690 10480989\n",
      "31928 2422102\n",
      "32013 2740527\n",
      "32072 153313159\n",
      "32112 3089175\n",
      "32165 120374210\n",
      "32214 56976\n",
      "32254 12847003\n",
      "32294 434688\n",
      "32350 80628248\n",
      "32530 9326499\n",
      "32594 512323\n",
      "32595 1746246\n",
      "32628 3191956\n",
      "32637 21015570\n",
      "32741 9174081\n",
      "32770 173990523\n",
      "32815 52155263\n",
      "32842 10250472\n",
      "32867 84842989\n",
      "32872 3265541\n",
      "32932 195750811\n",
      "33004 29151507\n",
      "33137 3265262\n",
      "33288 6008960\n",
      "33292 174802477\n",
      "33399 13888952\n",
      "33405 3380653\n",
      "33432 85518027\n",
      "33492 195776133\n",
      "33499 14980132\n",
      "33509 59986\n",
      "33657 15620570\n",
      "33775 12087925\n",
      "33784 395839\n",
      "33803 7116029\n",
      "33895 718342\n",
      "33901 52169534\n",
      "33977 49881509\n",
      "33979 932197\n",
      "34011 10009142\n",
      "34051 47019063\n",
      "34202 3205220\n",
      "34268 3266611\n",
      "34303 102350939\n",
      "34332 174798275\n",
      "34376 1119356\n",
      "34381 14519034\n",
      "34466 53082542\n",
      "34530 53217060\n",
      "34621 155093144\n",
      "34830 48354032\n",
      "34873 195345047\n",
      "34908 16960682\n",
      "34968 744471\n",
      "35017 1900253\n",
      "35147 102352298\n",
      "35183 52910554\n",
      "35200 52111211\n",
      "35369 9573708\n",
      "35406 198985976\n",
      "35456 1373479\n",
      "35489 7070844\n",
      "35510 14586568\n",
      "35570 84841767\n",
      "35589 167217880\n",
      "35621 189762439\n",
      "35738 60368\n",
      "35808 52078335\n",
      "35831 13747066\n",
      "35853 24609417\n",
      "35864 216107\n",
      "35900 5151070\n",
      "35960 53590103\n",
      "36000 14425690\n",
      "36030 51877560\n",
      "36050 10619801\n",
      "36072 8233374\n",
      "36220 608\n",
      "36309 2522459\n",
      "36360 53092624\n",
      "36376 3426453\n",
      "36428 131773668\n",
      "36514 14841563\n",
      "36523 9662991\n",
      "36540 111399\n",
      "36541 298145\n",
      "36813 2593903\n",
      "36901 153312586\n",
      "36909 5823158\n",
      "36917 6237722\n",
      "36983 12203896\n",
      "37048 53045504\n",
      "37114 182953211\n",
      "37165 173188167\n",
      "37217 186206852\n",
      "37287 1712853\n",
      "37297 52967399\n",
      "37308 7205411\n",
      "37365 196179173\n",
      "37497 6238748\n",
      "37584 9751558\n",
      "37585 102350407\n",
      "37677 9385494\n",
      "37788 383\n",
      "37809 53096414\n",
      "37816 1499080\n",
      "37902 14624362\n",
      "37920 3205890\n",
      "37964 165163819\n",
      "37973 1598703\n",
      "38047 148\n",
      "38062 14068440\n",
      "38103 905565\n",
      "38115 29161506\n",
      "38136 102352962\n",
      "38152 622152\n",
      "38154 456\n",
      "38236 173990158\n",
      "38244 44155085\n",
      "38252 5999791\n",
      "38321 9286820\n",
      "38333 16087821\n",
      "38420 184487889\n",
      "38431 174799374\n",
      "38476 195699881\n",
      "38501 182953261\n",
      "38506 58007087\n",
      "38507 94285\n",
      "38542 5620421\n",
      "38570 198147587\n",
      "38627 37390552\n",
      "38635 9911858\n",
      "38751 5087912\n",
      "38950 12176567\n",
      "38968 49901137\n",
      "38976 189898081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_more_grobid_bib_entr = []\n",
    "grobid_more_latex_bib_entr = []\n",
    "equal = []\n",
    "for num_art ,article in enumerate(all_articles):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        if len(article['latex_parse']['bib_entries']) > len(article['grobid_parse']['bib_entries']):\n",
    "            print(num_art,article['paper_id'])\n",
    "            latex_more_grobid_bib_entr.append(article['paper_id'])\n",
    "        elif len(article['latex_parse']['bib_entries']) < len(article['grobid_parse']['bib_entries']):\n",
    "#             print(num_art,article['paper_id'])\n",
    "            grobid_more_latex_bib_entr.append(article['paper_id'])\n",
    "        else:\n",
    "            equal.append(article['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(794, 2554, 691)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(latex_more_grobid_bib_entr),len(grobid_more_latex_bib_entr),len(equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "794+2554+691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 18| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 9\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 23| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 12| grobid_links = 42| together = 46\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 0| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 50| grobid_links = 46| together = 51\n",
      "====================\n",
      "latex_links = 9| grobid_links = 38| together = 37\n",
      "====================\n",
      "latex_links = 27| grobid_links = 13| together = 30\n",
      "====================\n",
      "latex_links = 31| grobid_links = 27| together = 32\n",
      "====================\n",
      "latex_links = 25| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 12| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 24| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 4| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 10| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 31| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 0| grobid_links = 22| together = 21\n",
      "====================\n",
      "latex_links = 43| grobid_links = 41| together = 45\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 2| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 48| grobid_links = 44| together = 48\n",
      "====================\n",
      "latex_links = 3| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 25| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 28| grobid_links = 36| together = 48\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 29| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 7| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 9| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 14| grobid_links = 12| together = 14\n",
      "====================\n",
      "latex_links = 19| grobid_links = 48| together = 49\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 6\n",
      "====================\n",
      "latex_links = 17| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 7| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 12| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 28| grobid_links = 43| together = 47\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 10| grobid_links = 11| together = 12\n",
      "====================\n",
      "latex_links = 14| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 10| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 5| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 9| grobid_links = 6| together = 10\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 18| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 20| grobid_links = 21| together = 26\n",
      "====================\n",
      "latex_links = 10| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 28| grobid_links = 23| together = 28\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 37| grobid_links = 43| together = 45\n",
      "====================\n",
      "latex_links = 2| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 13| grobid_links = 21| together = 24\n",
      "====================\n",
      "latex_links = 26| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 22| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 13| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 39| grobid_links = 34| together = 39\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 23| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 7\n",
      "====================\n",
      "latex_links = 36| grobid_links = 47| together = 48\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 36| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 22| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 3| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 21| grobid_links = 15| together = 21\n",
      "====================\n",
      "latex_links = 26| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 25| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 17| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 17| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 51| grobid_links = 36| together = 53\n",
      "====================\n",
      "latex_links = 5| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 24| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 9| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 23| grobid_links = 9| together = 24\n",
      "====================\n",
      "latex_links = 6| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 18| grobid_links = 39| together = 44\n",
      "====================\n",
      "latex_links = 23| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 13| grobid_links = 9| together = 14\n",
      "====================\n",
      "latex_links = 52| grobid_links = 34| together = 52\n",
      "====================\n",
      "latex_links = 30| grobid_links = 47| together = 50\n",
      "====================\n",
      "latex_links = 39| grobid_links = 39| together = 42\n",
      "====================\n",
      "latex_links = 9| grobid_links = 4| together = 9\n",
      "====================\n",
      "latex_links = 3| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 18| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 8| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 31| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 30| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 14| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 30| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 57| grobid_links = 60| together = 63\n",
      "====================\n",
      "latex_links = 5| grobid_links = 1| together = 5\n",
      "====================\n",
      "latex_links = 0| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 10| grobid_links = 4| together = 10\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 15| grobid_links = 0| together = 15\n",
      "====================\n",
      "latex_links = 40| grobid_links = 40| together = 39\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 4\n",
      "====================\n",
      "latex_links = 13| grobid_links = 4| together = 13\n",
      "====================\n",
      "latex_links = 40| grobid_links = 42| together = 44\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 26| grobid_links = 0| together = 26\n",
      "====================\n",
      "latex_links = 9| grobid_links = 6| together = 10\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 32| grobid_links = 40| together = 42\n",
      "====================\n",
      "latex_links = 0| grobid_links = 48| together = 48\n",
      "====================\n",
      "latex_links = 34| grobid_links = 50| together = 51\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 31| grobid_links = 47| together = 50\n",
      "====================\n",
      "latex_links = 1| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 10| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 0| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 0| grobid_links = 24| together = 23\n",
      "====================\n",
      "latex_links = 36| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 12| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 13| grobid_links = 44| together = 46\n",
      "====================\n",
      "latex_links = 23| grobid_links = 19| together = 24\n",
      "====================\n",
      "latex_links = 16| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 14| grobid_links = 11| together = 14\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 34\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 42| grobid_links = 42| together = 43\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 41| grobid_links = 39| together = 42\n",
      "====================\n",
      "latex_links = 2| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 2| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 35| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 23| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 25| grobid_links = 48| together = 51\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 39| grobid_links = 40| together = 43\n",
      "====================\n",
      "latex_links = 4| grobid_links = 5| together = 6\n",
      "====================\n",
      "latex_links = 29| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 23| grobid_links = 19| together = 26\n",
      "====================\n",
      "latex_links = 6| grobid_links = 3| together = 6\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 13| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 24| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 17| grobid_links = 39| together = 44\n",
      "====================\n",
      "latex_links = 11| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 12| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 13| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 17| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 21| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 20| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 21| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 11| grobid_links = 53| together = 55\n",
      "====================\n",
      "latex_links = 15| grobid_links = 3| together = 15\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 26\n",
      "====================\n",
      "latex_links = 14| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 44| grobid_links = 37| together = 43\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 46| grobid_links = 46| together = 53\n",
      "====================\n",
      "latex_links = 48| grobid_links = 43| together = 48\n",
      "====================\n",
      "latex_links = 1| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 42| grobid_links = 49| together = 51\n",
      "====================\n",
      "latex_links = 24| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 15| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 7| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 11| grobid_links = 16| together = 19\n",
      "====================\n",
      "latex_links = 56| grobid_links = 62| together = 63\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 9\n",
      "====================\n",
      "latex_links = 14| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 17| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 15| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 4| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 15| grobid_links = 10| together = 17\n",
      "====================\n",
      "latex_links = 15| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 26| grobid_links = 40| together = 41\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 17| grobid_links = 0| together = 17\n",
      "====================\n",
      "latex_links = 11| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 22| grobid_links = 16| together = 22\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 42| grobid_links = 27| together = 43\n",
      "====================\n",
      "latex_links = 17| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 36| grobid_links = 28| together = 38\n",
      "====================\n",
      "latex_links = 8| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 22\n",
      "====================\n",
      "latex_links = 9| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 25| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 26| grobid_links = 21| together = 27\n",
      "====================\n",
      "latex_links = 23| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 31| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 15| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 18| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 3| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 23\n",
      "====================\n",
      "latex_links = 10| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 27| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 14| grobid_links = 6| together = 15\n",
      "====================\n",
      "latex_links = 3| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 1| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 20| grobid_links = 18| together = 23\n",
      "====================\n",
      "latex_links = 1| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 18| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 0| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 5| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 18\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 6| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 23| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 22| grobid_links = 38| together = 43\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 15| grobid_links = 11| together = 15\n",
      "====================\n",
      "latex_links = 15| grobid_links = 7| together = 15\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 29| grobid_links = 27| together = 31\n",
      "====================\n",
      "latex_links = 31| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 20| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 14| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 18| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 29| grobid_links = 33| together = 35\n",
      "====================\n",
      "latex_links = 26| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 16| grobid_links = 9| together = 22\n",
      "====================\n",
      "latex_links = 8| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 27| grobid_links = 28| together = 33\n",
      "====================\n",
      "latex_links = 9| grobid_links = 5| together = 8\n",
      "====================\n",
      "latex_links = 8| grobid_links = 5| together = 8\n",
      "====================\n",
      "latex_links = 32| grobid_links = 28| together = 32\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 22| grobid_links = 55| together = 55\n",
      "====================\n",
      "latex_links = 29| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 21| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 6| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 8| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 11| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 13| grobid_links = 24| together = 24\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 20| grobid_links = 0| together = 20\n",
      "====================\n",
      "latex_links = 43| grobid_links = 53| together = 53\n",
      "====================\n",
      "latex_links = 0| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 27| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 28| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 15| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 22| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 17| grobid_links = 43| together = 45\n",
      "====================\n",
      "latex_links = 9| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 6| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 33| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 42| grobid_links = 40| together = 45\n",
      "====================\n",
      "latex_links = 18| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 31| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 15| grobid_links = 25| together = 33\n",
      "====================\n",
      "latex_links = 0| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 9| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 10| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 18| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 26| grobid_links = 20| together = 33\n",
      "====================\n",
      "latex_links = 40| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 11| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 1| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 26| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 18| grobid_links = 15| together = 21\n",
      "====================\n",
      "latex_links = 13| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 7| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 55| grobid_links = 53| together = 57\n",
      "====================\n",
      "latex_links = 20| grobid_links = 18| together = 22\n",
      "====================\n",
      "latex_links = 11| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 18| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 15| grobid_links = 11| together = 17\n",
      "====================\n",
      "latex_links = 17| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 44| grobid_links = 45| together = 48\n",
      "====================\n",
      "latex_links = 33| grobid_links = 46| together = 46\n",
      "====================\n",
      "latex_links = 27| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 18| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 11| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 5| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 24| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 14| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 28| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 34\n",
      "====================\n",
      "latex_links = 9| grobid_links = 21| together = 24\n",
      "====================\n",
      "latex_links = 29| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 20| grobid_links = 22| together = 26\n",
      "====================\n",
      "latex_links = 22| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 12| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 30| grobid_links = 33| together = 32\n",
      "====================\n",
      "latex_links = 30| grobid_links = 26| together = 31\n",
      "====================\n",
      "latex_links = 29| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 7| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 22| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 3| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 22| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 31| grobid_links = 33| together = 37\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 29| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 42| grobid_links = 42| together = 44\n",
      "====================\n",
      "latex_links = 34| grobid_links = 24| together = 34\n",
      "====================\n",
      "latex_links = 19| grobid_links = 8| together = 19\n",
      "====================\n",
      "latex_links = 27| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 10| grobid_links = 6| together = 10\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 40| grobid_links = 49| together = 51\n",
      "====================\n",
      "latex_links = 0| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 16| grobid_links = 12| together = 20\n",
      "====================\n",
      "latex_links = 17| grobid_links = 35| together = 36\n",
      "====================\n",
      "latex_links = 15| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 16| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 18| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 26| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 3| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 37\n",
      "====================\n",
      "latex_links = 21| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 13| grobid_links = 6| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 24| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 2| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 34| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 11| grobid_links = 7| together = 11\n",
      "====================\n",
      "latex_links = 13| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 20| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 6| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 45| grobid_links = 24| together = 45\n",
      "====================\n",
      "latex_links = 9| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 7| grobid_links = 23| together = 26\n",
      "====================\n",
      "latex_links = 14| grobid_links = 13| together = 15\n",
      "====================\n",
      "latex_links = 4| grobid_links = 3| together = 4\n",
      "====================\n",
      "latex_links = 40| grobid_links = 38| together = 40\n",
      "====================\n",
      "latex_links = 8| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 0| grobid_links = 1| together = 1\n",
      "====================\n",
      "latex_links = 24| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 7| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 27| grobid_links = 36| together = 39\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 22| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 24\n",
      "====================\n",
      "latex_links = 8| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 16| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 8| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 20| grobid_links = 15| together = 22\n",
      "====================\n",
      "latex_links = 9| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 3| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 9| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 0| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 16| grobid_links = 12| together = 19\n",
      "====================\n",
      "latex_links = 34| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 21| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 23| grobid_links = 20| together = 24\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 17| grobid_links = 1| together = 17\n",
      "====================\n",
      "latex_links = 38| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 14| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 16| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 12| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 6| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 19| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 10| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 10| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 2| grobid_links = 25| together = 24\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 41| grobid_links = 52| together = 53\n",
      "====================\n",
      "latex_links = 0| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 20| grobid_links = 14| together = 20\n",
      "====================\n",
      "latex_links = 12| grobid_links = 11| together = 12\n",
      "====================\n",
      "latex_links = 52| grobid_links = 50| together = 55\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 29| grobid_links = 23| together = 30\n",
      "====================\n",
      "latex_links = 44| grobid_links = 46| together = 46\n",
      "====================\n",
      "latex_links = 18| grobid_links = 3| together = 21\n",
      "====================\n",
      "latex_links = 5| grobid_links = 4| together = 5\n",
      "====================\n",
      "latex_links = 11| grobid_links = 7| together = 12\n",
      "====================\n",
      "latex_links = 18| grobid_links = 15| together = 19\n",
      "====================\n",
      "latex_links = 15| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 1| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 30| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 22| grobid_links = 20| together = 26\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 19| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 33| grobid_links = 35| together = 38\n",
      "====================\n",
      "latex_links = 19| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 31| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 22| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 39| grobid_links = 47| together = 49\n",
      "====================\n",
      "latex_links = 35| grobid_links = 45| together = 48\n",
      "====================\n",
      "latex_links = 38| grobid_links = 25| together = 39\n",
      "====================\n",
      "latex_links = 5| grobid_links = 38| together = 39\n",
      "====================\n",
      "latex_links = 10| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 29| grobid_links = 35| together = 39\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 31| grobid_links = 37| together = 37\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 9| grobid_links = 0| together = 9\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 12\n",
      "====================\n",
      "latex_links = 34| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 5| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 19| grobid_links = 16| together = 22\n",
      "====================\n",
      "latex_links = 3| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 46| grobid_links = 36| together = 60\n",
      "====================\n",
      "latex_links = 0| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 6| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 0| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 5| grobid_links = 4| together = 5\n",
      "====================\n",
      "latex_links = 3| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 19| grobid_links = 35| together = 39\n",
      "====================\n",
      "latex_links = 0| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 0| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 33| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 0| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 49| grobid_links = 44| together = 50\n",
      "====================\n",
      "latex_links = 83| grobid_links = 84| together = 88\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 25\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 11| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 21| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 24| grobid_links = 29| together = 34\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 7| grobid_links = 6| together = 8\n",
      "====================\n",
      "latex_links = 36| grobid_links = 40| together = 43\n",
      "====================\n",
      "latex_links = 15| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 21| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 32| grobid_links = 53| together = 55\n",
      "====================\n",
      "latex_links = 9| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 3| grobid_links = 43| together = 43\n",
      "====================\n",
      "latex_links = 11| grobid_links = 8| together = 11\n",
      "====================\n",
      "latex_links = 6| grobid_links = 6| together = 7\n",
      "====================\n",
      "latex_links = 2| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 17| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 26| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 18| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 16| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 5| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 22| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 18| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 10| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 13| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 5| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 17| grobid_links = 6| together = 17\n",
      "====================\n",
      "latex_links = 30| grobid_links = 27| together = 32\n",
      "====================\n",
      "latex_links = 29| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 16| grobid_links = 9| together = 16\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 16| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 12| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 30| grobid_links = 43| together = 44\n",
      "====================\n",
      "latex_links = 18| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 25| grobid_links = 42| together = 44\n",
      "====================\n",
      "latex_links = 27| grobid_links = 32| together = 35\n",
      "====================\n",
      "latex_links = 25| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 3| grobid_links = 62| together = 61\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 21| grobid_links = 39| together = 40\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 9| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 28| grobid_links = 61| together = 62\n",
      "====================\n",
      "latex_links = 18| grobid_links = 23| together = 22\n",
      "====================\n",
      "latex_links = 17| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 22| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 43| grobid_links = 50| together = 57\n",
      "====================\n",
      "latex_links = 14| grobid_links = 11| together = 14\n",
      "====================\n",
      "latex_links = 29| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 16| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 6| grobid_links = 12| together = 16\n",
      "====================\n",
      "latex_links = 0| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 10| grobid_links = 6| together = 9\n",
      "====================\n",
      "latex_links = 30| grobid_links = 25| together = 31\n",
      "====================\n",
      "latex_links = 18| grobid_links = 1| together = 19\n",
      "====================\n",
      "latex_links = 37| grobid_links = 41| together = 44\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 8\n",
      "====================\n",
      "latex_links = 22| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 22| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 18| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 20| grobid_links = 11| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 35| grobid_links = 39| together = 41\n",
      "====================\n",
      "latex_links = 21| grobid_links = 17| together = 24\n",
      "====================\n",
      "latex_links = 22| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 14| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 0| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 19| grobid_links = 16| together = 19\n",
      "====================\n",
      "latex_links = 30| grobid_links = 46| together = 46\n",
      "====================\n",
      "latex_links = 19| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 22| grobid_links = 28| together = 32\n",
      "====================\n",
      "latex_links = 4| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 14| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 25| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 14\n",
      "====================\n",
      "latex_links = 36| grobid_links = 35| together = 37\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 21| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 6| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 24| grobid_links = 35| together = 36\n",
      "====================\n",
      "latex_links = 17| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 28| grobid_links = 24| together = 35\n",
      "====================\n",
      "latex_links = 19| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 8| grobid_links = 6| together = 8\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 43| grobid_links = 42| together = 44\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 31| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 22| grobid_links = 17| together = 23\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 9\n",
      "====================\n",
      "latex_links = 14| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 34| grobid_links = 36| together = 39\n",
      "====================\n",
      "latex_links = 22| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 2| grobid_links = 4| together = 6\n",
      "====================\n",
      "latex_links = 17| grobid_links = 22| together = 27\n",
      "====================\n",
      "latex_links = 35| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 14| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 33| together = 35\n",
      "====================\n",
      "latex_links = 27| grobid_links = 9| together = 34\n",
      "====================\n",
      "latex_links = 43| grobid_links = 39| together = 44\n",
      "====================\n",
      "latex_links = 38| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 24| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 8| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 6| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 23| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 34| grobid_links = 38| together = 41\n",
      "====================\n",
      "latex_links = 37| grobid_links = 35| together = 44\n",
      "====================\n",
      "latex_links = 30| grobid_links = 25| together = 32\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 2| grobid_links = 0| together = 2\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 9| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 0| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 33| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 46| grobid_links = 51| together = 53\n",
      "====================\n",
      "latex_links = 10| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 21| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 24\n",
      "====================\n",
      "latex_links = 17| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 31| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 13| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 7| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 20| grobid_links = 12| together = 22\n",
      "====================\n",
      "latex_links = 33| grobid_links = 43| together = 43\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 25\n",
      "====================\n",
      "latex_links = 14| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 3| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 38| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 23| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 26| grobid_links = 22| together = 26\n",
      "====================\n",
      "latex_links = 6| grobid_links = 33| together = 33\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 1| grobid_links = 0| together = 1\n",
      "====================\n",
      "latex_links = 35| grobid_links = 46| together = 47\n",
      "====================\n",
      "latex_links = 17| grobid_links = 14| together = 25\n",
      "====================\n",
      "latex_links = 37| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 23| grobid_links = 13| together = 23\n",
      "====================\n",
      "latex_links = 17| grobid_links = 35| together = 36\n",
      "====================\n",
      "latex_links = 36| grobid_links = 29| together = 36\n",
      "====================\n",
      "latex_links = 11| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 12| grobid_links = 23| together = 27\n",
      "====================\n",
      "latex_links = 22| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 15| grobid_links = 17| together = 22\n",
      "====================\n",
      "latex_links = 30| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 6| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 16| grobid_links = 42| together = 42\n",
      "====================\n",
      "latex_links = 21| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 15| grobid_links = 35| together = 36\n",
      "====================\n",
      "latex_links = 22| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 25| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 3| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 32| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 26| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 13| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 16| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 18| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 9| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 26\n",
      "====================\n",
      "latex_links = 18| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 0| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 0| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 18| grobid_links = 43| together = 42\n",
      "====================\n",
      "latex_links = 17| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 22| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 19| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 2| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 0| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 7| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 4| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 22| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 39| grobid_links = 18| together = 41\n",
      "====================\n",
      "latex_links = 18| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 24\n",
      "====================\n",
      "latex_links = 13| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 0| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 28| grobid_links = 46| together = 46\n",
      "====================\n",
      "latex_links = 24| grobid_links = 17| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 18| grobid_links = 15| together = 20\n",
      "====================\n",
      "latex_links = 34| grobid_links = 31| together = 35\n",
      "====================\n",
      "latex_links = 8| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 18| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 11| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 5| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 19| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 16| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 30| grobid_links = 35| together = 37\n",
      "====================\n",
      "latex_links = 43| grobid_links = 41| together = 44\n",
      "====================\n",
      "latex_links = 0| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 25| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 25| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 29| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 2| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 12| grobid_links = 9| together = 12\n",
      "====================\n",
      "latex_links = 8| grobid_links = 7| together = 9\n",
      "====================\n",
      "latex_links = 13| grobid_links = 52| together = 54\n",
      "====================\n",
      "latex_links = 56| grobid_links = 55| together = 58\n",
      "====================\n",
      "latex_links = 8| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 53| grobid_links = 8| together = 53\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 8| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 8| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 12| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 42| grobid_links = 33| together = 46\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 9\n",
      "====================\n",
      "latex_links = 13| grobid_links = 11| together = 13\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 22| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 29| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 6| grobid_links = 2| together = 6\n",
      "====================\n",
      "latex_links = 35| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 15| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 31| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 8| grobid_links = 11| together = 12\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 27\n",
      "====================\n",
      "latex_links = 38| grobid_links = 38| together = 46\n",
      "====================\n",
      "latex_links = 26| grobid_links = 13| together = 25\n",
      "====================\n",
      "latex_links = 45| grobid_links = 23| together = 46\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 34| grobid_links = 0| together = 34\n",
      "====================\n",
      "latex_links = 33| grobid_links = 30| together = 33\n",
      "====================\n",
      "latex_links = 16| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 11| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 22| grobid_links = 17| together = 22\n",
      "====================\n",
      "latex_links = 15| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 14| grobid_links = 16| together = 21\n",
      "====================\n",
      "latex_links = 37| grobid_links = 36| together = 39\n",
      "====================\n",
      "latex_links = 23| grobid_links = 21| together = 24\n",
      "====================\n",
      "latex_links = 26| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 38| grobid_links = 10| together = 39\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 11| grobid_links = 0| together = 11\n",
      "====================\n",
      "latex_links = 8| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 15| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 14| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 1| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 33| grobid_links = 44| together = 45\n",
      "====================\n",
      "latex_links = 0| grobid_links = 38| together = 37\n",
      "====================\n",
      "latex_links = 49| grobid_links = 45| together = 50\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 15| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 18| grobid_links = 19| together = 18\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 0| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 44| grobid_links = 48| together = 50\n",
      "====================\n",
      "latex_links = 9| grobid_links = 2| together = 9\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 9| grobid_links = 35| together = 37\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 22| grobid_links = 0| together = 22\n",
      "====================\n",
      "latex_links = 33| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 12| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 44| grobid_links = 43| together = 45\n",
      "====================\n",
      "latex_links = 18| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 46| grobid_links = 44| together = 47\n",
      "====================\n",
      "latex_links = 9| grobid_links = 7| together = 10\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 0| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 7| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 73| grobid_links = 23| together = 72\n",
      "====================\n",
      "latex_links = 29| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 0| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 24| grobid_links = 8| together = 25\n",
      "====================\n",
      "latex_links = 23| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 31| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 0| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 36| grobid_links = 42| together = 45\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 17| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 13| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 31| together = 31\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 46| grobid_links = 0| together = 46\n",
      "====================\n",
      "latex_links = 5| grobid_links = 4| together = 5\n",
      "====================\n",
      "latex_links = 112| grobid_links = 82| together = 153\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 36| grobid_links = 27| together = 48\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 13\n",
      "====================\n",
      "latex_links = 22| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 48| grobid_links = 47| together = 50\n",
      "====================\n",
      "latex_links = 27| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 8\n",
      "====================\n",
      "latex_links = 14| grobid_links = 18| together = 21\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 16\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 21| grobid_links = 47| together = 50\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 49| grobid_links = 50| together = 53\n",
      "====================\n",
      "latex_links = 15| grobid_links = 21| together = 24\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 10| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 16| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 22| grobid_links = 15| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 8| grobid_links = 1| together = 9\n",
      "====================\n",
      "latex_links = 12| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 33| grobid_links = 39| together = 40\n",
      "====================\n",
      "latex_links = 10| grobid_links = 9| together = 10\n",
      "====================\n",
      "latex_links = 31| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 28| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 9| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 25| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 10| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 17| grobid_links = 2| together = 17\n",
      "====================\n",
      "latex_links = 30| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 18| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 21| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 41| grobid_links = 41| together = 42\n",
      "====================\n",
      "latex_links = 0| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 10| grobid_links = 6| together = 11\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 35| grobid_links = 0| together = 35\n",
      "====================\n",
      "latex_links = 43| grobid_links = 46| together = 48\n",
      "====================\n",
      "latex_links = 0| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 12| grobid_links = 45| together = 45\n",
      "====================\n",
      "latex_links = 24| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 5| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 28| grobid_links = 33| together = 32\n",
      "====================\n",
      "latex_links = 22| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 12| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 32| grobid_links = 13| together = 34\n",
      "====================\n",
      "latex_links = 16| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 13| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 31| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 22| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 4| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 13| grobid_links = 9| together = 14\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 16| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 30| grobid_links = 33| together = 37\n",
      "====================\n",
      "latex_links = 6| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 44| grobid_links = 41| together = 45\n",
      "====================\n",
      "latex_links = 25| grobid_links = 29| together = 34\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 5\n",
      "====================\n",
      "latex_links = 3| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 19| grobid_links = 13| together = 24\n",
      "====================\n",
      "latex_links = 18| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 19| grobid_links = 23| together = 26\n",
      "====================\n",
      "latex_links = 6| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 4| grobid_links = 25| together = 24\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 23| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 1| grobid_links = 41| together = 41\n",
      "====================\n",
      "latex_links = 2| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 15| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 0| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 15| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 28| grobid_links = 31| together = 33\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 16| grobid_links = 0| together = 16\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 32| grobid_links = 25| together = 33\n",
      "====================\n",
      "latex_links = 12| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 23| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 8| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 11| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 24| grobid_links = 16| together = 26\n",
      "====================\n",
      "latex_links = 29| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 5| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 14| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 16| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 16| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 38| grobid_links = 39| together = 40\n",
      "====================\n",
      "latex_links = 40| grobid_links = 39| together = 41\n",
      "====================\n",
      "latex_links = 19| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 17| grobid_links = 14| together = 18\n",
      "====================\n",
      "latex_links = 2| grobid_links = 2| together = 3\n",
      "====================\n",
      "latex_links = 1| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 6\n",
      "====================\n",
      "latex_links = 24| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 6| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 11| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 19| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 18| grobid_links = 30| together = 33\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 20| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 14| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 18| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 0| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 0| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 0| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 10| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 5| grobid_links = 4| together = 5\n",
      "====================\n",
      "latex_links = 9| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 5| grobid_links = 3| together = 7\n",
      "====================\n",
      "latex_links = 2| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 14| grobid_links = 10| together = 17\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 17| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 40| grobid_links = 45| together = 47\n",
      "====================\n",
      "latex_links = 8| grobid_links = 7| together = 9\n",
      "====================\n",
      "latex_links = 30| grobid_links = 26| together = 30\n",
      "====================\n",
      "latex_links = 18| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 15| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 9| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 5| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 4| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 20| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 15| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 46| grobid_links = 34| together = 55\n",
      "====================\n",
      "latex_links = 52| grobid_links = 2| together = 52\n",
      "====================\n",
      "latex_links = 35| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 0| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 29| grobid_links = 44| together = 46\n",
      "====================\n",
      "latex_links = 22| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 40| grobid_links = 41| together = 47\n",
      "====================\n",
      "latex_links = 31| grobid_links = 39| together = 40\n",
      "====================\n",
      "latex_links = 0| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 33| grobid_links = 17| together = 39\n",
      "====================\n",
      "latex_links = 0| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 25| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 22| grobid_links = 11| together = 25\n",
      "====================\n",
      "latex_links = 6| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 33| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 33| grobid_links = 32| together = 35\n",
      "====================\n",
      "latex_links = 31| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 8| grobid_links = 14| together = 13\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 9| grobid_links = 6| together = 9\n",
      "====================\n",
      "latex_links = 16| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 20| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 14| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 28| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 25| grobid_links = 35| together = 38\n",
      "====================\n",
      "latex_links = 15| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 27| grobid_links = 52| together = 54\n",
      "====================\n",
      "latex_links = 0| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 26| grobid_links = 35| together = 44\n",
      "====================\n",
      "latex_links = 13| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 17| grobid_links = 26| together = 31\n",
      "====================\n",
      "latex_links = 28| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 0| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 31| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 8| grobid_links = 25| together = 24\n",
      "====================\n",
      "latex_links = 0| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 14| grobid_links = 9| together = 17\n",
      "====================\n",
      "latex_links = 9| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 28| grobid_links = 25| together = 29\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 18\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 52| grobid_links = 0| together = 52\n",
      "====================\n",
      "latex_links = 9| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 6| grobid_links = 6| together = 8\n",
      "====================\n",
      "latex_links = 16| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 19| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 27| grobid_links = 32| together = 37\n",
      "====================\n",
      "latex_links = 28| grobid_links = 12| together = 28\n",
      "====================\n",
      "latex_links = 23| grobid_links = 34| together = 37\n",
      "====================\n",
      "latex_links = 14| grobid_links = 11| together = 14\n",
      "====================\n",
      "latex_links = 3| grobid_links = 1| together = 3\n",
      "====================\n",
      "latex_links = 9| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 16| grobid_links = 13| together = 18\n",
      "====================\n",
      "latex_links = 12| grobid_links = 36| together = 39\n",
      "====================\n",
      "latex_links = 31| grobid_links = 26| together = 31\n",
      "====================\n",
      "latex_links = 46| grobid_links = 44| together = 46\n",
      "====================\n",
      "latex_links = 0| grobid_links = 46| together = 45\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 28| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 10| grobid_links = 9| together = 10\n",
      "====================\n",
      "latex_links = 22| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 46| grobid_links = 42| together = 51\n",
      "====================\n",
      "latex_links = 28| grobid_links = 1| together = 28\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 6| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 12| grobid_links = 9| together = 12\n",
      "====================\n",
      "latex_links = 14| grobid_links = 13| together = 16\n",
      "====================\n",
      "latex_links = 25| grobid_links = 22| together = 27\n",
      "====================\n",
      "latex_links = 36| grobid_links = 45| together = 53\n",
      "====================\n",
      "latex_links = 5| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 23| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 22| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 13| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 6| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 7| grobid_links = 6| together = 7\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 12| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 8| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 29\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 33| grobid_links = 32| together = 36\n",
      "====================\n",
      "latex_links = 23| grobid_links = 38| together = 42\n",
      "====================\n",
      "latex_links = 18| grobid_links = 8| together = 18\n",
      "====================\n",
      "latex_links = 53| grobid_links = 54| together = 55\n",
      "====================\n",
      "latex_links = 22| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 11| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 0| grobid_links = 24| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 24| together = 28\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 34| together = 33\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 2| grobid_links = 0| together = 2\n",
      "====================\n",
      "latex_links = 51| grobid_links = 68| together = 70\n",
      "====================\n",
      "latex_links = 0| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 11| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 14| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 6| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 34| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 12| grobid_links = 2| together = 12\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 25| grobid_links = 0| together = 25\n",
      "====================\n",
      "latex_links = 16| grobid_links = 12| together = 18\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 10| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 27| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 21| grobid_links = 7| together = 21\n",
      "====================\n",
      "latex_links = 25| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 23| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 24| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 7\n",
      "====================\n",
      "latex_links = 10| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 39| grobid_links = 37| together = 47\n",
      "====================\n",
      "latex_links = 42| grobid_links = 14| together = 45\n",
      "====================\n",
      "latex_links = 9| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 49| grobid_links = 55| together = 58\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 39| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 26| grobid_links = 52| together = 53\n",
      "====================\n",
      "latex_links = 15| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 24| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 29| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 22| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 8| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 13| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 5| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 51| grobid_links = 48| together = 52\n",
      "====================\n",
      "latex_links = 16| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 22| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 31| grobid_links = 42| together = 43\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 16| grobid_links = 38| together = 37\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 32\n",
      "====================\n",
      "latex_links = 16| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 40| grobid_links = 23| together = 45\n",
      "====================\n",
      "latex_links = 5| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 3| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 8| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 28| grobid_links = 23| together = 29\n",
      "====================\n",
      "latex_links = 18| grobid_links = 15| together = 20\n",
      "====================\n",
      "latex_links = 29| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 17| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 43| grobid_links = 37| together = 42\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 33| grobid_links = 17| together = 33\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 38| grobid_links = 36| together = 40\n",
      "====================\n",
      "latex_links = 42| grobid_links = 41| together = 42\n",
      "====================\n",
      "latex_links = 10| grobid_links = 8| together = 10\n",
      "====================\n",
      "latex_links = 9| grobid_links = 5| together = 9\n",
      "====================\n",
      "latex_links = 26| grobid_links = 41| together = 42\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 21| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 18\n",
      "====================\n",
      "latex_links = 9| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 29| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 17| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 20| grobid_links = 27| together = 31\n",
      "====================\n",
      "latex_links = 10| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 10| grobid_links = 8| together = 10\n",
      "====================\n",
      "latex_links = 4| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 27| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 26| grobid_links = 53| together = 56\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 20| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 21| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 5| grobid_links = 3| together = 5\n",
      "====================\n",
      "latex_links = 48| grobid_links = 69| together = 71\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 31| grobid_links = 32| together = 36\n",
      "====================\n",
      "latex_links = 8| grobid_links = 1| together = 8\n",
      "====================\n",
      "latex_links = 15| grobid_links = 11| together = 17\n",
      "====================\n",
      "latex_links = 17| grobid_links = 13| together = 17\n",
      "====================\n",
      "latex_links = 8| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 18| grobid_links = 2| together = 19\n",
      "====================\n",
      "latex_links = 23| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 15| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 1| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 11| grobid_links = 9| together = 12\n",
      "====================\n",
      "latex_links = 21| grobid_links = 35| together = 37\n",
      "====================\n",
      "latex_links = 8| grobid_links = 42| together = 42\n",
      "====================\n",
      "latex_links = 11| grobid_links = 5| together = 11\n",
      "====================\n",
      "latex_links = 35| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 22| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 3| grobid_links = 2| together = 3\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 22| grobid_links = 20| together = 24\n",
      "====================\n",
      "latex_links = 25| grobid_links = 48| together = 50\n",
      "====================\n",
      "latex_links = 4| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 20| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 12| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 23| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 10| grobid_links = 9| together = 12\n",
      "====================\n",
      "latex_links = 5| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 24| grobid_links = 37| together = 41\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 5| grobid_links = 9| together = 12\n",
      "====================\n",
      "latex_links = 24| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 19| grobid_links = 15| together = 26\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 10| grobid_links = 0| together = 10\n",
      "====================\n",
      "latex_links = 27| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 42| grobid_links = 40| together = 42\n",
      "====================\n",
      "latex_links = 12| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 49| grobid_links = 41| together = 51\n",
      "====================\n",
      "latex_links = 13| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 15| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 19| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 5| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 0| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 25| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 33| grobid_links = 20| together = 39\n",
      "====================\n",
      "latex_links = 21| grobid_links = 10| together = 22\n",
      "====================\n",
      "latex_links = 3| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 38| grobid_links = 43| together = 44\n",
      "====================\n",
      "latex_links = 16| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 23| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 21| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 1| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 6| grobid_links = 3| together = 6\n",
      "====================\n",
      "latex_links = 8| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 22| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 45| grobid_links = 56| together = 57\n",
      "====================\n",
      "latex_links = 10| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 32| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 0| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 11| grobid_links = 9| together = 13\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 23| grobid_links = 0| together = 23\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 0| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 37| grobid_links = 38| together = 41\n",
      "====================\n",
      "latex_links = 13| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 7| grobid_links = 6| together = 7\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 3| grobid_links = 2| together = 4\n",
      "====================\n",
      "latex_links = 12| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 23| grobid_links = 18| together = 28\n",
      "====================\n",
      "latex_links = 46| grobid_links = 44| together = 47\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 39| grobid_links = 66| together = 66\n",
      "====================\n",
      "latex_links = 43| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 0| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 16| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 7| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 41| grobid_links = 32| together = 46\n",
      "====================\n",
      "latex_links = 28| grobid_links = 45| together = 48\n",
      "====================\n",
      "latex_links = 8| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 16| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 8| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 15| grobid_links = 12| together = 16\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 25| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 24| grobid_links = 14| together = 25\n",
      "====================\n",
      "latex_links = 12| grobid_links = 13| together = 18\n",
      "====================\n",
      "latex_links = 23| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 14| grobid_links = 24| together = 29\n",
      "====================\n",
      "latex_links = 0| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 19| grobid_links = 51| together = 52\n",
      "====================\n",
      "latex_links = 37| grobid_links = 34| together = 38\n",
      "====================\n",
      "latex_links = 39| grobid_links = 38| together = 43\n",
      "====================\n",
      "latex_links = 0| grobid_links = 48| together = 48\n",
      "====================\n",
      "latex_links = 28| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 0| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 1| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 52| grobid_links = 27| together = 53\n",
      "====================\n",
      "latex_links = 19| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 34| grobid_links = 32| together = 35\n",
      "====================\n",
      "latex_links = 31| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 0| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 20| grobid_links = 12| together = 22\n",
      "====================\n",
      "latex_links = 10| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 4| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 32| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 26| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 41| grobid_links = 40| together = 42\n",
      "====================\n",
      "latex_links = 23| grobid_links = 4| together = 23\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 3| grobid_links = 53| together = 54\n",
      "====================\n",
      "latex_links = 11| grobid_links = 10| together = 13\n",
      "====================\n",
      "latex_links = 23| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 5| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 29| grobid_links = 21| together = 31\n",
      "====================\n",
      "latex_links = 26| grobid_links = 35| together = 36\n",
      "====================\n",
      "latex_links = 62| grobid_links = 58| together = 63\n",
      "====================\n",
      "latex_links = 56| grobid_links = 56| together = 57\n",
      "====================\n",
      "latex_links = 24| grobid_links = 15| together = 29\n",
      "====================\n",
      "latex_links = 26| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 28| grobid_links = 53| together = 55\n",
      "====================\n",
      "latex_links = 32| grobid_links = 30| together = 38\n",
      "====================\n",
      "latex_links = 53| grobid_links = 44| together = 55\n",
      "====================\n",
      "latex_links = 14| grobid_links = 13| together = 16\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 5| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 30| grobid_links = 22| together = 33\n",
      "====================\n",
      "latex_links = 17| grobid_links = 43| together = 44\n",
      "====================\n",
      "latex_links = 9| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 9| grobid_links = 7| together = 10\n",
      "====================\n",
      "latex_links = 0| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 20| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 24| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 52| grobid_links = 51| together = 54\n",
      "====================\n",
      "latex_links = 9| grobid_links = 5| together = 11\n",
      "====================\n",
      "latex_links = 4| grobid_links = 28| together = 28\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 3| grobid_links = 0| together = 3\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 7| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 18| grobid_links = 14| together = 18\n",
      "====================\n",
      "latex_links = 38| grobid_links = 2| together = 38\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 32\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 10| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 22| grobid_links = 16| together = 27\n",
      "====================\n",
      "latex_links = 24| grobid_links = 15| together = 24\n",
      "====================\n",
      "latex_links = 0| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 14| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 37| grobid_links = 36| together = 39\n",
      "====================\n",
      "latex_links = 20| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 7| grobid_links = 6| together = 8\n",
      "====================\n",
      "latex_links = 12| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 32| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 22| grobid_links = 8| together = 23\n",
      "====================\n",
      "latex_links = 20| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 21\n",
      "====================\n",
      "latex_links = 29| grobid_links = 28| together = 32\n",
      "====================\n",
      "latex_links = 11| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 26| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 27| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 15| grobid_links = 13| together = 15\n",
      "====================\n",
      "latex_links = 21| grobid_links = 38| together = 41\n",
      "====================\n",
      "latex_links = 34| grobid_links = 35| together = 42\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 8| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 22| grobid_links = 19| together = 24\n",
      "====================\n",
      "latex_links = 9| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 8| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 21| grobid_links = 17| together = 21\n",
      "====================\n",
      "latex_links = 13| grobid_links = 18| together = 22\n",
      "====================\n",
      "latex_links = 19| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 47| grobid_links = 44| together = 47\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 46| grobid_links = 47| together = 47\n",
      "====================\n",
      "latex_links = 15| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 44| grobid_links = 52| together = 53\n",
      "====================\n",
      "latex_links = 0| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 24| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 22| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 8\n",
      "====================\n",
      "latex_links = 34| grobid_links = 23| together = 36\n",
      "====================\n",
      "latex_links = 0| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 22| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 9| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 24| grobid_links = 15| together = 28\n",
      "====================\n",
      "latex_links = 24| grobid_links = 36| together = 40\n",
      "====================\n",
      "latex_links = 39| grobid_links = 28| together = 47\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 11| grobid_links = 8| together = 11\n",
      "====================\n",
      "latex_links = 23| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 5\n",
      "====================\n",
      "latex_links = 6| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 6| grobid_links = 1| together = 6\n",
      "====================\n",
      "latex_links = 15| grobid_links = 11| together = 15\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 0| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 2| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 5| grobid_links = 2| together = 6\n",
      "====================\n",
      "latex_links = 2| grobid_links = 36| together = 35\n",
      "====================\n",
      "latex_links = 30| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 16| grobid_links = 18| together = 23\n",
      "====================\n",
      "latex_links = 25| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 9| grobid_links = 18| together = 22\n",
      "====================\n",
      "latex_links = 27| grobid_links = 23| together = 28\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 27| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 10\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 26\n",
      "====================\n",
      "latex_links = 7| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 35| grobid_links = 26| together = 40\n",
      "====================\n",
      "latex_links = 10| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 29| grobid_links = 50| together = 51\n",
      "====================\n",
      "latex_links = 5| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 16| together = 19\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 14| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 12| grobid_links = 11| together = 12\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 16| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 41| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 7| grobid_links = 33| together = 37\n",
      "====================\n",
      "latex_links = 2| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 11| grobid_links = 16| together = 16\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 14| grobid_links = 0| together = 14\n",
      "====================\n",
      "latex_links = 9| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 39| grobid_links = 18| together = 43\n",
      "====================\n",
      "latex_links = 0| grobid_links = 38| together = 37\n",
      "====================\n",
      "latex_links = 0| grobid_links = 63| together = 62\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 34| grobid_links = 43| together = 46\n",
      "====================\n",
      "latex_links = 22| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 25| grobid_links = 6| together = 25\n",
      "====================\n",
      "latex_links = 0| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 16| grobid_links = 7| together = 17\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 24| grobid_links = 19| together = 28\n",
      "====================\n",
      "latex_links = 8| grobid_links = 7| together = 13\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 33| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 20| grobid_links = 29| together = 34\n",
      "====================\n",
      "latex_links = 24| grobid_links = 4| together = 24\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 19\n",
      "====================\n",
      "latex_links = 27| grobid_links = 65| together = 66\n",
      "====================\n",
      "latex_links = 11| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 25| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 17\n",
      "====================\n",
      "latex_links = 20| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 26| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 21| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 2| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 40| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 4| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 8| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 5| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 10| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 11| grobid_links = 10| together = 13\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 14| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 29| grobid_links = 22| together = 32\n",
      "====================\n",
      "latex_links = 15| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 12| grobid_links = 7| together = 13\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 21\n",
      "====================\n",
      "latex_links = 19| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 21| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 22| grobid_links = 18| together = 23\n",
      "====================\n",
      "latex_links = 25| grobid_links = 46| together = 48\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 45| grobid_links = 50| together = 52\n",
      "====================\n",
      "latex_links = 19| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 27| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 7\n",
      "====================\n",
      "latex_links = 8| grobid_links = 8| together = 10\n",
      "====================\n",
      "latex_links = 14| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 11| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 15| grobid_links = 16| together = 24\n",
      "====================\n",
      "latex_links = 21| grobid_links = 12| together = 22\n",
      "====================\n",
      "latex_links = 15| grobid_links = 8| together = 16\n",
      "====================\n",
      "latex_links = 36| grobid_links = 1| together = 36\n",
      "====================\n",
      "latex_links = 21| grobid_links = 32| together = 38\n",
      "====================\n",
      "latex_links = 12| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 8| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 18| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 13| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 20| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 27| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 16| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 4| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 6\n",
      "====================\n",
      "latex_links = 12| grobid_links = 47| together = 47\n",
      "====================\n",
      "latex_links = 9| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 26| grobid_links = 39| together = 40\n",
      "====================\n",
      "latex_links = 8| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 30| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 22| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 16| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 42| grobid_links = 36| together = 42\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 17| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 29| grobid_links = 2| together = 29\n",
      "====================\n",
      "latex_links = 12| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 34| grobid_links = 14| together = 34\n",
      "====================\n",
      "latex_links = 1| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 9| grobid_links = 39| together = 38\n",
      "====================\n",
      "latex_links = 18| grobid_links = 20| together = 22\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 2| grobid_links = 0| together = 2\n",
      "====================\n",
      "latex_links = 50| grobid_links = 48| together = 51\n",
      "====================\n",
      "latex_links = 23| grobid_links = 18| together = 22\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 18| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 10| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 11| grobid_links = 9| together = 13\n",
      "====================\n",
      "latex_links = 11| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 16| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 13| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 13| grobid_links = 6| together = 13\n",
      "====================\n",
      "latex_links = 29| grobid_links = 25| together = 31\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 29| grobid_links = 40| together = 42\n",
      "====================\n",
      "latex_links = 29| grobid_links = 33| together = 35\n",
      "====================\n",
      "latex_links = 34| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 0| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 6| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 19| grobid_links = 12| together = 20\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 25| grobid_links = 32| together = 43\n",
      "====================\n",
      "latex_links = 30| grobid_links = 42| together = 43\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 20| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 7| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 21| grobid_links = 31| together = 35\n",
      "====================\n",
      "latex_links = 7| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 8| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 41| together = 40\n",
      "====================\n",
      "latex_links = 1| grobid_links = 16| together = 15\n",
      "====================\n",
      "latex_links = 22| grobid_links = 3| together = 22\n",
      "====================\n",
      "latex_links = 20| grobid_links = 53| together = 54\n",
      "====================\n",
      "latex_links = 38| grobid_links = 35| together = 40\n",
      "====================\n",
      "latex_links = 12| grobid_links = 11| together = 15\n",
      "====================\n",
      "latex_links = 23| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 31| grobid_links = 42| together = 43\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 8| grobid_links = 9| together = 14\n",
      "====================\n",
      "latex_links = 22| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 29| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 2| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 42| grobid_links = 42| together = 44\n",
      "====================\n",
      "latex_links = 36| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 10| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 1| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 15| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 8| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 2| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 30| grobid_links = 35| together = 37\n",
      "====================\n",
      "latex_links = 13| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 21| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 8| grobid_links = 6| together = 10\n",
      "====================\n",
      "latex_links = 0| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 33| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 17\n",
      "====================\n",
      "latex_links = 14| grobid_links = 7| together = 15\n",
      "====================\n",
      "latex_links = 43| grobid_links = 42| together = 45\n",
      "====================\n",
      "latex_links = 10| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 30| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 5| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 1| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 4| grobid_links = 2| together = 4\n",
      "====================\n",
      "latex_links = 27| grobid_links = 16| together = 27\n",
      "====================\n",
      "latex_links = 8| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 3| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 12| grobid_links = 2| together = 12\n",
      "====================\n",
      "latex_links = 40| grobid_links = 45| together = 48\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 4| grobid_links = 0| together = 4\n",
      "====================\n",
      "latex_links = 13| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 40| grobid_links = 39| together = 41\n",
      "====================\n",
      "latex_links = 33| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 0| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 33| grobid_links = 36| together = 36\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 36| grobid_links = 27| together = 36\n",
      "====================\n",
      "latex_links = 33| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 20| grobid_links = 13| together = 21\n",
      "====================\n",
      "latex_links = 15| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 28| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 41| grobid_links = 52| together = 53\n",
      "====================\n",
      "latex_links = 24| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 29| grobid_links = 26| together = 30\n",
      "====================\n",
      "latex_links = 44| grobid_links = 35| together = 47\n",
      "====================\n",
      "latex_links = 26| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 20| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 15| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 28| grobid_links = 54| together = 56\n",
      "====================\n",
      "latex_links = 0| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 18| grobid_links = 11| together = 21\n",
      "====================\n",
      "latex_links = 40| grobid_links = 43| together = 45\n",
      "====================\n",
      "latex_links = 10| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 39| grobid_links = 49| together = 51\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 43| grobid_links = 41| together = 44\n",
      "====================\n",
      "latex_links = 4| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 4| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 19| grobid_links = 36| together = 39\n",
      "====================\n",
      "latex_links = 20| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 16| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 33| grobid_links = 32| together = 34\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 9| grobid_links = 0| together = 9\n",
      "====================\n",
      "latex_links = 21| grobid_links = 6| together = 23\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 24| grobid_links = 36| together = 42\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 29| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 37| grobid_links = 34| together = 38\n",
      "====================\n",
      "latex_links = 22| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 65| grobid_links = 28| together = 73\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 21| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 61| grobid_links = 63| together = 64\n",
      "====================\n",
      "latex_links = 7| grobid_links = 4| together = 7\n",
      "====================\n",
      "latex_links = 35| grobid_links = 15| together = 35\n",
      "====================\n",
      "latex_links = 16| grobid_links = 33| together = 32\n",
      "====================\n",
      "latex_links = 22| grobid_links = 18| together = 23\n",
      "====================\n",
      "latex_links = 56| grobid_links = 55| together = 57\n",
      "====================\n",
      "latex_links = 20| grobid_links = 16| together = 21\n",
      "====================\n",
      "latex_links = 14| grobid_links = 67| together = 68\n",
      "====================\n",
      "latex_links = 8| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 40| grobid_links = 23| together = 48\n",
      "====================\n",
      "latex_links = 4| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 21| grobid_links = 44| together = 44\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 19\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 42| grobid_links = 0| together = 42\n",
      "====================\n",
      "latex_links = 10| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 36| grobid_links = 33| together = 35\n",
      "====================\n",
      "latex_links = 15| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 10| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 27| grobid_links = 21| together = 28\n",
      "====================\n",
      "latex_links = 17| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 23| grobid_links = 17| together = 24\n",
      "====================\n",
      "latex_links = 31| grobid_links = 27| together = 35\n",
      "====================\n",
      "latex_links = 10| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 18| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 24| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 15\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 35| grobid_links = 0| together = 35\n",
      "====================\n",
      "latex_links = 19| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 34| grobid_links = 31| together = 39\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 13| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 60| grobid_links = 58| together = 61\n",
      "====================\n",
      "latex_links = 22| grobid_links = 24| together = 26\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 23| grobid_links = 0| together = 23\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 7| grobid_links = 2| together = 9\n",
      "====================\n",
      "latex_links = 43| grobid_links = 55| together = 57\n",
      "====================\n",
      "latex_links = 29| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 27| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 4| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 35| grobid_links = 32| together = 36\n",
      "====================\n",
      "latex_links = 21| grobid_links = 18| together = 21\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 9\n",
      "====================\n",
      "latex_links = 11| grobid_links = 23| together = 26\n",
      "====================\n",
      "latex_links = 16| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 12| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 27| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 9| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 16| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 14| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 34| grobid_links = 42| together = 47\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 6\n",
      "====================\n",
      "latex_links = 26| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 29| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 17| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 18| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 8| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 17| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 49| grobid_links = 48| together = 51\n",
      "====================\n",
      "latex_links = 14| grobid_links = 21| together = 29\n",
      "====================\n",
      "latex_links = 0| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 19| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 25| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 1| grobid_links = 9| together = 9\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 39| grobid_links = 0| together = 39\n",
      "====================\n",
      "latex_links = 27| grobid_links = 48| together = 51\n",
      "====================\n",
      "latex_links = 41| grobid_links = 39| together = 41\n",
      "====================\n",
      "latex_links = 23| grobid_links = 42| together = 48\n",
      "====================\n",
      "latex_links = 11| grobid_links = 35| together = 45\n",
      "====================\n",
      "latex_links = 5| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 14| grobid_links = 13| together = 14\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 38| grobid_links = 0| together = 38\n",
      "====================\n",
      "latex_links = 25| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 5| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 13| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 29| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 7| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 1| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 72| grobid_links = 65| together = 80\n",
      "====================\n",
      "latex_links = 47| grobid_links = 48| together = 51\n",
      "====================\n",
      "latex_links = 6| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 14| grobid_links = 11| together = 14\n",
      "====================\n",
      "latex_links = 27| grobid_links = 43| together = 48\n",
      "====================\n",
      "latex_links = 5| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 11| grobid_links = 12| together = 14\n",
      "====================\n",
      "latex_links = 16| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 68| grobid_links = 86| together = 89\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 55| grobid_links = 53| together = 56\n",
      "====================\n",
      "latex_links = 0| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 13| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 34| grobid_links = 45| together = 47\n",
      "====================\n",
      "latex_links = 40| grobid_links = 56| together = 63\n",
      "====================\n",
      "latex_links = 25| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 1| grobid_links = 41| together = 41\n",
      "====================\n",
      "latex_links = 32| grobid_links = 43| together = 45\n",
      "====================\n",
      "latex_links = 15| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 18| grobid_links = 4| together = 18\n",
      "====================\n",
      "latex_links = 19| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 6| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 15| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 11| grobid_links = 8| together = 12\n",
      "====================\n",
      "latex_links = 12| grobid_links = 8| together = 12\n",
      "====================\n",
      "latex_links = 24| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 24| grobid_links = 25| together = 29\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 22\n",
      "====================\n",
      "latex_links = 5| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 47| grobid_links = 49| together = 52\n",
      "====================\n",
      "latex_links = 10| grobid_links = 104| together = 106\n",
      "====================\n",
      "latex_links = 16| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 17| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 32| grobid_links = 29| together = 33\n",
      "====================\n",
      "latex_links = 20| grobid_links = 16| together = 22\n",
      "====================\n",
      "latex_links = 14| grobid_links = 43| together = 43\n",
      "====================\n",
      "latex_links = 4| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 27| grobid_links = 49| together = 52\n",
      "====================\n",
      "latex_links = 0| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 22| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 4| grobid_links = 6| together = 7\n",
      "====================\n",
      "latex_links = 2| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 46| grobid_links = 42| together = 47\n",
      "====================\n",
      "latex_links = 23| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 26| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 21| grobid_links = 24| together = 29\n",
      "====================\n",
      "latex_links = 10| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 15| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 14| grobid_links = 31| together = 27\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 10| grobid_links = 6| together = 10\n",
      "====================\n",
      "latex_links = 30| grobid_links = 45| together = 49\n",
      "====================\n",
      "latex_links = 31| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 15| grobid_links = 11| together = 15\n",
      "====================\n",
      "latex_links = 36| grobid_links = 49| together = 52\n",
      "====================\n",
      "latex_links = 14| grobid_links = 6| together = 14\n",
      "====================\n",
      "latex_links = 15| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 41| grobid_links = 40| together = 43\n",
      "====================\n",
      "latex_links = 10| grobid_links = 13| together = 17\n",
      "====================\n",
      "latex_links = 34| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 16| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 10| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 22| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 15| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 20| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 29| grobid_links = 12| together = 31\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 7| grobid_links = 6| together = 8\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 22| grobid_links = 18| together = 24\n",
      "====================\n",
      "latex_links = 41| grobid_links = 32| together = 46\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 30| grobid_links = 17| together = 31\n",
      "====================\n",
      "latex_links = 1| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 28| grobid_links = 43| together = 47\n",
      "====================\n",
      "latex_links = 15| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 3| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 46| grobid_links = 37| together = 72\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 22| grobid_links = 24| together = 32\n",
      "====================\n",
      "latex_links = 14| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 11| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 8| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 3| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 30| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 10| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 23| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 15| grobid_links = 11| together = 15\n",
      "====================\n",
      "latex_links = 5| grobid_links = 5| together = 10\n",
      "====================\n",
      "latex_links = 26| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 39| grobid_links = 35| together = 40\n",
      "====================\n",
      "latex_links = 37| grobid_links = 35| together = 40\n",
      "====================\n",
      "latex_links = 0| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 24| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 54| grobid_links = 50| together = 55\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 55| grobid_links = 59| together = 60\n",
      "====================\n",
      "latex_links = 36| grobid_links = 44| together = 48\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 29\n",
      "====================\n",
      "latex_links = 38| grobid_links = 35| together = 38\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 28| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 8| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 12| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 5| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 27| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 6| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 38| grobid_links = 42| together = 48\n",
      "====================\n",
      "latex_links = 36| grobid_links = 29| together = 39\n",
      "====================\n",
      "latex_links = 17| grobid_links = 7| together = 17\n",
      "====================\n",
      "latex_links = 19| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 27| grobid_links = 31| together = 37\n",
      "====================\n",
      "latex_links = 16| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 13| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 29| grobid_links = 17| together = 40\n",
      "====================\n",
      "latex_links = 9| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 20| grobid_links = 17| together = 23\n",
      "====================\n",
      "latex_links = 27| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 15| grobid_links = 13| together = 16\n",
      "====================\n",
      "latex_links = 9| grobid_links = 16| together = 17\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 42| grobid_links = 0| together = 42\n",
      "====================\n",
      "latex_links = 42| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 18| grobid_links = 8| together = 18\n",
      "====================\n",
      "latex_links = 32| grobid_links = 29| together = 33\n",
      "====================\n",
      "latex_links = 29| grobid_links = 1| together = 29\n",
      "====================\n",
      "latex_links = 12| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 26| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 15\n",
      "====================\n",
      "latex_links = 37| grobid_links = 35| together = 38\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 49| grobid_links = 42| together = 49\n",
      "====================\n",
      "latex_links = 38| grobid_links = 38| together = 39\n",
      "====================\n",
      "latex_links = 37| grobid_links = 34| together = 38\n",
      "====================\n",
      "latex_links = 30| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 4| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 28| grobid_links = 23| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 1| grobid_links = 2| together = 3\n",
      "====================\n",
      "latex_links = 50| grobid_links = 72| together = 77\n",
      "====================\n",
      "latex_links = 47| grobid_links = 17| together = 48\n",
      "====================\n",
      "latex_links = 22| grobid_links = 57| together = 58\n",
      "====================\n",
      "latex_links = 34| grobid_links = 53| together = 54\n",
      "====================\n",
      "latex_links = 9| grobid_links = 7| together = 9\n",
      "====================\n",
      "latex_links = 11| grobid_links = 6| together = 11\n",
      "====================\n",
      "latex_links = 11| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 46| grobid_links = 44| together = 46\n",
      "====================\n",
      "latex_links = 16| grobid_links = 13| together = 18\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 35| grobid_links = 43| together = 43\n",
      "====================\n",
      "latex_links = 23| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 10| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 27| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 37| grobid_links = 44| together = 48\n",
      "====================\n",
      "latex_links = 18| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 38| grobid_links = 38| together = 40\n",
      "====================\n",
      "latex_links = 7| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 22| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 30| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 6| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 16| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 26\n",
      "====================\n",
      "latex_links = 38| grobid_links = 46| together = 49\n",
      "====================\n",
      "latex_links = 8| grobid_links = 7| together = 9\n",
      "====================\n",
      "latex_links = 32| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 16| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 39| grobid_links = 46| together = 51\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 25\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 17| grobid_links = 0| together = 17\n",
      "====================\n",
      "latex_links = 2| grobid_links = 20| together = 20\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 40| grobid_links = 0| together = 40\n",
      "====================\n",
      "latex_links = 7| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 5| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 5| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 39| grobid_links = 36| together = 41\n",
      "====================\n",
      "latex_links = 11| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 13| grobid_links = 10| together = 16\n",
      "====================\n",
      "latex_links = 30| grobid_links = 40| together = 44\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 38\n",
      "====================\n",
      "latex_links = 32| grobid_links = 34| together = 33\n",
      "====================\n",
      "latex_links = 2| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 17| grobid_links = 44| together = 45\n",
      "====================\n",
      "latex_links = 2| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 13| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 20| grobid_links = 16| together = 20\n",
      "====================\n",
      "latex_links = 25| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 24| grobid_links = 20| together = 26\n",
      "====================\n",
      "latex_links = 14| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 24\n",
      "====================\n",
      "latex_links = 27| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 25| grobid_links = 47| together = 48\n",
      "====================\n",
      "latex_links = 18| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 5| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 19| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 25| grobid_links = 52| together = 55\n",
      "====================\n",
      "latex_links = 13| grobid_links = 4| together = 13\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 11| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 29| grobid_links = 26| together = 31\n",
      "====================\n",
      "latex_links = 16| grobid_links = 12| together = 18\n",
      "====================\n",
      "latex_links = 13| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 18| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 19| grobid_links = 16| together = 21\n",
      "====================\n",
      "latex_links = 10| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 6| grobid_links = 6| together = 9\n",
      "====================\n",
      "latex_links = 23| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 16| grobid_links = 12| together = 17\n",
      "====================\n",
      "latex_links = 14| grobid_links = 12| together = 14\n",
      "====================\n",
      "latex_links = 28| grobid_links = 13| together = 28\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 10| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 30| grobid_links = 24| together = 34\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 23\n",
      "====================\n",
      "latex_links = 25| grobid_links = 36| together = 35\n",
      "====================\n",
      "latex_links = 33| grobid_links = 22| together = 35\n",
      "====================\n",
      "latex_links = 4| grobid_links = 11| together = 12\n",
      "====================\n",
      "latex_links = 26| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 28\n",
      "====================\n",
      "latex_links = 25| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 36| grobid_links = 35| together = 37\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 20| grobid_links = 0| together = 20\n",
      "====================\n",
      "latex_links = 17| grobid_links = 10| together = 18\n",
      "====================\n",
      "latex_links = 26| grobid_links = 14| together = 26\n",
      "====================\n",
      "latex_links = 3| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 14| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 37| grobid_links = 14| together = 37\n",
      "====================\n",
      "latex_links = 44| grobid_links = 29| together = 52\n",
      "====================\n",
      "latex_links = 0| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 45| grobid_links = 36| together = 52\n",
      "====================\n",
      "latex_links = 8| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 42| grobid_links = 36| together = 46\n",
      "====================\n",
      "latex_links = 29| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 12| grobid_links = 14| together = 24\n",
      "====================\n",
      "latex_links = 13| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 29\n",
      "====================\n",
      "latex_links = 0| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 18| grobid_links = 5| together = 21\n",
      "====================\n",
      "latex_links = 0| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 18\n",
      "====================\n",
      "latex_links = 0| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 7| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 33| grobid_links = 25| together = 34\n",
      "====================\n",
      "latex_links = 114| grobid_links = 147| together = 153\n",
      "====================\n",
      "latex_links = 7| grobid_links = 6| together = 8\n",
      "====================\n",
      "latex_links = 25| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 51| grobid_links = 6| together = 51\n",
      "====================\n",
      "latex_links = 27| grobid_links = 11| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 0| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 3| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 18| grobid_links = 14| together = 21\n",
      "====================\n",
      "latex_links = 42| grobid_links = 41| together = 45\n",
      "====================\n",
      "latex_links = 23| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 17| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 11| grobid_links = 9| together = 11\n",
      "====================\n",
      "latex_links = 34| grobid_links = 31| together = 37\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 19| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 7| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 28| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 24| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 21| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 7| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 33| grobid_links = 45| together = 46\n",
      "====================\n",
      "latex_links = 54| grobid_links = 49| together = 58\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 22| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 28| grobid_links = 24| together = 29\n",
      "====================\n",
      "latex_links = 4| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 42| grobid_links = 57| together = 57\n",
      "====================\n",
      "latex_links = 17| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 24| grobid_links = 7| together = 24\n",
      "====================\n",
      "latex_links = 41| grobid_links = 37| together = 44\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 26\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 20| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 38| together = 33\n",
      "====================\n",
      "latex_links = 11| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 16| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 22| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 13| grobid_links = 8| together = 14\n",
      "====================\n",
      "latex_links = 0| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 19| grobid_links = 16| together = 19\n",
      "====================\n",
      "latex_links = 15| grobid_links = 10| together = 15\n",
      "====================\n",
      "latex_links = 11| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 25| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 4| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 24| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 0| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 19| grobid_links = 1| together = 19\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 10| grobid_links = 27| together = 26\n",
      "====================\n",
      "latex_links = 13| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 30| grobid_links = 11| together = 30\n",
      "====================\n",
      "latex_links = 5| grobid_links = 4| together = 5\n",
      "====================\n",
      "latex_links = 12| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 4| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 14| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 8| grobid_links = 6| together = 8\n",
      "====================\n",
      "latex_links = 16| grobid_links = 12| together = 17\n",
      "====================\n",
      "latex_links = 11| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 2| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 11| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 10| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 5| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 24| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 5| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 35| grobid_links = 40| together = 42\n",
      "====================\n",
      "latex_links = 51| grobid_links = 46| together = 53\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 35| grobid_links = 48| together = 51\n",
      "====================\n",
      "latex_links = 12| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 22| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 21| grobid_links = 19| together = 21\n",
      "====================\n",
      "latex_links = 1| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 9| grobid_links = 22| together = 23\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 7| grobid_links = 0| together = 7\n",
      "====================\n",
      "latex_links = 29| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 21| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 25| grobid_links = 21| together = 29\n",
      "====================\n",
      "latex_links = 0| grobid_links = 34| together = 33\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 28\n",
      "====================\n",
      "latex_links = 4| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 14| grobid_links = 17| together = 21\n",
      "====================\n",
      "latex_links = 25| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 22| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 0| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 35| grobid_links = 33| together = 35\n",
      "====================\n",
      "latex_links = 23| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 18| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 23| grobid_links = 35| together = 37\n",
      "====================\n",
      "latex_links = 20| grobid_links = 15| together = 20\n",
      "====================\n",
      "latex_links = 28| grobid_links = 30| together = 33\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 20| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 35| grobid_links = 11| together = 35\n",
      "====================\n",
      "latex_links = 0| grobid_links = 29| together = 28\n",
      "====================\n",
      "latex_links = 25| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 30| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 22| grobid_links = 18| together = 23\n",
      "====================\n",
      "latex_links = 25| grobid_links = 5| together = 26\n",
      "====================\n",
      "latex_links = 19| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 10| grobid_links = 8| together = 10\n",
      "====================\n",
      "latex_links = 23| grobid_links = 31| together = 35\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 22\n",
      "====================\n",
      "latex_links = 19| grobid_links = 1| together = 19\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 13\n",
      "====================\n",
      "latex_links = 5| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 3| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 33| grobid_links = 7| together = 33\n",
      "====================\n",
      "latex_links = 30| grobid_links = 40| together = 40\n",
      "====================\n",
      "latex_links = 0| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 20| grobid_links = 16| together = 20\n",
      "====================\n",
      "latex_links = 19| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 18| grobid_links = 12| together = 20\n",
      "====================\n",
      "latex_links = 87| grobid_links = 85| together = 89\n",
      "====================\n",
      "latex_links = 70| grobid_links = 106| together = 108\n",
      "====================\n",
      "latex_links = 16| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 17| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 27| grobid_links = 35| together = 38\n",
      "====================\n",
      "latex_links = 8| grobid_links = 43| together = 43\n",
      "====================\n",
      "latex_links = 15| grobid_links = 13| together = 18\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 3| grobid_links = 6| together = 7\n",
      "====================\n",
      "latex_links = 25| grobid_links = 20| together = 26\n",
      "====================\n",
      "latex_links = 23| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 22| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 33| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 62| grobid_links = 62| together = 66\n",
      "====================\n",
      "latex_links = 20| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 21| grobid_links = 16| together = 24\n",
      "====================\n",
      "latex_links = 8| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 6| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 0| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 12| grobid_links = 8| together = 12\n",
      "====================\n",
      "latex_links = 21| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 40| grobid_links = 47| together = 48\n",
      "====================\n",
      "latex_links = 43| grobid_links = 44| together = 48\n",
      "====================\n",
      "latex_links = 0| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 8| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 35| grobid_links = 14| together = 35\n",
      "====================\n",
      "latex_links = 5| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 0| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 1| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 31| grobid_links = 22| together = 31\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 18| grobid_links = 27| together = 27\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 1| grobid_links = 0| together = 1\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 11| grobid_links = 8| together = 11\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 20| grobid_links = 44| together = 44\n",
      "====================\n",
      "latex_links = 17| grobid_links = 18| together = 21\n",
      "====================\n",
      "latex_links = 21| grobid_links = 14| together = 21\n",
      "====================\n",
      "latex_links = 16| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 37| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 17\n",
      "====================\n",
      "latex_links = 3| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 4| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 18| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 43| grobid_links = 50| together = 54\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 4| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 8| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 22| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 18| grobid_links = 23| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 3| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 24| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 0| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 26| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 1| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 17| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 29| grobid_links = 25| together = 29\n",
      "====================\n",
      "latex_links = 6| grobid_links = 49| together = 49\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 14| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 33| grobid_links = 25| together = 38\n",
      "====================\n",
      "latex_links = 34| grobid_links = 20| together = 36\n",
      "====================\n",
      "latex_links = 12| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 25| grobid_links = 50| together = 51\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 25| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 16| grobid_links = 13| together = 16\n",
      "====================\n",
      "latex_links = 20| grobid_links = 6| together = 20\n",
      "====================\n",
      "latex_links = 13| grobid_links = 10| together = 13\n",
      "====================\n",
      "latex_links = 11| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 20| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 38| grobid_links = 35| together = 39\n",
      "====================\n",
      "latex_links = 25| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 44| grobid_links = 20| together = 48\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 13| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 23| grobid_links = 28| together = 34\n",
      "====================\n",
      "latex_links = 13| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 4| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 17| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 39| grobid_links = 42| together = 45\n",
      "====================\n",
      "latex_links = 7| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 9\n",
      "====================\n",
      "latex_links = 0| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 36| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 48| grobid_links = 52| together = 59\n",
      "====================\n",
      "latex_links = 18| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 10| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 18| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 2| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 10| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 6| grobid_links = 42| together = 42\n",
      "====================\n",
      "latex_links = 25| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 11\n",
      "====================\n",
      "latex_links = 25| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 62| grobid_links = 58| together = 63\n",
      "====================\n",
      "latex_links = 13| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 9| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 40| grobid_links = 37| together = 42\n",
      "====================\n",
      "latex_links = 36| grobid_links = 33| together = 37\n",
      "====================\n",
      "latex_links = 22| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 16| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 29| grobid_links = 35| together = 38\n",
      "====================\n",
      "latex_links = 23| grobid_links = 36| together = 49\n",
      "====================\n",
      "latex_links = 24| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 31| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 19| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 22| grobid_links = 17| together = 27\n",
      "====================\n",
      "latex_links = 26| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 17\n",
      "====================\n",
      "latex_links = 38| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 7| grobid_links = 43| together = 43\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 22| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 40| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 55| grobid_links = 52| together = 55\n",
      "====================\n",
      "latex_links = 28| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 24| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 27| grobid_links = 38| together = 40\n",
      "====================\n",
      "latex_links = 38| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 45| grobid_links = 38| together = 47\n",
      "====================\n",
      "latex_links = 0| grobid_links = 48| together = 47\n",
      "====================\n",
      "latex_links = 55| grobid_links = 58| together = 62\n",
      "====================\n",
      "latex_links = 12| grobid_links = 9| together = 13\n",
      "====================\n",
      "latex_links = 4| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 17| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 18| grobid_links = 7| together = 18\n",
      "====================\n",
      "latex_links = 10| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 15| grobid_links = 7| together = 15\n",
      "====================\n",
      "latex_links = 25| grobid_links = 23| together = 30\n",
      "====================\n",
      "latex_links = 19| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 4| grobid_links = 49| together = 49\n",
      "====================\n",
      "latex_links = 25| grobid_links = 32| together = 35\n",
      "====================\n",
      "latex_links = 12| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 21| grobid_links = 23| together = 28\n",
      "====================\n",
      "latex_links = 71| grobid_links = 68| together = 72\n",
      "====================\n",
      "latex_links = 49| grobid_links = 41| together = 62\n",
      "====================\n",
      "latex_links = 25| grobid_links = 12| together = 25\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 31\n",
      "====================\n",
      "latex_links = 33| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 43| grobid_links = 51| together = 53\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 9\n",
      "====================\n",
      "latex_links = 12| grobid_links = 40| together = 39\n",
      "====================\n",
      "latex_links = 17| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 8\n",
      "====================\n",
      "latex_links = 0| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 2| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 28| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 2| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 14| grobid_links = 7| together = 14\n",
      "====================\n",
      "latex_links = 6| grobid_links = 21| together = 23\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 26| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 4| grobid_links = 20| together = 19\n",
      "====================\n",
      "latex_links = 29| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 9| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 17| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 15| grobid_links = 17| together = 21\n",
      "====================\n",
      "latex_links = 28| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 53| grobid_links = 38| together = 54\n",
      "====================\n",
      "latex_links = 16| grobid_links = 14| together = 19\n",
      "====================\n",
      "latex_links = 8| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 16| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 4| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 21| grobid_links = 18| together = 24\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 29| grobid_links = 24| together = 31\n",
      "====================\n",
      "latex_links = 17| grobid_links = 15| together = 18\n",
      "====================\n",
      "latex_links = 22| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 26| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 6| grobid_links = 4| together = 6\n",
      "====================\n",
      "latex_links = 3| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 51| grobid_links = 50| together = 53\n",
      "====================\n",
      "latex_links = 18| grobid_links = 19| together = 23\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 5| grobid_links = 0| together = 5\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 18| grobid_links = 39| together = 41\n",
      "====================\n",
      "latex_links = 9| grobid_links = 15| together = 19\n",
      "====================\n",
      "latex_links = 40| grobid_links = 37| together = 42\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 27| grobid_links = 29| together = 34\n",
      "====================\n",
      "latex_links = 16| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 19| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 0| grobid_links = 33| together = 32\n",
      "====================\n",
      "latex_links = 9| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 4| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 9| grobid_links = 6| together = 9\n",
      "====================\n",
      "latex_links = 17| grobid_links = 21| together = 23\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 5| grobid_links = 0| together = 5\n",
      "====================\n",
      "latex_links = 37| grobid_links = 29| together = 39\n",
      "====================\n",
      "latex_links = 67| grobid_links = 68| together = 69\n",
      "====================\n",
      "latex_links = 42| grobid_links = 45| together = 50\n",
      "====================\n",
      "latex_links = 38| grobid_links = 34| together = 38\n",
      "====================\n",
      "latex_links = 27| grobid_links = 21| together = 27\n",
      "====================\n",
      "latex_links = 22| grobid_links = 36| together = 38\n",
      "====================\n",
      "latex_links = 17| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 7| grobid_links = 9| together = 11\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 13| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 0| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 35| grobid_links = 22| together = 38\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 10\n",
      "====================\n",
      "latex_links = 32| grobid_links = 30| together = 34\n",
      "====================\n",
      "latex_links = 16| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 14| grobid_links = 9| together = 14\n",
      "====================\n",
      "latex_links = 56| grobid_links = 53| together = 57\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 24\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 7| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 13| grobid_links = 10| together = 18\n",
      "====================\n",
      "latex_links = 10| grobid_links = 28| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 13| grobid_links = 42| together = 44\n",
      "====================\n",
      "latex_links = 45| grobid_links = 51| together = 56\n",
      "====================\n",
      "latex_links = 31| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 11| grobid_links = 14| together = 18\n",
      "====================\n",
      "latex_links = 6| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 29| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 18| grobid_links = 34| together = 33\n",
      "====================\n",
      "latex_links = 31| grobid_links = 36| together = 39\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 17| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 43| grobid_links = 58| together = 62\n",
      "====================\n",
      "latex_links = 39| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 15| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 26| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 40| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 1| grobid_links = 45| together = 45\n",
      "====================\n",
      "latex_links = 37| grobid_links = 35| together = 37\n",
      "====================\n",
      "latex_links = 11| grobid_links = 8| together = 14\n",
      "====================\n",
      "latex_links = 10| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 29| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 8| grobid_links = 32| together = 31\n",
      "====================\n",
      "latex_links = 12| grobid_links = 23| together = 23\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 32| grobid_links = 0| together = 32\n",
      "====================\n",
      "latex_links = 24| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 45| grobid_links = 19| together = 46\n",
      "====================\n",
      "latex_links = 6| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 37| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 23| grobid_links = 20| together = 23\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 11| grobid_links = 0| together = 11\n",
      "====================\n",
      "latex_links = 21| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 57| grobid_links = 86| together = 86\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 10| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 11| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 24| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 7\n",
      "====================\n",
      "latex_links = 18| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 26| grobid_links = 30| together = 33\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 14| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 37| grobid_links = 20| together = 38\n",
      "====================\n",
      "latex_links = 12| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 8| grobid_links = 30| together = 31\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 4| grobid_links = 0| together = 4\n",
      "====================\n",
      "latex_links = 20| grobid_links = 37| together = 40\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 18\n",
      "====================\n",
      "latex_links = 25| grobid_links = 27| together = 34\n",
      "====================\n",
      "latex_links = 38| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 18| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 23| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 13| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 4| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 4| grobid_links = 3| together = 4\n",
      "====================\n",
      "latex_links = 28| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 4| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 8| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 26| grobid_links = 12| together = 27\n",
      "====================\n",
      "latex_links = 56| grobid_links = 53| together = 56\n",
      "====================\n",
      "latex_links = 11| grobid_links = 41| together = 41\n",
      "====================\n",
      "latex_links = 17| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 8| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 0| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 18| grobid_links = 15| together = 19\n",
      "====================\n",
      "latex_links = 18| grobid_links = 10| together = 18\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 33| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 44| grobid_links = 44| together = 46\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 42| grobid_links = 40| together = 42\n",
      "====================\n",
      "latex_links = 0| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 2| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 65| grobid_links = 101| together = 110\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 7\n",
      "====================\n",
      "latex_links = 36| grobid_links = 33| together = 38\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 30\n",
      "====================\n",
      "latex_links = 26| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 36\n",
      "====================latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 5| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 0| grobid_links = 26| together = 25\n",
      "====================\n",
      "latex_links = 51| grobid_links = 51| together = 52\n",
      "====================\n",
      "latex_links = 19| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 35| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 14| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 17| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 3| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 17| grobid_links = 13| together = 19\n",
      "====================\n",
      "latex_links = 18| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 34| grobid_links = 2| together = 34\n",
      "====================\n",
      "latex_links = 19| grobid_links = 8| together = 19\n",
      "====================\n",
      "latex_links = 4| grobid_links = 3| together = 5\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 26\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 16| grobid_links = 0| together = 16\n",
      "====================\n",
      "latex_links = 3| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 9\n",
      "====================\n",
      "latex_links = 12| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 32| grobid_links = 51| together = 54\n",
      "====================\n",
      "latex_links = 5| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 24| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 10| grobid_links = 9| together = 11\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 20| grobid_links = 26| together = 32\n",
      "====================\n",
      "latex_links = 18| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 12| grobid_links = 10| together = 13\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 18| grobid_links = 13| together = 19\n",
      "====================\n",
      "latex_links = 30| grobid_links = 21| together = 31\n",
      "====================\n",
      "latex_links = 15| grobid_links = 12| together = 16\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 21| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 18| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 38| grobid_links = 39| together = 42\n",
      "====================\n",
      "latex_links = 12| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 14| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 46| grobid_links = 46| together = 47\n",
      "====================\n",
      "latex_links = 0| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 4| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 27| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 13| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 45| grobid_links = 44| together = 49\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 13| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 32| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 9\n",
      "====================\n",
      "latex_links = 12| grobid_links = 13| together = 14\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 33| grobid_links = 0| together = 33\n",
      "====================\n",
      "latex_links = 24| grobid_links = 18| together = 29\n",
      "====================\n",
      "latex_links = 0| grobid_links = 49| together = 49\n",
      "====================\n",
      "latex_links = 2| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 13| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 41| grobid_links = 40| together = 42\n",
      "====================\n",
      "latex_links = 27| grobid_links = 28| together = 34\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 4| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 9| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 6| grobid_links = 9| together = 11\n",
      "====================\n",
      "latex_links = 0| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 20| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 5| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 20| grobid_links = 15| together = 20\n",
      "====================\n",
      "latex_links = 42| grobid_links = 42| together = 43\n",
      "====================\n",
      "latex_links = 8| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 5| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 12| grobid_links = 11| together = 12\n",
      "====================\n",
      "latex_links = 5| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 21| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 16\n",
      "====================\n",
      "latex_links = 4| grobid_links = 51| together = 51\n",
      "====================\n",
      "latex_links = 25| grobid_links = 26| together = 25\n",
      "====================\n",
      "latex_links = 31| grobid_links = 5| together = 31\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 4| grobid_links = 2| together = 4\n",
      "====================\n",
      "latex_links = 12| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 17| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 36| grobid_links = 43| together = 44\n",
      "====================\n",
      "latex_links = 17| grobid_links = 14| together = 17\n",
      "====================\n",
      "latex_links = 27| grobid_links = 32| together = 37\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 14| grobid_links = 9| together = 14\n",
      "====================\n",
      "latex_links = 26| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 17| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 11| grobid_links = 8| together = 12\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 11\n",
      "====================\n",
      "latex_links = 10| grobid_links = 9| together = 10\n",
      "====================\n",
      "latex_links = 17| grobid_links = 25| together = 29\n",
      "====================\n",
      "latex_links = 26| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 27| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 35| grobid_links = 11| together = 35\n",
      "====================\n",
      "latex_links = 7| grobid_links = 3| together = 7\n",
      "====================\n",
      "latex_links = 36| grobid_links = 37| together = 38\n",
      "====================\n",
      "latex_links = 19| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 25| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 16| grobid_links = 13| together = 16\n",
      "====================\n",
      "latex_links = 11| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 21| grobid_links = 15| together = 23\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 13| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 30| grobid_links = 35| together = 39\n",
      "====================\n",
      "latex_links = 33| grobid_links = 27| together = 35\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 11| grobid_links = 18| together = 17\n",
      "====================\n",
      "latex_links = 11| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 11| grobid_links = 12| together = 16\n",
      "====================\n",
      "latex_links = 7| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 36| grobid_links = 53| together = 55\n",
      "====================\n",
      "latex_links = 15| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 34| grobid_links = 43| together = 42\n",
      "====================\n",
      "latex_links = 31| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 28| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 43| grobid_links = 44| together = 50\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 37| grobid_links = 31| together = 40\n",
      "====================\n",
      "latex_links = 15| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 13| grobid_links = 39| together = 40\n",
      "====================\n",
      "latex_links = 20| grobid_links = 53| together = 54\n",
      "====================\n",
      "latex_links = 0| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 3| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 13| grobid_links = 14| together = 19\n",
      "====================\n",
      "latex_links = 24| grobid_links = 20| together = 24\n",
      "====================\n",
      "latex_links = 20| grobid_links = 18| together = 21\n",
      "====================\n",
      "latex_links = 16| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 11| grobid_links = 5| together = 11\n",
      "====================\n",
      "latex_links = 20| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 24| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 14| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 31| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 45| grobid_links = 45| together = 44\n",
      "====================\n",
      "latex_links = 27| grobid_links = 14| together = 28\n",
      "====================\n",
      "latex_links = 13| grobid_links = 10| together = 15\n",
      "====================\n",
      "latex_links = 31| grobid_links = 31| together = 35\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 9| grobid_links = 30| together = 30\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 19| grobid_links = 1| together = 19\n",
      "====================\n",
      "latex_links = 9| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 33| grobid_links = 4| together = 33\n",
      "====================\n",
      "latex_links = 44| grobid_links = 42| together = 45\n",
      "====================\n",
      "latex_links = 28| grobid_links = 14| together = 28\n",
      "====================\n",
      "latex_links = 14| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 26| grobid_links = 40| together = 43\n",
      "====================\n",
      "latex_links = 16| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 29| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 32| grobid_links = 60| together = 59\n",
      "====================\n",
      "latex_links = 25| grobid_links = 25| together = 25\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 15| grobid_links = 0| together = 15\n",
      "====================\n",
      "latex_links = 12| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 12| grobid_links = 37| together = 45\n",
      "====================\n",
      "latex_links = 19| grobid_links = 16| together = 23\n",
      "====================\n",
      "latex_links = 11| grobid_links = 6| together = 11\n",
      "====================\n",
      "latex_links = 14| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 49| grobid_links = 47| together = 50\n",
      "====================\n",
      "latex_links = 43| grobid_links = 43| together = 43\n",
      "====================\n",
      "latex_links = 4| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 26| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 9| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 14| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 17| grobid_links = 14| together = 19\n",
      "====================\n",
      "latex_links = 7| grobid_links = 17| together = 16\n",
      "====================\n",
      "latex_links = 24| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 24| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 29| grobid_links = 26| together = 30\n",
      "====================\n",
      "latex_links = 22| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 19| grobid_links = 31| together = 30\n",
      "====================\n",
      "latex_links = 2| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 13| grobid_links = 20| together = 23\n",
      "====================\n",
      "latex_links = 7| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 0| grobid_links = 53| together = 53\n",
      "====================\n",
      "latex_links = 28| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 4| grobid_links = 30| together = 31\n",
      "====================\n",
      "latex_links = 27| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 12| grobid_links = 25| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 2| together = 2\n",
      "====================\n",
      "latex_links = 8| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 48| grobid_links = 46| together = 48\n",
      "====================\n",
      "latex_links = 46| grobid_links = 46| together = 46\n",
      "====================\n",
      "latex_links = 19| grobid_links = 16| together = 19\n",
      "====================\n",
      "latex_links = 27| grobid_links = 11| together = 28\n",
      "====================\n",
      "latex_links = 5| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 0| grobid_links = 1| together = 1\n",
      "====================\n",
      "latex_links = 11| grobid_links = 1| together = 11\n",
      "====================\n",
      "latex_links = 0| grobid_links = 6| together = 6\n",
      "====================\n",
      "latex_links = 37| grobid_links = 36| together = 37\n",
      "====================\n",
      "latex_links = 39| grobid_links = 34| together = 39\n",
      "====================\n",
      "latex_links = 12| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 31| grobid_links = 33| together = 36\n",
      "====================\n",
      "latex_links = 2| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 44| grobid_links = 43| together = 46\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 36\n",
      "====================\n",
      "latex_links = 0| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 15| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 0| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 44| grobid_links = 41| together = 44\n",
      "====================\n",
      "latex_links = 17| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 25| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 46| grobid_links = 46| together = 45\n",
      "====================\n",
      "latex_links = 35| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 38| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 1| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 66| grobid_links = 33| together = 71\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 19\n",
      "====================\n",
      "latex_links = 23| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 67| grobid_links = 80| together = 89\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 18| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 37| grobid_links = 68| together = 68\n",
      "====================\n",
      "latex_links = 0| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 38| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 17| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 0| grobid_links = 4| together = 4\n",
      "====================\n",
      "latex_links = 8| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 11| grobid_links = 7| together = 11\n",
      "====================\n",
      "latex_links = 29| grobid_links = 29| together = 30\n",
      "====================\n",
      "latex_links = 19| grobid_links = 21| together = 27\n",
      "====================\n",
      "latex_links = 13| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 27| grobid_links = 1| together = 27\n",
      "====================\n",
      "latex_links = 2| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 26| grobid_links = 24| together = 27\n",
      "====================\n",
      "latex_links = 0| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 7| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 18| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 9| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 13| grobid_links = 8| together = 15\n",
      "====================\n",
      "latex_links = 16| grobid_links = 35| together = 36\n",
      "====================\n",
      "latex_links = 31| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 16| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 9| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 28| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 14| grobid_links = 5| together = 14\n",
      "====================\n",
      "latex_links = 40| grobid_links = 38| together = 41\n",
      "====================\n",
      "latex_links = 26| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 44| grobid_links = 45| together = 45\n",
      "====================\n",
      "latex_links = 21| grobid_links = 23| together = 24\n",
      "====================\n",
      "latex_links = 14| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 10| grobid_links = 18| together = 21\n",
      "====================\n",
      "latex_links = 41| grobid_links = 49| together = 51\n",
      "====================\n",
      "latex_links = 29| grobid_links = 39| together = 39\n",
      "====================\n",
      "latex_links = 34| grobid_links = 35| together = 37\n",
      "====================\n",
      "latex_links = 18| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 24| grobid_links = 43| together = 46\n",
      "====================\n",
      "latex_links = 26| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 13| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 27| grobid_links = 35| together = 39\n",
      "====================\n",
      "latex_links = 31| grobid_links = 23| together = 31\n",
      "====================\n",
      "latex_links = 13| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 11| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 10| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 15| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 23| grobid_links = 15| together = 23\n",
      "====================\n",
      "latex_links = 29| grobid_links = 21| together = 30\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 32\n",
      "====================\n",
      "latex_links = 25| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 29| grobid_links = 41| together = 44\n",
      "====================\n",
      "latex_links = 38| grobid_links = 34| together = 42\n",
      "====================\n",
      "latex_links = 31| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 29| grobid_links = 21| together = 29\n",
      "====================\n",
      "latex_links = 53| grobid_links = 52| together = 56\n",
      "====================\n",
      "latex_links = 32| grobid_links = 30| together = 33\n",
      "====================\n",
      "latex_links = 49| grobid_links = 47| together = 50\n",
      "====================\n",
      "latex_links = 41| grobid_links = 48| together = 47\n",
      "====================\n",
      "latex_links = 42| grobid_links = 41| together = 43\n",
      "====================\n",
      "latex_links = 31| grobid_links = 24| together = 31\n",
      "====================\n",
      "latex_links = 21| grobid_links = 19| together = 22\n",
      "====================\n",
      "latex_links = 15| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 34| grobid_links = 27| together = 36\n",
      "====================\n",
      "latex_links = 42| grobid_links = 39| together = 43\n",
      "====================\n",
      "latex_links = 4| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 31\n",
      "====================\n",
      "latex_links = 1| grobid_links = 24| together = 22\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 22| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 19| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 4| grobid_links = 3| together = 4\n",
      "====================\n",
      "latex_links = 24| grobid_links = 31| together = 36\n",
      "====================\n",
      "latex_links = 15| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 18| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 63| grobid_links = 56| together = 63\n",
      "====================\n",
      "latex_links = 55| grobid_links = 55| together = 57\n",
      "====================\n",
      "latex_links = 10| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 10| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 6| grobid_links = 9| together = 11\n",
      "====================\n",
      "latex_links = 0| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 10| grobid_links = 37| together = 39\n",
      "====================\n",
      "latex_links = 19| grobid_links = 24| together = 24\n",
      "====================\n",
      "latex_links = 20| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 14| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 17\n",
      "====================\n",
      "latex_links = 30| grobid_links = 29| together = 38\n",
      "====================\n",
      "latex_links = 36| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 6| grobid_links = 19| together = 19\n",
      "====================\n",
      "latex_links = 28| grobid_links = 31| together = 34\n",
      "====================\n",
      "latex_links = 0| grobid_links = 5| together = 5\n",
      "====================\n",
      "latex_links = 5| grobid_links = 17| together = 20\n",
      "====================\n",
      "latex_links = 16| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 35| grobid_links = 34| together = 37\n",
      "====================\n",
      "latex_links = 17| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 6| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 51| grobid_links = 64| together = 66\n",
      "====================\n",
      "latex_links = 34| grobid_links = 29| together = 35\n",
      "====================\n",
      "latex_links = 24| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 0| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 25| grobid_links = 33| together = 35\n",
      "====================\n",
      "latex_links = 7| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 17| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 3| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 0| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 0| grobid_links = 3| together = 3\n",
      "====================\n",
      "latex_links = 43| grobid_links = 41| together = 56\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 7| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 0| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 7| grobid_links = 1| together = 7\n",
      "====================\n",
      "latex_links = 7| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 38| grobid_links = 38| together = 39\n",
      "====================\n",
      "latex_links = 32| grobid_links = 30| together = 33\n",
      "====================\n",
      "latex_links = 5| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 27| grobid_links = 11| together = 33\n",
      "====================\n",
      "latex_links = 13| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 19| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 27| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 22| grobid_links = 23| together = 25\n",
      "====================\n",
      "latex_links = 9| grobid_links = 5| together = 9\n",
      "====================\n",
      "latex_links = 21| grobid_links = 41| together = 42\n",
      "====================\n",
      "latex_links = 10| grobid_links = 7| together = 10\n",
      "====================\n",
      "latex_links = 9| grobid_links = 10| together = 12\n",
      "====================\n",
      "latex_links = 25| grobid_links = 29| together = 33\n",
      "====================\n",
      "latex_links = 41| grobid_links = 42| together = 45\n",
      "====================\n",
      "latex_links = 14| grobid_links = 10| together = 16\n",
      "====================\n",
      "latex_links = 10| grobid_links = 13| together = 15\n",
      "====================\n",
      "latex_links = 18| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 0| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 6| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 11| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 10| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 9| together = 9\n",
      "====================\n",
      "latex_links = 54| grobid_links = 28| together = 59\n",
      "====================\n",
      "latex_links = 25| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 5| grobid_links = 33| together = 33\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 11\n",
      "====================\n",
      "latex_links = 27| grobid_links = 23| together = 30\n",
      "====================\n",
      "latex_links = 18| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 17| grobid_links = 11| together = 21\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 9\n",
      "====================\n",
      "latex_links = 18| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 4| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 6| grobid_links = 23| together = 23\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 13| grobid_links = 9| together = 13\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 6\n",
      "====================\n",
      "latex_links = 15| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 9| grobid_links = 13| together = 14\n",
      "====================\n",
      "latex_links = 13| grobid_links = 41| together = 47\n",
      "====================\n",
      "latex_links = 23| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 7| grobid_links = 4| together = 7\n",
      "====================\n",
      "latex_links = 13| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 19| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 9| grobid_links = 17| together = 21\n",
      "====================\n",
      "latex_links = 30| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 31| grobid_links = 31| together = 33\n",
      "====================\n",
      "latex_links = 9| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 27| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 13| grobid_links = 11| together = 14\n",
      "====================\n",
      "latex_links = 20| grobid_links = 17| together = 22\n",
      "====================\n",
      "latex_links = 21| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 43| grobid_links = 40| together = 52\n",
      "====================\n",
      "latex_links = 8| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 10| grobid_links = 9| together = 11\n",
      "====================\n",
      "latex_links = 26| grobid_links = 22| together = 27\n",
      "====================\n",
      "latex_links = 29| grobid_links = 22| together = 29\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 41| grobid_links = 0| together = 41\n",
      "====================\n",
      "latex_links = 22| grobid_links = 25| together = 28\n",
      "====================\n",
      "latex_links = 5| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 14| grobid_links = 16| together = 18\n",
      "====================\n",
      "latex_links = 4| grobid_links = 41| together = 41\n",
      "====================\n",
      "latex_links = 22| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 17\n",
      "====================\n",
      "latex_links = 25| grobid_links = 22| together = 25\n",
      "====================\n",
      "latex_links = 22| grobid_links = 22| together = 24\n",
      "====================\n",
      "latex_links = 17| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 31| grobid_links = 33| together = 34\n",
      "====================\n",
      "latex_links = 10| grobid_links = 15| together = 15\n",
      "====================\n",
      "latex_links = 18| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 0| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 41| grobid_links = 37| together = 41\n",
      "====================\n",
      "latex_links = 34| grobid_links = 44| together = 46\n",
      "====================\n",
      "latex_links = 23| grobid_links = 28| together = 30\n",
      "====================\n",
      "latex_links = 16| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 16| grobid_links = 32| together = 34\n",
      "====================\n",
      "latex_links = 14| grobid_links = 26| together = 26\n",
      "====================\n",
      "latex_links = 2| grobid_links = 38| together = 39\n",
      "====================\n",
      "latex_links = 17| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 23| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 15| grobid_links = 20| together = 22\n",
      "====================\n",
      "latex_links = 16| grobid_links = 11| together = 17\n",
      "====================\n",
      "latex_links = 15| grobid_links = 14| together = 15\n",
      "====================\n",
      "latex_links = 8| grobid_links = 18| together = 19\n",
      "====================\n",
      "latex_links = 11| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 16| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 28| grobid_links = 27| together = 29\n",
      "====================\n",
      "latex_links = 8| grobid_links = 5| together = 8\n",
      "====================\n",
      "latex_links = 9| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 28\n",
      "====================\n",
      "latex_links = 16| grobid_links = 14| together = 16\n",
      "====================\n",
      "latex_links = 17| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 12| grobid_links = 11| together = 14\n",
      "====================\n",
      "latex_links = 13| grobid_links = 13| together = 16\n",
      "====================\n",
      "latex_links = 3| grobid_links = 29| together = 29\n",
      "====================\n",
      "latex_links = 17| grobid_links = 17| together = 18\n",
      "====================\n",
      "latex_links = 23| grobid_links = 10| together = 24\n",
      "====================\n",
      "latex_links = 32| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 51| grobid_links = 61| together = 64\n",
      "====================\n",
      "latex_links = 17| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 25| grobid_links = 21| together = 26\n",
      "====================\n",
      "WOW!\n",
      "latex_links = 23| grobid_links = 0| together = 23\n",
      "====================\n",
      "latex_links = 23| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 0| grobid_links = 18| together = 18\n",
      "====================\n",
      "latex_links = 0| grobid_links = 45| together = 45\n",
      "====================\n",
      "latex_links = 21| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 16| grobid_links = 7| together = 19\n",
      "====================\n",
      "latex_links = 0| grobid_links = 16| together = 16\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for num_art ,article in enumerate(all_articles):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        latex_links = [v['links'] for k,v in article['latex_parse']['bib_entries'].items() if v['links']]\n",
    "        if article['grobid_parse']:\n",
    "            grobid_links =  [v['links'] for k,v in article['grobid_parse']['bib_entries'].items() if v['links']]\n",
    "            if len(latex_links)> 0 and len(grobid_links) == 0:\n",
    "                print('WOW!')\n",
    "            print(f'latex_links = {len(latex_links)}| grobid_links = {len(grobid_links)}| together = {len(set(grobid_links+latex_links))}')\n",
    "            print(10*'==')\n",
    "\n",
    "        else:\n",
    "            print(f'latex_links = {len(latex_links)}| grobid_links = 0| together = {len(set(latex_links))}')\n",
    "            print(10*'==')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка ситуации, когда есть **latex_parse**, но  нет **grobid_parse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_art ,article in enumerate(all_articles):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        if len(article['grobid_parse']['body_text'])==0:\n",
    "            print(num_art,articcle['paper_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выделение обзорной части статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Самый простой принцип построения - по максимальному количеству ссылок в абзаце:\n",
    " - **Решение**:\n",
    "    - подсчитать количество ссылок в каждой секции\n",
    "    - выбрать секцию с максимальным количеством ссылок (возможно ещё оставить ещё 1 секцию, в которой количество ссылок было больше половины чем в максимальной)\n",
    "    - для latex статей надо объединить текст одинаковых секций в 1 абзац\n",
    " - **Критерий**:\n",
    "    -  в части latex публикаций есть названия секций => после выделения обзорных часте можно посмотреть какие секции выделились: какие топ-3, сделать просмотр глазами и после этого решать что делать дальше.\n",
    "    - возможно логично сохранять для 2 максимального текста название статей\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers[all_articles[0]['paper_id']] = {\n",
    "    'paper_id':all_articles[0]['paper_id'],   'metadata':all_articles[0]['metadata'],\n",
    "    's2_pdf_hash':all_articles[0]['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10164018': {'paper_id': '10164018',\n",
       "  'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "   'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "   'year': '2017',\n",
       "   'arxiv_id': '1708.01065',\n",
       "   'acl_id': 'W17-4512',\n",
       "   'pmc_id': None,\n",
       "   'pubmed_id': None,\n",
       "   'doi': '10.18653/v1/w17-4512',\n",
       "   'venue': 'ArXiv',\n",
       "   'journal': 'ArXiv'},\n",
       "  's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       "  'grobid_parse': None,\n",
       "  'latex_parse': None}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BIBREF0', 'BIBREF1', 'BIBREF2', 'BIBREF3', 'BIBREF4', 'BIBREF5', 'BIBREF6', 'BIBREF7', 'BIBREF8', 'BIBREF9', 'BIBREF10', 'BIBREF11', 'BIBREF12', 'BIBREF13', 'BIBREF14', 'BIBREF15', 'BIBREF16', 'BIBREF17', 'BIBREF18', 'BIBREF19', 'BIBREF20', 'BIBREF21', 'BIBREF22'])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse']['bib_entries'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [{'start': 956, 'end': 964, 'text': 'Figure 1', 'latex': None, 'ref_id': 'FIGREF0'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 451, 'end': 468, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 12, 'end': 20, 'text': 'Figure 2', 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "6 {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [{'start': 32, 'end': 58, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 59, 'end': 79, 'text': 'Rezende et al., 2014', 'latex': None, 'ref_id': 'BIBREF18'}, {'start': 184, 'end': 200, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2384, 'end': 2401, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2433, 'end': 2456, 'text': '(Bahdanau et al., 2015;', 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 2457, 'end': 2476, 'text': 'Luong et al., 2015)', 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "3 {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .', 'cite_spans': [{'start': 86, 'end': 102, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 1133, 'end': 1158, 'text': '(Dantzig and Thapa, 2006)', 'latex': None, 'ref_id': 'BIBREF3'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 107, 'end': 118, 'text': '(Lin, 2004)', 'latex': None, 'ref_id': 'BIBREF12'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "4 {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [{'start': 351, 'end': 365, 'text': '(Wasson, 1998)', 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 492, 'end': 512, 'text': '(Radev et al., 2000)', 'latex': None, 'ref_id': 'BIBREF17'}, {'start': 700, 'end': 723, 'text': '(Erkan and Radev, 2004)', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 737, 'end': 763, 'text': '(Mihalcea and Tarau, 2004)', 'latex': None, 'ref_id': 'BIBREF14'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "2 {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .', 'cite_spans': [{'start': 498, 'end': 518, 'text': '(Kingma and Ba, 2014', 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 652, 'end': 674, 'text': '(Bastien et al., 2012)', 'latex': None, 'ref_id': 'BIBREF1'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "2 {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.', 'cite_spans': [{'start': 690, 'end': 707, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 908, 'end': 925, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 77, 'end': 84, 'text': 'Table 1', 'latex': None, 'ref_id': 'TABREF0'}, {'start': 746, 'end': 753, 'text': 'Table 2', 'latex': None, 'ref_id': 'TABREF1'}, {'start': 1184, 'end': 1191, 'text': 'Table 3', 'latex': None, 'ref_id': 'TABREF2'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "0 {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".', 'cite_spans': [], 'ref_spans': [{'start': 315, 'end': 322, 'text': 'Table 4', 'latex': None, 'ref_id': 'TABREF3'}], 'eq_spans': [], 'section': None}\n",
      "====================\n",
      "1 {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ', 'cite_spans': [{'start': 517, 'end': 868, 'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world', 'latex': None, 'ref_id': None}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "grobid_parse_overview = dict()\n",
    "for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "    grobid_parse_overview[num_sec] = sections\n",
    "    print(len(grobid_parse_overview[num_sec]['cite_spans']),sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'cite_spans': [], 'ref_spans': [{'start': 118, 'end': 125, 'text': None, 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'cite_spans': [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Overview 1\n",
      "{'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 19, 'end': 26, 'text': None, 'latex': None, 'ref_id': 'FIGREF7'}], 'eq_spans': [{'start': 212, 'end': 223, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 739, 'end': 750, 'text': 'ρ i ', 'latex': '\\\\rho _i', 'ref_id': None}, {'start': 774, 'end': 785, 'text': '𝐱 c i ', 'latex': '\\\\mathbf {x}_c^i', 'ref_id': None}, {'start': 807, 'end': 818, 'text': 'ρ∈ℝ n c  ', 'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}], 'section': 'Overview'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "{'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'cite_spans': [{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [{'start': 472, 'end': 483, 'text': 'p θ (𝐳)=𝒩(0,𝐈)', 'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})', 'ref_id': None}, {'start': 488, 'end': 499, 'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)', 'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})', 'ref_id': None}, {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None}, {'start': 524, 'end': 535, 'text': 'σ', 'latex': '\\\\sigma ', 'ref_id': None}, {'start': 799, 'end': 811, 'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\", 'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n', 'ref_id': 'EQREF9'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [{'start': 50, 'end': 56, 'text': None, 'latex': None, 'ref_id': 'EQREF9'}], 'eq_spans': [{'start': 131, 'end': 142, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 145, 'end': 157, 'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\", 'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ', 'ref_id': 'EQREF10'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': '𝐱', 'latex': '\\\\mathbf {x}', 'ref_id': None}, {'start': 76, 'end': 87, 'text': '𝐱 d ', 'latex': '\\\\mathbf {x}_d', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐱 c ', 'latex': '\\\\mathbf {x}_c', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 364, 'end': 375, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 444, 'end': 456, 'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )', 'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n', 'ref_id': 'EQREF11'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The calculation of INLINEFORM0 will be discussed later.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 19, 'end': 30, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 113, 'end': 124, 'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }', 'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ', 'ref_id': None}, {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 219, 'end': 230, 'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }', 'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ', 'ref_id': None}, {'start': 320, 'end': 331, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 335, 'end': 346, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 402, 'end': 413, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 416, 'end': 428, 'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )', 'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n', 'ref_id': 'EQREF12'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'cite_spans': [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [{'start': 304, 'end': 315, 'text': 's h i ', 'latex': 's^i_{h}', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'h d j ', 'latex': 'h^j_{d}', 'ref_id': None}, {'start': 401, 'end': 412, 'text': 'a d ∈ℝ n d  ', 'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}', 'ref_id': None}, {'start': 472, 'end': 483, 'text': 'h c j ', 'latex': 'h^j_{c}', 'ref_id': None}, {'start': 507, 'end': 518, 'text': 'a c ∈ℝ n c  ', 'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}, {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 672, 'end': 684, 'text': 'a ˜ c =a c ×ρ', 'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n', 'ref_id': 'EQREF13'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 30, 'end': 41, 'text': 'c d i ', 'latex': 'c_d^i', 'ref_id': None}, {'start': 79, 'end': 90, 'text': 'c c i ', 'latex': 'c_c^i', 'ref_id': None}, {'start': 240, 'end': 252, 'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )', 'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n', 'ref_id': 'EQREF14'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 64, 'end': 75, 'text': 's ˜ h i ', 'latex': '{{\\\\tilde{s}}_h^i}', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 15, 'end': 26, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 33, 'end': 44, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 220, 'end': 231, 'text': '𝐀 d ∈ℝ n d ×m ', 'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}', 'ref_id': None}, {'start': 297, 'end': 308, 'text': '𝐀 c ∈ℝ n c ×m ', 'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}', 'ref_id': None}, {'start': 569, 'end': 581, 'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )', 'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n', 'ref_id': 'EQREF15'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 170, 'end': 182, 'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )', 'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n', 'ref_id': 'EQREF16'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'Θ', 'latex': '\\\\Theta ', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐀 d ', 'latex': '\\\\mathbf {A}_d', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 434, 'end': 445, 'text': 'R∈ℝ n d ×n c  ', 'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}', 'ref_id': None}, {'start': 450, 'end': 462, 'text': 'R=X d ×X c T ', 'latex': '\\nR = X_d\\\\times X_c^T\\n', 'ref_id': 'EQREF17'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 93, 'end': 105, 'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]', 'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n', 'ref_id': 'EQREF18'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': '(0,1)', 'latex': '(0,1)', 'ref_id': None}, {'start': 84, 'end': 96, 'text': 'ρ=sigmoid(𝐫)', 'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n', 'ref_id': 'EQREF19'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 332, 'end': 343, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 346, 'end': 358, 'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ', 'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n', 'ref_id': 'EQREF20'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'ρ z ', 'latex': '\\\\rho _z', 'ref_id': None}, {'start': 22, 'end': 33, 'text': 'ρ x ', 'latex': '\\\\rho _x', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Summary Construction 2\n",
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'cite_spans': [{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [{'start': 485, 'end': 497, 'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },', 'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n', 'ref_id': 'EQREF22'}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Summary Construction 4\n",
      "{'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.', 'cite_spans': [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'α i ', 'latex': '\\\\alpha _i', 'ref_id': None}, {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None}, {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 126, 'end': 137, 'text': 'α ij ', 'latex': '\\\\alpha _{ij}', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'R ij ', 'latex': 'R_{ij}', 'ref_id': None}, {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 234, 'end': 245, 'text': 'P j ', 'latex': 'P_j', 'ref_id': None}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Data Description 0\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Description'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Properties 0\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}], 'eq_spans': [], 'section': 'Data Properties'}\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "{'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}], 'ref_spans': [{'start': 58, 'end': 66, 'text': None, 'latex': None, 'ref_id': 'SECREF28'}], 'eq_spans': [], 'section': 'Dataset and Metrics'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'cite_spans': [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'cite_spans': [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 2\n",
      "{'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Experimental Settings 2\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.', 'cite_spans': [{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': '|V|', 'latex': '|V|', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None}, {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 360, 'end': 371, 'text': 'm=5', 'latex': 'm = 5', 'ref_id': None}, {'start': 431, 'end': 442, 'text': 'd h =500', 'latex': 'd_h = 500', 'ref_id': None}, {'start': 463, 'end': 474, 'text': 'K=100', 'latex': 'K = 100', 'ref_id': None}, {'start': 495, 'end': 506, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 538, 'end': 549, 'text': 'λ p =0.2', 'latex': '\\\\lambda _p=0.2', 'ref_id': None}], 'section': 'Experimental Settings'}\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.', 'cite_spans': [], 'ref_spans': [{'start': 83, 'end': 91, 'text': None, 'latex': None, 'ref_id': 'TABREF40'}], 'eq_spans': [{'start': 240, 'end': 251, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Results on Our Dataset'}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', 'cite_spans': [{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'TABREF42'}], 'eq_spans': [{'start': 380, 'end': 391, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\", 'cite_spans': [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 305, 'end': 313, 'text': None, 'latex': None, 'ref_id': 'TABREF43'}], 'eq_spans': [], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Case Study 0\n",
      "{'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.', 'cite_spans': [], 'ref_spans': [{'start': 250, 'end': 258, 'text': None, 'latex': None, 'ref_id': 'TABREF45'}], 'eq_spans': [], 'section': 'Case Study'}\n",
      "====================\n",
      "Conclusions 0\n",
      "{'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusions'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "grobid_latex_overview = dict()\n",
    "for sections in all_articles[0]['latex_parse']['body_text']:\n",
    "    grobid_latex_overview[sections['section']] = sections\n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.\n",
      "====================\n",
      "Introduction 0\n",
      "With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\n",
      "====================\n",
      "Introduction 3\n",
      "One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.\n",
      "====================\n",
      "Introduction 3\n",
      "Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.\n",
      "====================\n",
      "Introduction 0\n",
      "There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.\n",
      "====================\n",
      "Introduction 0\n",
      "Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "====================\n",
      "Overview 1\n",
      "As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The calculation of INLINEFORM0 will be discussed later.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      " INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 \n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.\n",
      "====================\n",
      "Summary Construction 2\n",
      "In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 \n",
      "====================\n",
      "Summary Construction 4\n",
      "where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.\n",
      "====================\n",
      "Data Description 0\n",
      "In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.\n",
      "====================\n",
      "Background 0\n",
      "The definition of the terminology related to the dataset is given as follows.\n",
      "====================\n",
      "Background 0\n",
      "Topic: A topic refers to an event and it is composed of a set of news documents from different sources.\n",
      "====================\n",
      "Background 0\n",
      "Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.\n",
      "====================\n",
      "Background 0\n",
      "Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).\n",
      "====================\n",
      "Background 0\n",
      "Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.\n",
      "====================\n",
      "Background 0\n",
      "Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.\n",
      "====================\n",
      "Background 0\n",
      "Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.\n",
      "====================\n",
      "Data Collection 0\n",
      "The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .\n",
      "====================\n",
      "Data Collection 0\n",
      "For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.\n",
      "====================\n",
      "Data Collection 0\n",
      "After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.\n",
      "====================\n",
      "Data Properties 0\n",
      "The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n",
      "====================\n",
      "Comparative Methods 0\n",
      "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:\n",
      "====================\n",
      "Comparative Methods 1\n",
      "RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.\n",
      "====================\n",
      "Comparative Methods 2\n",
      "LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.\n",
      "====================\n",
      "Comparative Methods 1\n",
      "Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.\n",
      "====================\n",
      "Comparative Methods 0\n",
      "We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.\n",
      "====================\n",
      "Experimental Settings 2\n",
      "The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\n",
      "====================\n",
      "Case Study 0\n",
      "Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.\n",
      "====================\n",
      "Conclusions 0\n",
      "We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for article in all_articles[0]['latex_parse']['body_text']:\n",
    "    print(article['section'],len(article['cite_spans']))\n",
    "    print(article['text'])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'cite_spans': [], 'ref_spans': [{'start': 118, 'end': 125, 'text': None, 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'cite_spans': [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Overview 1\n",
      "{'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 19, 'end': 26, 'text': None, 'latex': None, 'ref_id': 'FIGREF7'}], 'eq_spans': [{'start': 212, 'end': 223, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 739, 'end': 750, 'text': 'ρ i ', 'latex': '\\\\rho _i', 'ref_id': None}, {'start': 774, 'end': 785, 'text': '𝐱 c i ', 'latex': '\\\\mathbf {x}_c^i', 'ref_id': None}, {'start': 807, 'end': 818, 'text': 'ρ∈ℝ n c  ', 'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}], 'section': 'Overview'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n",
      "{'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'cite_spans': [{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [{'start': 472, 'end': 483, 'text': 'p θ (𝐳)=𝒩(0,𝐈)', 'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})', 'ref_id': None}, {'start': 488, 'end': 499, 'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)', 'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})', 'ref_id': None}, {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None}, {'start': 524, 'end': 535, 'text': 'σ', 'latex': '\\\\sigma ', 'ref_id': None}, {'start': 799, 'end': 811, 'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\", 'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n', 'ref_id': 'EQREF9'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [{'start': 50, 'end': 56, 'text': None, 'latex': None, 'ref_id': 'EQREF9'}], 'eq_spans': [{'start': 131, 'end': 142, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 145, 'end': 157, 'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\", 'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ', 'ref_id': 'EQREF10'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': '𝐱', 'latex': '\\\\mathbf {x}', 'ref_id': None}, {'start': 76, 'end': 87, 'text': '𝐱 d ', 'latex': '\\\\mathbf {x}_d', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐱 c ', 'latex': '\\\\mathbf {x}_c', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 364, 'end': 375, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 444, 'end': 456, 'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )', 'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n', 'ref_id': 'EQREF11'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The calculation of INLINEFORM0 will be discussed later.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 19, 'end': 30, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 113, 'end': 124, 'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }', 'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ', 'ref_id': None}, {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 219, 'end': 230, 'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }', 'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ', 'ref_id': None}, {'start': 320, 'end': 331, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 335, 'end': 346, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 402, 'end': 413, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 416, 'end': 428, 'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )', 'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n', 'ref_id': 'EQREF12'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'cite_spans': [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [{'start': 304, 'end': 315, 'text': 's h i ', 'latex': 's^i_{h}', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'h d j ', 'latex': 'h^j_{d}', 'ref_id': None}, {'start': 401, 'end': 412, 'text': 'a d ∈ℝ n d  ', 'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}', 'ref_id': None}, {'start': 472, 'end': 483, 'text': 'h c j ', 'latex': 'h^j_{c}', 'ref_id': None}, {'start': 507, 'end': 518, 'text': 'a c ∈ℝ n c  ', 'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}, {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 672, 'end': 684, 'text': 'a ˜ c =a c ×ρ', 'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n', 'ref_id': 'EQREF13'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 30, 'end': 41, 'text': 'c d i ', 'latex': 'c_d^i', 'ref_id': None}, {'start': 79, 'end': 90, 'text': 'c c i ', 'latex': 'c_c^i', 'ref_id': None}, {'start': 240, 'end': 252, 'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )', 'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n', 'ref_id': 'EQREF14'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 64, 'end': 75, 'text': 's ˜ h i ', 'latex': '{{\\\\tilde{s}}_h^i}', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 15, 'end': 26, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 33, 'end': 44, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 220, 'end': 231, 'text': '𝐀 d ∈ℝ n d ×m ', 'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}', 'ref_id': None}, {'start': 297, 'end': 308, 'text': '𝐀 c ∈ℝ n c ×m ', 'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}', 'ref_id': None}, {'start': 569, 'end': 581, 'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )', 'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n', 'ref_id': 'EQREF15'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 170, 'end': 182, 'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )', 'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n', 'ref_id': 'EQREF16'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'Θ', 'latex': '\\\\Theta ', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐀 d ', 'latex': '\\\\mathbf {A}_d', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 434, 'end': 445, 'text': 'R∈ℝ n d ×n c  ', 'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}', 'ref_id': None}, {'start': 450, 'end': 462, 'text': 'R=X d ×X c T ', 'latex': '\\nR = X_d\\\\times X_c^T\\n', 'ref_id': 'EQREF17'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 93, 'end': 105, 'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]', 'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n', 'ref_id': 'EQREF18'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': '(0,1)', 'latex': '(0,1)', 'ref_id': None}, {'start': 84, 'end': 96, 'text': 'ρ=sigmoid(𝐫)', 'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n', 'ref_id': 'EQREF19'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 332, 'end': 343, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 346, 'end': 358, 'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ', 'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n', 'ref_id': 'EQREF20'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'ρ z ', 'latex': '\\\\rho _z', 'ref_id': None}, {'start': 22, 'end': 33, 'text': 'ρ x ', 'latex': '\\\\rho _x', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Summary Construction 2\n",
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'cite_spans': [{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [{'start': 485, 'end': 497, 'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },', 'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n', 'ref_id': 'EQREF22'}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Summary Construction 4\n",
      "{'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.', 'cite_spans': [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'α i ', 'latex': '\\\\alpha _i', 'ref_id': None}, {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None}, {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 126, 'end': 137, 'text': 'α ij ', 'latex': '\\\\alpha _{ij}', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'R ij ', 'latex': 'R_{ij}', 'ref_id': None}, {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 234, 'end': 245, 'text': 'P j ', 'latex': 'P_j', 'ref_id': None}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Data Description 0\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Description'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Properties 0\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}], 'eq_spans': [], 'section': 'Data Properties'}\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "{'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}], 'ref_spans': [{'start': 58, 'end': 66, 'text': None, 'latex': None, 'ref_id': 'SECREF28'}], 'eq_spans': [], 'section': 'Dataset and Metrics'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'cite_spans': [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'cite_spans': [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 2\n",
      "{'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Experimental Settings 2\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.', 'cite_spans': [{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': '|V|', 'latex': '|V|', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None}, {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 360, 'end': 371, 'text': 'm=5', 'latex': 'm = 5', 'ref_id': None}, {'start': 431, 'end': 442, 'text': 'd h =500', 'latex': 'd_h = 500', 'ref_id': None}, {'start': 463, 'end': 474, 'text': 'K=100', 'latex': 'K = 100', 'ref_id': None}, {'start': 495, 'end': 506, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 538, 'end': 549, 'text': 'λ p =0.2', 'latex': '\\\\lambda _p=0.2', 'ref_id': None}], 'section': 'Experimental Settings'}\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.', 'cite_spans': [], 'ref_spans': [{'start': 83, 'end': 91, 'text': None, 'latex': None, 'ref_id': 'TABREF40'}], 'eq_spans': [{'start': 240, 'end': 251, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Results on Our Dataset'}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', 'cite_spans': [{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'TABREF42'}], 'eq_spans': [{'start': 380, 'end': 391, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\", 'cite_spans': [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 305, 'end': 313, 'text': None, 'latex': None, 'ref_id': 'TABREF43'}], 'eq_spans': [], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Case Study 0\n",
      "{'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.', 'cite_spans': [], 'ref_spans': [{'start': 250, 'end': 258, 'text': None, 'latex': None, 'ref_id': 'TABREF45'}], 'eq_spans': [], 'section': 'Case Study'}\n",
      "====================\n",
      "Conclusions 0\n",
      "{'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusions'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "grobid_latex_overview = dict()\n",
    "for sections in all_articles[0]['latex_parse']['body_text']:\n",
    "    if sections['section'] in grobid_latex_overview:\n",
    "        if grobid_latex_overview[sections['section']] == sections:\n",
    "            continue\n",
    "        else:\n",
    "            grobid_latex_overview[sections['section']]['text'].append(sections['text'])\n",
    "            grobid_latex_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "            grobid_latex_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "            grobid_latex_overview[sections['section']]['section'].append(sections['section'])\n",
    "    else:\n",
    "        grobid_latex_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                      'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                      'section':[sections['section']]}\n",
    "        \n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "  \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "  'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "  'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "  'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "  'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       " 'cite_spans': [[{'start': 193,\n",
       "    'end': 200,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF0'},\n",
       "   {'start': 203,\n",
       "    'end': 210,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF1'},\n",
       "   {'start': 213,\n",
       "    'end': 220,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 223,\n",
       "    'end': 230,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF3'},\n",
       "   {'start': 233,\n",
       "    'end': 240,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 243,\n",
       "    'end': 250,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF5'},\n",
       "   {'start': 253,\n",
       "    'end': 260,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'}],\n",
       "  [],\n",
       "  [{'start': 527,\n",
       "    'end': 534,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 537,\n",
       "    'end': 544,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF8'},\n",
       "   {'start': 802,\n",
       "    'end': 809,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'}],\n",
       "  [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "   {'start': 159,\n",
       "    'end': 167,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF10'},\n",
       "   {'start': 170,\n",
       "    'end': 178,\n",
       "    'text': None,\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'}],\n",
       "  [],\n",
       "  []],\n",
       " 'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       " 'section': ['Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction',\n",
       "  'Introduction']}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_latex_overview['Introduction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers[all_articles[0]['paper_id']]['grobid_parse'] = grobid_parse_overview\n",
    "overview_papers[all_articles[0]['paper_id']]['latex_parse'] = grobid_latex_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10164018': {'paper_id': '10164018',\n",
       "  'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "   'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "   'year': '2017',\n",
       "   'arxiv_id': '1708.01065',\n",
       "   'acl_id': 'W17-4512',\n",
       "   'pmc_id': None,\n",
       "   'pubmed_id': None,\n",
       "   'doi': '10.18653/v1/w17-4512',\n",
       "   'venue': 'ArXiv',\n",
       "   'journal': 'ArXiv'},\n",
       "  's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       "  'grobid_parse': {0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "    'cite_spans': [{'start': 192,\n",
       "      'end': 216,\n",
       "      'text': '(Goldstein et al., 2000;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF6'},\n",
       "     {'start': 217,\n",
       "      'end': 239,\n",
       "      'text': 'Erkan and Radev, 2004;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 240,\n",
       "      'end': 257,\n",
       "      'text': 'Wan et al., 2007;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 258,\n",
       "      'end': 284,\n",
       "      'text': 'Nenkova and McKeown, 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF16'},\n",
       "     {'start': 285,\n",
       "      'end': 302,\n",
       "      'text': 'Min et al., 2012;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF15'},\n",
       "     {'start': 303,\n",
       "      'end': 319,\n",
       "      'text': 'Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 773,\n",
       "      'end': 797,\n",
       "      'text': '(Project Code: 14203414)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2288,\n",
       "      'end': 2305,\n",
       "      'text': '(Hu et al., 2008;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF7'},\n",
       "     {'start': 2306,\n",
       "      'end': 2324,\n",
       "      'text': 'Yang et al., 2011)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF22'},\n",
       "     {'start': 2582,\n",
       "      'end': 2598,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 2911,\n",
       "      'end': 2927,\n",
       "      'text': 'Li et al. (2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 3069,\n",
       "      'end': 3095,\n",
       "      'text': '(Kingma and Welling, 2014;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 3096,\n",
       "      'end': 3117,\n",
       "      'text': 'Rezende et al., 2014)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'}],\n",
       "    'ref_spans': [{'start': 956,\n",
       "      'end': 964,\n",
       "      'text': 'Figure 1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF0'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   1: {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "    'cite_spans': [{'start': 451,\n",
       "      'end': 468,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [{'start': 12,\n",
       "      'end': 20,\n",
       "      'text': 'Figure 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   2: {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "    'cite_spans': [{'start': 32,\n",
       "      'end': 58,\n",
       "      'text': '(Kingma and Welling, 2014;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 59,\n",
       "      'end': 79,\n",
       "      'text': 'Rezende et al., 2014',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 184,\n",
       "      'end': 200,\n",
       "      'text': 'Li et al. (2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2384,\n",
       "      'end': 2401,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 2433,\n",
       "      'end': 2456,\n",
       "      'text': '(Bahdanau et al., 2015;',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF0'},\n",
       "     {'start': 2457,\n",
       "      'end': 2476,\n",
       "      'text': 'Luong et al., 2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF13'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   3: {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "    'cite_spans': [{'start': 86,\n",
       "      'end': 102,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 971,\n",
       "      'end': 987,\n",
       "      'text': 'Li et al. (2015)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF2'},\n",
       "     {'start': 1133,\n",
       "      'end': 1158,\n",
       "      'text': '(Dantzig and Thapa, 2006)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF3'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   4: {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   5: {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   6: {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   7: {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   8: {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "    'cite_spans': [{'start': 107,\n",
       "      'end': 118,\n",
       "      'text': '(Lin, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF12'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   9: {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "    'cite_spans': [{'start': 351,\n",
       "      'end': 365,\n",
       "      'text': '(Wasson, 1998)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF20'},\n",
       "     {'start': 492,\n",
       "      'end': 512,\n",
       "      'text': '(Radev et al., 2000)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'},\n",
       "     {'start': 700,\n",
       "      'end': 723,\n",
       "      'text': '(Erkan and Radev, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF4'},\n",
       "     {'start': 737,\n",
       "      'end': 763,\n",
       "      'text': '(Mihalcea and Tarau, 2004)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF14'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   10: {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "    'cite_spans': [{'start': 498,\n",
       "      'end': 518,\n",
       "      'text': '(Kingma and Ba, 2014',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF8'},\n",
       "     {'start': 652,\n",
       "      'end': 674,\n",
       "      'text': '(Bastien et al., 2012)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   11: {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "    'cite_spans': [{'start': 690,\n",
       "      'end': 707,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'},\n",
       "     {'start': 908,\n",
       "      'end': 925,\n",
       "      'text': '(Li et al., 2017)',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF11'}],\n",
       "    'ref_spans': [{'start': 77,\n",
       "      'end': 84,\n",
       "      'text': 'Table 1',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF0'},\n",
       "     {'start': 746,\n",
       "      'end': 753,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF1'},\n",
       "     {'start': 1184,\n",
       "      'end': 1191,\n",
       "      'text': 'Table 3',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF2'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   12: {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 315,\n",
       "      'end': 322,\n",
       "      'text': 'Table 4',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF3'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   13: {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "    'cite_spans': [{'start': 517,\n",
       "      'end': 868,\n",
       "      'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None}},\n",
       "  'latex_parse': {'Introduction': {'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "     \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "     'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "     'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "     'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "     'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       "    'cite_spans': [[{'start': 193,\n",
       "       'end': 200,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF0'},\n",
       "      {'start': 203,\n",
       "       'end': 210,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'},\n",
       "      {'start': 213,\n",
       "       'end': 220,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 223,\n",
       "       'end': 230,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF3'},\n",
       "      {'start': 233,\n",
       "       'end': 240,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 243,\n",
       "       'end': 250,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'},\n",
       "      {'start': 253,\n",
       "       'end': 260,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     [],\n",
       "     [{'start': 527,\n",
       "       'end': 534,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF7'},\n",
       "      {'start': 537,\n",
       "       'end': 544,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF8'},\n",
       "      {'start': 802,\n",
       "       'end': 809,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'}],\n",
       "     [{'start': 10,\n",
       "       'end': 17,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 159,\n",
       "       'end': 167,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF10'},\n",
       "      {'start': 170,\n",
       "       'end': 178,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     [],\n",
       "     []],\n",
       "    'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       "    'section': ['Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction',\n",
       "     'Introduction']},\n",
       "   'Overview': {'text': ['As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.'],\n",
       "    'cite_spans': [[{'start': 489,\n",
       "       'end': 496,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}]],\n",
       "    'cite_span_lens': [1],\n",
       "    'section': ['Overview']},\n",
       "   'Reader-Aware Salience Estimation': {'text': ['Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "     'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "     'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "     'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "     'The calculation of INLINEFORM0 will be discussed later.',\n",
       "     'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "     'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "     'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "     'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "     ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "     'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "     'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "     'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "     'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "     'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "     'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "     'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.'],\n",
       "    'cite_spans': [[{'start': 32,\n",
       "       'end': 40,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF10'},\n",
       "      {'start': 43,\n",
       "       'end': 51,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 154,\n",
       "       'end': 161,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [{'start': 7,\n",
       "       'end': 14,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 46,\n",
       "       'end': 54,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF12'},\n",
       "      {'start': 57,\n",
       "       'end': 65,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF13'}],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     [],\n",
       "     []],\n",
       "    'cite_span_lens': [3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "    'section': ['Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation',\n",
       "     'Reader-Aware Salience Estimation']},\n",
       "   'Summary Construction': {'text': ['In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "     'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.'],\n",
       "    'cite_spans': [[{'start': 82,\n",
       "       'end': 89,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'},\n",
       "      {'start': 94,\n",
       "       'end': 101,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'}],\n",
       "     [{'start': 466,\n",
       "       'end': 474,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF14'},\n",
       "      {'start': 477,\n",
       "       'end': 484,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'},\n",
       "      {'start': 491,\n",
       "       'end': 498,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 644,\n",
       "       'end': 652,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF15'}]],\n",
       "    'cite_span_lens': [2, 4],\n",
       "    'section': ['Summary Construction', 'Summary Construction']},\n",
       "   'Data Description': {'text': ['In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.'],\n",
       "    'cite_spans': [[]],\n",
       "    'cite_span_lens': [0],\n",
       "    'section': ['Data Description']},\n",
       "   'Background': {'text': ['The definition of the terminology related to the dataset is given as follows.',\n",
       "     'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "     'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "     'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "     'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "     'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "     'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.'],\n",
       "    'cite_spans': [[], [], [], [], [], [], []],\n",
       "    'cite_span_lens': [0, 0, 0, 0, 0, 0, 0],\n",
       "    'section': ['Background',\n",
       "     'Background',\n",
       "     'Background',\n",
       "     'Background',\n",
       "     'Background',\n",
       "     'Background',\n",
       "     'Background']},\n",
       "   'Data Collection': {'text': ['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "     'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "     'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "     'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.'],\n",
       "    'cite_spans': [[], [], [], []],\n",
       "    'cite_span_lens': [0, 0, 0, 0],\n",
       "    'section': ['Data Collection',\n",
       "     'Data Collection',\n",
       "     'Data Collection',\n",
       "     'Data Collection']},\n",
       "   'Data Properties': {'text': ['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.'],\n",
       "    'cite_spans': [[]],\n",
       "    'cite_span_lens': [0],\n",
       "    'section': ['Data Properties']},\n",
       "   'Dataset and Metrics': {'text': ['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.'],\n",
       "    'cite_spans': [[{'start': 113,\n",
       "       'end': 121,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF16'}]],\n",
       "    'cite_span_lens': [1],\n",
       "    'section': ['Dataset and Metrics']},\n",
       "   'Comparative Methods': {'text': ['To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "     'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "     'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "     'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "     'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "     'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "     'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.'],\n",
       "    'cite_spans': [[],\n",
       "     [{'start': 10,\n",
       "       'end': 17,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'}],\n",
       "     [{'start': 5,\n",
       "       'end': 13,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF17'}],\n",
       "     [{'start': 9,\n",
       "       'end': 17,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'}],\n",
       "     [{'start': 8,\n",
       "       'end': 15,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'},\n",
       "      {'start': 29,\n",
       "       'end': 37,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF19'}],\n",
       "     [{'start': 8,\n",
       "       'end': 15,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'}],\n",
       "     []],\n",
       "    'cite_span_lens': [0, 1, 1, 1, 2, 1, 0],\n",
       "    'section': ['Comparative Methods',\n",
       "     'Comparative Methods',\n",
       "     'Comparative Methods',\n",
       "     'Comparative Methods',\n",
       "     'Comparative Methods',\n",
       "     'Comparative Methods',\n",
       "     'Comparative Methods']},\n",
       "   'Experimental Settings': {'text': ['The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.'],\n",
       "    'cite_spans': [[{'start': 557,\n",
       "       'end': 565,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF20'},\n",
       "      {'start': 697,\n",
       "       'end': 705,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF21'}]],\n",
       "    'cite_span_lens': [2],\n",
       "    'section': ['Experimental Settings']},\n",
       "   'Results on Our Dataset': {'text': ['The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.'],\n",
       "    'cite_spans': [[]],\n",
       "    'cite_span_lens': [0],\n",
       "    'section': ['Results on Our Dataset']},\n",
       "   'Further Investigation of Our Framework ': {'text': ['To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "     \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\"],\n",
       "    'cite_spans': [[{'start': 208,\n",
       "       'end': 215,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     [{'start': 33,\n",
       "       'end': 40,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}]],\n",
       "    'cite_span_lens': [1, 1],\n",
       "    'section': ['Further Investigation of Our Framework ',\n",
       "     'Further Investigation of Our Framework ']},\n",
       "   'Case Study': {'text': ['Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.'],\n",
       "    'cite_spans': [[]],\n",
       "    'cite_span_lens': [0],\n",
       "    'section': ['Case Study']},\n",
       "   'Conclusions': {'text': ['We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.'],\n",
       "    'cite_spans': [[]],\n",
       "    'cite_span_lens': [0],\n",
       "    'section': ['Conclusions']}}}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения глазами ссылка на [статью](https://arxiv.org/pdf/1708.01065.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Introduction 13\n",
      "1 Overview 1\n",
      "2 Reader-Aware Salience Estimation 6\n",
      "3 Summary Construction 6\n",
      "4 Data Description 0\n",
      "5 Background 0\n",
      "6 Data Collection 0\n",
      "7 Data Properties 0\n",
      "8 Dataset and Metrics 1\n",
      "9 Comparative Methods 6\n",
      "10 Experimental Settings 2\n",
      "11 Results on Our Dataset 0\n",
      "12 Further Investigation of Our Framework  2\n",
      "13 Case Study 0\n",
      "14 Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for num_tex,(k,v) in enumerate(grobid_latex_overview.items()):\n",
    "    print(num_tex,k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "1 1\n",
      "2 6\n",
      "3 3\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 1\n",
      "9 4\n",
      "10 2\n",
      "11 2\n",
      "12 0\n",
      "13 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in grobid_parse_overview.items():\n",
    "    print(k,len(v['cite_spans']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), key=lambda item: len(item[1]['cite_spans']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_latex_overview = {k: v for k, v in sorted(grobid_latex_overview.items(), key=lambda item: sum(item[1]['cite_span_lens']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Introduction': {'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "   \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "   'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "   'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "   'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "   'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       "  'cite_spans': [[{'start': 193,\n",
       "     'end': 200,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF0'},\n",
       "    {'start': 203,\n",
       "     'end': 210,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 213,\n",
       "     'end': 220,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF2'},\n",
       "    {'start': 223,\n",
       "     'end': 230,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF3'},\n",
       "    {'start': 233,\n",
       "     'end': 240,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF4'},\n",
       "    {'start': 243,\n",
       "     'end': 250,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 253,\n",
       "     'end': 260,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   [],\n",
       "   [{'start': 527,\n",
       "     'end': 534,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF7'},\n",
       "    {'start': 537,\n",
       "     'end': 544,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF8'},\n",
       "    {'start': 802,\n",
       "     'end': 809,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "    {'start': 159,\n",
       "     'end': 167,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 170,\n",
       "     'end': 178,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'}],\n",
       "   [],\n",
       "   []],\n",
       "  'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       "  'section': ['Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction']},\n",
       " 'Reader-Aware Salience Estimation': {'text': ['Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "   'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "   'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "   'The calculation of INLINEFORM0 will be discussed later.',\n",
       "   'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "   'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "   'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "   'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "   ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "   'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "   'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "   'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "   'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "   'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.'],\n",
       "  'cite_spans': [[{'start': 32,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 43,\n",
       "     'end': 51,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'},\n",
       "    {'start': 154,\n",
       "     'end': 161,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "    {'start': 46,\n",
       "     'end': 54,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF12'},\n",
       "    {'start': 57,\n",
       "     'end': 65,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF13'}],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   [],\n",
       "   []],\n",
       "  'cite_span_lens': [3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'section': ['Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation',\n",
       "   'Reader-Aware Salience Estimation']},\n",
       " 'Summary Construction': {'text': ['In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "   'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.'],\n",
       "  'cite_spans': [[{'start': 82,\n",
       "     'end': 89,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 94,\n",
       "     'end': 101,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   [{'start': 466,\n",
       "     'end': 474,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF14'},\n",
       "    {'start': 477,\n",
       "     'end': 484,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 491,\n",
       "     'end': 498,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'},\n",
       "    {'start': 644,\n",
       "     'end': 652,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF15'}]],\n",
       "  'cite_span_lens': [2, 4],\n",
       "  'section': ['Summary Construction', 'Summary Construction']},\n",
       " 'Comparative Methods': {'text': ['To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "   'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "   'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "   'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "   'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "   'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "   'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.'],\n",
       "  'cite_spans': [[],\n",
       "   [{'start': 10,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   [{'start': 5,\n",
       "     'end': 13,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF17'}],\n",
       "   [{'start': 9,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF18'}],\n",
       "   [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'},\n",
       "    {'start': 29,\n",
       "     'end': 37,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF19'}],\n",
       "   [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}],\n",
       "   []],\n",
       "  'cite_span_lens': [0, 1, 1, 1, 2, 1, 0],\n",
       "  'section': ['Comparative Methods',\n",
       "   'Comparative Methods',\n",
       "   'Comparative Methods',\n",
       "   'Comparative Methods',\n",
       "   'Comparative Methods',\n",
       "   'Comparative Methods',\n",
       "   'Comparative Methods']},\n",
       " 'Experimental Settings': {'text': ['The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.'],\n",
       "  'cite_spans': [[{'start': 557,\n",
       "     'end': 565,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF20'},\n",
       "    {'start': 697,\n",
       "     'end': 705,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF21'}]],\n",
       "  'cite_span_lens': [2],\n",
       "  'section': ['Experimental Settings']},\n",
       " 'Further Investigation of Our Framework ': {'text': ['To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "   \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\"],\n",
       "  'cite_spans': [[{'start': 208,\n",
       "     'end': 215,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   [{'start': 33,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}]],\n",
       "  'cite_span_lens': [1, 1],\n",
       "  'section': ['Further Investigation of Our Framework ',\n",
       "   'Further Investigation of Our Framework ']},\n",
       " 'Overview': {'text': ['As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.'],\n",
       "  'cite_spans': [[{'start': 489,\n",
       "     'end': 496,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}]],\n",
       "  'cite_span_lens': [1],\n",
       "  'section': ['Overview']},\n",
       " 'Dataset and Metrics': {'text': ['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.'],\n",
       "  'cite_spans': [[{'start': 113,\n",
       "     'end': 121,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF16'}]],\n",
       "  'cite_span_lens': [1],\n",
       "  'section': ['Dataset and Metrics']},\n",
       " 'Background': {'text': ['The definition of the terminology related to the dataset is given as follows.',\n",
       "   'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "   'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "   'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "   'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "   'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "   'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.'],\n",
       "  'cite_spans': [[], [], [], [], [], [], []],\n",
       "  'cite_span_lens': [0, 0, 0, 0, 0, 0, 0],\n",
       "  'section': ['Background',\n",
       "   'Background',\n",
       "   'Background',\n",
       "   'Background',\n",
       "   'Background',\n",
       "   'Background',\n",
       "   'Background']},\n",
       " 'Data Collection': {'text': ['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "   'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "   'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "   'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.'],\n",
       "  'cite_spans': [[], [], [], []],\n",
       "  'cite_span_lens': [0, 0, 0, 0],\n",
       "  'section': ['Data Collection',\n",
       "   'Data Collection',\n",
       "   'Data Collection',\n",
       "   'Data Collection']},\n",
       " 'Data Description': {'text': ['In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.'],\n",
       "  'cite_spans': [[]],\n",
       "  'cite_span_lens': [0],\n",
       "  'section': ['Data Description']},\n",
       " 'Data Properties': {'text': ['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.'],\n",
       "  'cite_spans': [[]],\n",
       "  'cite_span_lens': [0],\n",
       "  'section': ['Data Properties']},\n",
       " 'Results on Our Dataset': {'text': ['The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.'],\n",
       "  'cite_spans': [[]],\n",
       "  'cite_span_lens': [0],\n",
       "  'section': ['Results on Our Dataset']},\n",
       " 'Case Study': {'text': ['Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.'],\n",
       "  'cite_spans': [[]],\n",
       "  'cite_span_lens': [0],\n",
       "  'section': ['Case Study']},\n",
       " 'Conclusions': {'text': ['We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.'],\n",
       "  'cite_spans': [[]],\n",
       "  'cite_span_lens': [0],\n",
       "  'section': ['Conclusions']}}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_latex_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 13\n",
      "Reader-Aware Salience Estimation 6\n",
      "Summary Construction 6\n",
      "Comparative Methods 6\n",
      "Experimental Settings 2\n",
      "Further Investigation of Our Framework  2\n",
      "Overview 1\n",
      "Dataset and Metrics 1\n",
      "Background 0\n",
      "Data Collection 0\n",
      "Data Description 0\n",
      "Data Properties 0\n",
      "Results on Our Dataset 0\n",
      "Case Study 0\n",
      "Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for k,v in grobid_latex_overview.items():\n",
    "    print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим для части и посмотрим глазами как выполняется сбор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "for num_artic,article in enumerate(all_articles):\n",
    "    if num_artic ==5:\n",
    "        break\n",
    "        \n",
    "    overview_papers[article['paper_id']] = { 'paper_id':article['paper_id'],   'metadata':article['metadata'],\n",
    "                                             's2_pdf_hash':article['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}\n",
    "    grobid_parse_overview = None\n",
    "    if article['grobid_parse'] and article['grobid_parse']['body_text']:\n",
    "        grobid_parse_overview = dict()\n",
    "        for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "            grobid_parse_overview[num_sec] = sections\n",
    "        \n",
    "        grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), key=lambda item: len(item[1]['cite_spans']), reverse=True)}\n",
    "\n",
    "    grobid_latex_overview = None\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        grobid_latex_overview = dict()\n",
    "        for sections in article['latex_parse']['body_text']:\n",
    "            if sections['section'] in grobid_latex_overview:\n",
    "                if grobid_latex_overview[sections['section']] == sections:\n",
    "                    continue\n",
    "                else:\n",
    "                    grobid_latex_overview[sections['section']]['text'].append(sections['text'])\n",
    "                    grobid_latex_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "                    grobid_latex_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "                    grobid_latex_overview[sections['section']]['section'].append(sections['section'])\n",
    "            else:\n",
    "                grobid_latex_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                              'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                              'section':[sections['section']]}\n",
    "        grobid_latex_overview = {k: v for k, v in sorted(grobid_latex_overview.items(), key=lambda item: item[1]['cite_span_lens'], reverse=True)}\n",
    "    overview_papers[article['paper_id']]['grobid_parse'] = grobid_parse_overview\n",
    "    overview_papers[article['paper_id']]['latex_parse'] = grobid_latex_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['10164018', '14472576', '17302615', '3243536', '3248240'])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a Semantic Parser Overnight \n",
      "----\n",
      "grobid\n",
      "14 18\n",
      "1 7\n",
      "0 6\n",
      "8 5\n",
      "2 3\n",
      "7 3\n",
      "3 1\n",
      "13 1\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "9 0\n",
      "10 0\n",
      "11 0\n",
      "12 0\n"
     ]
    }
   ],
   "source": [
    "key_id = '14472576'\n",
    "print(overview_papers[key_id]['metadata']['title'],'\\n----')\n",
    "if overview_papers[key_id]['grobid_parse']:\n",
    "    print('grobid')\n",
    "    for k,v in overview_papers[key_id]['grobid_parse'].items():\n",
    "        print(k,len(v['cite_spans']))\n",
    "\n",
    "if overview_papers[key_id]['latex_parse']:\n",
    "    print('---\\nlatex')\n",
    "    for k,v in overview_papers[key_id]['latex_parse'].items():\n",
    "        print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Building a Semantic Parser Overnight](https://www.aclweb.org/anthology/P15-1129.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим Related Work является последнмм перед References абзацем => работает верно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLUE: Simple and robust methods for polarity classification \n",
      "----\n",
      "grobid\n",
      "3 6\n",
      "9 4\n",
      "0 3\n",
      "1 1\n",
      "2 1\n",
      "6 1\n",
      "7 1\n",
      "4 0\n",
      "5 0\n",
      "8 0\n"
     ]
    }
   ],
   "source": [
    "key_id = '17302615'\n",
    "print(overview_papers[key_id]['metadata']['title'],'\\n----')\n",
    "if overview_papers[key_id]['grobid_parse']:\n",
    "    print('grobid')\n",
    "    for k,v in overview_papers[key_id]['grobid_parse'].items():\n",
    "        print(k,len(v['cite_spans']))\n",
    "\n",
    "if overview_papers[key_id]['latex_parse']:\n",
    "    print('---\\nlatex')\n",
    "    for k,v in overview_papers[key_id]['latex_parse'].items():\n",
    "        print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Widely-used algorithms such as SentiStrength (Thelwall et al., 2010) rely heavily on dictionaries containing sentiment ratings of words and/or phrases. We use features based on an extended version of AFINN-111 (Nielsen, 2011) . 4 The AFINN sentiment dictionary contains sentiment ratings ranging from −5 (very negative) to 5 (very positive) for 2 476 word forms. In order to obtain a better coverage, we extended the dictionary with distributionally similar words. For this purpose, large-vocabulary distributional semantic models (DSM) were constructed from a version of the English Wikipedia 5 and the Google Web 1T 5-Grams database (Brants and Franz, 2006) . The Wikipedia DSM consists of 122 281 case-folded word forms as target terms and 30 484 mid-frequency content words (lemmatised) as feature terms; the Web1T5 DSM of 241 583 case-folded word forms as target terms and 100 063 case-folded word forms as feature terms. Both DSMs use a context window of two words to the left and right, and were reduced to 300 latent dimensions using randomized singular value decomposition (Halko et al., 2009 ).For each AFINN entry, the 30 nearest neighbours according to each DSM were considered as extension candidates. Sentiment ratings for the new candidates were computed by averaging over the 30 nearest neighbours of the respective candidate term (with scores set to 0 for all neighbours not listed in AFINN), and rescaling to the range [−5, 5]. 6 After some initial experiments, only candidates with a computed rating ≤ −2.5 or ≥ 2.5 were retained, resulting in an extended dictionary of 2 820 word forms.As with the bag of words model, we make use of a simple heuristic treatment of negation: following a negation marker, the polarity of the next sentimentcarrying token up to a distance of at most four tokens is multiplied by −1.The sentiment dictionary is used to extract four features: I) the number of tokens that express a positive sentiment, II) the number of tokens that express a negative sentiment, III) the total number of tokens that express a sentiment according to our sentiment dictionary and IV) the arithmetic mean of all the sentiment scores from the sentiment dictionary in the message.'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse'][3]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We use a resource-lean approach, relying only on three external resources: a stemmer, a relatively small sentiment dictionary and an even smaller list of emotion markers. Stemmers are already available for many languages and both kinds of lexical resources can be gathered relatively easily for other languages. The list of emotion markers should apply to most languages. This makes our whole system relatively language-independent, provided that a similar amount of manually labelled training data is available. 13 In fact, the learning curve for our system ( Fig. 1) suggests that even as few as 3 000-3 500 labelled messages might be sufficient. The similar Figure 1: Learning curve of our system for the \"Message Polarity Classification\" task, evaluated on the Twitter data evaluation results for the Twitter and the SMS data show that not relying on Twitter-specific features like hashtags pays off: by making our system as generic as possible, it is robust, not overfitted to the training data, and generalizes well to other types of data. The methods discussed in the current paper are particularly well suited to the \"Message Polarity Classification\" task, our system ranking amongst the best. It turns out, however, that simply applying the same approach to the \"Contextual Polarity Disambiguation\" task yields only mediocre results.In the future, we would like to experiment with a couple of additional features. Determining the nearest neighbors of a message based on Latent Semantic Analysis might be a useful addition, as might be the use of part-of-speech tags created by an in-domain POS tagger (Gimpel et al., 2011) 14 . We would also like to find out whether a heuristic treatment of intensifiers and detensifiers, the normalization of character repetitions, or the inclusion of some punctuationbased features could further improve classifier performance. 13 For task B, even the extended unigram bag-of-words model by itself, without any additional resources, would have performed quite well as the 9th best constrained system on the Twitter test set (13th best system overall) and the 5th best system on the SMS test set.14 http://www.ark.cs.cmu.edu/TweetNLP/'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse'][9]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[KLUE: Simple and robust methods for polarity classification ](https://www.aclweb.org/anthology/S13-2065.pdf)\n",
    "\n",
    "Здесь нет ярко выраженной главы Related Work,на мой взгляд, всё выделилось верно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting Variant Recognition With Light Semantics \n",
      "----\n",
      "grobid\n",
      "0 5\n",
      "4 3\n",
      "9 2\n",
      "1 1\n",
      "5 1\n",
      "10 1\n",
      "11 1\n",
      "2 0\n",
      "3 0\n",
      "6 0\n",
      "7 0\n",
      "8 0\n",
      "12 0\n"
     ]
    }
   ],
   "source": [
    "key_id = '3243536'\n",
    "print(overview_papers[key_id]['metadata']['title'],'\\n----')\n",
    "if overview_papers[key_id]['grobid_parse']:\n",
    "    print('grobid')\n",
    "    for k,v in overview_papers[key_id]['grobid_parse'].items():\n",
    "        print(k,len(v['cite_spans']))\n",
    "\n",
    "if overview_papers[key_id]['latex_parse']:\n",
    "    print('---\\nlatex')\n",
    "    for k,v in overview_papers[key_id]['latex_parse'].items():\n",
    "        print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The recognition ,of paraphrases and variants is an important issue in several areas of infornmtion retrieval and text mlderstanding. Merging paraphrastic sentences ilnproves summarization by avoiding redundancy (Barzilay et al., 1999) . Term variant conilation enhances recall in intbrmation retrieval by pointing at documents that contain linguistic variants of (tuery terms (Arampatzis et al., 1998) .In (Jacquemin and Tzoukermann, 1999 ), a technique is proposed for the conflation of morpho-syntactic variants that relies solely on morphological and low-level syntactic features (part-of-speech category, munber agreement, morphological relationships, and phrase structure). An analysis of these results shows the limitation of this approach: correct and incorrect variants cannot be separated satisfactorily on a purely morpho-syntactic basis. Sonic additional lexical semantics must be taken into consideration.In this study we propose a reasonably simple, domain-independent, large-scale approach of lexical semantics to noun-to-verb variant recognition. It relies on the mere addition of two t)oolean syntactic features to 449 verbs and two boolean morpho-semantic features to 574 nouns. It result,; in a significant enhancement of precision of 30% with a slight decrease in recall of 10%. This new al)proaeh to semantics--human-based, ettlcient, involving simple linguistic tbatures --convincingly illustrates the positive role of linguistic knowledge in information processing. It confirms that verbs and their semantics play a significant role in document analysis (Klavans and Kan, 1998) . Nomino-verbal Variation In order to illustrate the contribution of semantics to the detection of paraphrastic structures, we focus on a specific type of wtriation: the vo.rl)al varbmts of Noun-Preposition-Noun terms o1\" compounds in French. For example, les corttraintes rdsiduellcs darts les coques sont anaIysdes (the residual constraints in the shells ~re mialyzed) is such a vert)al variant of analyse de corttraintc (constraint analysis).'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Boosting Variant Recognition With Light Semantics](https://www.aclweb.org/anthology/C00-1039.pdf)\n",
    "\n",
    "Все верно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multilingual, Efficient and Easy NLP Processing with IXA Pipeline \n",
      "----\n",
      "grobid\n",
      "5 6\n",
      "1 5\n",
      "4 5\n",
      "6 3\n",
      "7 3\n",
      "8 2\n",
      "3 1\n",
      "0 0\n",
      "2 0\n",
      "9 0\n"
     ]
    }
   ],
   "source": [
    "key_id = '3248240'\n",
    "print(overview_papers[key_id]['metadata']['title'],'\\n----')\n",
    "if overview_papers[key_id]['grobid_parse']:\n",
    "    print('grobid')\n",
    "    for k,v in overview_papers[key_id]['grobid_parse'].items():\n",
    "        print(k,len(v['cite_spans']))\n",
    "\n",
    "if overview_papers[key_id]['latex_parse']:\n",
    "    print('---\\nlatex')\n",
    "    for k,v in overview_papers[key_id]['latex_parse'].items():\n",
    "        print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most of the NER systems nowdays consist of language independent systems (sometimes enriched with gazeteers) based on automatic learning of statistical models. ixa-pipe-nerc provides Named Entity Recognition (NER) for English and Spanish. The named entity types are based on the CONLL 2002 13 and 2003 14 tasks which were focused on language-independent supervised named entity recognition (NER) for four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. We currently provide two very fast language independent models using a rather simple baseline featureset (e.g., similar to that of Curran and Clark (2003), except POS tag features). For English, perceptron models have been trained using CoNLL 2003 dataset. We currenly obtain 84.80 F1 which is coherent with other results reported with these features (Clark and Curran, 2003; Ratinov and Roth, 2009 ). The best Stanford NER model reported on this dataset achieves 86.86 F1 (Finkel et al., 2005) , whereas the best system on this dataset achieves 90.80 F1 (Ratinov and Roth, 2009 ), using non local features and substantial external knowledge.For Spanish we currently obtain best results training Maximum Entropy models on the CoNLL 2002 dataset. Our best model obtains 79.92 F1 vs 81.39 F1 (Carreras et al., 2002) , the best result so far on this dataset. Their result uses external knowledge and without it, their system obtains 79.28 F1.'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse'][5]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 http://www.apache.org/licenses/LICENSE-2.0.html 2 Architecture IXA pipeline is primarily conceived as a set of ready to use tools that can provide efficient and accurate linguistic annotation without any installation/configuration/compilation effort. As in Unix-like operative systems, IXA pipeline consists of a set of processes chained by their standard streams, in a way that the output of each process feeds directly as input to the next one. The Unix pipeline metaphor has been applied for NLP tools by adopting a very simple and well known data centric architecture, in which every module/pipe is interchangeable for another one as long as it takes and produces the required data format.The data format in which both the input and output of the modules needs to be formatted to represent and filter linguistic annotations is KAF (Bosma et al., 2009) . KAF is a language neutral annotation format representing both morpho-syntactic and semantic annotation in a structured format. KAF was originally designed in the Kyoto European project 2 , but it has since been in continuous development 3 . Our Java modules all use kaflib 4 library for easy integration.Every module in the IXA pipeline, except the coreference resolution, is implemented in Java, and requires Java JDK1.7+ to compile. The integration of the Java modules in the IXA pipeline is performed using Maven 5 . Maven is used to take care of classpaths configurations and third-party tool dependencies. This means that the binaries produced and distributed will work off-the-self. The coreference module uses pip 6 to provide an easy, one step installation. If the source code of an ixa-pipe-$module is cloned from the remote repository, one command to compile and have ready the tools will suffice. Some modules in IXA pipeline provide linguistic annotation based on probabilistic supervised approaches such as POS tagging, NER and Syntactic Parsing. IXA pipeline uses two well known machine learning algorithms, namely, Maximum Entropy and the Perceptron. Both Perceptron (Collins, 2002; Collins, 2003) and Maximum Entropy models (Ratnaparkhi, 1999) are adaptable algorithms which have been successfully applied to NLP tasks such as POS tagging, NER and Parsing with state of the art results. To avoid duplication of efforts, IXA pipeline uses the already available open-source Apache OpenNLP API 7 to train POS, NER and parsing probabilistic models using these two approaches.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse'][1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ixa-pipe-pos provides POS tagging and lemmatization for English and Spanish. We have obtained the best results so far with the same featureset as in Collins's (2002) paper. Perceptron models for English have been trained and evaluated on the WSJ treebank using the usual partitions (e.g., as explained in Toutanova et al. (2003) . We currently obtain a performance of 97.07% vs 97.24% obtained by Toutanova et al., (2003) ). For Spanish, Maximum Entropy models have been trained and evaluated using the Ancora corpus; it was randomly divided in 90% for training and 10% for testing. This corresponds to 440K words used for training and 70K words for testing. We obtain a performance of 98.88% (the corpus partitions are available for reproducibility). Giménez and Marquez (2004) report 98.86%, although they train and test on a different subset of the Ancora corpus.Lemmatization is currently performed via 3 different dictionary lookup methods: (i) Simple Lemmatizer: It is based on HashMap lookups on a plain text dictionary. Currently we use dictionaries from the LanguageTool project 10 under their distribution licenses. The English 8 http://www-nlp.stanford.edu/software/tokenizer.shtml 9 http://jflex.de/ 10 http://languagetool.org/ dictionary contains 300K lemmas whereas the Spanish provides over 600K; (ii) Morfologik-stemming 11 : The Morfologik library provides routines to produce binary dictionaries, from dictionaries such as the one used by the Simple Lemmatizer above, as finite state automata. This method is convenient whenever lookups on very large dictionaries are required because it reduces the memory footprint to 10% of the memory required for the equivalent plain text dictionary; and (iii) We also provide lemmatization by lookup in WordNet-3.0 (Fellbaum and Miller, 1998) via the JWNL API 12 . Note that this method is only available for English.\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse'][4]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Multilingual, Efficient and Easy NLP Processing with IXA Pipeline](https://www.aclweb.org/anthology/E14-2002.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Other NLP toolkits exist providing similar or more extensive functionalities than the IXA pipeline tools, although not many of them provide multilingual support. GATE (Cunningham, 2002) is an extensive framework supporting annotation of text. GATE has some capacity for wrapping Apache UIMA components 16 , so should be able to manage distributed NLP components. However, GATE is a very large and complex system, with a corresponding steep learning curve.Freeling (Padró and Stanilovsky, 2012) provides multilingual processing for a number of languages, incluing Spanish and English. As opposed to IXA pipeline, Freeling is a monolithic toolkit written in C++ which needs to be compiled natively. The Stanford CoreNLP 17 is a monolithic suite, which makes it difficult to integrate other tools in its chain.IXA pipeline tools can easily be used piping the input with the output of another too, and it is also possible to easily replace or extend the toolchain with a third-party tool. IXA pipeline is already being used to do extensive parallel processing in the FP7 European projects OpeNER 18 and NewsReader 19 .'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers[key_id]['grobid_parse'][8]['text'] # RW section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**контрпример:** Как видим не всегда большее количество ссылок в RW части."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
