{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def get_citation_contexts(paper: Dict, toks_in_context=10) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Retrieve citation contexts from GORC paper\n",
    "    :param paper:\n",
    "    :param toks_in_context:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not paper:\n",
    "        return []\n",
    "\n",
    "    if not paper['grobid_parse']:\n",
    "        return []\n",
    "\n",
    "    if not paper['grobid_parse']['body_text']:\n",
    "        return []\n",
    "\n",
    "    contexts = []\n",
    "\n",
    "    for paragraph in paper['grobid_parse']['body_text']:\n",
    "        for cite_span in paragraph['cite_spans']:\n",
    "            # get cited paper id, skip if none\n",
    "            cite_ref = cite_span['ref_id']\n",
    "            cited_paper_id = None\n",
    "            if cite_ref in paper['grobid_parse']['bib_entries']:\n",
    "                cited_paper_id = paper['grobid_parse']['bib_entries'][cite_ref]['links']\n",
    "            if not cited_paper_id:\n",
    "                continue\n",
    "\n",
    "            # get pre and post tokens\n",
    "            pre_span_tokens = paragraph['text'][:cite_span['start']].split(' ')[-toks_in_context:]\n",
    "            post_span_tokens = paragraph['text'][cite_span['end']:].split(' ')[:toks_in_context]\n",
    "            pre_string = ' '.join(pre_span_tokens)\n",
    "            post_string = ' '.join(post_span_tokens)\n",
    "            full_context = pre_string + cite_span['text'] + post_string\n",
    "\n",
    "            contexts.append({\n",
    "                \"paper_id\": paper['paper_id'],\n",
    "                \"context_string\": full_context,\n",
    "                \"cite_start\": len(pre_string),\n",
    "                \"cite_end\": len(pre_string) + len(cite_span['text']),\n",
    "                \"cite_str\": cite_span['text'],\n",
    "                \"cited_paper_id\": cited_paper_id\n",
    "            })\n",
    "\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_DATA_FILE = '../data/example_papers.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_contexts = []\n",
    "all_papers = []\n",
    "context_dict = dict()\n",
    "with open(EXAMPLE_DATA_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        gorc_obj = json.loads(line)\n",
    "        all_papers.append(gorc_obj)\n",
    "        all_contexts += get_citation_contexts(gorc_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "f_zips = []\n",
    "for (dirpath, dirnames, filenames) in walk('../../../gorc/'):\n",
    "    f_zips.extend(filenames)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10002,\n",
       " ['0.jsonl.gz',\n",
       "  '1.jsonl.gz',\n",
       "  '10.jsonl.gz',\n",
       "  '100.jsonl.gz',\n",
       "  '1000.jsonl.gz',\n",
       "  '1001.jsonl.gz',\n",
       "  '1002.jsonl.gz',\n",
       "  '1003.jsonl.gz',\n",
       "  '1004.jsonl.gz',\n",
       "  '1005.jsonl.gz',\n",
       "  '1006.jsonl.gz',\n",
       "  '1007.jsonl.gz',\n",
       "  '1008.jsonl.gz',\n",
       "  '1009.jsonl.gz',\n",
       "  '101.jsonl.gz',\n",
       "  '1010.jsonl.gz',\n",
       "  '1011.jsonl.gz',\n",
       "  '1012.jsonl.gz',\n",
       "  '1013.jsonl.gz',\n",
       "  '1014.jsonl.gz',\n",
       "  '1015.jsonl.gz',\n",
       "  '1016.jsonl.gz',\n",
       "  '1017.jsonl.gz',\n",
       "  '1018.jsonl.gz',\n",
       "  '1019.jsonl.gz',\n",
       "  '102.jsonl.gz',\n",
       "  '1020.jsonl.gz',\n",
       "  '1021.jsonl.gz',\n",
       "  '1022.jsonl.gz',\n",
       "  '1023.jsonl.gz',\n",
       "  '1024.jsonl.gz',\n",
       "  '1025.jsonl.gz',\n",
       "  '1026.jsonl.gz',\n",
       "  '1027.jsonl.gz',\n",
       "  '1028.jsonl.gz',\n",
       "  '1029.jsonl.gz',\n",
       "  '103.jsonl.gz',\n",
       "  '1030.jsonl.gz',\n",
       "  '1031.jsonl.gz',\n",
       "  '1032.jsonl.gz',\n",
       "  '1033.jsonl.gz',\n",
       "  '1034.jsonl.gz',\n",
       "  '1035.jsonl.gz',\n",
       "  '1036.jsonl.gz',\n",
       "  '1037.jsonl.gz',\n",
       "  '1038.jsonl.gz',\n",
       "  '1039.jsonl.gz',\n",
       "  '104.jsonl.gz',\n",
       "  '1040.jsonl.gz',\n",
       "  '1041.jsonl.gz',\n",
       "  '1042.jsonl.gz',\n",
       "  '1043.jsonl.gz',\n",
       "  '1044.jsonl.gz',\n",
       "  '1045.jsonl.gz',\n",
       "  '1046.jsonl.gz',\n",
       "  '1047.jsonl.gz',\n",
       "  '1048.jsonl.gz',\n",
       "  '1049.jsonl.gz',\n",
       "  '105.jsonl.gz',\n",
       "  '1050.jsonl.gz',\n",
       "  '1051.jsonl.gz',\n",
       "  '1052.jsonl.gz',\n",
       "  '1053.jsonl.gz',\n",
       "  '1054.jsonl.gz',\n",
       "  '1055.jsonl.gz',\n",
       "  '1056.jsonl.gz',\n",
       "  '1057.jsonl.gz',\n",
       "  '1058.jsonl.gz',\n",
       "  '1059.jsonl.gz',\n",
       "  '106.jsonl.gz',\n",
       "  '1060.jsonl.gz',\n",
       "  '1061.jsonl.gz',\n",
       "  '1062.jsonl.gz',\n",
       "  '1063.jsonl.gz',\n",
       "  '1064.jsonl.gz',\n",
       "  '1065.jsonl.gz',\n",
       "  '1066.jsonl.gz',\n",
       "  '1067.jsonl.gz',\n",
       "  '1068.jsonl.gz',\n",
       "  '1069.jsonl.gz',\n",
       "  '107.jsonl.gz',\n",
       "  '1070.jsonl.gz',\n",
       "  '1071.jsonl.gz',\n",
       "  '1072.jsonl.gz',\n",
       "  '1073.jsonl.gz',\n",
       "  '1074.jsonl.gz',\n",
       "  '1075.jsonl.gz',\n",
       "  '1076.jsonl.gz',\n",
       "  '1077.jsonl.gz',\n",
       "  '1078.jsonl.gz',\n",
       "  '1079.jsonl.gz',\n",
       "  '108.jsonl.gz',\n",
       "  '1080.jsonl.gz',\n",
       "  '1081.jsonl.gz',\n",
       "  '1082.jsonl.gz',\n",
       "  '1083.jsonl.gz',\n",
       "  '1084.jsonl.gz',\n",
       "  '1085.jsonl.gz',\n",
       "  '1086.jsonl.gz',\n",
       "  '1087.jsonl.gz',\n",
       "  '1088.jsonl.gz',\n",
       "  '1089.jsonl.gz',\n",
       "  '109.jsonl.gz',\n",
       "  '1090.jsonl.gz',\n",
       "  '1091.jsonl.gz',\n",
       "  '1092.jsonl.gz',\n",
       "  '1093.jsonl.gz',\n",
       "  '1094.jsonl.gz',\n",
       "  '1095.jsonl.gz',\n",
       "  '1096.jsonl.gz',\n",
       "  '1097.jsonl.gz',\n",
       "  '1098.jsonl.gz',\n",
       "  '1099.jsonl.gz',\n",
       "  '11.jsonl.gz',\n",
       "  '110.jsonl.gz',\n",
       "  '1100.jsonl.gz',\n",
       "  '1101.jsonl.gz',\n",
       "  '1102.jsonl.gz',\n",
       "  '1103.jsonl.gz',\n",
       "  '1104.jsonl.gz',\n",
       "  '1105.jsonl.gz',\n",
       "  '1106.jsonl.gz',\n",
       "  '1107.jsonl.gz',\n",
       "  '1108.jsonl.gz',\n",
       "  '1109.jsonl.gz',\n",
       "  '111.jsonl.gz',\n",
       "  '1110.jsonl.gz',\n",
       "  '1111.jsonl.gz',\n",
       "  '1112.jsonl.gz',\n",
       "  '1113.jsonl.gz',\n",
       "  '1114.jsonl.gz',\n",
       "  '1115.jsonl.gz',\n",
       "  '1116.jsonl.gz',\n",
       "  '1117.jsonl.gz',\n",
       "  '1118.jsonl.gz',\n",
       "  '1119.jsonl.gz',\n",
       "  '112.jsonl.gz',\n",
       "  '1120.jsonl.gz',\n",
       "  '1121.jsonl.gz',\n",
       "  '1122.jsonl.gz',\n",
       "  '1123.jsonl.gz',\n",
       "  '1124.jsonl.gz',\n",
       "  '1125.jsonl.gz',\n",
       "  '1126.jsonl.gz',\n",
       "  '1127.jsonl.gz',\n",
       "  '1128.jsonl.gz',\n",
       "  '1129.jsonl.gz',\n",
       "  '113.jsonl.gz',\n",
       "  '1130.jsonl.gz',\n",
       "  '1131.jsonl.gz',\n",
       "  '1132.jsonl.gz',\n",
       "  '1133.jsonl.gz',\n",
       "  '1134.jsonl.gz',\n",
       "  '1135.jsonl.gz',\n",
       "  '1136.jsonl.gz',\n",
       "  '1137.jsonl.gz',\n",
       "  '1138.jsonl.gz',\n",
       "  '1139.jsonl.gz',\n",
       "  '114.jsonl.gz',\n",
       "  '1140.jsonl.gz',\n",
       "  '1141.jsonl.gz',\n",
       "  '1142.jsonl.gz',\n",
       "  '1143.jsonl.gz',\n",
       "  '1144.jsonl.gz',\n",
       "  '1145.jsonl.gz',\n",
       "  '1146.jsonl.gz',\n",
       "  '1147.jsonl.gz',\n",
       "  '1148.jsonl.gz',\n",
       "  '1149.jsonl.gz',\n",
       "  '115.jsonl.gz',\n",
       "  '1150.jsonl.gz',\n",
       "  '1151.jsonl.gz',\n",
       "  '1152.jsonl.gz',\n",
       "  '1153.jsonl.gz',\n",
       "  '1154.jsonl.gz',\n",
       "  '1155.jsonl.gz',\n",
       "  '1156.jsonl.gz',\n",
       "  '1157.jsonl.gz',\n",
       "  '1158.jsonl.gz',\n",
       "  '1159.jsonl.gz',\n",
       "  '116.jsonl.gz',\n",
       "  '1160.jsonl.gz',\n",
       "  '1161.jsonl.gz',\n",
       "  '1162.jsonl.gz',\n",
       "  '1163.jsonl.gz',\n",
       "  '1164.jsonl.gz',\n",
       "  '1165.jsonl.gz',\n",
       "  '1166.jsonl.gz',\n",
       "  '1167.jsonl.gz',\n",
       "  '1168.jsonl.gz',\n",
       "  '1169.jsonl.gz',\n",
       "  '117.jsonl.gz',\n",
       "  '1170.jsonl.gz',\n",
       "  '1171.jsonl.gz',\n",
       "  '1172.jsonl.gz',\n",
       "  '1173.jsonl.gz',\n",
       "  '1174.jsonl.gz',\n",
       "  '1175.jsonl.gz',\n",
       "  '1176.jsonl.gz',\n",
       "  '1177.jsonl.gz',\n",
       "  '1178.jsonl.gz',\n",
       "  '1179.jsonl.gz',\n",
       "  '118.jsonl.gz',\n",
       "  '1180.jsonl.gz',\n",
       "  '1181.jsonl.gz',\n",
       "  '1182.jsonl.gz',\n",
       "  '1183.jsonl.gz',\n",
       "  '1184.jsonl.gz',\n",
       "  '1185.jsonl.gz',\n",
       "  '1186.jsonl.gz',\n",
       "  '1187.jsonl.gz',\n",
       "  '1188.jsonl.gz',\n",
       "  '1189.jsonl.gz',\n",
       "  '119.jsonl.gz',\n",
       "  '1190.jsonl.gz',\n",
       "  '1191.jsonl.gz',\n",
       "  '1192.jsonl.gz',\n",
       "  '1193.jsonl.gz',\n",
       "  '1194.jsonl.gz',\n",
       "  '1195.jsonl.gz',\n",
       "  '1196.jsonl.gz',\n",
       "  '1197.jsonl.gz',\n",
       "  '1198.jsonl.gz',\n",
       "  '1199.jsonl.gz',\n",
       "  '12.jsonl.gz',\n",
       "  '120.jsonl.gz',\n",
       "  '1200.jsonl.gz',\n",
       "  '1201.jsonl.gz',\n",
       "  '1202.jsonl.gz',\n",
       "  '1203.jsonl.gz',\n",
       "  '1204.jsonl.gz',\n",
       "  '1205.jsonl.gz',\n",
       "  '1206.jsonl.gz',\n",
       "  '1207.jsonl.gz',\n",
       "  '1208.jsonl.gz',\n",
       "  '1209.jsonl.gz',\n",
       "  '121.jsonl.gz',\n",
       "  '1210.jsonl.gz',\n",
       "  '1211.jsonl.gz',\n",
       "  '1212.jsonl.gz',\n",
       "  '1213.jsonl.gz',\n",
       "  '1214.jsonl.gz',\n",
       "  '1215.jsonl.gz',\n",
       "  '1216.jsonl.gz',\n",
       "  '1217.jsonl.gz',\n",
       "  '1218.jsonl.gz',\n",
       "  '1219.jsonl.gz',\n",
       "  '122.jsonl.gz',\n",
       "  '1220.jsonl.gz',\n",
       "  '1221.jsonl.gz',\n",
       "  '1222.jsonl.gz',\n",
       "  '1223.jsonl.gz',\n",
       "  '1224.jsonl.gz',\n",
       "  '1225.jsonl.gz',\n",
       "  '1226.jsonl.gz',\n",
       "  '1227.jsonl.gz',\n",
       "  '1228.jsonl.gz',\n",
       "  '1229.jsonl.gz',\n",
       "  '123.jsonl.gz',\n",
       "  '1230.jsonl.gz',\n",
       "  '1231.jsonl.gz',\n",
       "  '1232.jsonl.gz',\n",
       "  '1233.jsonl.gz',\n",
       "  '1234.jsonl.gz',\n",
       "  '1235.jsonl.gz',\n",
       "  '1236.jsonl.gz',\n",
       "  '1237.jsonl.gz',\n",
       "  '1238.jsonl.gz',\n",
       "  '1239.jsonl.gz',\n",
       "  '124.jsonl.gz',\n",
       "  '1240.jsonl.gz',\n",
       "  '1241.jsonl.gz',\n",
       "  '1242.jsonl.gz',\n",
       "  '1243.jsonl.gz',\n",
       "  '1244.jsonl.gz',\n",
       "  '1245.jsonl.gz',\n",
       "  '1246.jsonl.gz',\n",
       "  '1247.jsonl.gz',\n",
       "  '1248.jsonl.gz',\n",
       "  '1249.jsonl.gz',\n",
       "  '125.jsonl.gz',\n",
       "  '1250.jsonl.gz',\n",
       "  '1251.jsonl.gz',\n",
       "  '1252.jsonl.gz',\n",
       "  '1253.jsonl.gz',\n",
       "  '1254.jsonl.gz',\n",
       "  '1255.jsonl.gz',\n",
       "  '1256.jsonl.gz',\n",
       "  '1257.jsonl.gz',\n",
       "  '1258.jsonl.gz',\n",
       "  '1259.jsonl.gz',\n",
       "  '126.jsonl.gz',\n",
       "  '1260.jsonl.gz',\n",
       "  '1261.jsonl.gz',\n",
       "  '1262.jsonl.gz',\n",
       "  '1263.jsonl.gz',\n",
       "  '1264.jsonl.gz',\n",
       "  '1265.jsonl.gz',\n",
       "  '1266.jsonl.gz',\n",
       "  '1267.jsonl.gz',\n",
       "  '1268.jsonl.gz',\n",
       "  '1269.jsonl.gz',\n",
       "  '127.jsonl.gz',\n",
       "  '1270.jsonl.gz',\n",
       "  '1271.jsonl.gz',\n",
       "  '1272.jsonl.gz',\n",
       "  '1273.jsonl.gz',\n",
       "  '1274.jsonl.gz',\n",
       "  '1275.jsonl.gz',\n",
       "  '1276.jsonl.gz',\n",
       "  '1277.jsonl.gz',\n",
       "  '1278.jsonl.gz',\n",
       "  '1279.jsonl.gz',\n",
       "  '128.jsonl.gz',\n",
       "  '1280.jsonl.gz',\n",
       "  '1281.jsonl.gz',\n",
       "  '1282.jsonl.gz',\n",
       "  '1283.jsonl.gz',\n",
       "  '1284.jsonl.gz',\n",
       "  '1285.jsonl.gz',\n",
       "  '1286.jsonl.gz',\n",
       "  '1287.jsonl.gz',\n",
       "  '1288.jsonl.gz',\n",
       "  '1289.jsonl.gz',\n",
       "  '129.jsonl.gz',\n",
       "  '1290.jsonl.gz',\n",
       "  '1291.jsonl.gz',\n",
       "  '1292.jsonl.gz',\n",
       "  '1293.jsonl.gz',\n",
       "  '1294.jsonl.gz',\n",
       "  '1295.jsonl.gz',\n",
       "  '1296.jsonl.gz',\n",
       "  '1297.jsonl.gz',\n",
       "  '1298.jsonl.gz',\n",
       "  '1299.jsonl.gz',\n",
       "  '13.jsonl.gz',\n",
       "  '130.jsonl.gz',\n",
       "  '1300.jsonl.gz',\n",
       "  '1301.jsonl.gz',\n",
       "  '1302.jsonl.gz',\n",
       "  '1303.jsonl.gz',\n",
       "  '1304.jsonl.gz',\n",
       "  '1305.jsonl.gz',\n",
       "  '1306.jsonl.gz',\n",
       "  '1307.jsonl.gz',\n",
       "  '1308.jsonl.gz',\n",
       "  '1309.jsonl.gz',\n",
       "  '131.jsonl.gz',\n",
       "  '1310.jsonl.gz',\n",
       "  '1311.jsonl.gz',\n",
       "  '1312.jsonl.gz',\n",
       "  '1313.jsonl.gz',\n",
       "  '1314.jsonl.gz',\n",
       "  '1315.jsonl.gz',\n",
       "  '1316.jsonl.gz',\n",
       "  '1317.jsonl.gz',\n",
       "  '1318.jsonl.gz',\n",
       "  '1319.jsonl.gz',\n",
       "  '132.jsonl.gz',\n",
       "  '1320.jsonl.gz',\n",
       "  '1321.jsonl.gz',\n",
       "  '1322.jsonl.gz',\n",
       "  '1323.jsonl.gz',\n",
       "  '1324.jsonl.gz',\n",
       "  '1325.jsonl.gz',\n",
       "  '1326.jsonl.gz',\n",
       "  '1327.jsonl.gz',\n",
       "  '1328.jsonl.gz',\n",
       "  '1329.jsonl.gz',\n",
       "  '133.jsonl.gz',\n",
       "  '1330.jsonl.gz',\n",
       "  '1331.jsonl.gz',\n",
       "  '1332.jsonl.gz',\n",
       "  '1333.jsonl.gz',\n",
       "  '1334.jsonl.gz',\n",
       "  '1335.jsonl.gz',\n",
       "  '1336.jsonl.gz',\n",
       "  '1337.jsonl.gz',\n",
       "  '1338.jsonl.gz',\n",
       "  '1339.jsonl.gz',\n",
       "  '134.jsonl.gz',\n",
       "  '1340.jsonl.gz',\n",
       "  '1341.jsonl.gz',\n",
       "  '1342.jsonl.gz',\n",
       "  '1343.jsonl.gz',\n",
       "  '1344.jsonl.gz',\n",
       "  '1345.jsonl.gz',\n",
       "  '1346.jsonl.gz',\n",
       "  '1347.jsonl.gz',\n",
       "  '1348.jsonl.gz',\n",
       "  '1349.jsonl.gz',\n",
       "  '135.jsonl.gz',\n",
       "  '1350.jsonl.gz',\n",
       "  '1351.jsonl.gz',\n",
       "  '1352.jsonl.gz',\n",
       "  '1353.jsonl.gz',\n",
       "  '1354.jsonl.gz',\n",
       "  '1355.jsonl.gz',\n",
       "  '1356.jsonl.gz',\n",
       "  '1357.jsonl.gz',\n",
       "  '1358.jsonl.gz',\n",
       "  '1359.jsonl.gz',\n",
       "  '136.jsonl.gz',\n",
       "  '1360.jsonl.gz',\n",
       "  '1361.jsonl.gz',\n",
       "  '1362.jsonl.gz',\n",
       "  '1363.jsonl.gz',\n",
       "  '1364.jsonl.gz',\n",
       "  '1365.jsonl.gz',\n",
       "  '1366.jsonl.gz',\n",
       "  '1367.jsonl.gz',\n",
       "  '1368.jsonl.gz',\n",
       "  '1369.jsonl.gz',\n",
       "  '137.jsonl.gz',\n",
       "  '1370.jsonl.gz',\n",
       "  '1371.jsonl.gz',\n",
       "  '1372.jsonl.gz',\n",
       "  '1373.jsonl.gz',\n",
       "  '1374.jsonl.gz',\n",
       "  '1375.jsonl.gz',\n",
       "  '1376.jsonl.gz',\n",
       "  '1377.jsonl.gz',\n",
       "  '1378.jsonl.gz',\n",
       "  '1379.jsonl.gz',\n",
       "  '138.jsonl.gz',\n",
       "  '1380.jsonl.gz',\n",
       "  '1381.jsonl.gz',\n",
       "  '1382.jsonl.gz',\n",
       "  '1383.jsonl.gz',\n",
       "  '1384.jsonl.gz',\n",
       "  '1385.jsonl.gz',\n",
       "  '1386.jsonl.gz',\n",
       "  '1387.jsonl.gz',\n",
       "  '1388.jsonl.gz',\n",
       "  '1389.jsonl.gz',\n",
       "  '139.jsonl.gz',\n",
       "  '1390.jsonl.gz',\n",
       "  '1391.jsonl.gz',\n",
       "  '1392.jsonl.gz',\n",
       "  '1393.jsonl.gz',\n",
       "  '1394.jsonl.gz',\n",
       "  '1395.jsonl.gz',\n",
       "  '1396.jsonl.gz',\n",
       "  '1397.jsonl.gz',\n",
       "  '1398.jsonl.gz',\n",
       "  '1399.jsonl.gz',\n",
       "  '14.jsonl.gz',\n",
       "  '140.jsonl.gz',\n",
       "  '1400.jsonl.gz',\n",
       "  '1401.jsonl.gz',\n",
       "  '1402.jsonl.gz',\n",
       "  '1403.jsonl.gz',\n",
       "  '1404.jsonl.gz',\n",
       "  '1405.jsonl.gz',\n",
       "  '1406.jsonl.gz',\n",
       "  '1407.jsonl.gz',\n",
       "  '1408.jsonl.gz',\n",
       "  '1409.jsonl.gz',\n",
       "  '141.jsonl.gz',\n",
       "  '1410.jsonl.gz',\n",
       "  '1411.jsonl.gz',\n",
       "  '1412.jsonl.gz',\n",
       "  '1413.jsonl.gz',\n",
       "  '1414.jsonl.gz',\n",
       "  '1415.jsonl.gz',\n",
       "  '1416.jsonl.gz',\n",
       "  '1417.jsonl.gz',\n",
       "  '1418.jsonl.gz',\n",
       "  '1419.jsonl.gz',\n",
       "  '142.jsonl.gz',\n",
       "  '1420.jsonl.gz',\n",
       "  '1421.jsonl.gz',\n",
       "  '1422.jsonl.gz',\n",
       "  '1423.jsonl.gz',\n",
       "  '1424.jsonl.gz',\n",
       "  '1425.jsonl.gz',\n",
       "  '1426.jsonl.gz',\n",
       "  '1427.jsonl.gz',\n",
       "  '1428.jsonl.gz',\n",
       "  '1429.jsonl.gz',\n",
       "  '143.jsonl.gz',\n",
       "  '1430.jsonl.gz',\n",
       "  '1431.jsonl.gz',\n",
       "  '1432.jsonl.gz',\n",
       "  '1433.jsonl.gz',\n",
       "  '1434.jsonl.gz',\n",
       "  '1435.jsonl.gz',\n",
       "  '1436.jsonl.gz',\n",
       "  '1437.jsonl.gz',\n",
       "  '1438.jsonl.gz',\n",
       "  '1439.jsonl.gz',\n",
       "  '144.jsonl.gz',\n",
       "  '1440.jsonl.gz',\n",
       "  '1441.jsonl.gz',\n",
       "  '1442.jsonl.gz',\n",
       "  '1443.jsonl.gz',\n",
       "  '1444.jsonl.gz',\n",
       "  '1445.jsonl.gz',\n",
       "  '1446.jsonl.gz',\n",
       "  '1447.jsonl.gz',\n",
       "  '1448.jsonl.gz',\n",
       "  '1449.jsonl.gz',\n",
       "  '145.jsonl.gz',\n",
       "  '1450.jsonl.gz',\n",
       "  '1451.jsonl.gz',\n",
       "  '1452.jsonl.gz',\n",
       "  '1453.jsonl.gz',\n",
       "  '1454.jsonl.gz',\n",
       "  '1455.jsonl.gz',\n",
       "  '1456.jsonl.gz',\n",
       "  '1457.jsonl.gz',\n",
       "  '1458.jsonl.gz',\n",
       "  '1459.jsonl.gz',\n",
       "  '146.jsonl.gz',\n",
       "  '1460.jsonl.gz',\n",
       "  '1461.jsonl.gz',\n",
       "  '1462.jsonl.gz',\n",
       "  '1463.jsonl.gz',\n",
       "  '1464.jsonl.gz',\n",
       "  '1465.jsonl.gz',\n",
       "  '1466.jsonl.gz',\n",
       "  '1467.jsonl.gz',\n",
       "  '1468.jsonl.gz',\n",
       "  '1469.jsonl.gz',\n",
       "  '147.jsonl.gz',\n",
       "  '1470.jsonl.gz',\n",
       "  '1471.jsonl.gz',\n",
       "  '1472.jsonl.gz',\n",
       "  '1473.jsonl.gz',\n",
       "  '1474.jsonl.gz',\n",
       "  '1475.jsonl.gz',\n",
       "  '1476.jsonl.gz',\n",
       "  '1477.jsonl.gz',\n",
       "  '1478.jsonl.gz',\n",
       "  '1479.jsonl.gz',\n",
       "  '148.jsonl.gz',\n",
       "  '1480.jsonl.gz',\n",
       "  '1481.jsonl.gz',\n",
       "  '1482.jsonl.gz',\n",
       "  '1483.jsonl.gz',\n",
       "  '1484.jsonl.gz',\n",
       "  '1485.jsonl.gz',\n",
       "  '1486.jsonl.gz',\n",
       "  '1487.jsonl.gz',\n",
       "  '1488.jsonl.gz',\n",
       "  '1489.jsonl.gz',\n",
       "  '149.jsonl.gz',\n",
       "  '1490.jsonl.gz',\n",
       "  '1491.jsonl.gz',\n",
       "  '1492.jsonl.gz',\n",
       "  '1493.jsonl.gz',\n",
       "  '1494.jsonl.gz',\n",
       "  '1495.jsonl.gz',\n",
       "  '1496.jsonl.gz',\n",
       "  '1497.jsonl.gz',\n",
       "  '1498.jsonl.gz',\n",
       "  '1499.jsonl.gz',\n",
       "  '15.jsonl.gz',\n",
       "  '150.jsonl.gz',\n",
       "  '1500.jsonl.gz',\n",
       "  '1501.jsonl.gz',\n",
       "  '1502.jsonl.gz',\n",
       "  '1503.jsonl.gz',\n",
       "  '1504.jsonl.gz',\n",
       "  '1505.jsonl.gz',\n",
       "  '1506.jsonl.gz',\n",
       "  '1507.jsonl.gz',\n",
       "  '1508.jsonl.gz',\n",
       "  '1509.jsonl.gz',\n",
       "  '151.jsonl.gz',\n",
       "  '1510.jsonl.gz',\n",
       "  '1511.jsonl.gz',\n",
       "  '1512.jsonl.gz',\n",
       "  '1513.jsonl.gz',\n",
       "  '1514.jsonl.gz',\n",
       "  '1515.jsonl.gz',\n",
       "  '1516.jsonl.gz',\n",
       "  '1517.jsonl.gz',\n",
       "  '1518.jsonl.gz',\n",
       "  '1519.jsonl.gz',\n",
       "  '152.jsonl.gz',\n",
       "  '1520.jsonl.gz',\n",
       "  '1521.jsonl.gz',\n",
       "  '1522.jsonl.gz',\n",
       "  '1523.jsonl.gz',\n",
       "  '1524.jsonl.gz',\n",
       "  '1525.jsonl.gz',\n",
       "  '1526.jsonl.gz',\n",
       "  '1527.jsonl.gz',\n",
       "  '1528.jsonl.gz',\n",
       "  '1529.jsonl.gz',\n",
       "  '153.jsonl.gz',\n",
       "  '1530.jsonl.gz',\n",
       "  '1531.jsonl.gz',\n",
       "  '1532.jsonl.gz',\n",
       "  '1533.jsonl.gz',\n",
       "  '1534.jsonl.gz',\n",
       "  '1535.jsonl.gz',\n",
       "  '1536.jsonl.gz',\n",
       "  '1537.jsonl.gz',\n",
       "  '1538.jsonl.gz',\n",
       "  '1539.jsonl.gz',\n",
       "  '154.jsonl.gz',\n",
       "  '1540.jsonl.gz',\n",
       "  '1541.jsonl.gz',\n",
       "  '1542.jsonl.gz',\n",
       "  '1543.jsonl.gz',\n",
       "  '1544.jsonl.gz',\n",
       "  '1545.jsonl.gz',\n",
       "  '1546.jsonl.gz',\n",
       "  '1547.jsonl.gz',\n",
       "  '1548.jsonl.gz',\n",
       "  '1549.jsonl.gz',\n",
       "  '155.jsonl.gz',\n",
       "  '1550.jsonl.gz',\n",
       "  '1551.jsonl.gz',\n",
       "  '1552.jsonl.gz',\n",
       "  '1553.jsonl.gz',\n",
       "  '1554.jsonl.gz',\n",
       "  '1555.jsonl.gz',\n",
       "  '1556.jsonl.gz',\n",
       "  '1557.jsonl.gz',\n",
       "  '1558.jsonl.gz',\n",
       "  '1559.jsonl.gz',\n",
       "  '156.jsonl.gz',\n",
       "  '1560.jsonl.gz',\n",
       "  '1561.jsonl.gz',\n",
       "  '1562.jsonl.gz',\n",
       "  '1563.jsonl.gz',\n",
       "  '1564.jsonl.gz',\n",
       "  '1565.jsonl.gz',\n",
       "  '1566.jsonl.gz',\n",
       "  '1567.jsonl.gz',\n",
       "  '1568.jsonl.gz',\n",
       "  '1569.jsonl.gz',\n",
       "  '157.jsonl.gz',\n",
       "  '1570.jsonl.gz',\n",
       "  '1571.jsonl.gz',\n",
       "  '1572.jsonl.gz',\n",
       "  '1573.jsonl.gz',\n",
       "  '1574.jsonl.gz',\n",
       "  '1575.jsonl.gz',\n",
       "  '1576.jsonl.gz',\n",
       "  '1577.jsonl.gz',\n",
       "  '1578.jsonl.gz',\n",
       "  '1579.jsonl.gz',\n",
       "  '158.jsonl.gz',\n",
       "  '1580.jsonl.gz',\n",
       "  '1581.jsonl.gz',\n",
       "  '1582.jsonl.gz',\n",
       "  '1583.jsonl.gz',\n",
       "  '1584.jsonl.gz',\n",
       "  '1585.jsonl.gz',\n",
       "  '1586.jsonl.gz',\n",
       "  '1587.jsonl.gz',\n",
       "  '1588.jsonl.gz',\n",
       "  '1589.jsonl.gz',\n",
       "  '159.jsonl.gz',\n",
       "  '1590.jsonl.gz',\n",
       "  '1591.jsonl.gz',\n",
       "  '1592.jsonl.gz',\n",
       "  '1593.jsonl.gz',\n",
       "  '1594.jsonl.gz',\n",
       "  '1595.jsonl.gz',\n",
       "  '1596.jsonl.gz',\n",
       "  '1597.jsonl.gz',\n",
       "  '1598.jsonl.gz',\n",
       "  '1599.jsonl.gz',\n",
       "  '16.jsonl.gz',\n",
       "  '160.jsonl.gz',\n",
       "  '1600.jsonl.gz',\n",
       "  '1601.jsonl.gz',\n",
       "  '1602.jsonl.gz',\n",
       "  '1603.jsonl.gz',\n",
       "  '1604.jsonl.gz',\n",
       "  '1605.jsonl.gz',\n",
       "  '1606.jsonl.gz',\n",
       "  '1607.jsonl.gz',\n",
       "  '1608.jsonl.gz',\n",
       "  '1609.jsonl.gz',\n",
       "  '161.jsonl.gz',\n",
       "  '1610.jsonl.gz',\n",
       "  '1611.jsonl.gz',\n",
       "  '1612.jsonl.gz',\n",
       "  '1613.jsonl.gz',\n",
       "  '1614.jsonl.gz',\n",
       "  '1615.jsonl.gz',\n",
       "  '1616.jsonl.gz',\n",
       "  '1617.jsonl.gz',\n",
       "  '1618.jsonl.gz',\n",
       "  '1619.jsonl.gz',\n",
       "  '162.jsonl.gz',\n",
       "  '1620.jsonl.gz',\n",
       "  '1621.jsonl.gz',\n",
       "  '1622.jsonl.gz',\n",
       "  '1623.jsonl.gz',\n",
       "  '1624.jsonl.gz',\n",
       "  '1625.jsonl.gz',\n",
       "  '1626.jsonl.gz',\n",
       "  '1627.jsonl.gz',\n",
       "  '1628.jsonl.gz',\n",
       "  '1629.jsonl.gz',\n",
       "  '163.jsonl.gz',\n",
       "  '1630.jsonl.gz',\n",
       "  '1631.jsonl.gz',\n",
       "  '1632.jsonl.gz',\n",
       "  '1633.jsonl.gz',\n",
       "  '1634.jsonl.gz',\n",
       "  '1635.jsonl.gz',\n",
       "  '1636.jsonl.gz',\n",
       "  '1637.jsonl.gz',\n",
       "  '1638.jsonl.gz',\n",
       "  '1639.jsonl.gz',\n",
       "  '164.jsonl.gz',\n",
       "  '1640.jsonl.gz',\n",
       "  '1641.jsonl.gz',\n",
       "  '1642.jsonl.gz',\n",
       "  '1643.jsonl.gz',\n",
       "  '1644.jsonl.gz',\n",
       "  '1645.jsonl.gz',\n",
       "  '1646.jsonl.gz',\n",
       "  '1647.jsonl.gz',\n",
       "  '1648.jsonl.gz',\n",
       "  '1649.jsonl.gz',\n",
       "  '165.jsonl.gz',\n",
       "  '1650.jsonl.gz',\n",
       "  '1651.jsonl.gz',\n",
       "  '1652.jsonl.gz',\n",
       "  '1653.jsonl.gz',\n",
       "  '1654.jsonl.gz',\n",
       "  '1655.jsonl.gz',\n",
       "  '1656.jsonl.gz',\n",
       "  '1657.jsonl.gz',\n",
       "  '1658.jsonl.gz',\n",
       "  '1659.jsonl.gz',\n",
       "  '166.jsonl.gz',\n",
       "  '1660.jsonl.gz',\n",
       "  '1661.jsonl.gz',\n",
       "  '1662.jsonl.gz',\n",
       "  '1663.jsonl.gz',\n",
       "  '1664.jsonl.gz',\n",
       "  '1665.jsonl.gz',\n",
       "  '1666.jsonl.gz',\n",
       "  '1667.jsonl.gz',\n",
       "  '1668.jsonl.gz',\n",
       "  '1669.jsonl.gz',\n",
       "  '167.jsonl.gz',\n",
       "  '1670.jsonl.gz',\n",
       "  '1671.jsonl.gz',\n",
       "  '1672.jsonl.gz',\n",
       "  '1673.jsonl.gz',\n",
       "  '1674.jsonl.gz',\n",
       "  '1675.jsonl.gz',\n",
       "  '1676.jsonl.gz',\n",
       "  '1677.jsonl.gz',\n",
       "  '1678.jsonl.gz',\n",
       "  '1679.jsonl.gz',\n",
       "  '168.jsonl.gz',\n",
       "  '1680.jsonl.gz',\n",
       "  '1681.jsonl.gz',\n",
       "  '1682.jsonl.gz',\n",
       "  '1683.jsonl.gz',\n",
       "  '1684.jsonl.gz',\n",
       "  '1685.jsonl.gz',\n",
       "  '1686.jsonl.gz',\n",
       "  '1687.jsonl.gz',\n",
       "  '1688.jsonl.gz',\n",
       "  '1689.jsonl.gz',\n",
       "  '169.jsonl.gz',\n",
       "  '1690.jsonl.gz',\n",
       "  '1691.jsonl.gz',\n",
       "  '1692.jsonl.gz',\n",
       "  '1693.jsonl.gz',\n",
       "  '1694.jsonl.gz',\n",
       "  '1695.jsonl.gz',\n",
       "  '1696.jsonl.gz',\n",
       "  '1697.jsonl.gz',\n",
       "  '1698.jsonl.gz',\n",
       "  '1699.jsonl.gz',\n",
       "  '17.jsonl.gz',\n",
       "  '170.jsonl.gz',\n",
       "  '1700.jsonl.gz',\n",
       "  '1701.jsonl.gz',\n",
       "  '1702.jsonl.gz',\n",
       "  '1703.jsonl.gz',\n",
       "  '1704.jsonl.gz',\n",
       "  '1705.jsonl.gz',\n",
       "  '1706.jsonl.gz',\n",
       "  '1707.jsonl.gz',\n",
       "  '1708.jsonl.gz',\n",
       "  '1709.jsonl.gz',\n",
       "  '171.jsonl.gz',\n",
       "  '1710.jsonl.gz',\n",
       "  '1711.jsonl.gz',\n",
       "  '1712.jsonl.gz',\n",
       "  '1713.jsonl.gz',\n",
       "  '1714.jsonl.gz',\n",
       "  '1715.jsonl.gz',\n",
       "  '1716.jsonl.gz',\n",
       "  '1717.jsonl.gz',\n",
       "  '1718.jsonl.gz',\n",
       "  '1719.jsonl.gz',\n",
       "  '172.jsonl.gz',\n",
       "  '1720.jsonl.gz',\n",
       "  '1721.jsonl.gz',\n",
       "  '1722.jsonl.gz',\n",
       "  '1723.jsonl.gz',\n",
       "  '1724.jsonl.gz',\n",
       "  '1725.jsonl.gz',\n",
       "  '1726.jsonl.gz',\n",
       "  '1727.jsonl.gz',\n",
       "  '1728.jsonl.gz',\n",
       "  '1729.jsonl.gz',\n",
       "  '173.jsonl.gz',\n",
       "  '1730.jsonl.gz',\n",
       "  '1731.jsonl.gz',\n",
       "  '1732.jsonl.gz',\n",
       "  '1733.jsonl.gz',\n",
       "  '1734.jsonl.gz',\n",
       "  '1735.jsonl.gz',\n",
       "  '1736.jsonl.gz',\n",
       "  '1737.jsonl.gz',\n",
       "  '1738.jsonl.gz',\n",
       "  '1739.jsonl.gz',\n",
       "  '174.jsonl.gz',\n",
       "  '1740.jsonl.gz',\n",
       "  '1741.jsonl.gz',\n",
       "  '1742.jsonl.gz',\n",
       "  '1743.jsonl.gz',\n",
       "  '1744.jsonl.gz',\n",
       "  '1745.jsonl.gz',\n",
       "  '1746.jsonl.gz',\n",
       "  '1747.jsonl.gz',\n",
       "  '1748.jsonl.gz',\n",
       "  '1749.jsonl.gz',\n",
       "  '175.jsonl.gz',\n",
       "  '1750.jsonl.gz',\n",
       "  '1751.jsonl.gz',\n",
       "  '1752.jsonl.gz',\n",
       "  '1753.jsonl.gz',\n",
       "  '1754.jsonl.gz',\n",
       "  '1755.jsonl.gz',\n",
       "  '1756.jsonl.gz',\n",
       "  '1757.jsonl.gz',\n",
       "  '1758.jsonl.gz',\n",
       "  '1759.jsonl.gz',\n",
       "  '176.jsonl.gz',\n",
       "  '1760.jsonl.gz',\n",
       "  '1761.jsonl.gz',\n",
       "  '1762.jsonl.gz',\n",
       "  '1763.jsonl.gz',\n",
       "  '1764.jsonl.gz',\n",
       "  '1765.jsonl.gz',\n",
       "  '1766.jsonl.gz',\n",
       "  '1767.jsonl.gz',\n",
       "  '1768.jsonl.gz',\n",
       "  '1769.jsonl.gz',\n",
       "  '177.jsonl.gz',\n",
       "  '1770.jsonl.gz',\n",
       "  '1771.jsonl.gz',\n",
       "  '1772.jsonl.gz',\n",
       "  '1773.jsonl.gz',\n",
       "  '1774.jsonl.gz',\n",
       "  '1775.jsonl.gz',\n",
       "  '1776.jsonl.gz',\n",
       "  '1777.jsonl.gz',\n",
       "  '1778.jsonl.gz',\n",
       "  '1779.jsonl.gz',\n",
       "  '178.jsonl.gz',\n",
       "  '1780.jsonl.gz',\n",
       "  '1781.jsonl.gz',\n",
       "  '1782.jsonl.gz',\n",
       "  '1783.jsonl.gz',\n",
       "  '1784.jsonl.gz',\n",
       "  '1785.jsonl.gz',\n",
       "  '1786.jsonl.gz',\n",
       "  '1787.jsonl.gz',\n",
       "  '1788.jsonl.gz',\n",
       "  '1789.jsonl.gz',\n",
       "  '179.jsonl.gz',\n",
       "  '1790.jsonl.gz',\n",
       "  '1791.jsonl.gz',\n",
       "  '1792.jsonl.gz',\n",
       "  '1793.jsonl.gz',\n",
       "  '1794.jsonl.gz',\n",
       "  '1795.jsonl.gz',\n",
       "  '1796.jsonl.gz',\n",
       "  '1797.jsonl.gz',\n",
       "  '1798.jsonl.gz',\n",
       "  '1799.jsonl.gz',\n",
       "  '18.jsonl.gz',\n",
       "  '180.jsonl.gz',\n",
       "  '1800.jsonl.gz',\n",
       "  '1801.jsonl.gz',\n",
       "  '1802.jsonl.gz',\n",
       "  '1803.jsonl.gz',\n",
       "  '1804.jsonl.gz',\n",
       "  '1805.jsonl.gz',\n",
       "  '1806.jsonl.gz',\n",
       "  '1807.jsonl.gz',\n",
       "  '1808.jsonl.gz',\n",
       "  '1809.jsonl.gz',\n",
       "  '181.jsonl.gz',\n",
       "  '1810.jsonl.gz',\n",
       "  '1811.jsonl.gz',\n",
       "  '1812.jsonl.gz',\n",
       "  '1813.jsonl.gz',\n",
       "  '1814.jsonl.gz',\n",
       "  '1815.jsonl.gz',\n",
       "  '1816.jsonl.gz',\n",
       "  '1817.jsonl.gz',\n",
       "  '1818.jsonl.gz',\n",
       "  '1819.jsonl.gz',\n",
       "  '182.jsonl.gz',\n",
       "  '1820.jsonl.gz',\n",
       "  '1821.jsonl.gz',\n",
       "  '1822.jsonl.gz',\n",
       "  '1823.jsonl.gz',\n",
       "  '1824.jsonl.gz',\n",
       "  '1825.jsonl.gz',\n",
       "  '1826.jsonl.gz',\n",
       "  '1827.jsonl.gz',\n",
       "  '1828.jsonl.gz',\n",
       "  '1829.jsonl.gz',\n",
       "  '183.jsonl.gz',\n",
       "  '1830.jsonl.gz',\n",
       "  '1831.jsonl.gz',\n",
       "  '1832.jsonl.gz',\n",
       "  '1833.jsonl.gz',\n",
       "  '1834.jsonl.gz',\n",
       "  '1835.jsonl.gz',\n",
       "  '1836.jsonl.gz',\n",
       "  '1837.jsonl.gz',\n",
       "  '1838.jsonl.gz',\n",
       "  '1839.jsonl.gz',\n",
       "  '184.jsonl.gz',\n",
       "  '1840.jsonl.gz',\n",
       "  '1841.jsonl.gz',\n",
       "  '1842.jsonl.gz',\n",
       "  '1843.jsonl.gz',\n",
       "  '1844.jsonl.gz',\n",
       "  '1845.jsonl.gz',\n",
       "  '1846.jsonl.gz',\n",
       "  '1847.jsonl.gz',\n",
       "  '1848.jsonl.gz',\n",
       "  '1849.jsonl.gz',\n",
       "  '185.jsonl.gz',\n",
       "  '1850.jsonl.gz',\n",
       "  '1851.jsonl.gz',\n",
       "  '1852.jsonl.gz',\n",
       "  '1853.jsonl.gz',\n",
       "  '1854.jsonl.gz',\n",
       "  '1855.jsonl.gz',\n",
       "  '1856.jsonl.gz',\n",
       "  '1857.jsonl.gz',\n",
       "  '1858.jsonl.gz',\n",
       "  '1859.jsonl.gz',\n",
       "  '186.jsonl.gz',\n",
       "  '1860.jsonl.gz',\n",
       "  '1861.jsonl.gz',\n",
       "  '1862.jsonl.gz',\n",
       "  '1863.jsonl.gz',\n",
       "  '1864.jsonl.gz',\n",
       "  '1865.jsonl.gz',\n",
       "  '1866.jsonl.gz',\n",
       "  '1867.jsonl.gz',\n",
       "  '1868.jsonl.gz',\n",
       "  '1869.jsonl.gz',\n",
       "  '187.jsonl.gz',\n",
       "  '1870.jsonl.gz',\n",
       "  '1871.jsonl.gz',\n",
       "  '1872.jsonl.gz',\n",
       "  '1873.jsonl.gz',\n",
       "  '1874.jsonl.gz',\n",
       "  '1875.jsonl.gz',\n",
       "  '1876.jsonl.gz',\n",
       "  '1877.jsonl.gz',\n",
       "  '1878.jsonl.gz',\n",
       "  '1879.jsonl.gz',\n",
       "  '188.jsonl.gz',\n",
       "  '1880.jsonl.gz',\n",
       "  '1881.jsonl.gz',\n",
       "  '1882.jsonl.gz',\n",
       "  '1883.jsonl.gz',\n",
       "  '1884.jsonl.gz',\n",
       "  '1885.jsonl.gz',\n",
       "  '1886.jsonl.gz',\n",
       "  '1887.jsonl.gz',\n",
       "  '1888.jsonl.gz',\n",
       "  '1889.jsonl.gz',\n",
       "  '189.jsonl.gz',\n",
       "  '1890.jsonl.gz',\n",
       "  '1891.jsonl.gz',\n",
       "  '1892.jsonl.gz',\n",
       "  '1893.jsonl.gz',\n",
       "  '1894.jsonl.gz',\n",
       "  '1895.jsonl.gz',\n",
       "  '1896.jsonl.gz',\n",
       "  '1897.jsonl.gz',\n",
       "  '1898.jsonl.gz',\n",
       "  ...])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_zips),f_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s2orc-master.zip']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in f_zips if '.gz' not in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '104172',\n",
       " 'metadata': {'title': 'Nonlinear inversion of tilt-affected very long period records of explosive eruptions at Fuego volcano: INVERSION OF TILT-AFFECTED VLP EVENTS',\n",
       "  'authors': [{'first': 'Gregory',\n",
       "    'middle': ['P.'],\n",
       "    'last': 'Waite',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Federica', 'middle': [], 'last': 'Lanza', 'suffix': ''}],\n",
       "  'abstract': None,\n",
       "  'year': '2016',\n",
       "  'arxiv_id': None,\n",
       "  'acl_id': None,\n",
       "  'pmc_id': None,\n",
       "  'pubmed_id': None,\n",
       "  'doi': '10.1002/2016jb013287',\n",
       "  'venue': 'Journal of Geophysical Research: Solid Earth',\n",
       "  'journal': 'Journal of Geophysical Research'},\n",
       " 's2_pdf_hash': '73ed8076fc747e77c41845cb5f18b40ece350865',\n",
       " 'grobid_parse': {'abstract': [],\n",
       "  'body_text': [{'text': 'solution to this is to evaluate long wavelength, very-long-period (VLP) data that are relativelyFuego is a 3800 m stratovolcano that regularly produces Strombolian and weak 76Vulcanian explosions. The dynamics of these explosive events have been examined in the VLP 77 band [Lyons and Waite, 2011] and modeled together with infrasound and gas emission data. At 78 least three different styles of VLP event have been observed and attributed to eruptions from 79 either the summit vent or a flank vent [Waite et al., 2013] . The strongest recorded explosions 80 generated impulsive infrasound and seismic signals, ejected incandescent bombs and tephra, and 81were associated with repetitive VLP seismicity. The previous studies of Fuego VLP events 82 focused on periods from 30-10 seconds, where the influence of ground tilt is negligible given the 83 distances to the source and relatively short VLP wavelengths. Although the station geometry was 84 somewhat limited by the logistical and safety considerations, Lyons and Waite [2011] found that 85 the data best fit a source with a centroid 300 m below and 300 m to the west of the summit with a 86 moment tensor representative of primarily a dipping crack. The 30-10 second VLP captures the 87 inflation-deflation-reinflation cycle of this portion of the conduit as a small eruption occurs. 88 Lyons et al. [2012] examined the tilt signal associated with these same small explosions 89 at periods below the instrument corners. They found a significant tilt signal beginning up to 30 90 minutes prior to explosive eruptions. Forward modeling of the tilt from stations that were close 91 enough to record it suggested a shallow source midway between the VLP source centroid and the 92 summit. A full waveform inversion was not attempted. 93In this study, we perform full waveform inversions of stacks of events associated with 94 summit vent explosions in periods from 400 -10 seconds using a combined rotation-translation 95 approach similar to that of Maeda et al. [2011] . Inversions were performed in different bands to 96 explore the increasing influence of tilt with increasing period. While events with periods of 100s 97 of seconds are sometimes called Ultra-Long-Period events [e.g., Johnson et al., 2009], we simply 98 use the term VLP to cover the range of periods we investigate here. To improve the signal to 99 noise ratio, and ensure a representative dataset, inversions were performed on a set of phase-100 weighted, stacked seismograms from six explosions. The cleaner signals that resulted from 101 stacking also allowed for a larger number of seismic channels to be used than in previous studies. 102In order to constrain the uncertainty on the source type, we performed a nonlinear inversion for 103 moment tensor source type. This involves a grid search over all possible moment tensor types 104 and orientations at the best-fit centroid location, providing quantitative constraint on the source 105 type.',\n",
       "    'cite_spans': [{'start': 274,\n",
       "      'end': 297,\n",
       "      'text': '[Lyons and Waite, 2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'},\n",
       "     {'start': 500,\n",
       "      'end': 520,\n",
       "      'text': '[Waite et al., 2013]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF27'},\n",
       "     {'start': 1027,\n",
       "      'end': 1033,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1342, 'end': 1344, 'text': '88', 'latex': None, 'ref_id': None},\n",
       "     {'start': 1358,\n",
       "      'end': 1364,\n",
       "      'text': '[2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2016,\n",
       "      'end': 2022,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'In order to avoid problems associated with ground rotation contaminating the low 109 frequency seismic signal and, in fact, to take advantage of the tilt signal to investigate 110 frequencies below the typical VLP band, we invert jointly for translation and rotation. The 111 approach was first laid out by Maeda et al. [2011] and has been successfully applied at Kīlauea 112 [Chouet and Dawson, 2013] and Asama volcanoes ] . An important 113 aspect of this method is that the instrument responses are not deconvolved from the 114 seismograms. Instead, the responses to tilt and displacement are convolved with corresponding 115Green\\'s functions. Deconvolution effectively occurs during the inversion. For component n of 116 displacement at time t and receiver position € ! r , we can write the relationship between source and 117 receiver through rotation and translation Green\\'s functions in the frequency domain: 118 with respect to the q-coordinate. The translation response is derived from the poles and zeros in a 126 standard way, while the tilt response is defined as I rot = g I trans (iω) -2 , where 127 g is gravitational acceleration. We assume no tilt response on the vertical components. Although 128 it is not specified in equation (1), the G matrices naturally depend on the source position as well. 129u n seis ! r,ω ( ) = I n rot ω ( ) G np,q rot ! r,ω ( ) + I n trans ω ( ) G np,q trans ! r,ω ( ) ! \" # $ M pq ω ( ) ,(1)In practice, we do not invert directly for the source position, instead searching over a volume of 130 possible source locations and determining the source position from the inversion results. 131Equation 1 can be recast in matrix form as 132u ω ( ) = G ω ( ) s ω ( ) ,(2) 133where u is the R x 1 vector of Fourier-transformed ground displacement components, s is the 6 x 134 number of observed seismic traces. The matrix G is R x 6, composed from Fourier transform of 136 the terms inside the square brackets in equation 1. The equation is solved in a least-squares 137 inversion one frequency at a time, as described in Waite et al. [2008] . Initially, the inversion is 138 unconstrained, and the model moment tensor source time function is constructed from the 139 inverse Fourier transform of the inversion results. Although some studies have found that single 140 forces can be important in VLP source processes, especially when related to a reaction to vertical 141 mass ejection, previous work at Fuego [Lyons and Waite, 2011] showed they were not 142 significant. Single forces are unlikely to contribute to even lower frequency source models, 143 which are dominated by pre-explosion signal, so they were not considered in our study. 144',\n",
       "    'cite_spans': [{'start': 320,\n",
       "      'end': 326,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 376,\n",
       "      'end': 401,\n",
       "      'text': '[Chouet and Dawson, 2013]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF1'},\n",
       "     {'start': 2074,\n",
       "      'end': 2080,\n",
       "      'text': '[2008]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2449,\n",
       "      'end': 2472,\n",
       "      'text': '[Lyons and Waite, 2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': \"The translation and rotation Green's functions were calculated using a finite-difference 146 method [Ohminato and Chouet, 1997 ] with a homogeneous model that includes the three-147 dimensional topography of Fuego volcano. Given the long wavelengths of the VLP signals, the 148 homogeneous structure is appropriate [see, e.g., Waite et al., 2008] . We used a compressional-149 wave velocity of 3.5 km/s, shear-wave velocity of 2 km/s, and density of 2650 kg/m 3 . In practice, 150 the error in the velocity model introduces little effect on the VLP inversion [Waite et al., 2008] . 151The model is centered on the summit of Fuego and extends 11.72 km east-west, 8.96 km north-152 south, and 6 km vertically with a 40 m grid spacing. Green's functions for synthetic sources over 153 a volume 680 m east-west, 480 m north-south, and 920 m vertically from the summit down. This 154 volume is centered west of the summit, because the VLP source found by Lyons and Waite 155[2011] was ~300 m west of the summit vent. In order to speed the calculations, we skip 156 alternating grid nodes, so the synthetic source volume has a spacing of 80 m. All other details 157 about the finite-difference modeling are as described in Lyons and Waite [2011] . The rotation 158Green's functions are computed from the curl of the displacement field (two times the tilt) during 159 the finite-difference simulations. 160\",\n",
       "    'cite_spans': [{'start': 100,\n",
       "      'end': 126,\n",
       "      'text': '[Ohminato and Chouet, 1997',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF18'},\n",
       "     {'start': 327,\n",
       "      'end': 346,\n",
       "      'text': 'Waite et al., 2008]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 559,\n",
       "      'end': 579,\n",
       "      'text': '[Waite et al., 2008]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF26'},\n",
       "     {'start': 1233,\n",
       "      'end': 1239,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The unconstrained inversion at hundreds of points inside a volume within the summit of 162Fuego volcano provides a spatial estimate of the location and uncertainty on the location (see 163 section below). Interpretation of the moment tensor source time function can be done by point-164by-point eigenvector decomposition, which provides the orientation and mechanism type in caseswhere the moment components are in phase throughout the source time function. We expand on 166 this approach to explore the uncertainty in the moment tensor type, following the nonlinear 167 inversion approach described below. 168Our approach involves a grid search over a total of five parameters for all possible 169 moment tensor types and orientations. For each model, we constrain the moment tensor using the 170 method of Lagrange multipliers and compute the data fit. First, we search over possible moment 171 tensor types using the fundamental lune source-type definition of Tape and Tape [2012] , which 172 involves two parameters that describe the ratios of the moment tensor eigenvalues (Figure 1) . 173The latitude parameter, δ, ranges from -90º to 90º and the longitude parameter, γ, ranges from -174 30º to 30º. Some example source types, the associated γ and δ, along with relative eigenvalues 175 (λ) are given in The search over the parameters γ and δ uses the surface spline method described by Tape  183 and Tape [2012] to evenly sample the moment tensor source type space. This method results in a 184 substantial computational savings over an evenly sampled grid [Wang and Dahlen, 1995] . We 185 evaluate γ from -30º to 30º, but because the lower half of the lune is simply the opposite sign of 186 the upper half (e.g., volume decrease versus volume increase), we evaluate δ from 0 to 90º. For each of the 1,300,536 trial moment tensor solutions, the six independent moment 195 tensor components of the trial tensor are used to constrain the inversion through a system of 196 equations, that fix the ratio of the moment tensor components [Menke, 1989] . For a given trial 197 tensor, H is a matrix that contains the ratios of matrix components to arbitrarily selected 198component M 11 (n) : 199 H = 1 − M 11 (n) M 22 (n) 0 0 0 0 1 0 − M 11 (n) M 33 (n) 0 0 0 1 0 0 − M 11 (n) M 12 (n) 0 0 1 0 0 0 − M 11 (n) M 23 (n) 0 1 0 0 0 0 − M 11 (n) M 13 (n) \" # $ $ $ $ $ $ $ $ % & \\' \\' \\' \\' \\' \\' \\' \\' 200where the notation M 11 (n) indicates the nth trial moment tensor. The constraint equations are 201 solved simultaneously in the inversion using least squares: 202G T G H T H Z ! \" # # $ % & & s l ! \" # $ % & = G T u h ! \" # # $ % & & 203where Z is a 5 x 5 matrix of zeros, h is a 5 x 1 vector of zeros, and s and u are the model and 204 data vectors as in equation 2. The vector l consists of the Lagrange multipliers. 205Given the large number of possible solutions, we perform the full grid search for only the 206 best-fit locations of each frequency band. Given that the source time function does not vary 207 rapidly spatially [Lyons and Waite, 2011] , this is a reasonable approach. 208',\n",
       "    'cite_spans': [{'start': 977,\n",
       "      'end': 983,\n",
       "      'text': '[2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1412,\n",
       "      'end': 1418,\n",
       "      'text': '[2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1564,\n",
       "      'end': 1587,\n",
       "      'text': '[Wang and Dahlen, 1995]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF28'},\n",
       "     {'start': 2040,\n",
       "      'end': 2053,\n",
       "      'text': '[Menke, 1989]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF17'},\n",
       "     {'start': 3028,\n",
       "      'end': 3051,\n",
       "      'text': '[Lyons and Waite, 2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [{'start': 1078,\n",
       "      'end': 1088,\n",
       "      'text': '(Figure 1)',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1393,\n",
       "      'end': 1402,\n",
       "      'text': 'Tape  183',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': \"The nonlinear approaches to finding the best centroid location and moment tensor lend 210 themselves to quantitative error analysis. Misfit information is used to define uncertainty on thesource centroid, and in this work we extend this to examine uncertainty in the moment tensor 212 type. A similar approach was used by Ford et al. [2010] to examine non-double-couple 213 components of explosion and earthquake sources. In many applications of moment tensor 214 inversion at volcanoes, a weighted squared error is used, which is either normalized by station or 215 channel [Chouet et al., 2003; Ohminato et al., 1998 ]. As our approach follows this methodology, 216we adopt the squared error measure described as E 2 : 217E 2 = 1 N r u n 0 pΔt ( ) − u n s pΔt ( ) ( ) 2 p=1 N s ∑ 1 3 ∑ u n 0 pΔt ( ) ( ) 2 p=1 N s ∑ 1 3 ∑ $ % & & & & & ' ( ) ) ) ) ) n=1 N r ∑ (3) 218where is the pth sample of the nth data trace, is the pth sample of the nth 219 The threat of vandalism or theft prohibited deployment of solar panels at some sites, so 231 continuous operation of all 10 stations was restricted to 19 -21 January. Two of the stations are 232 very close to F9A, F9B, and F9C, and were not used in the study. Station F9SE, at 6 km from the 233 summit, did not record the VLP signals. In total, seven stations (21 channels) were used in the 234 inversions. More complete details of the experiment are given in Lyons and Waite [2011] . 235€ u n 0 pΔt ( ) € u n s pΔt ( )\",\n",
       "    'cite_spans': [{'start': 334,\n",
       "      'end': 340,\n",
       "      'text': '[2010]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 575,\n",
       "      'end': 596,\n",
       "      'text': '[Chouet et al., 2003;',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 597,\n",
       "      'end': 618,\n",
       "      'text': 'Ohminato et al., 1998',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF19'},\n",
       "     {'start': 1425,\n",
       "      'end': 1431,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'During this deployment, all of the strong explosions issued from the summit vent and 237 produced clear VLP signals at the nearest stations. The closest station, which also happened to be 238 the most reliable, F900, recorded hundreds of similar explosions during the 19-day deployment. 239Lyons and Waite [2011] selected one of these events (19 January 2009 at 16:09:30) that was well 240 recorded on the network for their inversion. In order to improve the signal-to-noise ratio of the 241 data on some of the more distant channels, especially at periods below 30 seconds, we computed 242 phase-weighted stacks of similar events in multiple frequency bands for inversion. 243The 19 January event was used as a master event and compared against all the data at 244 station F900. With this procedure, we identified 209 events for which the combined three-245 channel cross-correlation coefficient was 2.0 or greater (roughly one event every 2 hours). Of 246 these, only 21 events were simultaneously recorded on all the stations from 19-21 January. The 247 waveform similarity generally degraded with increased distance from the source. While the 248 waveforms were generally consistent from event to event on a given channel in the 30-10 second 249 band, and produced a clean stack, the weaker signals and possibly greater noise at lower 250 frequencies led to poor correlations on many channels below 30 seconds. As an objective of this 251 study was to investigate frequencies below the instrument corner, we sought to use only events 252 that were similar. We visually inspected all the data channel-by-channel, and removed all data 253 from events where one or more channels was extremely noisy or had inverted polarity in the 400 254 to 60 or 60 to 10 seconds band. This left just six events to stack for the inversion. 255We followed the phase-weighted stacking approach of Schimmel and Paulssen [1997] 256 and found that it produced a vastly improved signal to noise ratio (Figure 3 ). This is particularly 257 important for the lower frequencies, which contain substantially more noise on a single trace. 258Even with just six events in the stack, the phase-weighted stacking procedure resulted in clean 259 waveforms at nearly every channel and every station. For comparison, we show the linear stack 260 along with the individual waveforms from all the events for the vertical component at three 261 stations in two non-overlapping frequency bands. In general, the phase-weighted stack is slightly 262 lower in amplitude but has much less pre-or post-event noise. In particular, single-event noise 263 that contributes substantially to the linear stack is absent in the phase-weighted stack. 264The gray bars in Figure 3 into one value using the square root of the sum of the squared standard deviations. Since Lyons 308and Waite [2011] found the 30-10 second period VLP source to be dominated by a dipping 309 crack, we used moment tensor eigenvalue ratios of 2:1:1. We also examined g for ratios of 3:1:1 310 and 1:1:1 and, although the values for g varied, we found no difference in which of the solutions 311 had the lowest g. Because it implies the most consistency throughout the source time function, 312 the solution within 5% of the minimum E 2 error that had the lowest g was chosen as the best 313 solution. 314',\n",
       "    'cite_spans': [{'start': 306,\n",
       "      'end': 312,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1903,\n",
       "      'end': 1909,\n",
       "      'text': '[1997]',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2835,\n",
       "      'end': 2847,\n",
       "      'text': 'Waite [2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF9'}],\n",
       "    'ref_spans': [{'start': 1981,\n",
       "      'end': 1990,\n",
       "      'text': '(Figure 3',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF3'},\n",
       "     {'start': 2723,\n",
       "      'end': 2731,\n",
       "      'text': 'Figure 3',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF3'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'The best-fit centroid locations do not vary significantly with bandpass. The minimum E 2 316 solutions (E 2min ) are identical for bands from 60-10 s through 400-10 s, all very near the surface 317 and just west of the summit (see Figure 5 and Table 2 ). When the minimum g is also considered, 318 the best solutions are deeper, and the 60-10 s and 90-10 s solutions are 320 m east of the summit. 319The 120-10 s and 400-10 s solutions are about 80 m south of the E 2min solutions. The lowest 320 frequency (400-60 s) g min solution is at a shallower depth and location nearer the summit than the 321 E 2min solution. The best 30-10 s solutions are west of the summit when both E 2min and g min are 322considered; it only migrates south to north. The E 2min solution is within 200 m of the best fit 323 found by Lyons and Waite [2011] when inverting in the same 30-10 s band, although we use a 324 stack of 6 events that allows us to invert data from an additional station. We do not attempt tointerpret the fine details of the centroid locations, given the limitations in the station coverage, 326 but consider the locations to be geologically reasonable. 327Given the similarities in models for some of the intermediate bandpasses, we focus on 328 four representative bands: 30-10 s; 90-10 s; 120-10 s; and 400-60 s. The E 2min and g min centroid 329locations are shown in Figure 5 . In Table 2 , the E 2min and g min solutions for each bandpass are 330 shown, with the E 2min in the first row and g min in the second row. 331 Figure 6 shows data (black) and synthetic waveforms for each of the four bandpasses. 332The free inversion synthetics, shown in red, generally fit the data quite well. The error measure 333 we use weights stations, not channels, equally. The advantage to this is that stations that are 334 farther away or have lower amplitude because of the radiation pattern have equal importance. To 335give a sense of the fits on individual channels, we computed cross-correlation coefficients and 336 lag times between the data and free inversion synthetics. At F900, for example, the fits at all 337 channels in the 30-10 s bandpass are above 0.97 at lags of between -0.04 and 0.28 s. In contrast, 338 the north channel at F9A, which clearly fits less well, has a correlation coefficient of 0.83 at 0.02 339 s lag. The worst-fitting channel, the east channel of F9NW, has a correlation of 0.53 at 6.5 s lag. 340More than half of the correlation coefficients are above 0.9 in the 90-10 s band, and none of the 341 lags are more than 1 s. In each of the remaining two bands, at least 5 channels correlate above 342 0.9. The range of lag times increases slightly in the longer period bands but, even in the 400-60 s 343 band, well over half of the lags are below 5 sec. 344The synthetics from the fixed tensor solutions, described below, have slightly greater 345 misfits, as expected for models with fewer free parameters. The synthetics for the 90-10 s band 346 are notably much poorer for the fixed inversion. The correlation coefficients and lag times 347 between the data and both the free inversions and the fixed tensor inversions for each of the 348 channels are given in supplementary tables S1 and S2, respectively. 349The source time functions for these same four representative bandpasses are shown in 350 for 17 channels, compared with 19 channels used in this study, which led to much larger misfits. 359For the remaining rows, the first row associated with each band represents details of the E 2min 360 solution and the second row shows details of the g min solution. 361 362',\n",
       "    'cite_spans': [{'start': 828,\n",
       "      'end': 834,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [{'start': 231,\n",
       "      'end': 239,\n",
       "      'text': 'Figure 5',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 244,\n",
       "      'end': 251,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1375,\n",
       "      'end': 1383,\n",
       "      'text': 'Figure 5',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1389,\n",
       "      'end': 1396,\n",
       "      'text': 'Table 2',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1529,\n",
       "      'end': 1537,\n",
       "      'text': 'Figure 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'For each of the representative bandpasses, the centroid location with the lowest g that 364 was also within 5% of E 2min was used to investigate the constraint on the source type. The 365 nonlinear analysis demonstrates that the range of models with similar misfits is fairly broad. 366 Figure 8 shows the misfits at each moment tensor type for the lowest misfit tensor orientation. 367The point-by-point source time function eigenvalue analysis for the best free inversion is shown 368 using red dots. These correspond to the γ and δ at each point in the source time function when 369 the amplitude was within 50% of the maximum. Points with negative δ were projected to the 370 upper portion of the plot. The misfits for the constrained inversion results are much larger than 371 those of the free inversions, because the moment tensor is fixed for the whole source time 372function. This means there is essentially one model parameter in the constrained inversions,compared to six independent model parameters for the free inversions. Each point in this lune 374 plot represents the lowest misfit value for all moment-tensor orientations with the γ -δ pair. 375Considering, first, the solution in the 30-10 s band, the minimum misfit in the constrained 376 inversions is 0.43, compared to 0.32 for the g min solution. The point-by-point γ−δ pairs are tightly 377 constrained, and the median ratio of eigenvalues is (1.0, 0.28, 0.12). But the nonlinear inversion 378for source type suggests a wider range of possible mechanism types fit the data. In Figure 8a , the 379 0.46 misfit contour encompasses the points in the free inversion and is about 5% above the 380 minimum, and so might be considered a reasonable level to explore the range of possible models. 381This includes sources from a tension crack to a DC. 382One interpretation of this is that the source represents a composite of two or more sources 383 that have different orientations. In fact, Lyons and Waite [2011] found that a combination of a 384 single dipping crack and nearly vertical pipe or crack provided the best-fitting model for the 30-385 10 s VLP. The two sources were out of phase and interpreted to represent the opposite sense of 386 volume change. That is, when the vertical dike or pipe was deflating, the dipping sill was 387 inflating. We computed the γ -δ pair for the combined crack-pipe tensor of Lyons and Waite 388 indicates multiple source types can fit the data well. 393As is clear from Figure 8b , the 90-10 s bandpass data do not constrain the source type. 394Clearly, even the g min solution is much less stable than the 30-10 s solution, and this is born out 395 in the nonlinear inversion for source type. Very little can be interpreted from this solution, but 396 note that the median eigenvalue ratios include a negative minimum and the point-by-point γ -397 δ pairs are all below the region in which all the components have the same sign. The poorly 398 constrained solution is probably mainly due to the complexity of the source. On some stations, 399 especially in the horizontal components, the spectra show two peaks between 90 and 10 s period, 400 which we might infer are tilt dominated, and translation dominated. These may reflect two 401 sources, distinct in space and with peak moment release occurring at distinct times. 402At the wider 120-10 s bandpass, the lower frequencies tend to dominate and the nonlinear 403 inversion for source type is somewhat better constrained. The free inversion is better constrainedthan the 90-10 s band, but the point-by-point γ -δ pairs still range widely. In this case, however, 405 the points are all in the region of a volumetric source. The median ratio of eigenvalues is (1.0, 406 0.73, 0.30) the nonlinear inversion (Figure 8c ) points to a source somewhat like a pipe. 407The final example we present is the 400-60 s band, below the corners of any of the 408 sensors. This inversion is much better constrained in both the free inversion and constrained 409 nonlinear inversion. The γ -δ pairs from the free g min inversion cluster near the point defined for 410 a pipe (1.5, 1.5, 1) and the median eigenvalues are (1.0, 0.78, 0.21). The range of misfit values is 411 much larger than in the previous inversions, and the region surrounding 5% above the minimum 412 occupies a relatively small area. In this case, the source type is well constrained to a volumetric 413 source, and eigenvalues suggest a mechanism like a pipe. Based on the eigenvectors a pipe 414would be dipping about 30º to the NNE, which is reasonable given the centroid location. 415',\n",
       "    'cite_spans': [{'start': 1976,\n",
       "      'end': 1982,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [{'start': 287,\n",
       "      'end': 295,\n",
       "      'text': 'Figure 8',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 1552,\n",
       "      'end': 1561,\n",
       "      'text': 'Figure 8a',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 2483,\n",
       "      'end': 2492,\n",
       "      'text': 'Figure 8b',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 3772,\n",
       "      'end': 3782,\n",
       "      'text': '(Figure 8c',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'These results do not necessarily simplify the interpretation of these events, but the 417 nonlinear inversion for source type provides a better means of assessing the reliability of the 418 models. In some cases, like the 400-60 s band result, the model is tightly constrained and can be 419 interpreted in terms of a portion of the conduit just below the vent that inflates and deflates 420 during each explosion. In other cases, such as the 30-10 s band, the nonlinear inversion results 421 suggest that a range of solutions from double-couple to a tension crack can explain the data 422 equally well. This can also be seen as evidence for a complex source that involves two or more 423 sources acting more-or-less coincidentally, but with opposite phase. Finally, the 90-10 s band is 424 an example that cannot be constrained to a model. In this case, we suggest that it is because it 425 includes both the crack-like source of the 30-10 s band west of the summit and the pipe-like 426 source of the 400-60 s band closer to the summit. 427Our interpretation is consistent with earlier work at Fuego, and we refer to Lyons and from four broadband stations to the north of the summit. In their study, they found that tilt 433 signals could be described by a spherical source just below the summit. While our inversionssuggest a source somewhat more like a dipping pipe than a pure sphere, the location and sense of 435 volume change are consistent. 436The tilt-dominated signal begins with inflation several minutes prior to the explosion 437 (Figure 7d ). Although we limited the band to 400 seconds, a much longer lead-up time can be 438 seen in the raw data when integrated [Lyons et al., 2012] . This is interpreted as pressurization 439 following sealing of the conduit in the summit area. The model for the 30-10 s VLP proposed by 440Lyons and Waite [2011] has pre-explosion inflation, mainly in a dipping sill, followed by 441 deflation of that portion of the conduit during the eruption. Our single model moment tensor 442 inversions are consistent with this and we favor this interpretation. 443',\n",
       "    'cite_spans': [{'start': 1678,\n",
       "      'end': 1698,\n",
       "      'text': '[Lyons et al., 2012]',\n",
       "      'latex': None,\n",
       "      'ref_id': 'BIBREF10'},\n",
       "     {'start': 1857,\n",
       "      'end': 1863,\n",
       "      'text': '[2011]',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'ref_spans': [{'start': 1544,\n",
       "      'end': 1554,\n",
       "      'text': '(Figure 7d',\n",
       "      'latex': None,\n",
       "      'ref_id': 'FIGREF4'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'Volcanic seismic events offer great complexity given the wide range of frequencies, 445 influence of near-field effects including ground rotation, and wide range of source types. We 446 adapt a moment-tensor inversion methodology for including tilt-affected seismograms so that we 447 can examine explosion signals over a range from 400-10 s period, well below the corner periods 448 of the instruments used. These data clearly include translation below the instrument corner, as 449 seen in the vertical components, but likely also include some tilt signal near the instrument 450 corner frequency. We show how a nonlinear inversion for source type can aid in interpretation of 451 the mechanism. In some of the intermediate ranges, the contributions from the longer period, 452 presumably tilt-affected signals and those of the translation dominated signals, mean that a single 453 moment-tensor source cannot fit the data. In other pass bands where either the translation or tilt-454 affected data dominate, the moment-tensor source types are better constrained. 455The broadening of the inversion to periods greater than 6 minutes allows for a more 456 complete description of the source. In this case, we find a result that is largely consistent with the 457 forward modeling done in prior work, but offers more detail about the mechanism type. Our 591 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616 ',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 1354,\n",
       "      'end': 1476,\n",
       "      'text': '591 593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616',\n",
       "      'latex': None,\n",
       "      'ref_id': 'TABREF0'}],\n",
       "    'eq_spans': [],\n",
       "    'section': None},\n",
       "   {'text': 'These supplementary tables include the scaled correlation coefficients and associated lag times for each data channel with the corresponding model synthetics. These tables can be compared with the waveform plots in Figure 6 . Although this information is not used in the inversion, it provides additional information about the model fit that may be compared to other waveform similarity studies. Note that the correlations are scaled so that absolute amplitude information is not preserved. Table S1 . The correlations between each data channel and synthetic waveform from the freeinversion models, and corresponding lag times, for each channel used. The maximum allowable lag time of +/-9.98 seconds was reached in some cases. These statistics were not computed for two channels that were not used in the inversions. Table S2 . The correlations between each data channel and synthetic waveform from the fixed-inversion models, and corresponding lag times, for each channel used. The maximum allowable lag time of +/-9.98 seconds was reached in some cases. These statistics were not computed for two channels that were not used in the inversions.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [{'start': 215,\n",
       "      'end': 223,\n",
       "      'text': 'Figure 6',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 491,\n",
       "      'end': 499,\n",
       "      'text': 'Table S1',\n",
       "      'latex': None,\n",
       "      'ref_id': None},\n",
       "     {'start': 818,\n",
       "      'end': 826,\n",
       "      'text': 'Table S2',\n",
       "      'latex': None,\n",
       "      'ref_id': None}],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {'FIGREF0': {'text': \") is the Fourier transform of the n component of the seismogram without 120 instrument correction, p and q are direction indices x, y, and z, M pq (ω) is the Fourier transform 121 of the time history of the pq-component of the moment tensor, I rot and I trans represent tilt and 122 translation instrument response functions, the matrices G rot and G trans are Green's functions that 123 relate the n-component of tilt or translation at the receiver position, € ! r , with the moment at the 124 source position, and ω is angular frequency. The notation p,q indicates spatial differentiation 125\",\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF1': {'text': \"To explore the full moment tensor space requires modeling the range of possible 188 orientations. We rotate the moment tensor at 10º intervals using a sequence of three rotations 189 about the initial coordinate system of the moment tensor. Full sampling of the symmetric tensor 190 involves a 360º range about the z axis, 180º of dip, and 90º of rotation about the new, rotated z' 191 axis [Goldstein et al., 2001]. Tests with finer intervals showed little difference in the pattern of 192 misfits on the lune and minor variation in the misfit values. This involves 5832 combinations of 193 rotation angles combined with 223 γ -δ pairs. 194\",\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF2': {'text': 'synthetic trace, N s is the number of samples in each trace, and N r is the number of three- 220 component receivers. Here the squared error is normalized by station, so that stations with 221 varying amplitude contribute equally to the error.data were recorded on a network of 10 three-component broadband 225 seismometers from 8-26 January 2009, 8 of which are shown in Figure 2. The network 226 configuration was limited by the steep topography, deep ravines, thick jungle, and safety 227 considerations due to the eruptive activity. Six of the sites were equipped with Güralp CMG 40T 228 sensors (0.02-30 s) and four sites featured Güralp 3 ESPC sensors (0.02-60 s). Data were 229 recorded on 10 Reftek 130 digitizers operating in continuous mode at 100 samples per second. 230',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF3': {'text': 'Figure 3 also highlights the difference between the vertical components in the low-270 frequency band. We expect no tilt on the vertical components, yet the events are evident in the 271 400-60 s band, especially on the 60 s stations (F9NW, F9B, F9SW, F9NE). This suggests that 272 the events produce translational motion at frequencies below the instrument corner. Figure 4, 273 which shows the spectra for each of the bands analyzed at three stations, further highlights the 274 vertical component signal well below the instrument corner. The spectra also demonstrate the 275 large amplitude of the horizontal components at the lowest frequencies. Given the known 276 influence of tilt on the horizontal components at those low frequencies, a joint inversion 277 approach is required to investigate frequencies below the corner of the sensors. Only station 278 F9SE, which was ~6 km from the vent, did not record signal in the VLP band, so it was omitted 279 from further analysis. 280',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF4': {'text': 'Figure 7. They share some common features, with the dipole components in phase, and apart 351 from the 90-10 s band, dominating the source time function. There are differences in the 352 eigenvectors and eigenvalues, although the major difference is between the 90-10 s band and the 353 others. The median ratios of minimum and intermediate eigenvalues to the largest are shown in 354',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF5': {'text': ', which is dominated by the crack, and plotted it in Figure 8a. This combined mechanism 389 is actually closer to a single crack than the free inversion solution, with γ =-22 and δ=58. The 390 two-crack model of Lyons and Waite [2011] is in a similar location (γ =-20 and δ=63). The 391 relatively small range of E 2min values, compared to Figure 8c and d, throughout the lune space 392',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF6': {'text': '[2011] for detailed discussion of the source dynamics of the VLP events. Apart from the 429 quantitative description of the source-type uncertainty, the key new results involve the formal 430 moment-tensor solutions for periods below 30 s. The results of inversion for tilt-affected and tilt- 431 dominated signals are consistent with forward modeling by Lyons et al. [2012] of tilt derived 432',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF7': {'text': 'Figure 2. Locations of stations used in this study (circles) plotted on a shaded relief map of 545 Fuego volcano (a). Contour interval is 200 m. The red dots represent stations with 30 s sensors; 546 the blue dots represent 60 s sensors. The orange star marks the approximate location of the 547 summit vent. Part (b) shows the location of Fuego (arrow) in the chain of active Guatemalan 548 volcanoes. Station F9SE was not used in the inversions because the VLP signal was not observed 549 at that distance from the source. 550 551',\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'FIGREF8': {'text': \"Locations of best-fit source centroids cluster near the summit. Both E 2min and g min 619 solutions are plotted by bandpass. Uncertainties on centroid locations, not shown for simplicity, 620 are 200-350 m for solutions within 5% of the minimum. Contour interval is 200 m; the 621 topography model plotted in map view and in the cross sections is the same as that used to 622 compute the Green's functions. Seismic stations are shown with dots, with red dots representing 623 stations with 30 s sensors; blue dots represent 60 s sensors as in Figure 2. The hexagon shows the 624 best-fit location from Lyons and Waite [2011]. 625 626\",\n",
       "    'latex': None,\n",
       "    'type': 'figure'},\n",
       "   'TABREF0': {'text': 'and in Figure 1. We refer readers to Tape and Tape [2012] for a 176 thorough description of the lune parameterization. 177Relationship between moment tensor eigenvalues, lune latitude and lune longitude 178Examples for two different Poisson ratios are shown for the crack and pipe models. 181',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF1': {'text': 'The source types are described in the next section. 355Best solution information as a function of bandpass 357Lyons and Waite [2011] is shown for comparison. In that study, error was computed 358',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF2': {'text': 'band channel corr. coef. lag [s] channel corr. coef. lag [s] channel corr. coef. lag [s]',\n",
       "    'latex': None,\n",
       "    'type': 'table'},\n",
       "   'TABREF3': {'text': 'band channel corr. coef. lag [s] channel corr. coef. lag [s] channel corr. coef. lag [s]',\n",
       "    'latex': None,\n",
       "    'type': 'table'}},\n",
       "  'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "    'title': 'Tilt change recorded by broadband seismometer prior to 476 small phreatic explosion of Meakan-dake volcano',\n",
       "    'authors': [{'first': 'H', 'middle': [], 'last': 'Aoyama', 'suffix': ''},\n",
       "     {'first': 'H', 'middle': [], 'last': 'Oshima', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'Geophys. Res. Lett',\n",
       "    'volume': '477',\n",
       "    'issn': '6',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF1': {'ref_id': 'b1',\n",
       "    'title': 'Very long period conduit oscillations induced by rockfalls at 479',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF4': {'ref_id': 'b4',\n",
       "    'title': 'Source mechanisms of explosions at Stromboli Volcano',\n",
       "    'authors': [{'first': 'G', 'middle': [], 'last': 'Milana', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': [], 'last': 'Scarpa', 'suffix': ''}],\n",
       "    'year': 2003,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF5': {'ref_id': 'b5',\n",
       "    'title': 'determined from moment-tensor inversions of very-long-period data',\n",
       "    'authors': [{'first': '', 'middle': [], 'last': 'Italy', 'suffix': ''}],\n",
       "    'year': None,\n",
       "    'venue': 'J. Geophys. 483 Res',\n",
       "    'volume': '108',\n",
       "    'issn': 'B1',\n",
       "    'pages': '',\n",
       "    'other_ids': {'doi': ['10.1029/2002JB001919']},\n",
       "    'links': None},\n",
       "   'BIBREF6': {'ref_id': 'b6',\n",
       "    'title': 'A multi-decadal view of seismic methods for detecting 485 precursors of magma movement and eruption',\n",
       "    'authors': [{'first': 'B', 'middle': [], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'R', 'middle': ['S'], 'last': 'Matoza', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'J. Volcanol. Geotherm. Res',\n",
       "    'volume': '252',\n",
       "    'issn': '',\n",
       "    'pages': '108--175',\n",
       "    'other_ids': {'doi': ['10.1016/j.jvolgeores.2012.11.013']},\n",
       "    'links': '129636416'},\n",
       "   'BIBREF7': {'ref_id': 'b7',\n",
       "    'title': 'Network Sensitivity Solutions for Regional 488 Moment-Tensor Inversions',\n",
       "    'authors': [{'first': 'S', 'middle': ['R'], 'last': 'Ford', 'suffix': ''},\n",
       "     {'first': 'D', 'middle': ['S'], 'last': 'Dreger', 'suffix': ''},\n",
       "     {'first': 'W', 'middle': ['R'], 'last': 'Walter', 'suffix': ''}],\n",
       "    'year': 1962,\n",
       "    'venue': 'Bull. Seism. Soc. Am',\n",
       "    'volume': '100',\n",
       "    'issn': '5A',\n",
       "    'pages': '',\n",
       "    'other_ids': {'doi': ['10.1785/0120090140']},\n",
       "    'links': '73647128'},\n",
       "   'BIBREF8': {'ref_id': 'b8',\n",
       "    'title': 'Classical Mechanics',\n",
       "    'authors': [{'first': 'H',\n",
       "      'middle': [],\n",
       "      'last': 'Goldstein',\n",
       "      'suffix': ''},\n",
       "     {'first': 'C', 'middle': ['P J'], 'last': 'Poole', 'suffix': ''},\n",
       "     {'first': 'L', 'middle': ['L'], 'last': 'Safko', 'suffix': ''}],\n",
       "    'year': 2001,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF9': {'ref_id': 'b9',\n",
       "    'title': 'Dynamics of explosive volcanism at Fuego volcano imaged 492 with very long period seismicity',\n",
       "    'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "     {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '116',\n",
       "    'issn': 'B9',\n",
       "    'pages': '',\n",
       "    'other_ids': {'doi': ['10.1029/2011jb008521']},\n",
       "    'links': '129058472'},\n",
       "   'BIBREF10': {'ref_id': 'b10',\n",
       "    'title': 'Tilt prior to explosions and the 494 effect of topography on ultra-long-period seismic records at Fuego volcano',\n",
       "    'authors': [{'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "     {'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Ichihara', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['M'], 'last': 'Lees', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF12': {'ref_id': 'b12',\n",
       "    'title': 'Very-long-period pulses at Asama volcano, central Japan, 497 inferred from dense seismic observations',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '185',\n",
       "    'issn': '1',\n",
       "    'pages': '265--282',\n",
       "    'other_ids': {'doi': ['10.1111/j.1365-246X.2011.04938.x']},\n",
       "    'links': '131264104'},\n",
       "   'BIBREF13': {'ref_id': 'b13',\n",
       "    'title': 'A waveform inversion including tilt: method and 500 simple tests',\n",
       "    'authors': [{'first': 'Y', 'middle': [], 'last': 'Maeda', 'suffix': ''},\n",
       "     {'first': 'M', 'middle': [], 'last': 'Takeo', 'suffix': ''},\n",
       "     {'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''}],\n",
       "    'year': 2011,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '184',\n",
       "    'issn': '2',\n",
       "    'pages': '907--918',\n",
       "    'other_ids': {'doi': ['10.1111/j.1365-246X.2010.04892.x']},\n",
       "    'links': None},\n",
       "   'BIBREF15': {'ref_id': 'b15',\n",
       "    'title': 'Source mechanism of small long-period events at 503',\n",
       "    'authors': [{'first': 'T', 'middle': ['D'], 'last': 'Moran', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Mikesell', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF16': {'ref_id': 'b16',\n",
       "    'title': 'Helens in July 2005 using template matching, phase-weighted stacking, and 504 full-waveform inversion',\n",
       "    'authors': [{'first': 'Mount', 'middle': [], 'last': 'St', 'suffix': ''}],\n",
       "    'year': None,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '120',\n",
       "    'issn': '9',\n",
       "    'pages': '6351--6364',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF17': {'ref_id': 'b17',\n",
       "    'title': 'Geophysical Data Analysis: Discrete Inverse Theory',\n",
       "    'authors': [{'first': 'W', 'middle': [], 'last': 'Menke', 'suffix': ''}],\n",
       "    'year': 1989,\n",
       "    'venue': '',\n",
       "    'volume': '507',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '117957106'},\n",
       "   'BIBREF18': {'ref_id': 'b18',\n",
       "    'title': 'A free-surface boundary condition for including 3D 509 topography in the finite-difference method',\n",
       "    'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''}],\n",
       "    'year': 1997,\n",
       "    'venue': 'Bull. Seism. Soc. Am',\n",
       "    'volume': '87',\n",
       "    'issn': '2',\n",
       "    'pages': '494--515',\n",
       "    'other_ids': {},\n",
       "    'links': '129694378'},\n",
       "   'BIBREF19': {'ref_id': 'b19',\n",
       "    'title': 'Waveform inversion of very long 511 period impulsive signals associated with magmatic injection beneath Kilauea Volcano',\n",
       "    'authors': [{'first': 'T', 'middle': [], 'last': 'Ohminato', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': [], 'last': 'Dawson', 'suffix': ''},\n",
       "     {'first': 'S', 'middle': [], 'last': 'Kedar', 'suffix': ''}],\n",
       "    'year': 1998,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF21': {'ref_id': 'b21',\n",
       "    'title': 'The response of the horizontal pendulum seismometer to Rayleigh and 514 Love waves, tilt, and free oscillations of the Earth',\n",
       "    'authors': [{'first': 'P',\n",
       "      'middle': ['W'],\n",
       "      'last': 'Rogers',\n",
       "      'suffix': ''}],\n",
       "    'year': 1968,\n",
       "    'venue': 'Bull. Seism. Soc. Am',\n",
       "    'volume': '58',\n",
       "    'issn': '5',\n",
       "    'pages': '1384--515',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF22': {'ref_id': 'b22',\n",
       "    'title': 'Noise reduction and detection of weak, coherent signals 517 through phase-weighted stacks',\n",
       "    'authors': [{'first': 'M', 'middle': [], 'last': 'Schimmel', 'suffix': ''},\n",
       "     {'first': 'H', 'middle': [], 'last': 'Paulssen', 'suffix': ''}],\n",
       "    'year': 1997,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '130',\n",
       "    'issn': '',\n",
       "    'pages': '497--505',\n",
       "    'other_ids': {},\n",
       "    'links': '41492712'},\n",
       "   'BIBREF23': {'ref_id': 'b23',\n",
       "    'title': 'A geometric setting for moment tensors',\n",
       "    'authors': [{'first': 'W', 'middle': [], 'last': 'Tape', 'suffix': ''},\n",
       "     {'first': 'C', 'middle': [], 'last': 'Tape', 'suffix': ''}],\n",
       "    'year': 2012,\n",
       "    'venue': 'Geophys. J. Int',\n",
       "    'volume': '190',\n",
       "    'issn': '',\n",
       "    'pages': '476--498',\n",
       "    'other_ids': {'doi': ['10.1111/j.1365-246X.2012.05491.x']},\n",
       "    'links': '122638175'},\n",
       "   'BIBREF24': {'ref_id': 'b24',\n",
       "    'title': 'Very-Long-Period Seismicity at Active Volcanoes: Source Mechanisms',\n",
       "    'authors': [{'first': 'G',\n",
       "      'middle': ['P'],\n",
       "      'last': 'Waite',\n",
       "      'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': '522 Encyclopedia of Earthquake Engineering',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '135418931'},\n",
       "   'BIBREF26': {'ref_id': 'b26',\n",
       "    'title': 'Eruption dynamics at Mount St. Helens 526 imaged from broadband seismic waveforms: Interaction of the shallow magmatic and 527 hydrothermal systems',\n",
       "    'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "     {'first': 'B', 'middle': ['A'], 'last': 'Chouet', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': ['B'], 'last': 'Dawson', 'suffix': ''}],\n",
       "    'year': 2008,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '113',\n",
       "    'issn': 'B2',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '14550325'},\n",
       "   'BIBREF27': {'ref_id': 'b27',\n",
       "    'title': 'Variability in eruption style and associated 529 very-long-period earthquakes at Fuego volcano, Guatemala',\n",
       "    'authors': [{'first': 'G', 'middle': ['P'], 'last': 'Waite', 'suffix': ''},\n",
       "     {'first': 'J', 'middle': ['J'], 'last': 'Lyons', 'suffix': ''},\n",
       "     {'first': 'P', 'middle': ['A'], 'last': 'Nadeau', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'J. Geophys. Res',\n",
       "    'volume': '118',\n",
       "    'issn': '4',\n",
       "    'pages': '1526--1533',\n",
       "    'other_ids': {'doi': ['10.1002/jgrb.50075']},\n",
       "    'links': None},\n",
       "   'BIBREF28': {'ref_id': 'b28',\n",
       "    'title': 'Spherical-spline parameterization of three-dimensional Earth 532 models',\n",
       "    'authors': [{'first': 'Z', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "     {'first': 'F', 'middle': ['A'], 'last': 'Dahlen', 'suffix': ''}],\n",
       "    'year': 1995,\n",
       "    'venue': 'Geophys. Res. Lett',\n",
       "    'volume': '22',\n",
       "    'issn': '22',\n",
       "    'pages': '3099--3102',\n",
       "    'other_ids': {'doi': ['10.1029/95GL03080']},\n",
       "    'links': '129880072'},\n",
       "   'BIBREF29': {'ref_id': 'b29',\n",
       "    'title': 'Data (black), free-inversion synthetics (red), and best-fitting constrained inversion 651 synthetics (blue) for each of the four bandpasses: a) 30-10 s; b) 90-10 s; c) 120-10 s',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '6',\n",
       "    'issn': '',\n",
       "    'pages': '400--652',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF30': {'ref_id': 'b30',\n",
       "    'title': 'The free-inversion results are generally better than the constrained inversion, as expected',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF31': {'ref_id': 'b31',\n",
       "    'title': 'Source time functions from free inversions in the a) 30-10, b) 90-10, c) 120-10',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF32': {'ref_id': 'b32',\n",
       "    'title': '400-60 s pass bands are largely consistent, although the 90-10 s band has notable differences',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF33': {'ref_id': 'b33',\n",
       "    'title': 'Note the difference in amplitude and time scales for each of the four plots',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF34': {'ref_id': 'b34',\n",
       "    'title': 'Error (E 2 ) by fixed moment-tensor solution plotted using the indicated color scale for 710 each of the bandpasses in Figure 7: a) 30-10, b) 90-10, c) 120-10, and d) 400-60 s. The γ−δ pairs 711 computed from the point-by-point eigenvalue analysis for the best free inversion for each of four 712 bandpasses are plotted in red. The lack of consistency in the free inversion for the 90-10 s band 713 is reflected in the lack of a resolved moment-tensor type',\n",
       "    'authors': [],\n",
       "    'year': None,\n",
       "    'venue': '',\n",
       "    'volume': '8',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None},\n",
       "   'BIBREF35': {'ref_id': 'b35',\n",
       "    'title': 'The combined crack-pipe tensor of Lyons and Waite',\n",
       "    'authors': [],\n",
       "    'year': 2011,\n",
       "    'venue': '',\n",
       "    'volume': '715',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': None}}},\n",
       " 'latex_parse': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_papers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41660\n"
     ]
    }
   ],
   "source": [
    "with open(\"../acl_only_json/acl_only_json_list_10000.json\", \"r\") as read_file:\n",
    "    all_articles = json.load(read_file)\n",
    "print(len(all_articles))\n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ подборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### проверка наличия названия секции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_paper_ids = [article['paper_id'] for article in all_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Удалим статьи,у которых нет body_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_ids_not_body_text = [article['paper_id'] for article in all_articles if not article['grobid_parse']['body_text'] or not article['grobid_parse']['bib_entries']]\n",
    "len(acl_ids_not_body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_items_from_papers(acl_ids_not_body_text,acl_only_articles):\n",
    "    del_items = []\n",
    "    del_num_items = []\n",
    "    for num_artic, article in enumerate(acl_only_articles):\n",
    "        if article['paper_id'] in acl_ids_not_body_text:\n",
    "            del_items.append(article['paper_id'])\n",
    "            del_num_items.append(num_artic)\n",
    "            \n",
    "    del_num_items = np.array(del_num_items)\n",
    "    acl_only_articles = np.array(acl_only_articles)\n",
    "    acl_only_articles = np.delete(acl_only_articles,del_num_items)\n",
    "    return acl_only_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39763"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles = delete_items_from_papers(acl_ids_not_body_text,all_articles)\n",
    "len(all_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### удалим дублированные статьи по acl_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39763"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_paper_ids = [article['metadata']['acl_id'] for article in all_articles if article['metadata']['acl_id']]\n",
    "len(acl_paper_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "284"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubled_acl_id_papers = []\n",
    "for ind,cnt_of_acl_id in zip(pd.Series(acl_paper_ids).value_counts().index,pd.Series(acl_paper_ids).value_counts()):\n",
    "    if cnt_of_acl_id >=2:\n",
    "        doubled_acl_id_papers.append(ind)\n",
    "len(doubled_acl_id_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_items = []\n",
    "del_num_items = []\n",
    "for num_artic, article in enumerate(all_articles):\n",
    "    if article['metadata']['acl_id'] in doubled_acl_id_papers:\n",
    "        del_items.append(article['paper_id'])\n",
    "        del_num_items.append(num_artic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(del_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = np.delete(all_articles,del_num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_items = []\n",
    "del_num_items = []\n",
    "for num_artic, article in enumerate(all_articles):\n",
    "    if article['metadata']['acl_id'] in doubled_acl_id_papers:\n",
    "        del_items.append(article['paper_id'])\n",
    "        del_num_items.append(num_artic)\n",
    "len(del_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39191"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acl_paper_ids = [article['paper_id'] for article in all_articles if article['metadata']['acl_id']]\n",
    "len(acl_paper_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### проверка наличия текста и названия секций во всех статьях в grobid части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39191"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_articles if article['grobid_parse']['body_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_bofy_text = [article['paper_id'] for article in all_articles if not article['grobid_parse']['body_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_with_sect = dict()\n",
    "for article in all_articles:\n",
    "    for sections in article['grobid_parse']['body_text']:\n",
    "        if sections['section']:\n",
    "            if article['paper_id'] in article_with_sect:\n",
    "                article_with_sect[article['paper_id']] +=1\n",
    "            else:\n",
    "                article_with_sect[article['paper_id']] = 1\n",
    "article_with_sect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### как видим нет названия секций у grobid_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "0 {'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "1 {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "2 {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "3 {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "4 {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "5 {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "6 {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "7 {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "8 {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "9 {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "10 {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "11 {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "12 {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google's big developers' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "----\n",
      "1 1\n",
      "0 {'start': 451, 'end': 468, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.\n",
      "----\n",
      "2 6\n",
      "0 {'start': 32, 'end': 58, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "1 {'start': 59, 'end': 79, 'text': 'Rezende et al., 2014', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "2 {'start': 184, 'end': 200, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "3 {'start': 2384, 'end': 2401, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "4 {'start': 2433, 'end': 2456, 'text': '(Bahdanau et al., 2015;', 'latex': None, 'ref_id': 'BIBREF0'}\n",
      "5 {'start': 2457, 'end': 2476, 'text': 'Luong et al., 2015)', 'latex': None, 'ref_id': 'BIBREF13'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.\n",
      "----\n",
      "3 3\n",
      "0 {'start': 86, 'end': 102, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "1 {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "2 {'start': 1133, 'end': 1158, 'text': '(Dantzig and Thapa, 2006)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .\n",
      "----\n",
      "8 1\n",
      "0 {'start': 107, 'end': 118, 'text': '(Lin, 2004)', 'latex': None, 'ref_id': 'BIBREF12'}\n",
      "The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n",
      "----\n",
      "9 4\n",
      "0 {'start': 351, 'end': 365, 'text': '(Wasson, 1998)', 'latex': None, 'ref_id': 'BIBREF20'}\n",
      "1 {'start': 492, 'end': 512, 'text': '(Radev et al., 2000)', 'latex': None, 'ref_id': 'BIBREF17'}\n",
      "2 {'start': 700, 'end': 723, 'text': '(Erkan and Radev, 2004)', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "3 {'start': 737, 'end': 763, 'text': '(Mihalcea and Tarau, 2004)', 'latex': None, 'ref_id': 'BIBREF14'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.\n",
      "----\n",
      "10 2\n",
      "0 {'start': 498, 'end': 518, 'text': '(Kingma and Ba, 2014', 'latex': None, 'ref_id': 'BIBREF8'}\n",
      "1 {'start': 652, 'end': 674, 'text': '(Bastien et al., 2012)', 'latex': None, 'ref_id': 'BIBREF1'}\n",
      "The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .\n",
      "----\n",
      "11 2\n",
      "0 {'start': 690, 'end': 707, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "1 {'start': 908, 'end': 925, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"'Bitcoin Mt. Gox Offlile\"' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.\n",
      "----\n",
      "13 1\n",
      "0 {'start': 517, 'end': 868, 'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world', 'latex': None, 'ref_id': None}\n",
      "We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world \n",
      "----\n",
      "====================\n",
      "0 6\n",
      "0 {'start': 173, 'end': 197, 'text': '(Zelle and Mooney, 1996;', 'latex': None, 'ref_id': 'BIBREF25'}\n",
      "1 {'start': 198, 'end': 228, 'text': 'Zettlemoyer and Collins, 2005;', 'latex': None, 'ref_id': 'BIBREF26'}\n",
      "2 {'start': 229, 'end': 248, 'text': 'Liang et al., 2011;', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "3 {'start': 249, 'end': 269, 'text': 'Berant et al., 2013;', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "4 {'start': 270, 'end': 295, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "5 {'start': 296, 'end': 323, 'text': 'Kushman and Barzilay, 2013)', 'latex': None, 'ref_id': 'BIBREF10'}\n",
      "By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Kushman and Barzilay, 2013) . Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain-for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start?In this paper, we advocate a functionalitydriven process for rapidly building a semantic * Both authors equally contributed to the paper. ...\n",
      "----\n",
      "1 7\n",
      "0 {'start': 1832, 'end': 1853, 'text': '(Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "1 {'start': 2121, 'end': 2142, 'text': '(Rangel et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "2 {'start': 2415, 'end': 2441, 'text': '(Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "3 {'start': 2442, 'end': 2465, 'text': 'Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "4 {'start': 3174, 'end': 3194, 'text': '(Liang et al., 2011;', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "5 {'start': 3195, 'end': 3220, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "6 {'start': 3221, 'end': 3244, 'text': 'Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) by builder (∼30 minutes)(2) via domain-general grammar (3) via crowdsourcing (∼5 hours) (4) by training a paraphrasing model Figure 1 : Functionality-driven process for building semantic parsers. The two red boxes are the domain-specific parts provided by the builder of the semantic parser, and the other two are generated by the framework.parser in a new domain. At a high-level, we seek to minimize the amount of work needed for a new domain by factoring out the domaingeneral aspects (done by our framework) from the domain-specific ones (done by the builder of the semantic parser). We assume that the builder already has the desired functionality of the semantic parser in mind-e.g., the publications database is set up and the schema is fixed. Figure 1 depicts the functionality-driven process: First, the builder writes a seed lexicon specifying a canonical phrase (\"publication date\") for each predicate (publicationDate).Second, our framework uses a domain-general grammar, along with the seed lexicon and the database, to automatically generate a few hundred canonical utterances paired with their logical forms (e.g., \"article that has the largest publication date\" and arg max(type.article, publicationDate)). These utterances need not be the most elegant, but they should retain the semantics of the logical forms. Third, the builder leverages crowdsourcing to paraphrase each canonical utterance into a few natural utterances (e.g., \"what is the newest published article?\"). Finally, our framework uses this data to train a semantic parser.Practical advantages. There are two main advantages of our approach: completeness and ease of supervision. Traditionally, training data is collected in a best-effort manner, which can result in an incomplete coverage of functionality. For example, the WebQuestions dataset (Berant et al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014) . In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule.In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014) . Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain.Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny, as the structural mismatch between logic and language is the chief source of difficulty in semantic parsing (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014) .There are two important questions here. First, is it possible to design a simple grammar that simultaneously generates both logical forms and canonical utterances so that the utterances are understandable by a human? In Section 3, we show how to choose appropriate canonical utterances to maximize alignment with the logical forms.Second, our grammar can generate an infinite number of canonical utterances. How many do we need for adequate coverage? Certainly, single relations is insufficient: just knowing that \"publication date of X\" paraphrases to \"when X was published\" would offer insufficient information to generalize to \"articles that came after X\" mapping to \"article whose publication date is larger than publication date of X\". We call this phenomena sublexical compositionality-when a short lexical unit (\"came after\") maps onto a multi-predicate logical form. Our hypothesis is that the sublexical compositional units are small, so we only need to crowdsource a small number of canonical utterances to learn about most of the language variability in the given domain (Section 4).We applied our functionality-driven process to seven domains, which were chosen to explore particular types of phenomena, such as spatial language, temporal language, and high-arity relations. This resulted in seven new semantic parsing datasets, totaling 12.6K examples. Our approach, which was not tuned on any one domain, was able to obtain an average accuracy of 59% over all domains. On the day of this paper submission, we created an eighth domain and trained a semantic parser overnight.\n",
      "----\n",
      "2 3\n",
      "0 {'start': 327, 'end': 342, 'text': 'article1, 2015)', 'latex': None, 'ref_id': None}\n",
      "1 {'start': 1308, 'end': 1311, 'text': '(r)', 'latex': None, 'ref_id': None}\n",
      "2 {'start': 2374, 'end': 2397, 'text': 'Berant and Liang (2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "In our functionality-driven process (Figure 1) , there are two parties: the builder, who provides domain-specific information, and the framework, which provides domain-general information. We assume that the builder has a fixed database w, represented as a set of triples (e 1 , p, e 2 ), where e 1 and e 2 are entities (e.g., article1, 2015) and p is a property (e.g., publicationDate). The database w can be queried using lambda DCS logical forms, described further in Section 2.1.The builder supplies a seed lexicon L, which contains for each database property p (e.g., publicationDate) a lexical entry of the form t → s[p] , where t is a natural language phrase (e.g., \"publication date\") and s is a syntactic cat-egory (e.g., RELNP). In addition, L contains two typical entities for each semantic type in the database (e.g., alice → NP[alice] for the type person). The purpose of L is to simply connect each predicate with some representation in natural language.The framework supplies a grammar G, which specifies the modes of composition, both on logical forms and canonical utterances. Formally, G is a set of rules of the form α 1 . . . α n → s[z] , where α 1 . . . α n is a sequence of tokens or categories, s is a syntactic category and z is the logical form constructed. For example, one rule in (r) .x] , which constructs z by reversing the binary predicate r and joining it with a the unary predicate x. We use the rules G ∪ L to generate a set of (z, c) pairs, where z is a logical form (e.g., R(publicationDate).article1), and c is the corresponding canonical utterance (e.g., \"publication date of article 1\"). The set of (z, c) is denoted by GEN(G ∪ L). See Section 3 for details.G is RELNP[r] of NP[x] → NP[RNext, the builder (backed by crowdsourcing) paraphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., \"when was article 1 published?\"). This defines a set of training examples D = {(x, c, z)}, for each (z, c) ∈ GEN(G ∪ L) and x ∈ P(c). The crowdsourcing setup is detailed in Section 5.Finally, the framework trains a semantic parser on D. Our semantic parser is a log-linear distribution p θ (z, c | x, w) over logical forms and canonical utterances specified by the grammar G. Note that the grammar G will in general not parse x, so the semantic parsing model will be based on paraphrasing, in the spirit of Berant and Liang (2014) .To summarize, (1) the builder produces a seed lexicon L; (2) the framework produces logical forms and canonical utterances GEN(G ∪ L) = {(z, c)}; (3) the builder (via crowdsourcing) uses P(·) to produce a dataset D = {(x, c, z)}; and (4) the framework uses D to train a semantic parser p θ (z, c | x, w).\n",
      "----\n",
      "3 1\n",
      "0 {'start': 167, 'end': 179, 'text': 'Liang (2013)', 'latex': None, 'ref_id': 'BIBREF14'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our logical forms are represented in lambda DCS, a logical language where composition operates on sets rather than truth values. Here we give a brief description; see Liang (2013) for details.Every logical form z in this paper is either a unary (denoting a set of entities) or a binary (denoting a set of entity-pairs). In the base case, each entity e (e.g., 2015) is a unary denoting the singleton set: e w = {e}; and each property p (e.g., publicationDate) is a binary denoting all entitypairs (e 1 , e 2 ) that satisfy the property p. Unaries and binaries can be composed: Given a binary b and unary u, the join b.u denotes all entities e 1 for which there exists an e 2 ∈ u w with (e 1 , e 2 ) ∈ b w . For example, publicationDate.2015 denote entities published in 2015.The intersection u 1 u 2 , union u 1 u 2 , complement ¬u denote the corresponding set operations on the denotations. We let R(b) denote the reversal of b: (e 1 , e 2 ) ∈ b w iff (e 2 , e 1 ) ∈ R(b) w . This allows us to define R(publicationDate).article1 as the publication date of article 1. We also include aggregation operations (count(u), sum(u) and average(u, b)), and superlatives (argmax(u, b)).Finally, we can construct binaries using lambda abstraction: λx.u denotes a set of (e 1 , e 2 ) where e 1 ∈ u[x/e 2 ] w and u[x/e 2 ] is the logical form where free occurrences of x are replaced with e 2 . For example, R(λx.count(R(cites).x)) denotes the set of entities (e 1 , e 2 ), where e 2 is the number of entities that e 1 cites.\n",
      "----\n",
      "7 3\n",
      "0 {'start': 442, 'end': 448, 'text': '[2015]', 'latex': None, 'ref_id': None}\n",
      "1 {'start': 1435, 'end': 1458, 'text': 'Berant and Liang (2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "2 {'start': 1494, 'end': 1518, 'text': 'Pasupat and Liang (2015)', 'latex': None, 'ref_id': 'BIBREF17'}\n",
      "Our semantic parsing model defines a distribution over logical forms given by the domaingeneral grammar G and additional rules triggered by the input utterance x. Specifically, given an utterance x, we detect numbers, dates, and perform string matching with database entities to recognize named entities. This results in a set of rules T(x). For example, if x is \"article published in 2015 that cites article 1\", then T(x) contains 2015 → NP [2015] andarticle 1 → NP[article1]. Let L x be the rules in the seed lexicon L where the entity rules (e.g., alice → NP[alice] ) are replaced by T(x). Our semantic parsing model defines a loglinear distribution over candidate pairs (z, c) ∈ GEN(G ∪ L x ):p θ (z, c | x, w) ∝ exp(φ(c, z, x, w) θ),(1)where φ(z, c, x, w) ∈ R d is a feature vector and θ ∈ R d is a parameter vector. To generate candidate logical forms, we use a simple beam search: For each search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance \"article\" would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015) .Training. We train our model by maximizing the regularized log-likelihood O(θ) = Domain # pred. # ex. Phenomena Example CALENDAR 22 837 temporal language x: \"Show me meetings after the weekly standup day\" c: \"meeting whose date is at least date of weekly standup\" z: type.meeting date. > R(date).weeklyStandup BLOCKS 19 1995 spatial language x: \"Select the brick that is to the furthest left.\" c: \"block that the most number of block is right of\" z: argmax(type.block, R(λx.count(R(right).x))) HOUSING 24 941 measurement units x: \"Housing that is 800 square feet or bigger?\" c: \"housing unit whose size is at least 800 square feet\" z: type.housingUnit area. > .800 RESTAURANTS 32 1657 long unary relations x: \"What restaurant can you eat lunch outside at?\" c: \"restaurant that has outdoor seating and that serves lunch\" z: type.restaurant hasOutdoorSeating serveslunch PUBLICATIONS 15 801 sublexical compositionality x: \"Who has co-authored articles with Efron?\" c: \"person that is author of article whose author is efron\" z: type.person R(author).(type.article author.efron) SOCIAL 45 4419 multi-arity relations x: \"When did alice start attending brown university?\" c: \"start date of student alice whose university is brown university\" z: R(date). Table 3 : We experimented on seven domains, covering a variety of phenomena. For each domain, we show the number of predicates, number of examples, and a (c, z) generated by our framework along with a paraphrased utterance x. Table 4 : Features for the paraphrasing model. pos(x i:i ) is the POS tag; type( z w ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c.\n",
      "----\n",
      "8 5\n",
      "0 {'start': 70, 'end': 90, 'text': '(Duchi et al., 2010)', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "1 {'start': 253, 'end': 280, 'text': '(Ganitkevitch et al., 2013)', 'latex': None, 'ref_id': 'BIBREF8'}\n",
      "2 {'start': 411, 'end': 431, 'text': '(Liang et al., 2006)', 'latex': None, 'ref_id': 'BIBREF12'}\n",
      "3 {'start': 774, 'end': 793, 'text': '(Och and Ney, 2004)', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "4 {'start': 1213, 'end': 1234, 'text': '(Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 . To optimize, we use AdaGrad (Duchi et al., 2010) .Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) . We count the number of exact matches, PPDB matches, and unmatched words.To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4 ). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) . We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .\n",
      "----\n",
      "13 1\n",
      "0 {'start': 205, 'end': 229, 'text': '(Zelle and Mooney, 1996)', 'latex': None, 'ref_id': 'BIBREF25'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo880. To test how our parser generalizes to utterances independent of our framework, we created a semantic parser for the domain of US geography, and tested on the standard 280 test examples from GEO880 (Zelle and Mooney, 1996) . We did not use the standard 600 training examples. Our parser obtained 56.4% accuracy, which is substantially lower than state-of-the-art (∼ 90%).We performed error analysis on 100 random sentences from the development set where accuracy was 60%. We found that the parser learns from the training data to prefer shorter paraphrases, which accounts for 30% of the errors. In most of these cases, the correct logical form is ranked at the top-3 results (accuracy for the top-3 derivations is 73%). GEO880 contains highly compositional utterances, and in 25% of the errors the correct derivation tree exceeds the maximum depth used for our parser. Another 17.5% of the errors are caused by problems in the paraphrasing model. For example, in the utterance \"what is the size of california\", the model learns that \"size\" corresponds to \"population\" rather than \"area\". Errors related to reordering and the syntactic structure of the input utterance account for 7.5% of the errors. For example, the utterance \"what is the area of the largest state\" is paraphrased to \"state that has the largest area\".Calendar. In Section 7.1, we evaluated on utterances obtained by paraphrasing canonical utterances from the grammar. To examine the coverage of our parser on independently-produced utterances, we asked AMT workers to freely come up with queries. We collected 186 such queries; 5 were spam and discarded. We replaced all entities (people, dates, etc.) with entities from our seed lexicon to avoid focusing on entity detection.We were able to annotate 52% of the utterances with logical forms from our grammar. We could not annotate 20% of the utterances due to relative time references, such as \"What time is my next meeting?\". 14% of the utterances were not covered due to binary predicates not in the grammar (\"What is the agenda of the meeting?\") or missing entities (\"When is Dan's birthday?\"). Another 2% required unsupported calculations (\"How much free time do I have tomorrow?\"), and the rest are out of scope for other reasons (\"When does my Verizon data plan start over?\").We evaluated our trained semantic parser on the 95 utterances annotated with logical forms. Our parser obtained an accuracy of 46.3% and oracle accuracy of 84.2%, which measures how often the correct denotation is on the final beam. The large gap shows that there is considerable room for improvement in the paraphrasing model.\n",
      "----\n",
      "14 18\n",
      "0 {'start': 101, 'end': 122, 'text': '(Cai and Yates, 2013;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "1 {'start': 123, 'end': 148, 'text': 'Kwiatkowski et al., 2013;', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "2 {'start': 149, 'end': 169, 'text': 'Berant et al., 2013)', 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "3 {'start': 410, 'end': 438, 'text': '(Kushman and Barzilay, 2013)', 'latex': None, 'ref_id': 'BIBREF10'}\n",
      "4 {'start': 466, 'end': 489, 'text': '(Matuszek et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "5 {'start': 490, 'end': 510, 'text': 'Tellex et al., 2011;', 'latex': None, 'ref_id': 'BIBREF21'}\n",
      "6 {'start': 511, 'end': 542, 'text': 'Krishnamurthy and Kollar, 2013)', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "7 {'start': 756, 'end': 787, 'text': '(Zettlemoyer and Collins, 2005;', 'latex': None, 'ref_id': 'BIBREF26'}\n",
      "8 {'start': 788, 'end': 810, 'text': 'Wong and Mooney, 2007)', 'latex': None, 'ref_id': 'BIBREF23'}\n",
      "9 {'start': 826, 'end': 847, 'text': '(Clarke et al., 2010;', 'latex': None, 'ref_id': 'BIBREF5'}\n",
      "10 {'start': 848, 'end': 867, 'text': 'Liang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF13'}\n",
      "11 {'start': 888, 'end': 917, 'text': '(Artzi and Zettlemoyer, 2011;', 'latex': None, 'ref_id': 'BIBREF0'}\n",
      "12 {'start': 918, 'end': 937, 'text': 'Reddy et al., 2014)', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "13 {'start': 1202, 'end': 1222, 'text': '(Fader et al., 2013)', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "14 {'start': 1244, 'end': 1268, 'text': '(Berant and Liang, 2014)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "15 {'start': 1393, 'end': 1413, 'text': '(Woods et al., 1972;', 'latex': None, 'ref_id': 'BIBREF24'}\n",
      "16 {'start': 1414, 'end': 1439, 'text': 'Warren and Pereira, 1982)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "17 {'start': 1472, 'end': 1489, 'text': '(Schwitter, 2010)', 'latex': None, 'ref_id': 'BIBREF20'}\n",
      "Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013) . However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013) . Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal.Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014) . All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing.Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014) . We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010) . However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance.In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a promising way to build semantic parsers, and in future work, we would like to extend it to handle anaphora and nested quantification.\n",
      "----\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "# article_with_sect = dict()\n",
    "for num,article in enumerate(all_articles):\n",
    "    if num == 2:\n",
    "        break\n",
    "    for cnt_sect,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "        if sections['cite_spans']:\n",
    "            print(cnt_sect,len(sections['cite_spans']))\n",
    "            for cnt_cite,cite in enumerate(sections['cite_spans']):\n",
    "                print(cnt_cite,cite)\n",
    "            print(sections['text'])\n",
    "            print('----')\n",
    "    print(10*'==')\n",
    "#             if article['paper_id'] in article_with_sect:\n",
    "#                 article_with_sect[article['paper_id']] +=1\n",
    "#             else:\n",
    "#                 article_with_sect[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_results[0] - Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset∗\n",
    "\n",
    "\n",
    "\n",
    "Result \n",
    "\n",
    "- 0 {'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
    "- 1 {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
    "- 2 {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
    "- 3 {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
    "- 4 {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
    "- 5 {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
    "\n",
    "True \n",
    "\n",
    "- 0 (Goldstein et al., 2000; \n",
    "- 1 Erkan and Radev,2004; \n",
    "- 2 Wan et al., 2007; \n",
    "- 3 Nenkova and McKeown, 2012; \n",
    "- 4 Min et al., 2012; \n",
    "- 5 Bing et al., 2015; \n",
    "- 6 Li et al.,2017)\n",
    "\n",
    "\n",
    "Result\n",
    "\n",
    "- {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
    "\n",
    "True\n",
    "\n",
    "- Woodsend and Lapata (2012), Bing et al. (2015), and Li et al. (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**При большом перечислении подряд ссылок, GROBID не выделяет предпоследнюю ссылку**\n",
    "\n",
    "**Также он не срабатывает на части ссылок**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### проверка наличия текста и названия секций во всех статьях в latex части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3868"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([article['paper_id'] for article in all_articles if article['latex_parse'] and article['latex_parse']['body_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_ids_not_body_text_tex = [article['paper_id'] for article in all_articles if not ( article['latex_parse'] and article['latex_parse']['body_text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35323"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(acl_ids_not_body_text_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 None\n",
      "====================\n",
      "2 None\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for num,paper_id in enumerate(acl_ids_not_body_text_tex):\n",
    "    if num == 2:\n",
    "        break\n",
    "    id_lst = acl_paper_ids.index(paper_id)\n",
    "    print(id_lst,all_articles[id_lst]['latex_parse'])\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "  'title': 'Multi-document summarization by sentence extraction',\n",
       "  'authors': [{'first': 'Jade',\n",
       "    'middle': [],\n",
       "    'last': 'Goldstein',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "   {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "   {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'NAACL-ANLPWorkshop',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '40--48',\n",
       "  'other_ids': {},\n",
       "  'links': '8294822'},\n",
       " 'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "  'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "  'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '4',\n",
       "  'issn': '',\n",
       "  'pages': '365--371',\n",
       "  'other_ids': {},\n",
       "  'links': '10418456'},\n",
       " 'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "  'title': 'Auto-encoding variational bayes',\n",
       "  'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "   {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '15789289'},\n",
       " 'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "  'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "  'authors': [{'first': 'Danilo',\n",
       "    'middle': [],\n",
       "    'last': 'Jimenez Rezende',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "   {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICML',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1278--1286',\n",
       "  'other_ids': {},\n",
       "  'links': '16895865'},\n",
       " 'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "  'title': 'Effective approaches to attention-based neural machine translation',\n",
       "  'authors': [{'first': 'Minh-Thang',\n",
       "    'middle': [],\n",
       "    'last': 'Luong',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "   {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'EMNLP',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1412--1421',\n",
       "  'other_ids': {},\n",
       "  'links': '1998416'},\n",
       " 'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "  'title': 'Multiple aspect summarization using integer linear programming',\n",
       "  'authors': [{'first': 'Kristian',\n",
       "    'middle': [],\n",
       "    'last': 'Woodsend',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'EMNLP-CNLL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '233--243',\n",
       "  'other_ids': {},\n",
       "  'links': '17497992'},\n",
       " 'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "  'title': 'Linear programming 1: introduction',\n",
       "  'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "   {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "  'year': 2006,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '53739754'},\n",
       " 'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "  'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "  'authors': [{'first': 'Mark', 'middle': [], 'last': 'Wasson', 'suffix': ''}],\n",
       "  'year': 1998,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1364--1368',\n",
       "  'other_ids': {},\n",
       "  'links': '12681629'},\n",
       " 'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "  'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "  'authors': [{'first': 'Hongyan',\n",
       "    'middle': [],\n",
       "    'last': 'Dragomir R Radev',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "  'year': 2000,\n",
       "  'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '21--30',\n",
       "  'other_ids': {},\n",
       "  'links': '1320'},\n",
       " 'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "  'title': 'Textrank: Bringing order into texts',\n",
       "  'authors': [{'first': 'Rada',\n",
       "    'middle': [],\n",
       "    'last': 'Mihalcea',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "  'year': 2004,\n",
       "  'venue': '',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '577937'},\n",
       " 'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "  'title': 'Adam: A method for stochastic optimization',\n",
       "  'authors': [{'first': 'Diederik',\n",
       "    'middle': [],\n",
       "    'last': 'Kingma',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "  'year': 2014,\n",
       "  'venue': 'ICLR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '',\n",
       "  'other_ids': {},\n",
       "  'links': '6628106'},\n",
       " 'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "  'title': 'A survey of text summarization techniques',\n",
       "  'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "   {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'Mining Text Data',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '43--76',\n",
       "  'other_ids': {},\n",
       "  'links': '556431'},\n",
       " 'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "  'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "  'authors': [{'first': 'Yen',\n",
       "    'middle': ['Kan'],\n",
       "    'last': 'Ziheng Lin Min',\n",
       "    'suffix': ''},\n",
       "   {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "   {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "  'year': 2012,\n",
       "  'venue': 'COLING',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '2093--2108',\n",
       "  'other_ids': {},\n",
       "  'links': '6317274'},\n",
       " 'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "  'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "  'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "   {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'ACL',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1587--1597',\n",
       "  'other_ids': {},\n",
       "  'links': '8377315'},\n",
       " 'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "  'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "  'year': 2017,\n",
       "  'venue': 'AAAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '3497--3503',\n",
       "  'other_ids': {},\n",
       "  'links': '29562039'},\n",
       " 'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "  'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "  'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "   {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "   {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "  'year': 2008,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '291--298',\n",
       "  'other_ids': {},\n",
       "  'links': '13723748'},\n",
       " 'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "  'title': 'Social context summarization',\n",
       "  'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "   {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "   {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "   {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "   {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "   {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "  'year': 2011,\n",
       "  'venue': 'SIGIR',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '255--264',\n",
       "  'other_ids': {},\n",
       "  'links': '704517'},\n",
       " 'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "  'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "  'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "   {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "   {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "   {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "  'year': 2015,\n",
       "  'venue': 'IJCAI',\n",
       "  'volume': '',\n",
       "  'issn': '',\n",
       "  'pages': '1270--1276',\n",
       "  'other_ids': {},\n",
       "  'links': '14777460'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: all_articles[0]['latex_parse']['bib_entries'][k] for k in sorted(all_articles[0]['latex_parse']['bib_entries'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': [],\n",
       " 'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "   'cite_spans': [{'start': 193,\n",
       "     'end': 200,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF0'},\n",
       "    {'start': 203,\n",
       "     'end': 210,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 213,\n",
       "     'end': 220,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF2'},\n",
       "    {'start': 223,\n",
       "     'end': 230,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF3'},\n",
       "    {'start': 233,\n",
       "     'end': 240,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF4'},\n",
       "    {'start': 243,\n",
       "     'end': 250,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 253,\n",
       "     'end': 260,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 118,\n",
       "     'end': 125,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF2'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "   'cite_spans': [{'start': 527,\n",
       "     'end': 534,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF7'},\n",
       "    {'start': 537,\n",
       "     'end': 544,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF8'},\n",
       "    {'start': 802,\n",
       "     'end': 809,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "   'cite_spans': [{'start': 10,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'},\n",
       "    {'start': 159,\n",
       "     'end': 167,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 170,\n",
       "     'end': 178,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "   'cite_spans': [{'start': 489,\n",
       "     'end': 496,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 19,\n",
       "     'end': 26,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF7'}],\n",
       "   'eq_spans': [{'start': 212,\n",
       "     'end': 223,\n",
       "     'text': 'X d ',\n",
       "     'latex': 'X_d',\n",
       "     'ref_id': None},\n",
       "    {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None},\n",
       "    {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None},\n",
       "    {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None},\n",
       "    {'start': 739,\n",
       "     'end': 750,\n",
       "     'text': 'ρ i ',\n",
       "     'latex': '\\\\rho _i',\n",
       "     'ref_id': None},\n",
       "    {'start': 774,\n",
       "     'end': 785,\n",
       "     'text': '𝐱 c i ',\n",
       "     'latex': '\\\\mathbf {x}_c^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 807,\n",
       "     'end': 818,\n",
       "     'text': 'ρ∈ℝ n c  ',\n",
       "     'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Overview'},\n",
       "  {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 32,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 43,\n",
       "     'end': 51,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'},\n",
       "    {'start': 154,\n",
       "     'end': 161,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 472,\n",
       "     'end': 483,\n",
       "     'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "     'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "     'ref_id': None},\n",
       "    {'start': 488,\n",
       "     'end': 499,\n",
       "     'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "     'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "     'ref_id': None},\n",
       "    {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None},\n",
       "    {'start': 524,\n",
       "     'end': 535,\n",
       "     'text': 'σ',\n",
       "     'latex': '\\\\sigma ',\n",
       "     'ref_id': None},\n",
       "    {'start': 799,\n",
       "     'end': 811,\n",
       "     'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "     'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "     'ref_id': 'EQREF9'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 50,\n",
       "     'end': 56,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'EQREF9'}],\n",
       "   'eq_spans': [{'start': 131,\n",
       "     'end': 142,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 145,\n",
       "     'end': 157,\n",
       "     'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "     'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "     'ref_id': 'EQREF10'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': '𝐱',\n",
       "     'latex': '\\\\mathbf {x}',\n",
       "     'ref_id': None},\n",
       "    {'start': 76,\n",
       "     'end': 87,\n",
       "     'text': '𝐱 d ',\n",
       "     'latex': '\\\\mathbf {x}_d',\n",
       "     'ref_id': None},\n",
       "    {'start': 110,\n",
       "     'end': 121,\n",
       "     'text': '𝐱 c ',\n",
       "     'latex': '\\\\mathbf {x}_c',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 364,\n",
       "     'end': 375,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 444,\n",
       "     'end': 456,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "     'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "     'ref_id': 'EQREF11'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 19,\n",
       "     'end': 30,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 113,\n",
       "     'end': 124,\n",
       "     'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "     'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "    {'start': 219,\n",
       "     'end': 230,\n",
       "     'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "     'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 320,\n",
       "     'end': 331,\n",
       "     'text': '𝐒 z ',\n",
       "     'latex': '\\\\mathbf {S}_z',\n",
       "     'ref_id': None},\n",
       "    {'start': 335,\n",
       "     'end': 346,\n",
       "     'text': '𝐒 h ',\n",
       "     'latex': '\\\\mathbf {S}_h',\n",
       "     'ref_id': None},\n",
       "    {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "    {'start': 402,\n",
       "     'end': 413,\n",
       "     'text': '𝐒 x ',\n",
       "     'latex': '\\\\mathbf {S}_x',\n",
       "     'ref_id': None},\n",
       "    {'start': 416,\n",
       "     'end': 428,\n",
       "     'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "     'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "     'ref_id': 'EQREF12'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 7,\n",
       "     'end': 14,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'},\n",
       "    {'start': 46,\n",
       "     'end': 54,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF12'},\n",
       "    {'start': 57,\n",
       "     'end': 65,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF13'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 304,\n",
       "     'end': 315,\n",
       "     'text': 's h i ',\n",
       "     'latex': 's^i_{h}',\n",
       "     'ref_id': None},\n",
       "    {'start': 366,\n",
       "     'end': 377,\n",
       "     'text': 'h d j ',\n",
       "     'latex': 'h^j_{d}',\n",
       "     'ref_id': None},\n",
       "    {'start': 401,\n",
       "     'end': 412,\n",
       "     'text': 'a d ∈ℝ n d  ',\n",
       "     'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "     'ref_id': None},\n",
       "    {'start': 472,\n",
       "     'end': 483,\n",
       "     'text': 'h c j ',\n",
       "     'latex': 'h^j_{c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 507,\n",
       "     'end': 518,\n",
       "     'text': 'a c ∈ℝ n c  ',\n",
       "     'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 672,\n",
       "     'end': 684,\n",
       "     'text': 'a ˜ c =a c ×ρ',\n",
       "     'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "     'ref_id': 'EQREF13'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 30,\n",
       "     'end': 41,\n",
       "     'text': 'c d i ',\n",
       "     'latex': 'c_d^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 79,\n",
       "     'end': 90,\n",
       "     'text': 'c c i ',\n",
       "     'latex': 'c_c^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 240,\n",
       "     'end': 252,\n",
       "     'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "     'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "     'ref_id': 'EQREF14'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 64,\n",
       "     'end': 75,\n",
       "     'text': 's ˜ h i ',\n",
       "     'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': '𝐒 z ',\n",
       "     'latex': '\\\\mathbf {S}_z',\n",
       "     'ref_id': None},\n",
       "    {'start': 15,\n",
       "     'end': 26,\n",
       "     'text': '𝐒 h ',\n",
       "     'latex': '\\\\mathbf {S}_h',\n",
       "     'ref_id': None},\n",
       "    {'start': 33,\n",
       "     'end': 44,\n",
       "     'text': '𝐒 x ',\n",
       "     'latex': '\\\\mathbf {S}_x',\n",
       "     'ref_id': None},\n",
       "    {'start': 220,\n",
       "     'end': 231,\n",
       "     'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "     'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "     'ref_id': None},\n",
       "    {'start': 297,\n",
       "     'end': 308,\n",
       "     'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "     'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "     'ref_id': None},\n",
       "    {'start': 569,\n",
       "     'end': 581,\n",
       "     'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "     'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "     'ref_id': 'EQREF15'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 70,\n",
       "     'end': 81,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 170,\n",
       "     'end': 182,\n",
       "     'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "     'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "     'ref_id': 'EQREF16'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'Θ',\n",
       "     'latex': '\\\\Theta ',\n",
       "     'ref_id': None},\n",
       "    {'start': 110,\n",
       "     'end': 121,\n",
       "     'text': '𝐀 d ',\n",
       "     'latex': '\\\\mathbf {A}_d',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 94,\n",
       "     'end': 105,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None},\n",
       "    {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None},\n",
       "    {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None},\n",
       "    {'start': 434,\n",
       "     'end': 445,\n",
       "     'text': 'R∈ℝ n d ×n c  ',\n",
       "     'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 450,\n",
       "     'end': 462,\n",
       "     'text': 'R=X d ×X c T ',\n",
       "     'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "     'ref_id': 'EQREF17'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 93,\n",
       "     'end': 105,\n",
       "     'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "     'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "     'ref_id': 'EQREF18'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 70,\n",
       "     'end': 81,\n",
       "     'text': '(0,1)',\n",
       "     'latex': '(0,1)',\n",
       "     'ref_id': None},\n",
       "    {'start': 84,\n",
       "     'end': 96,\n",
       "     'text': 'ρ=sigmoid(𝐫)',\n",
       "     'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "     'ref_id': 'EQREF19'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 332,\n",
       "     'end': 343,\n",
       "     'text': 'λ p ',\n",
       "     'latex': '\\\\lambda _p',\n",
       "     'ref_id': None},\n",
       "    {'start': 346,\n",
       "     'end': 358,\n",
       "     'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "     'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "     'ref_id': 'EQREF20'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'ρ z ',\n",
       "     'latex': '\\\\rho _z',\n",
       "     'ref_id': None},\n",
       "    {'start': 22,\n",
       "     'end': 33,\n",
       "     'text': 'ρ x ',\n",
       "     'latex': '\\\\rho _x',\n",
       "     'ref_id': None},\n",
       "    {'start': 142,\n",
       "     'end': 153,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 82,\n",
       "     'end': 89,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 94,\n",
       "     'end': 101,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 485,\n",
       "     'end': 497,\n",
       "     'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "     'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "     'ref_id': 'EQREF22'}],\n",
       "   'section': 'Summary Construction'},\n",
       "  {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "   'cite_spans': [{'start': 466,\n",
       "     'end': 474,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF14'},\n",
       "    {'start': 477,\n",
       "     'end': 484,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 491,\n",
       "     'end': 498,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'},\n",
       "    {'start': 644,\n",
       "     'end': 652,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF15'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'α i ',\n",
       "     'latex': '\\\\alpha _i',\n",
       "     'ref_id': None},\n",
       "    {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "    {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 126,\n",
       "     'end': 137,\n",
       "     'text': 'α ij ',\n",
       "     'latex': '\\\\alpha _{ij}',\n",
       "     'ref_id': None},\n",
       "    {'start': 142,\n",
       "     'end': 153,\n",
       "     'text': 'R ij ',\n",
       "     'latex': 'R_{ij}',\n",
       "     'ref_id': None},\n",
       "    {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 234,\n",
       "     'end': 245,\n",
       "     'text': 'P j ',\n",
       "     'latex': 'P_j',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Summary Construction'},\n",
       "  {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Description'},\n",
       "  {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 222,\n",
       "     'end': 229,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF7'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Properties'},\n",
       "  {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "   'cite_spans': [{'start': 113,\n",
       "     'end': 121,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF16'}],\n",
       "   'ref_spans': [{'start': 58,\n",
       "     'end': 66,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF28'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Dataset and Metrics'},\n",
       "  {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "   'cite_spans': [{'start': 10,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "   'cite_spans': [{'start': 5,\n",
       "     'end': 13,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF17'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "   'cite_spans': [{'start': 9,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF18'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "   'cite_spans': [{'start': 8,\n",
       "     'end': 15,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 29,\n",
       "     'end': 37,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF19'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "   'cite_spans': [{'start': 8,\n",
       "     'end': 15,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "   'cite_spans': [{'start': 557,\n",
       "     'end': 565,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF20'},\n",
       "    {'start': 697,\n",
       "     'end': 705,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF21'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 94,\n",
       "     'end': 105,\n",
       "     'text': '|V|',\n",
       "     'latex': '|V|',\n",
       "     'ref_id': None},\n",
       "    {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "    {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None},\n",
       "    {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None},\n",
       "    {'start': 360,\n",
       "     'end': 371,\n",
       "     'text': 'm=5',\n",
       "     'latex': 'm = 5',\n",
       "     'ref_id': None},\n",
       "    {'start': 431,\n",
       "     'end': 442,\n",
       "     'text': 'd h =500',\n",
       "     'latex': 'd_h = 500',\n",
       "     'ref_id': None},\n",
       "    {'start': 463,\n",
       "     'end': 474,\n",
       "     'text': 'K=100',\n",
       "     'latex': 'K = 100',\n",
       "     'ref_id': None},\n",
       "    {'start': 495,\n",
       "     'end': 506,\n",
       "     'text': 'λ p ',\n",
       "     'latex': '\\\\lambda _p',\n",
       "     'ref_id': None},\n",
       "    {'start': 538,\n",
       "     'end': 549,\n",
       "     'text': 'λ p =0.2',\n",
       "     'latex': '\\\\lambda _p=0.2',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Experimental Settings'},\n",
       "  {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 83,\n",
       "     'end': 91,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF40'}],\n",
       "   'eq_spans': [{'start': 240,\n",
       "     'end': 251,\n",
       "     'text': 'p<0.05',\n",
       "     'latex': 'p<0.05',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Results on Our Dataset'},\n",
       "  {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "   'cite_spans': [{'start': 208,\n",
       "     'end': 215,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 260,\n",
       "     'end': 268,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF42'}],\n",
       "   'eq_spans': [{'start': 380,\n",
       "     'end': 391,\n",
       "     'text': 'p<0.05',\n",
       "     'latex': 'p<0.05',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Further Investigation of Our Framework '},\n",
       "  {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "   'cite_spans': [{'start': 33,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 305,\n",
       "     'end': 313,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF43'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Further Investigation of Our Framework '},\n",
       "  {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 250,\n",
       "     'end': 258,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF45'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Case Study'},\n",
       "  {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Conclusions'}],\n",
       " 'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "   'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "   'ref_id': 'EQREF9',\n",
       "   'type': 'equation'},\n",
       "  'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "   'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "   'ref_id': 'EQREF10',\n",
       "   'type': 'equation'},\n",
       "  'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "   'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "   'ref_id': 'EQREF11',\n",
       "   'type': 'equation'},\n",
       "  'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "   'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "   'ref_id': 'EQREF12',\n",
       "   'type': 'equation'},\n",
       "  'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "   'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "   'ref_id': 'EQREF13',\n",
       "   'type': 'equation'},\n",
       "  'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "   'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "   'ref_id': 'EQREF14',\n",
       "   'type': 'equation'},\n",
       "  'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "   'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "   'ref_id': 'EQREF15',\n",
       "   'type': 'equation'},\n",
       "  'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "   'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "   'ref_id': 'EQREF16',\n",
       "   'type': 'equation'},\n",
       "  'EQREF17': {'text': 'R=X d ×X c T',\n",
       "   'latex': 'R = X_d\\\\times X_c^T',\n",
       "   'ref_id': 'EQREF17',\n",
       "   'type': 'equation'},\n",
       "  'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "   'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "   'ref_id': 'EQREF18',\n",
       "   'type': 'equation'},\n",
       "  'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "   'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "   'ref_id': 'EQREF19',\n",
       "   'type': 'equation'},\n",
       "  'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "   'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "   'ref_id': 'EQREF20',\n",
       "   'type': 'equation'},\n",
       "  'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "   'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "   'ref_id': 'EQREF22',\n",
       "   'type': 'equation'},\n",
       "  'FIGREF2': {'text': '1',\n",
       "   'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "   'latex': None,\n",
       "   'ref_id': 'FIGREF2',\n",
       "   'type': 'figure'},\n",
       "  'FIGREF7': {'text': '2',\n",
       "   'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "   'latex': None,\n",
       "   'ref_id': 'FIGREF7',\n",
       "   'type': 'figure'},\n",
       "  'TABREF40': {'text': '1',\n",
       "   'caption': 'Summarization performance.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF40',\n",
       "   'type': 'table'},\n",
       "  'TABREF42': {'text': '2',\n",
       "   'caption': 'Further investigation of RAVAESum.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF42',\n",
       "   'type': 'table'},\n",
       "  'TABREF43': {'text': '3',\n",
       "   'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF43',\n",
       "   'type': 'table'},\n",
       "  'TABREF45': {'text': '4',\n",
       "   'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF45',\n",
       "   'type': 'table'},\n",
       "  'TABREF46': {'text': '5',\n",
       "   'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF46',\n",
       "   'type': 'table'},\n",
       "  'SECREF1': {'text': 'Introduction',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF1',\n",
       "   'type': 'section'},\n",
       "  'SECREF2': {'text': 'Framework',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF2',\n",
       "   'type': 'section'},\n",
       "  'SECREF6': {'text': 'Conclusions',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF6',\n",
       "   'type': 'section'},\n",
       "  'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF8',\n",
       "   'type': 'section'},\n",
       "  'SECREF21': {'text': 'Summary Construction',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF21',\n",
       "   'type': 'section'},\n",
       "  'SECREF3': {'text': 'Data Description',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF3',\n",
       "   'type': 'section'},\n",
       "  'SECREF24': {'text': 'Background',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF24',\n",
       "   'type': 'section'},\n",
       "  'SECREF26': {'text': 'Data Collection',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF26',\n",
       "   'type': 'section'},\n",
       "  'SECREF28': {'text': 'Data Properties',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF28',\n",
       "   'type': 'section'},\n",
       "  'SECREF4': {'text': 'Experimental Setup',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF4',\n",
       "   'type': 'section'},\n",
       "  'SECREF29': {'text': 'Dataset and Metrics',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF29',\n",
       "   'type': 'section'},\n",
       "  'SECREF31': {'text': 'Comparative Methods',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF31',\n",
       "   'type': 'section'},\n",
       "  'SECREF37': {'text': 'Experimental Settings',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF37',\n",
       "   'type': 'section'},\n",
       "  'SECREF5': {'text': 'Results and Discussions',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF5',\n",
       "   'type': 'section'},\n",
       "  'SECREF39': {'text': 'Results on Our Dataset',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF39',\n",
       "   'type': 'section'},\n",
       "  'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF41',\n",
       "   'type': 'section'},\n",
       "  'SECREF44': {'text': 'Case Study',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF44',\n",
       "   'type': 'section'},\n",
       "  'SECREF7': {'text': 'Topics',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF7',\n",
       "   'type': 'section'}},\n",
       " 'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "   'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "   'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "    {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'ACL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1587--1597',\n",
       "   'other_ids': {},\n",
       "   'links': '8377315'},\n",
       "  'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "   'title': 'Linear programming 1: introduction',\n",
       "   'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "    {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "   'year': 2006,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '53739754'},\n",
       "  'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "   'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "   'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "   'year': 2004,\n",
       "   'venue': 'EMNLP',\n",
       "   'volume': '4',\n",
       "   'issn': '',\n",
       "   'pages': '365--371',\n",
       "   'other_ids': {},\n",
       "   'links': '10418456'},\n",
       "  'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "   'title': 'Multi-document summarization by sentence extraction',\n",
       "   'authors': [{'first': 'Jade',\n",
       "     'middle': [],\n",
       "     'last': 'Goldstein',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "    {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "    {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "   'year': 2000,\n",
       "   'venue': 'NAACL-ANLPWorkshop',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '40--48',\n",
       "   'other_ids': {},\n",
       "   'links': '8294822'},\n",
       "  'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "   'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "   'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "    {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "    {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "   'year': 2008,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '291--298',\n",
       "   'other_ids': {},\n",
       "   'links': '13723748'},\n",
       "  'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "   'title': 'Adam: A method for stochastic optimization',\n",
       "   'authors': [{'first': 'Diederik',\n",
       "     'middle': [],\n",
       "     'last': 'Kingma',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '6628106'},\n",
       "  'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "   'title': 'Auto-encoding variational bayes',\n",
       "   'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "    {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '15789289'},\n",
       "  'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "   'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'IJCAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1270--1276',\n",
       "   'other_ids': {},\n",
       "   'links': '14777460'},\n",
       "  'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "   'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'AAAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '3497--3503',\n",
       "   'other_ids': {},\n",
       "   'links': '29562039'},\n",
       "  'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "   'title': 'Effective approaches to attention-based neural machine translation',\n",
       "   'authors': [{'first': 'Minh-Thang',\n",
       "     'middle': [],\n",
       "     'last': 'Luong',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "    {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'EMNLP',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1412--1421',\n",
       "   'other_ids': {},\n",
       "   'links': '1998416'},\n",
       "  'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "   'title': 'Textrank: Bringing order into texts',\n",
       "   'authors': [{'first': 'Rada',\n",
       "     'middle': [],\n",
       "     'last': 'Mihalcea',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "   'year': 2004,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '577937'},\n",
       "  'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "   'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "   'authors': [{'first': 'Yen',\n",
       "     'middle': ['Kan'],\n",
       "     'last': 'Ziheng Lin Min',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'COLING',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '2093--2108',\n",
       "   'other_ids': {},\n",
       "   'links': '6317274'},\n",
       "  'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "   'title': 'A survey of text summarization techniques',\n",
       "   'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "    {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'Mining Text Data',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '43--76',\n",
       "   'other_ids': {},\n",
       "   'links': '556431'},\n",
       "  'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "   'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "   'authors': [{'first': 'Hongyan',\n",
       "     'middle': [],\n",
       "     'last': 'Dragomir R Radev',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "   'year': 2000,\n",
       "   'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '21--30',\n",
       "   'other_ids': {},\n",
       "   'links': '1320'},\n",
       "  'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "   'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "   'authors': [{'first': 'Danilo',\n",
       "     'middle': [],\n",
       "     'last': 'Jimenez Rezende',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "    {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1278--1286',\n",
       "   'other_ids': {},\n",
       "   'links': '16895865'},\n",
       "  'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "   'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "   'authors': [{'first': 'Mark',\n",
       "     'middle': [],\n",
       "     'last': 'Wasson',\n",
       "     'suffix': ''}],\n",
       "   'year': 1998,\n",
       "   'venue': 'ACL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1364--1368',\n",
       "   'other_ids': {},\n",
       "   'links': '12681629'},\n",
       "  'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "   'title': 'Multiple aspect summarization using integer linear programming',\n",
       "   'authors': [{'first': 'Kristian',\n",
       "     'middle': [],\n",
       "     'last': 'Woodsend',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'EMNLP-CNLL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '233--243',\n",
       "   'other_ids': {},\n",
       "   'links': '17497992'},\n",
       "  'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "   'title': 'Social context summarization',\n",
       "   'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "    {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "    {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "    {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "    {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '255--264',\n",
       "   'other_ids': {},\n",
       "   'links': '704517'}}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_with_sect_latex = dict()\n",
    "for article in all_articles:\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']: \n",
    "        for sections in article['latex_parse']['body_text']:\n",
    "            if sections['section']:\n",
    "                if article['paper_id'] in article_with_sect_latex:\n",
    "                    article_with_sect_latex[article['paper_id']] +=1\n",
    "                else:\n",
    "                    article_with_sect_latex[article['paper_id']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Количество статей, в которых есть названия секций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3502"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_with_sect_latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сравнение количества выделенных ссылок в grobid_parse & latex_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 16050464\n",
      "96 173990592\n",
      "273 29245285\n",
      "304 100300\n",
      "346 2558\n",
      "504 86813509\n",
      "552 1703535\n",
      "601 14170854\n",
      "651 49358911\n",
      "682 371926\n",
      "696 682772\n",
      "700 2840197\n",
      "735 2411\n",
      "773 16273304\n",
      "827 17511008\n",
      "886 3101294\n",
      "892 5740960\n",
      "920 2145766\n",
      "1062 5079594\n",
      "1135 298504\n",
      "1141 870921\n",
      "1183 53217693\n",
      "1302 1438450\n",
      "1323 309476\n",
      "1324 44278\n",
      "1335 7669927\n",
      "1481 711424\n",
      "1513 1423962\n",
      "1701 370914\n",
      "1743 15600925\n",
      "1758 184488087\n",
      "1811 10086161\n",
      "1866 11492268\n",
      "2028 534431\n",
      "2062 15986631\n",
      "2109 189998202\n",
      "2125 21665312\n",
      "2166 9371149\n",
      "2168 311594\n",
      "2170 6256345\n",
      "2271 14974\n",
      "2287 3933075\n",
      "2339 1571038\n",
      "2345 2862211\n",
      "2351 6210126\n",
      "2553 15881253\n",
      "2630 5201435\n",
      "2666 5054582\n",
      "2754 10324034\n",
      "2826 7021843\n",
      "3006 3152424\n",
      "3077 139106285\n",
      "3094 14922772\n",
      "3095 2652169\n",
      "3120 3204831\n",
      "3180 12245103\n",
      "3218 85543217\n",
      "3230 17297069\n",
      "3279 621025\n",
      "3326 85556928\n",
      "3355 13661068\n",
      "3356 1765384\n",
      "3371 3025759\n",
      "3543 1871596\n",
      "3588 27914547\n",
      "3611 53082704\n",
      "3622 118680003\n",
      "3627 11451871\n",
      "3772 3132651\n",
      "3797 20995314\n",
      "3800 85529973\n",
      "3801 14401063\n",
      "3808 2213896\n",
      "3834 3204935\n",
      "3861 537\n",
      "3971 1031444\n",
      "3989 5112203\n",
      "3996 465\n",
      "4015 174798366\n",
      "4065 52123817\n",
      "4114 102353614\n",
      "4137 4943905\n",
      "4252 3262717\n",
      "4357 2373337\n",
      "4395 195658138\n",
      "4476 84842\n",
      "4592 5848469\n",
      "4596 91183987\n",
      "4717 969555\n",
      "4759 1707814\n",
      "4817 3543642\n",
      "4881 2315102\n",
      "4994 10636826\n",
      "4997 6444309\n",
      "5000 14330405\n",
      "5068 52112703\n",
      "5070 30617432\n",
      "5164 14817153\n",
      "5167 174802484\n",
      "5295 6985080\n",
      "5303 6160155\n",
      "5330 72940921\n",
      "5387 6519154\n",
      "5405 15314162\n",
      "5414 102351489\n",
      "5472 174802490\n",
      "5576 119190241\n",
      "5714 53080846\n",
      "5722 6508854\n",
      "5841 174798343\n",
      "5850 5062433\n",
      "5860 4328410\n",
      "6063 3115914\n",
      "6075 5536079\n",
      "6111 173188095\n",
      "6115 85518248\n",
      "6128 11805625\n",
      "6135 3725815\n",
      "6201 12051021\n",
      "6202 102353391\n",
      "6218 21678954\n",
      "6304 49865798\n",
      "6322 52009840\n",
      "6384 10694414\n",
      "6414 155091369\n",
      "6456 6972699\n",
      "6625 195218358\n",
      "6657 1175070\n",
      "6698 29622196\n",
      "6705 6278197\n",
      "6724 2162648\n",
      "6750 10182773\n",
      "6823 1138221\n",
      "6827 2367456\n",
      "6907 384520\n",
      "7007 2476229\n",
      "7103 160010264\n",
      "7120 8703520\n",
      "7168 278288\n",
      "7233 1477127\n",
      "7298 13533556\n",
      "7339 174797774\n",
      "7352 1662415\n",
      "7452 53083054\n",
      "7475 102351751\n",
      "7550 8821211\n",
      "7710 13949438\n",
      "7725 7105713\n",
      "7818 2782776\n",
      "7881 2612150\n",
      "7948 11022639\n",
      "8156 1076\n",
      "8185 6188348\n",
      "8193 1461182\n",
      "8214 18733074\n",
      "8318 57759363\n",
      "8366 8314118\n",
      "8449 52126537\n",
      "8508 14036650\n",
      "8583 1696837\n",
      "8616 9884935\n",
      "8752 8932111\n",
      "8755 1931472\n",
      "8766 6981674\n",
      "8874 13699041\n",
      "8880 10159938\n",
      "8913 1618800\n",
      "8957 1572802\n",
      "8987 7250091\n",
      "9004 184487983\n",
      "9025 2783746\n",
      "9049 1065088\n",
      "9238 159041465\n",
      "9313 102354684\n",
      "9326 56657817\n",
      "9331 15412473\n",
      "9338 1356419\n",
      "9460 5357119\n",
      "9571 51874724\n",
      "9572 6612964\n",
      "9609 119284742\n",
      "9727 44083182\n",
      "9855 1171264\n",
      "10024 18176945\n",
      "10080 60121\n",
      "10116 1057469\n",
      "10163 1148525\n",
      "10171 5370615\n",
      "10319 67856712\n",
      "10364 9889475\n",
      "10516 13742419\n",
      "10638 11113728\n",
      "10641 7156590\n",
      "10646 13530374\n",
      "10696 52281331\n",
      "10801 2468783\n",
      "10808 118716003\n",
      "10871 8451212\n",
      "10889 17646396\n",
      "10942 2061521\n",
      "11141 3157058\n",
      "11187 2556\n",
      "11219 8495258\n",
      "11224 6462501\n",
      "11230 172552\n",
      "11257 3263890\n",
      "11267 2360770\n",
      "11290 14656950\n",
      "11318 397556\n",
      "11319 182953113\n",
      "11342 46938852\n",
      "11370 3266250\n",
      "11371 5247929\n",
      "11522 2328808\n",
      "11658 5574231\n",
      "11826 2414511\n",
      "11923 9345583\n",
      "11951 7940109\n",
      "11974 10458880\n",
      "12010 5039697\n",
      "12086 195750470\n",
      "12183 12245213\n",
      "12226 67856532\n",
      "12241 398440\n",
      "12283 10128510\n",
      "12291 189898046\n",
      "12319 386957\n",
      "12355 5430734\n",
      "12375 52153793\n",
      "12494 14043792\n",
      "12496 3265566\n",
      "12578 67855494\n",
      "12584 3264822\n",
      "12588 46935811\n",
      "12599 102350941\n",
      "12642 7130747\n",
      "12696 10017527\n",
      "12706 989572\n",
      "12850 44084020\n",
      "12930 118674834\n",
      "12987 2868247\n",
      "12993 15690223\n",
      "13037 6478896\n",
      "13142 211597\n",
      "13183 59910\n",
      "13219 195873924\n",
      "13248 53295957\n",
      "13378 19662556\n",
      "13442 195345703\n",
      "13487 14081838\n",
      "13509 2574224\n",
      "13572 14151217\n",
      "13629 14434979\n",
      "13643 52074232\n",
      "13659 15124020\n",
      "13689 67855882\n",
      "13715 6821575\n",
      "13728 162183964\n",
      "13739 1353980\n",
      "13754 12147910\n",
      "13811 46939562\n",
      "13852 15065468\n",
      "13855 4993665\n",
      "13932 311942\n",
      "13976 22615716\n",
      "13977 52112576\n",
      "14036 2395426\n",
      "14064 9752897\n",
      "14195 12075649\n",
      "14237 2704974\n",
      "14281 52171279\n",
      "14324 174797999\n",
      "14375 102350691\n",
      "14380 665441\n",
      "14393 58116\n",
      "14444 6506243\n",
      "14469 2734446\n",
      "14482 7343584\n",
      "14490 2300310\n",
      "14557 3266340\n",
      "14560 5484698\n",
      "14561 6670611\n",
      "14636 2680887\n",
      "14707 119298411\n",
      "14716 186206974\n",
      "14783 11904633\n",
      "14900 13375869\n",
      "14926 8555434\n",
      "14937 9134916\n",
      "14941 1196960\n",
      "14986 52098907\n",
      "15039 39487\n",
      "15187 12615602\n",
      "15196 184488346\n",
      "15212 9763372\n",
      "15218 4528610\n",
      "15313 2957\n",
      "15381 2717698\n",
      "15397 6509354\n",
      "15401 182952524\n",
      "15495 1440229\n",
      "15546 195886443\n",
      "15604 44009215\n",
      "15617 207754\n",
      "15631 29170917\n",
      "15643 15201331\n",
      "15695 3263993\n",
      "15717 6694311\n",
      "15761 14008742\n",
      "15820 83458688\n",
      "15923 8749656\n",
      "15974 1282\n",
      "16029 52158174\n",
      "16035 381439\n",
      "16119 195584301\n",
      "16326 3546207\n",
      "16395 5586\n",
      "16474 776002\n",
      "16484 15326443\n",
      "16519 14909391\n",
      "16538 49430466\n",
      "16657 16538528\n",
      "16760 52160496\n",
      "16786 1918127\n",
      "16793 5034059\n",
      "16869 14874026\n",
      "16998 3549266\n",
      "17144 8074746\n",
      "17185 182952390\n",
      "17215 76666127\n",
      "17275 979\n",
      "17305 4438009\n",
      "17337 14321437\n",
      "17389 53645946\n",
      "17424 3199424\n",
      "17431 53025882\n",
      "17476 7212475\n",
      "17518 13214003\n",
      "17651 1488466\n",
      "17657 198980526\n",
      "17698 11157751\n",
      "17744 598478\n",
      "18059 2687883\n",
      "18090 5983351\n",
      "18162 12744871\n",
      "18171 7807216\n",
      "18187 594\n",
      "18262 1964946\n",
      "18314 13691704\n",
      "18358 12851711\n",
      "18448 184486914\n",
      "18467 195069392\n",
      "18468 52058047\n",
      "18500 3576631\n",
      "18652 1040\n",
      "18773 67856188\n",
      "19016 5905515\n",
      "19100 195750836\n",
      "19135 8688235\n",
      "19324 174801519\n",
      "19335 168170148\n",
      "19405 10549256\n",
      "19472 52962051\n",
      "19541 3008394\n",
      "19606 46985924\n",
      "19607 926149\n",
      "19661 645515\n",
      "19726 9150889\n",
      "19791 1210515\n",
      "19835 1733985\n",
      "19846 894481\n",
      "19914 1321\n",
      "19935 155089628\n",
      "19944 8395799\n",
      "19988 21717477\n",
      "19992 9206785\n",
      "20024 13753765\n",
      "20073 5590763\n",
      "20088 434304\n",
      "20261 102350747\n",
      "20416 195791725\n",
      "20566 48358519\n",
      "20673 21669304\n",
      "20737 51877905\n",
      "20755 14544878\n",
      "20813 14527730\n",
      "20986 174798022\n",
      "21006 3231502\n",
      "21028 17844260\n",
      "21159 1277731\n",
      "21347 12317426\n",
      "21409 2014049\n",
      "21425 195886070\n",
      "21468 77529\n",
      "21480 19603477\n",
      "21525 6278207\n",
      "21540 11054466\n",
      "21593 14651385\n",
      "21690 29051190\n",
      "21739 52281487\n",
      "21755 8940645\n",
      "21943 57721278\n",
      "22024 13444726\n",
      "22030 153313061\n",
      "22151 18193214\n",
      "22163 174797779\n",
      "22189 21731209\n",
      "22232 174798018\n",
      "22312 44127108\n",
      "22343 10479248\n",
      "22355 8894136\n",
      "22403 198229871\n",
      "22427 173188049\n",
      "22460 102352781\n",
      "22538 195345550\n",
      "22613 8409243\n",
      "22686 186206883\n",
      "22803 14275144\n",
      "22821 1198964\n",
      "22840 2382442\n",
      "22885 49525534\n",
      "22916 85518425\n",
      "22968 6946103\n",
      "23006 10553280\n",
      "23010 6667804\n",
      "23019 14131077\n",
      "23029 6618571\n",
      "23036 182952931\n",
      "23190 998001\n",
      "23224 5527031\n",
      "23226 14908221\n",
      "23262 21697648\n",
      "23263 2329174\n",
      "23294 5591459\n",
      "23344 44019606\n",
      "23350 88524743\n",
      "23449 989721\n",
      "23501 1100249\n",
      "23573 196170479\n",
      "23612 15980568\n",
      "23712 7480074\n",
      "23755 6880969\n",
      "23825 195700014\n",
      "23851 9164227\n",
      "23878 2098562\n",
      "23966 195064954\n",
      "24106 13747175\n",
      "24202 15870937\n",
      "24246 7682221\n",
      "24308 9716222\n",
      "24352 9683221\n",
      "24354 10565222\n",
      "24362 9777884\n",
      "24396 7833469\n",
      "24415 8699915\n",
      "24438 3860960\n",
      "24480 67856193\n",
      "24514 13624075\n",
      "24523 102350997\n",
      "24556 129945536\n",
      "24565 6315454\n",
      "24580 102485992\n",
      "24589 2012188\n",
      "24663 3206604\n",
      "24705 16173223\n",
      "24735 4941839\n",
      "24780 3074496\n",
      "24786 12829285\n",
      "24849 156053191\n",
      "25021 7586460\n",
      "25043 202660972\n",
      "25068 53034346\n",
      "25089 6157443\n",
      "25179 15827373\n",
      "25204 52138366\n",
      "25208 2131122\n",
      "25228 2202801\n",
      "25260 7421176\n",
      "25327 3264135\n",
      "25573 2075553\n",
      "25608 51868339\n",
      "25693 27191574\n",
      "25696 1918428\n",
      "25710 53181926\n",
      "25776 174803587\n",
      "25817 170078852\n",
      "25818 18706304\n",
      "25846 5029790\n",
      "25882 2209093\n",
      "25910 5332396\n",
      "25920 402805\n",
      "25982 174799410\n",
      "25989 12304778\n",
      "26103 11642690\n",
      "26167 6116433\n",
      "26217 46918664\n",
      "26267 10460485\n",
      "26335 15818617\n",
      "26361 52943910\n",
      "26377 6217983\n",
      "26396 13115581\n",
      "26482 8661576\n",
      "26496 1241421\n",
      "26519 3882054\n",
      "26565 652921\n",
      "26602 195069169\n",
      "26670 13313668\n",
      "26689 12921197\n",
      "26778 12742243\n",
      "26798 1045\n",
      "26836 159041722\n",
      "26908 13750296\n",
      "26985 13694466\n",
      "27024 2499521\n",
      "27027 1253333\n",
      "27094 1689426\n",
      "27123 6528378\n",
      "27125 119117163\n",
      "27271 1768042\n",
      "27426 52171904\n",
      "27432 174799480\n",
      "27463 1173840\n",
      "27555 189999604\n",
      "27591 9204815\n",
      "27647 3718988\n",
      "27692 6359641\n",
      "27710 1516923\n",
      "27750 14451951\n",
      "27777 85517978\n",
      "27855 201632901\n",
      "27874 371\n",
      "27903 61629\n",
      "27912 16484912\n",
      "27952 3266594\n",
      "27999 8506049\n",
      "28067 51875355\n",
      "28079 13951114\n",
      "28220 1747018\n",
      "28231 52802182\n",
      "28252 3264213\n",
      "28265 67855733\n",
      "28317 155100086\n",
      "28358 52136564\n",
      "28426 2075057\n",
      "28454 12203802\n",
      "28516 58302\n",
      "28540 139105363\n",
      "28642 63346\n",
      "28694 4792796\n",
      "28748 1144073\n",
      "28756 498\n",
      "28770 9564958\n",
      "28830 31924\n",
      "28924 52124023\n",
      "29020 2616110\n",
      "29029 196181734\n",
      "29069 1929239\n",
      "29185 2104869\n",
      "29189 1437\n",
      "29249 3068831\n",
      "29252 14039866\n",
      "29257 102350748\n",
      "29292 159041758\n",
      "29295 48353722\n",
      "29301 13508500\n",
      "29309 3202289\n",
      "29316 14727144\n",
      "29537 29162884\n",
      "29560 51838647\n",
      "29565 8443958\n",
      "29619 9471964\n",
      "29788 10480989\n",
      "30013 2422102\n",
      "30091 2740527\n",
      "30187 3089175\n",
      "30239 120374210\n",
      "30287 56976\n",
      "30325 12847003\n",
      "30415 80628248\n",
      "30580 9326499\n",
      "30642 512323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30643 1746246\n",
      "30673 3191956\n",
      "30681 21015570\n",
      "30780 9174081\n",
      "30807 173990523\n",
      "30851 52155263\n",
      "30875 10250472\n",
      "30900 84842989\n",
      "30904 3265541\n",
      "30961 195750811\n",
      "31029 29151507\n",
      "31155 3265262\n",
      "31296 6008960\n",
      "31300 174802477\n",
      "31401 13888952\n",
      "31406 3380653\n",
      "31431 85518027\n",
      "31488 195776133\n",
      "31494 14980132\n",
      "31503 59986\n",
      "31640 15620570\n",
      "31748 12087925\n",
      "31757 395839\n",
      "31776 7116029\n",
      "31866 718342\n",
      "31872 52169534\n",
      "31943 49881509\n",
      "31945 932197\n",
      "31975 10009142\n",
      "32015 47019063\n",
      "32156 3205220\n",
      "32219 3266611\n",
      "32251 102350939\n",
      "32277 174798275\n",
      "32316 1119356\n",
      "32320 14519034\n",
      "32400 53082542\n",
      "32462 53217060\n",
      "32550 155093144\n",
      "32741 48354032\n",
      "32783 195345047\n",
      "32815 16960682\n",
      "32873 744471\n",
      "32920 1900253\n",
      "33043 102352298\n",
      "33094 52111211\n",
      "33255 9573708\n",
      "33291 198985976\n",
      "33338 1373479\n",
      "33388 14586568\n",
      "33445 84841767\n",
      "33464 167217880\n",
      "33493 189762439\n",
      "33602 60368\n",
      "33707 24609417\n",
      "33718 216107\n",
      "33753 5151070\n",
      "33811 53590103\n",
      "33850 14425690\n",
      "33877 51877560\n",
      "33896 10619801\n",
      "33917 8233374\n",
      "34056 608\n",
      "34139 2522459\n",
      "34185 53092624\n",
      "34200 3426453\n",
      "34250 131773668\n",
      "34331 14841563\n",
      "34340 9662991\n",
      "34357 111399\n",
      "34358 298145\n",
      "34613 2593903\n",
      "34695 153312586\n",
      "34703 5823158\n",
      "34711 6237722\n",
      "34774 12203896\n",
      "34898 182953211\n",
      "34946 173188167\n",
      "34993 186206852\n",
      "35057 1712853\n",
      "35067 52967399\n",
      "35077 7205411\n",
      "35130 196179173\n",
      "35258 6238748\n",
      "35339 9751558\n",
      "35424 9385494\n",
      "35528 383\n",
      "35552 1499080\n",
      "35634 14624362\n",
      "35651 3205890\n",
      "35693 165163819\n",
      "35699 1598703\n",
      "35770 148\n",
      "35785 14068440\n",
      "35825 905565\n",
      "35837 29161506\n",
      "35857 102352962\n",
      "35872 622152\n",
      "35874 456\n",
      "35955 173990158\n",
      "35963 44155085\n",
      "35971 5999791\n",
      "36034 9286820\n",
      "36046 16087821\n",
      "36127 184487889\n",
      "36137 174799374\n",
      "36179 195699881\n",
      "36203 182953261\n",
      "36208 58007087\n",
      "36209 94285\n",
      "36242 5620421\n",
      "36324 37390552\n",
      "36331 9911858\n",
      "36440 5087912\n",
      "36628 12176567\n",
      "36646 49901137\n",
      "36653 189898081\n",
      "36735 67856413\n",
      "36756 199501730\n",
      "36786 13686145\n",
      "36862 9707387\n",
      "36888 52943615\n",
      "36939 4955787\n",
      "37046 49544037\n",
      "37144 189898\n",
      "37174 32228701\n",
      "37180 196211654\n",
      "37242 195767436\n",
      "37316 16126936\n",
      "37318 4314399\n",
      "37448 7306663\n",
      "37509 1513472\n",
      "37539 189928358\n",
      "37582 140210852\n",
      "37838 4940747\n",
      "38176 1868\n",
      "38186 13747110\n",
      "38217 1045792\n",
      "38229 15659560\n",
      "38250 5836739\n",
      "38260 3226120\n",
      "38323 6825479\n",
      "38336 14593464\n",
      "38376 1240\n",
      "38398 6713452\n",
      "38537 375478\n",
      "38641 7928230\n",
      "38692 10085501\n",
      "38702 186206964\n",
      "38715 5033497\n",
      "38866 6360322\n",
      "38965 111430\n",
      "38987 184487062\n",
      "38995 6204420\n",
      "39078 52124999\n",
      "39083 5213476\n",
      "39133 173188642\n",
      "39188 3264132\n"
     ]
    }
   ],
   "source": [
    "latex_more_grobid_bib_entr = []\n",
    "grobid_more_latex_bib_entr = []\n",
    "equal = []\n",
    "for num_art ,article in enumerate(all_articles):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        if len(article['latex_parse']['bib_entries']) > len(article['grobid_parse']['bib_entries']):\n",
    "            print(num_art,article['paper_id'])\n",
    "            latex_more_grobid_bib_entr.append(article['paper_id'])\n",
    "        elif len(article['latex_parse']['bib_entries']) < len(article['grobid_parse']['bib_entries']):\n",
    "#             print(num_art,article['paper_id'])\n",
    "            grobid_more_latex_bib_entr.append(article['paper_id'])\n",
    "        else:\n",
    "            equal.append(article['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739, 2480, 649)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(latex_more_grobid_bib_entr),len(grobid_more_latex_bib_entr),len(equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3868"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "739+2480+649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 18| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 9\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 23| grobid_links = 32| together = 33\n",
      "====================\n",
      "latex_links = 21| grobid_links = 21| together = 21\n",
      "====================\n",
      "latex_links = 0| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 50| grobid_links = 46| together = 51\n",
      "====================\n",
      "latex_links = 9| grobid_links = 38| together = 37\n",
      "====================\n",
      "latex_links = 27| grobid_links = 13| together = 30\n",
      "====================\n",
      "latex_links = 31| grobid_links = 27| together = 32\n",
      "====================\n",
      "latex_links = 25| grobid_links = 28| together = 31\n",
      "====================\n",
      "latex_links = 12| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 24| grobid_links = 30| together = 32\n",
      "====================\n",
      "latex_links = 4| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 10| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 31| grobid_links = 31| together = 32\n",
      "====================\n",
      "latex_links = 0| grobid_links = 22| together = 21\n",
      "====================\n",
      "latex_links = 43| grobid_links = 41| together = 45\n",
      "====================\n",
      "latex_links = 28| grobid_links = 26| together = 28\n",
      "====================\n",
      "latex_links = 0| grobid_links = 8| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 17| together = 19\n",
      "====================\n",
      "latex_links = 2| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 48| grobid_links = 44| together = 48\n",
      "====================\n",
      "latex_links = 3| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 25| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 28| grobid_links = 36| together = 48\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 29| grobid_links = 30| together = 30\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 7| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 9| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 14| grobid_links = 12| together = 14\n",
      "====================\n",
      "latex_links = 19| grobid_links = 48| together = 49\n",
      "====================\n",
      "latex_links = 12| grobid_links = 12| together = 13\n",
      "====================\n",
      "latex_links = 7| grobid_links = 5| together = 6\n",
      "====================\n",
      "latex_links = 17| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 7| grobid_links = 18| together = 20\n",
      "====================\n",
      "latex_links = 12| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 28| grobid_links = 43| together = 47\n",
      "====================\n",
      "latex_links = 10| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 10| grobid_links = 11| together = 12\n",
      "====================\n",
      "latex_links = 14| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 10| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 5| grobid_links = 11| together = 13\n",
      "====================\n",
      "latex_links = 9| grobid_links = 6| together = 10\n",
      "====================\n",
      "latex_links = 20| grobid_links = 20| together = 20\n",
      "====================\n",
      "latex_links = 0| grobid_links = 12| together = 12\n",
      "====================\n",
      "latex_links = 18| grobid_links = 22| together = 23\n",
      "====================\n",
      "latex_links = 20| grobid_links = 21| together = 26\n",
      "====================\n",
      "latex_links = 10| grobid_links = 28| together = 28\n",
      "====================\n",
      "latex_links = 28| grobid_links = 23| together = 28\n",
      "====================\n",
      "latex_links = 7| grobid_links = 7| together = 8\n",
      "====================\n",
      "latex_links = 19| grobid_links = 20| together = 21\n",
      "====================\n",
      "latex_links = 37| grobid_links = 43| together = 45\n",
      "====================\n",
      "latex_links = 2| grobid_links = 22| together = 22\n",
      "====================\n",
      "latex_links = 13| grobid_links = 21| together = 24\n",
      "====================\n",
      "latex_links = 26| grobid_links = 28| together = 29\n",
      "====================\n",
      "latex_links = 9| grobid_links = 8| together = 12\n",
      "====================\n",
      "latex_links = 0| grobid_links = 10| together = 10\n",
      "====================\n",
      "latex_links = 14| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 22| grobid_links = 40| together = 41\n",
      "====================\n",
      "latex_links = 0| grobid_links = 0| together = 0\n",
      "====================\n",
      "latex_links = 13| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 39| grobid_links = 34| together = 39\n",
      "====================\n",
      "latex_links = 27| grobid_links = 27| together = 27\n",
      "====================\n",
      "latex_links = 23| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 6| grobid_links = 5| together = 7\n",
      "====================\n",
      "latex_links = 36| grobid_links = 47| together = 48\n",
      "====================\n",
      "latex_links = 24| grobid_links = 21| together = 25\n",
      "====================\n",
      "latex_links = 36| grobid_links = 37| together = 37\n",
      "====================\n",
      "latex_links = 22| grobid_links = 19| together = 23\n",
      "====================\n",
      "latex_links = 3| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 21| grobid_links = 15| together = 21\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 35\n",
      "====================\n",
      "latex_links = 25| grobid_links = 24| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 32| together = 32\n",
      "====================\n",
      "latex_links = 17| grobid_links = 26| together = 27\n",
      "====================\n",
      "latex_links = 23| grobid_links = 23| together = 23\n",
      "====================\n",
      "latex_links = 13| grobid_links = 12| together = 15\n",
      "====================\n",
      "latex_links = 22| grobid_links = 21| together = 22\n",
      "====================\n",
      "latex_links = 17| grobid_links = 25| together = 25\n",
      "====================\n",
      "latex_links = 51| grobid_links = 36| together = 53\n",
      "====================\n",
      "latex_links = 5| grobid_links = 13| together = 13\n",
      "====================\n",
      "latex_links = 24| grobid_links = 38| together = 38\n",
      "====================\n",
      "latex_links = 14| grobid_links = 14| together = 14\n",
      "====================\n",
      "latex_links = 9| grobid_links = 10| together = 11\n",
      "====================\n",
      "latex_links = 23| grobid_links = 9| together = 24\n",
      "====================\n",
      "latex_links = 6| grobid_links = 16| together = 16\n",
      "====================\n",
      "latex_links = 18| grobid_links = 39| together = 44\n",
      "====================\n",
      "latex_links = 23| grobid_links = 21| together = 23\n",
      "====================\n",
      "latex_links = 13| grobid_links = 9| together = 14\n",
      "====================\n",
      "latex_links = 52| grobid_links = 34| together = 52\n",
      "====================\n",
      "latex_links = 30| grobid_links = 47| together = 50\n",
      "====================\n",
      "latex_links = 39| grobid_links = 39| together = 42\n",
      "====================\n",
      "latex_links = 9| grobid_links = 4| together = 9\n",
      "====================\n",
      "latex_links = 3| grobid_links = 16| together = 17\n",
      "====================\n",
      "latex_links = 18| grobid_links = 24| together = 25\n",
      "====================\n",
      "latex_links = 8| grobid_links = 19| together = 20\n",
      "====================\n",
      "latex_links = 34| grobid_links = 34| together = 34\n",
      "====================\n",
      "latex_links = 30| grobid_links = 27| together = 30\n",
      "====================\n",
      "latex_links = 14| grobid_links = 25| together = 26\n",
      "====================\n",
      "latex_links = 0| grobid_links = 36| together = 36\n",
      "====================\n",
      "latex_links = 16| grobid_links = 15| together = 16\n",
      "====================\n",
      "latex_links = 30| grobid_links = 31| together = 31\n",
      "====================\n",
      "latex_links = 57| grobid_links = 60| together = 63\n",
      "====================\n",
      "latex_links = 5| grobid_links = 1| together = 5\n",
      "====================\n",
      "latex_links = 0| grobid_links = 35| together = 35\n",
      "====================\n",
      "latex_links = 10| grobid_links = 4| together = 10\n",
      "====================\n",
      "latex_links = 40| grobid_links = 40| together = 39\n",
      "====================\n",
      "latex_links = 3| grobid_links = 3| together = 4\n",
      "====================\n",
      "latex_links = 13| grobid_links = 4| together = 13\n",
      "====================\n",
      "latex_links = 40| grobid_links = 42| together = 44\n",
      "====================\n",
      "latex_links = 9| grobid_links = 6| together = 10\n",
      "====================\n",
      "latex_links = 20| grobid_links = 19| together = 20\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latex_links = 32| grobid_links = 40| together = 42\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for num_art ,article in enumerate(all_articles):\n",
    "    if num_art > 1000:\n",
    "        break\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        latex_links = [v['links'] for k,v in article['latex_parse']['bib_entries'].items() if v['links']]\n",
    "        if article['grobid_parse']:\n",
    "            grobid_links =  [v['links'] for k,v in article['grobid_parse']['bib_entries'].items() if v['links']]\n",
    "            if len(latex_links)> 0 and len(grobid_links) == 0:\n",
    "                print('WOW!')\n",
    "            print(f'latex_links = {len(latex_links)}| grobid_links = {len(grobid_links)}| together = {len(set(grobid_links+latex_links))}')\n",
    "            print(10*'==')\n",
    "\n",
    "        else:\n",
    "            print(f'latex_links = {len(latex_links)}| grobid_links = 0| together = {len(set(latex_links))}')\n",
    "            print(10*'==')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка ситуации, когда есть **latex_parse**, но  нет **grobid_parse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_art ,article in enumerate(all_articles):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        if len(article['grobid_parse']['body_text'])==0:\n",
    "            print(num_art,articcle['paper_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не бывает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выделение обзорной части статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Самый простой принцип построения - ***по максимальному количеству ссылок в абзаце***:\n",
    "<br> **(СМОТРИ В collect ACL papers-evaluate covering overview titiles_16_05 & collect ACL papers-evaluate covering overview titiles_8_05)**\n",
    " - **Решение**:\n",
    "    - подсчитать количество ссылок в каждой секции\n",
    "    - выбрать секцию с максимальным количеством ссылок (возможно ещё оставить ещё 1 секцию, в которой количество ссылок было больше половины чем в максимальной)\n",
    "    - для latex статей надо объединить текст одинаковых секций в 1 абзац\n",
    " - **Критерий**:\n",
    "    -  в части latex публикаций есть названия секций => после выделения обзорных часте можно посмотреть какие секции выделились: какие топ-3, сделать просмотр глазами и после этого решать что делать дальше.\n",
    "    - возможно логично сохранять для 2 максимального текста название статей\n",
    "\n",
    "2. Сделать ***ML - модель на признаках***:\n",
    " - **Сбор выборки**: (считаем признаки для каждой секции статьи - наша выборка)\n",
    "     - подсчитать $\\text{густота ссылок} =  \\frac{\\text{кол-во ссылок}}{\\text{длина абзаца}}$\n",
    "     - подсчитать кол-во непрерывных предложений, в которых есть хотя бы 1 ссылка\n",
    "     - расположение секции в документе (мб нормализовать)\n",
    "     - усреднение позиции in-line ссылок в каждой секции\n",
    " - **Сбор таргета**: (для статей с latex)\n",
    "     - вытаскиваем название секции как колонку \n",
    " - **Формирование финальной таблицы**:\n",
    "     - объединяем таблицы\n",
    "     - в зависимости от наличия в секции: RW, background, overview - назначаем 1 или 0\n",
    "     - разделяем выборку на train & test\n",
    " - **Обучаем различные классификаторы и смотрим качество**:\n",
    "     - LogReg\n",
    "     - Xgboost\n",
    "     - DT\n",
    "     - RF\n",
    "     - SVM\n",
    " - Также мб посмотреть если делаем учет по топ 2 секциям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сбор выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['abstract', 'body_text', 'ref_entries', 'bib_entries'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['latex_parse'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers[all_articles[0]['paper_id']] = {\n",
    "    'paper_id':all_articles[0]['paper_id'],   'metadata':all_articles[0]['metadata'],\n",
    "    's2_pdf_hash':all_articles[0]['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10164018': {'paper_id': '10164018',\n",
       "  'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "   'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "   'year': '2017',\n",
       "   'arxiv_id': '1708.01065',\n",
       "   'acl_id': 'W17-4512',\n",
       "   'pmc_id': None,\n",
       "   'pubmed_id': None,\n",
       "   'doi': '10.18653/v1/w17-4512',\n",
       "   'venue': 'ArXiv',\n",
       "   'journal': 'ArXiv'},\n",
       "  's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       "  'grobid_parse': None,\n",
       "  'latex_parse': None}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'cite_spans', 'ref_spans', 'eq_spans', 'section'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[0]['grobid_parse']['body_text'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def text2sentences(text):\n",
    "    sentences = []\n",
    "    text = text.replace('?','. ').replace('!','. ').replace('Mt.','Mt').replace('.)','. ').replace('•','').split('.')\n",
    "    #.replace('al.','al ').replace('i.e.','ie').replace('e.g.','eg').replace('...','. ')\n",
    "    for line in text:\n",
    "#         print(line,len(sentences))\n",
    "        if len(sentences) == 0:\n",
    "            if len(line)>25:\n",
    "                sentences.append(line)\n",
    "            elif len(text)>1:\n",
    "                text[1] = text[0]+'.'+ text[1]\n",
    "            else:\n",
    "                continue\n",
    "        elif len(line.split())>2:\n",
    "            if len(line)>2 and (line[0].isupper() or line[1].isupper()):\n",
    "                sentences.append(line)\n",
    "            elif len(line)>4 and ( line[1].isdigit() or line[2].isdigit()) and not (line[0].isdigit() and line[1].isdigit()): #line[0].isdigit() or\n",
    "                new_line = line.replace('(','').replace(')','')\n",
    "                if (new_line[2].isupper() or new_line[3].isupper()):\n",
    "                    sentences.append(line)\n",
    "                else:\n",
    "                    line = '.'+line\n",
    "                    sentences[-1] += line\n",
    "            else:\n",
    "                line = '.'+line\n",
    "                sentences[-1] += line \n",
    "        else:\n",
    "            sentences[-1] += '.'+line\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тестирование разбиения на предложения\n",
    "- **1 Тест**\n",
    "    - посмотреть на предложения, у которых длина маленькая и проверить все ли верно распарсили\n",
    "- **2 Тест**\n",
    "    - посмотреть длинные предложения, в которых много заглавных букв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Тест**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "paper_id = 10164018 | paper_num = 0 \n",
      "[[53, 18, 21, 57, 22, 16, 8, 36, 8, 10, 17, 27, 15, 18, 27, 27, 11, 26, 20, 32, 25, 30, 45, 19, 10, 11, 8, 20, 11, 21, 11, 19, 20], [27, 32, 25, 39, 22], [29, 24, 16, 53, 13, 152, 25, 13, 15, 45, 13, 49, 76, 20, 30, 22, 55, 49, 10, 21, 15, 30, 112, 26, 19, 27, 25, 95, 18, 45, 20], [33, 18, 84, 10, 17, 15, 7, 18, 12], [11, 7], [13, 21, 10, 16, 7, 29, 9, 9, 21, 18, 34, 43, 23], [7, 11, 9, 12, 11, 22, 20, 20, 21, 19, 17, 17, 20, 14, 10, 21, 13, 24, 14, 16, 10], [10, 14, 10, 10, 13, 13, 41, 7], [11, 14, 8], [33, 21, 22, 33, 26, 16, 8, 9, 11, 11], [16, 27, 15, 21, 15, 17, 19], [16, 15, 29, 13, 20, 14, 25, 19, 33, 13, 21, 29], [41, 10, 18, 11, 22, 13, 34], [14, 21, 16, 19, 47]]\n",
      "--------------------------------------------------\n",
      "paper_id = 14472576 | paper_num = 1 \n",
      "[[47, 36, 21, 5, 22], [28, 29, 38, 27, 23, 40, 18, 23, 12, 13, 18, 27, 25, 10, 12, 20, 40, 15, 10, 16, 34, 6, 45, 11, 51, 18, 36, 26, 11, 22, 18], [24, 41, 16, 44, 22, 16, 19, 47, 26, 36, 11, 5, 33, 24, 8, 9, 25, 29, 59], [20, 11, 24, 43, 41, 8, 23, 27, 13, 14, 45, 26], [18, 31, 28, 16, 18, 9, 15, 31, 17, 25, 13, 18, 14, 18, 10, 28, 52, 24, 16, 9, 30, 36, 91, 28, 21, 18, 23, 30, 46, 96], [25, 32, 16, 37, 26, 43, 9, 22, 4, 19, 38, 24, 18, 24, 47, 19, 15, 15, 18, 45, 49, 28], [8, 20, 16, 27, 8, 7, 10, 19, 10, 29, 46], [25, 20, 8, 24, 24, 51, 45, 19, 11, 10, 30, 16, 177, 13, 26, 32, 10], [13, 9, 6, 28, 12, 28, 17, 9, 22, 19, 12, 29, 15], [22, 33, 20, 15], [14, 28, 16, 12, 8, 13, 21, 22, 13, 10, 30, 47, 13], [19, 13, 32, 45, 76, 8, 16, 24], [26, 13, 13, 17], [38, 9, 13, 16, 21, 22, 36, 22, 18, 21, 15, 20, 10, 19, 15, 53, 33, 14, 24, 15], [26, 17, 33, 20, 42, 16, 14, 19, 14, 20, 25, 9, 36, 19, 29]]\n",
      "--------------------------------------------------\n",
      "paper_id = 17302615 | paper_num = 2 \n",
      "[[60, 49, 33, 49, 21, 23, 10], [32, 18, 64, 41, 15], [25, 13, 19, 16, 14, 32, 9, 25, 13], [20, 12, 22, 15, 29, 44, 31, 17, 39, 28, 41, 61], [20, 15, 17, 36], [29, 25, 62, 19], [37, 10, 17, 87, 20, 40, 61, 12, 14, 36, 18], [37, 22, 14, 113, 16, 19, 3, 36, 42, 22], [5], [26, 21, 10, 45, 65, 23, 20, 14, 36, 79]]\n",
      "--------------------------------------------------\n",
      "paper_id = 3243536 | paper_num = 3 \n",
      "[[19, 12, 23, 34, 24, 9, 18, 22, 18, 21, 18, 34, 29], [31, 17, 11, 16, 37, 10, 55, 56], [48, 24, 25, 29, 38, 35, 12, 69, 36, 14, 24, 44, 98], [39], [36, 39, 18, 38, 33, 18], [26, 41, 23, 55, 45, 64, 4], [37, 38], [14, 27, 25], [104, 14, 13, 22, 26, 23, 25, 24, 35, 10, 82], [39, 16, 15, 19, 20, 17, 43, 19, 21, 15, 12], [33, 15, 8], [14, 24, 21], [31, 31, 43, 21, 33]]\n",
      "--------------------------------------------------\n",
      "paper_id = 3248240 | paper_num = 4 \n",
      "[[37, 23, 22, 27, 69, 24, 36, 22, 19, 35, 28, 26, 20, 7, 12, 14, 7, 10, 7], [28, 35, 41, 29, 17, 20, 11, 20, 15, 13, 11, 13, 24, 21, 15, 36, 28], [28, 15, 24, 15], [15, 13, 18, 32, 29], [10, 17, 25, 16, 26, 13, 13, 19, 24, 13, 47, 49, 9], [32, 48, 28, 11, 24, 40, 16, 22, 13], [9, 21, 22, 32, 15, 19, 25], [24, 19, 9, 21, 29, 28, 35, 16, 17, 17], [23, 11, 20, 15, 17, 19, 19, 32, 22], [15, 18, 15, 22, 29, 13, 10]]\n",
      "--------------------------------------------------\n",
      "paper_id = 2223737 | paper_num = 5 \n",
      "[[22, 29, 15, 19, 50, 6, 33, 13, 7, 14, 11, 24, 58, 26, 106, 13, 20, 32, 12, 13, 5, 49, 16, 17, 21, 38], [14, 27, 13, 10, 21, 29, 41], [19, 48, 19, 19], [11, 24, 25, 20, 44, 21, 14, 10], [28, 22, 12, 16, 32, 14, 10, 7, 18, 12, 19, 10, 8, 14], [6, 19, 22, 23, 34, 31, 15, 15, 15, 17, 31, 14], [51, 52, 176, 47, 43, 31, 18], [19, 14, 37, 18, 27, 33], [7, 71, 70, 12, 13, 11, 22, 39, 20, 23, 3], [28, 59, 20], [73, 72, 54, 143], [30, 9, 23, 26, 51], [19, 26, 33, 10, 12, 56, 10, 11, 20, 26, 14, 45, 26, 11, 18, 9, 20, 29, 16, 64], [40, 32, 6, 52, 38, 28, 7, 7, 7, 17, 37, 12, 22, 45, 42, 17, 37, 8, 33, 35, 48, 16, 22, 33, 50, 51, 55], [42, 23, 15]]\n",
      "--------------------------------------------------\n",
      "paper_id = 488 | paper_num = 6 \n",
      "[[21, 19, 9, 29, 16, 17, 7, 29, 19, 11, 18, 14, 22, 7, 46, 36, 4, 23, 53], [27, 44, 14, 28, 14, 22, 16, 21, 26, 19, 28, 16, 20, 8, 4, 16, 18, 7, 17, 25, 27, 6, 15, 9], [11, 24, 36, 21, 8, 21, 11, 23, 19, 9, 12, 13, 18, 11, 23, 18, 28, 77, 10, 14, 11, 28], [18, 9, 8, 17, 10, 5, 6, 8], [33, 10], [14, 23, 14, 9, 9, 23, 9, 52, 14, 15, 11, 20, 9, 20, 12, 11, 16, 27, 20, 17, 13, 11, 18, 9], [38, 21, 39, 18], [26, 22, 7, 9, 28, 44, 23, 15, 8, 18, 9, 5, 20, 30, 9, 37, 29, 12, 16, 27, 26, 12, 24, 18, 8, 10, 16, 27, 28], [7, 10, 11, 24, 17], [32, 14, 16, 14, 14, 11, 21, 22, 11, 14, 35, 18, 15, 13, 17, 10, 14, 17, 11, 15, 11, 30, 12, 23, 15, 13, 13, 6, 29, 19, 7, 15, 14, 8, 17, 14, 30, 28, 25, 28, 16, 14], [11, 15, 11, 28, 6, 16, 11, 23, 14, 15, 21, 30, 9, 39, 16, 27, 7, 12, 22, 10, 48, 10, 6, 18, 18, 27, 31, 10, 10, 12, 14, 34, 6, 8, 30], [20, 27, 17, 15, 30, 18, 20, 14, 14, 13, 16, 31, 15, 15, 6, 8, 20]]\n",
      "--------------------------------------------------\n",
      "paper_id = 14323173 | paper_num = 7 \n",
      "[[17, 54, 16, 13, 34, 38, 22], [11, 18, 14, 27, 41, 32], [36, 77, 17], [24, 28, 10, 13, 6, 10, 9, 5, 21], [13, 29, 13, 9, 19, 27, 29, 25, 41, 23, 21, 17, 41, 27, 20, 39, 18, 48], [18, 91, 30, 25, 47, 37, 19, 40, 27, 19, 35, 20, 31, 6, 28, 37], [17, 37, 19, 18, 14, 44]]\n",
      "--------------------------------------------------\n",
      "paper_id = 15251605 | paper_num = 8 \n",
      "[[30, 19, 14, 8, 51, 41, 14, 20, 44], [17, 31, 19, 23, 45, 16, 25, 62, 29, 45, 42, 51, 18, 89], [48, 13, 79, 55, 20, 40, 27, 26, 64, 20, 24, 34, 27, 10, 34, 24, 15, 13, 5, 26, 22, 20, 7, 12, 34, 27, 29, 23, 25, 17, 13, 22, 10, 34, 25, 22, 20, 22, 21, 9, 29], [22, 19, 16, 21, 17, 22, 17, 17, 18, 18], [12, 21, 52, 11, 35, 27, 12, 20], [21, 16, 10, 21, 27, 10, 37, 23], [15, 21, 25, 32, 35, 11, 26, 27, 34, 112, 16, 34, 38, 49, 19, 39, 3, 29, 10], [15, 33, 15, 21, 34, 17, 3, 50], [40, 19, 26, 16], [14, 20, 9, 21, 25, 8, 25, 16], [47, 38, 27, 26, 51, 56], [48, 14, 38, 46, 14, 21, 10, 27, 43, 21, 48, 11, 20, 11, 27, 27, 20, 16, 21], [16, 28, 31, 39, 46]]\n",
      "--------------------------------------------------\n",
      "paper_id = 8260435 | paper_num = 9 \n",
      "[[18, 9, 25, 31, 38, 40, 42, 24, 17, 13, 27, 22, 44, 63], [28, 18, 15, 30, 33, 23, 28, 27, 9, 13, 25, 18, 24, 27, 31], [31, 12, 17, 12, 8, 24, 54, 15, 20, 13, 16, 66, 15, 22, 4, 21, 25, 13, 25, 20, 17, 24], [22], [30, 30, 22, 14, 19, 21, 4, 17, 24, 16, 12, 18, 22, 24, 19, 20, 29], [27, 57, 32, 33, 12, 23, 21], [22, 22, 10, 21, 19, 16, 20], [13, 19, 25, 19, 21], [47, 16, 18, 17, 19, 28, 14, 27, 17, 27, 24, 32, 5], [12, 16, 28, 25, 12, 64, 15, 14, 42, 17, 34, 17, 16, 13, 13, 28, 34, 12, 17, 28, 23, 9], [22, 20, 17, 20, 19, 19]]\n",
      "--------------------------------------------------\n",
      "paper_id = 14421213 | paper_num = 10 \n",
      "[[16, 15, 21, 33, 17, 11, 39, 21, 26, 10, 24, 17, 17, 11, 25, 14, 36, 17, 25, 15, 21, 16], [16, 19, 18, 43, 25, 98, 24, 25, 48, 14], [27, 18, 66, 25, 30, 47], [15, 25, 20, 64, 31, 72, 24, 15, 11], [21, 15, 30, 18, 25, 17, 33, 21, 14, 14, 33, 19, 14, 15, 31, 25, 20, 10, 16, 38, 29, 45, 51], [6, 25, 20, 28, 25, 19, 24, 12, 20, 30, 31, 28, 11, 21, 21, 30, 25, 41], [18, 13, 23, 14, 28, 36]]\n",
      "--------------------------------------------------\n",
      "paper_id = 2338256 | paper_num = 11 \n",
      "[[14, 15, 9, 59, 8, 25, 105, 20, 43, 131], [23, 11, 16, 19, 18, 8, 16, 48, 16, 23, 26, 26, 11, 16, 48], [28, 30, 22, 16, 23, 14, 27, 16, 12, 10, 20, 13], [23, 22, 15, 28], [32, 34, 16, 15, 27, 10, 19, 15, 16, 8, 24], [], [346, 50, 23, 25, 24, 17, 9, 21, 40, 22, 42, 30, 26, 12], [34, 8], [20, 23, 38, 24], [24, 52, 18, 29, 28, 24, 13, 32, 18, 10, 14, 16, 71, 29, 18, 13, 17, 41, 29, 20, 7, 21, 15], [43, 20, 19, 14, 37, 23, 20, 11, 22, 28, 9, 13, 11], [23, 16, 28, 19, 17, 19, 8], [22, 16, 33, 13, 21, 10, 13, 12, 13, 21, 7], [11, 18, 18, 48, 15, 33, 28, 16, 17], [11, 12], [20, 66, 10, 23, 29, 12], [18, 6, 38, 5, 51, 47, 8, 30, 7]]\n",
      "--------------------------------------------------\n",
      "paper_id = 41095560 | paper_num = 12 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19, 24, 13, 49, 29, 20, 24, 20], [20, 10, 23, 18, 11, 16, 51, 33, 7, 16, 20, 40, 26, 11, 26, 20, 17, 30, 20, 14, 12, 18, 17, 32, 24], [12, 13, 24, 21, 21, 26, 14, 24, 49, 35, 11, 11, 11, 14], [23, 31], [34, 19, 4, 29, 27, 12, 23, 16, 9, 13, 25, 19, 9, 21, 45, 36, 16, 22, 20, 28, 22, 18, 25, 24, 8, 16, 26, 11, 34, 21, 19, 38, 25, 16, 22, 19, 31, 46, 16, 15, 71, 23], [26, 13, 12, 16, 15, 12, 15, 20, 14, 15]]\n",
      "--------------------------------------------------\n",
      "paper_id = 823637 | paper_num = 13 \n",
      "[[24, 23, 13, 10, 30, 10, 29, 23, 23, 43, 21, 16, 17, 12, 17, 27, 39, 28, 16, 14, 22, 15, 10, 17, 12, 19, 31, 25, 16, 26], [46, 14, 24, 30, 9, 39, 39, 12, 26, 11, 8], [15, 12, 42, 7, 15, 24, 29, 23, 42, 51, 55, 17, 7, 9, 26, 13, 34], [6, 17, 32, 29, 30, 22, 23, 24, 35, 57, 24, 19, 23, 27, 17, 20, 55, 50, 21, 10, 20, 21, 11, 22, 20, 13, 10, 18, 12, 17, 46, 13, 12, 27, 13, 27, 10, 19, 25, 13], [19, 4, 21, 16, 18, 16, 10, 22, 11, 6, 8, 28, 11, 14, 5, 8, 17, 16, 7, 21, 16, 15, 12, 23, 12, 30, 15, 12, 40, 25, 14, 16], [16, 43, 10, 27, 13, 25, 33]]\n",
      "--------------------------------------------------\n",
      "paper_id = 13460410 | paper_num = 14 \n",
      "[[20, 17, 20, 28, 19, 29, 12, 25, 36, 18, 30, 20, 11, 19, 19, 24], [10, 12, 15, 16, 24, 29, 18, 41, 29, 22, 16, 14, 8, 10, 12, 24, 21, 7], [15, 14, 24, 24, 27, 21, 18, 17], [28, 20, 33, 11, 15, 14, 17, 24, 8, 19, 19, 21, 11, 22, 17, 18, 16, 17, 14, 12], [17, 22, 37, 6, 11, 20, 13, 37, 23, 9, 15, 74, 31, 25], [18, 12, 13, 19, 6, 11, 16, 24, 23, 24, 15, 29, 25, 15, 7, 28, 11, 15, 38, 28, 26, 12, 35, 29, 23, 16], [26, 13], [26, 33, 13, 15, 18, 14, 39, 100], [46], [11, 41, 14, 26, 28, 10], [30, 8, 11, 20, 17, 17, 17, 11, 48, 8], [7, 9, 11, 22, 10, 14, 21, 37, 23, 18, 44, 30, 32, 23], [31, 17, 10, 13, 8, 12, 13, 32, 22, 13, 32, 32, 13], [14, 11, 13, 14, 17, 31, 21, 23, 8, 16, 24, 29, 31, 20, 8], [12, 30, 10, 46, 41, 27, 27, 21]]\n",
      "--------------------------------------------------\n",
      "paper_id = 15416031 | paper_num = 15 \n",
      "[[24, 38, 30, 28, 18, 29, 23, 42, 39, 33, 22, 15, 15, 27, 22, 20, 30], [14, 13, 35, 16, 15, 24, 23, 16, 40, 11, 19, 19, 21, 32, 28, 14, 32, 17, 21, 16, 12, 19, 18, 21, 34, 15, 185, 8], [35, 26, 17, 25, 22, 28, 39, 10, 13, 20], [24, 47, 13, 35, 10, 67, 13, 37, 38, 23, 19, 35], [50, 29, 13, 22, 26, 10, 52, 18, 16, 51, 22], [10, 26, 7], [12, 16, 35, 31, 15, 10, 23], [40, 29, 15, 18, 33, 26, 24, 10, 11, 39, 16, 23, 12, 26, 25, 34, 17, 159, 7, 19, 14, 18, 23, 17, 12, 17, 10, 22, 17, 23, 21, 17, 11, 51, 11, 14, 37, 23, 24, 18, 24, 22, 14, 65], [31, 24, 15, 10, 25, 22], [42, 32, 19, 16, 23, 26, 17, 44, 15, 38, 36, 20, 30, 24], [18, 26, 16, 13, 21]]\n",
      "--------------------------------------------------\n",
      "paper_id = 24957784 | paper_num = 16 \n",
      "[[18, 23, 20, 73, 34, 27, 57, 25, 9, 19, 12, 9, 39], [5]]\n",
      "--------------------------------------------------\n",
      "paper_id = 442560 | paper_num = 17 \n",
      "[[16, 12, 31, 27, 40, 9], [12, 10, 13, 23, 21, 13], [21, 32, 24], [43, 16, 51, 54, 9, 11, 12, 16, 13, 11, 12, 13, 16, 4, 32, 17, 19, 12, 40, 15, 17, 40, 12, 16, 10, 26, 41, 24, 13, 25, 17, 26, 14, 26, 14, 25], [12, 15, 43, 13, 12, 13, 23, 42], [30, 17], [9, 14, 15], [18, 38, 17, 23], [9, 29, 14, 20], [11, 21, 36, 13], [18], [22, 13, 16, 33, 12, 8, 17, 15, 4, 29, 28, 8], [16, 9, 9, 34, 20, 8, 3, 11, 33], [15, 17], [9, 24, 23, 34, 24, 16, 20, 19, 23, 9, 7, 10, 24, 20, 6, 14, 12, 28]]\n",
      "--------------------------------------------------\n",
      "paper_id = 44112954 | paper_num = 18 \n",
      "[[17, 26, 20, 13, 19, 30, 12, 33, 21, 21, 27, 22, 17, 28, 96, 24, 23, 43, 23, 20, 10], [24, 28, 25, 32, 22, 33, 14, 27, 19, 40, 24, 31, 19, 18, 23, 14, 41, 10, 42, 22, 27, 32, 10, 7, 24, 12, 17, 77, 15, 20, 21, 19, 32, 22, 17, 34, 39, 36, 27, 32], [25, 11, 22, 16, 21, 14, 24, 59, 21, 50, 23, 21, 36, 38, 18, 41, 17, 34, 72, 12, 18, 4], [24], [13, 96, 14, 19, 21, 21], [44, 38, 34, 10, 23, 26, 20, 45, 52, 62], [25, 9, 25, 9, 11, 26, 11, 19, 15, 11, 18, 22, 13, 19, 20, 29, 24, 16, 32, 48, 27, 68, 36, 20, 20, 14, 8, 10], [31, 31, 19, 10, 24, 31, 16, 16, 39, 14, 10, 21, 17, 14, 27, 32, 13, 10, 22, 4, 18, 13, 24], [18, 22, 25, 19, 11, 35, 24]]\n",
      "--------------------------------------------------\n",
      "paper_id = 8387007 | paper_num = 19 \n",
      "[[16, 33, 21, 20, 9, 12, 29, 17, 37, 30, 18, 32, 28, 19, 35, 44, 32, 16, 24, 29, 20], [22, 12], [17, 20, 17, 40, 47, 15, 24, 31, 49, 18, 18], [25, 19, 26, 12, 24, 15, 16, 19, 20], [25, 26, 6, 23, 42, 21, 9, 8, 40, 24, 3, 43], [37, 16, 13, 44, 14, 7, 38, 20, 31, 42, 49], [19, 14, 15, 13], [17, 24, 19, 31, 36, 26, 13, 8, 19, 15, 12, 16, 19, 18, 17, 15, 17, 12, 16, 22, 22, 62, 48, 7, 30, 21, 27], [18, 9, 23, 18, 22, 21, 12, 9, 27, 28, 30, 28, 21, 16, 17, 25, 7, 7, 22, 19, 40, 21, 38, 28, 14, 18, 33], [22, 35, 12, 24, 10, 29, 17, 26, 15, 4, 33, 14], [46, 31], [36, 13, 22, 15, 40, 12, 35, 12, 24], [11, 21, 23, 10, 9, 8], [11, 13, 14, 27, 14, 17, 14, 20, 57, 15, 12, 17], [21, 36], [42, 25, 17, 30], [21, 13, 25, 40, 25, 22], [16, 24, 24, 36, 33, 27, 14, 15, 14, 27, 20, 43, 33, 17, 17, 17, 10, 11, 14, 20, 18, 16, 23, 39, 14, 18, 20], [19, 29, 13, 14, 42, 16, 32, 15, 26, 19, 40, 22]]\n",
      "--------------------------------------------------\n",
      "paper_id = 8983536 | paper_num = 20 \n",
      "[[38], [20, 27], [24, 14, 16, 13], [26, 22, 12, 6, 8, 9, 8, 6], [9, 19, 11, 22, 19, 8], [8, 10, 7, 37], [17, 31, 29, 23], [27, 23, 14, 22], [23, 16], [19], [18, 26, 105, 10], [9, 22, 12], [24, 26, 17, 13, 30, 12, 19], [20, 19, 43, 24, 26, 22, 13, 20, 12, 15, 12, 32, 14, 26, 18, 24], [14, 20, 23, 9, 32]]\n"
     ]
    }
   ],
   "source": [
    "for num_art, article in enumerate(all_articles):\n",
    "    sentences_article = []\n",
    "    if num_art >20:\n",
    "        break\n",
    "    for num_sec, section in enumerate(article['grobid_parse']['body_text']):\n",
    "        sentences_sec = text2sentences(section['text'])\n",
    "#         sentences_sec_len = [len(sentence) for sentence in sentences_sec]\n",
    "        sentences_sec_len = []\n",
    "        for sentence in sentences_sec:\n",
    "            if len(sentence.split()) < 3:\n",
    "                print(10*'==')\n",
    "                print('paper_num = {0} paper_sec = {1} '.format(num_art,num_sec))\n",
    "                print(len(sentence.split()),sentence)\n",
    "                print(10*'==')\n",
    "            sentences_sec_len.append(len(sentence.split()))\n",
    "        sentences_article.append(sentences_sec_len)\n",
    "    print(50*'-')\n",
    "    print('paper_id = {0} | paper_num = {1} '.format(article['paper_id'],num_art))\n",
    "    print(sentences_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_art = 1\n",
    "num_sect = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 . To optimize, we use AdaGrad (Duchi et al., 2010) .Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) . We count the number of exact matches, PPDB matches, and unmatched words.To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4 ). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) . We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(x,c,z)∈D log p θ (z, c | x, w) − λ θ 1 ',\n",
       " ' To optimize, we use AdaGrad (Duchi et al., 2010) ',\n",
       " 'Features Table 4 describes the features',\n",
       " ' Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013) ',\n",
       " ' We count the number of exact matches, PPDB matches, and unmatched words',\n",
       " 'To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another',\n",
       " ' Based on these probabilities we compute a maximum weight alignment A between words in x and c',\n",
       " ' We define features over A (see Table 4 )',\n",
       " ' We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) ',\n",
       " ' We define an indicator feature for every phrase pair of x and c that appear in the phrase table',\n",
       " ' Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is',\n",
       " ' Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training',\n",
       " ' Finally, we define standard features on logical forms and denotations (Berant et al., 2013) .']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2sentences(all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('This year comes the third') # меньше символов в секции уирать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 Тест**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "len(re.findall(r'[A-Z]',all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "paper_num = 0 paper_sec = 0 \n",
      "13 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 0 \n",
      "18 With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 0 \n",
      "15 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 0 \n",
      "12 Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 0 \n",
      "12  (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 2 \n",
      "16  All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c \n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 2 \n",
      "16  Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 2 \n",
      "26  The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 2 \n",
      "13  For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 3 \n",
      "18  The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 5 \n",
      "18  There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other)\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 5 \n",
      "53  For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\"\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 5 \n",
      "14  Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 5 \n",
      "18  Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 8 \n",
      "18  Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 9 \n",
      "21 To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods: RA-Sparse : It is a framework to tackle the RA-MDS problem\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 9 \n",
      "11  LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 11 \n",
      "13  Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) \n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 11 \n",
      "15  The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05)\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 11 \n",
      "26  Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"'Bitcoin Mt Gox Offlile\"' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 \n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 12 \n",
      "30 Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 13 \n",
      "12  To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments\n",
      "====================\n",
      "====================\n",
      "paper_num = 0 paper_sec = 13 \n",
      "11  Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world \n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 10164018 | paper_num = 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 1 paper_sec = 2 \n",
      "13 G is RELNP[r] of NP[x] → NP[RNext, the builder (backed by crowdsourcing) paraphrases each canonical utterance c output above into a set of natural utterances P(c) (e.g., \"when was article 1 published. \")\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 4 \n",
      "12  Broadly speaking, the rules (R1)-(R4), (C1)-(C4) take a binary and a noun phrase, and compose them (optionally via comparatives, counting, and negation) to produce a complementizer phrase CP representing a unary (e.g., \"that cites article 1\" or \"that cites more than three article\"). (G3) combines these CP's with an NP (e.g., \"article\")\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 4 \n",
      "19  For example, in the SOCIAL domain, Alice's education can be represented in the database as five triples: All five properties here are represented as RELNP's, with the first one designated as the subject (RELNP 0 )\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 4 \n",
      "43  We support two ways of querying multi-arity relations: \"student whose university is ucla\" (T2) and \"university of student Alice whose start date is 2005\" (T3).birthdate → RELNP[birthdate] person|university|field → TYPENP[person| · · · ] company|job title → TYPENP[company| · · · ] student|university|field of study → RELNP[student| · · · ] employee|employer|job title → RELNP[employee| · · · ] start date|end date → RELNP[startDate| · · · ] is friends with → VP/NP[friends| · · · ]Generating directly from the grammar in Table 2 would result in many uninterpretable canonical utterances\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 4 \n",
      "102  Handling these would require a more radical change to the grammar, but is still within scope. [glue] (G1) ENTITYNP[x] → NP[x] (G2) TYPENP[x] → NP[type.x] (G3) NP[x] CP[f ] (and CP[g])* → NP[x f g] [simple] (R0) that VP[x] → CP[x] (R1.z)))] [transformation] (T1) RELNP[r] of NP[y] → NP[R(r).y] (T2) RELNP 0 [h]CP[f ] (and CP[g])* → NP[R(h).(f g)] (T3) RELNP[r] of RELNP 0 [h] NP[x] CP[f ] (and CP[g])* → NP[R(r).(h.x f g)] (T4) NP[x] or NP[y] → NP[x y] [aggregation] (A1) number of NP[x] → NP[count(x)] (A2) total|average RELNP[r] of NP[x] → NP[sum|average(x, r)]\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 7 \n",
      "68  Phenomena Example CALENDAR 22 837 temporal language x: \"Show me meetings after the weekly standup day\" c: \"meeting whose date is at least date of weekly standup\" z: type.meeting date. > R(date).weeklyStandup BLOCKS 19 1995 spatial language x: \"Select the brick that is to the furthest left.\" c: \"block that the most number of block is right of\" z: argmax(type.block, R(λx.count(R(right).x))) HOUSING 24 941 measurement units x: \"Housing that is 800 square feet or bigger. \" c: \"housing unit whose size is at least 800 square feet\" z: type.housingUnit area. > .800 RESTAURANTS 32 1657 long unary relations x: \"What restaurant can you eat lunch outside at. \" c: \"restaurant that has outdoor seating and that serves lunch\" z: type.restaurant hasOutdoorSeating serveslunch PUBLICATIONS 15 801 sublexical compositionality x: \"Who has co-authored articles with Efron. \" c: \"person that is author of article whose author is efron\" z: type.person R(author).(type.article author.efron) SOCIAL 45 4419 multi-arity relations x: \"When did alice start attending brown university. \" c: \"start date of student alice whose university is brown university\" z: R(date)\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 8 \n",
      "13  Examples from the PUBLICATIONS domain include fewestleast number and by-whose author is\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 10 \n",
      "11  To corroborate this, we compare our full system to NOLEX, a baseline that omits all lexical features (Table 4 ), but uses PPDB as a domain-general paraphrasing component\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 10 \n",
      "11  We perform the complementary experiment and compare to NOPPDB, a baseline that omits PPDB match features\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 10 \n",
      "13  We also run BASELINE, where we omit both lexical and PPDB features\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 10 \n",
      "11  The accuracy of NOPPDB is only slightly lower than FULL, showing that most of the required paraphrases can be learned during training\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 10 \n",
      "14  As expected, removing both lexical and PPDB features results in poor performance (BASELINE).Analysis\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 11 \n",
      "11  First, we define by NP 0 , NP 1 , and NP 2 the set of utterances generated by an NP that has exactly zero, one, and two NPs embedded in it\n",
      "====================\n",
      "====================\n",
      "paper_num = 1 paper_sec = 11 \n",
      "19  Our Scenario Acc. Scenario Acc. 0 → 1 22.9 0 ∪ 1 → 2 28.1 0 ∪ 1 → 1 85.8 0 ∪ 1 ∪ 2 → 2 47.5 Table 6 : Test set results in the CALENDAR domain on bounded non-compositionality. hypothesis is that generalization on 0 ∪ 1 → 2 should be better than for 0 → 1, since NP 1 utterances have non-compositional paraphrases, but training on NP 0 does not expose them\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 14472576 | paper_num = 1 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 2 paper_sec = 3 \n",
      "12  For this purpose, large-vocabulary distributional semantic models (DSM) were constructed from a version of the English Wikipedia 5 and the Google Web 1T 5-Grams database (Brants and Franz, 2006) \n",
      "====================\n",
      "====================\n",
      "paper_num = 2 paper_sec = 6 \n",
      "15 Experiments with just a simple unigram bag-ofwords model show that for both the Twitter (Tab. 3) and the SMS data (Tab. 4) the Maximum Entropy classifier outperforms multinomial Naive Bayes and Linear SVM by a considerable margin\n",
      "====================\n",
      "====================\n",
      "paper_num = 2 paper_sec = 7 \n",
      "11 The results for task A are similar to those for task B in that Maximum Entropy is the best classifier for the unigram bag-of-words model for both the Twitter (Tab. 5) and the SMS data (Tab. 6)\n",
      "====================\n",
      "====================\n",
      "paper_num = 2 paper_sec = 7 \n",
      "12 European Exchanges open with a slight rise: (AGI) Rome, October 24 -European Exchanges opened with a slight ris... http://t.co/mAljf6eTThis problem is probably a major factor in the misclassification of many negative and positive messages as neutral\n",
      "====================\n",
      "====================\n",
      "paper_num = 2 paper_sec = 9 \n",
      "12  The similar Figure 1: Learning curve of our system for the \"Message Polarity Classification\" task, evaluated on the Twitter data evaluation results for the Twitter and the SMS data show that not relying on Twitter-specific features like hashtags pays off: by making our system as generic as possible, it is robust, not overfitted to the training data, and generalizes well to other types of data\n",
      "====================\n",
      "====================\n",
      "paper_num = 2 paper_sec = 9 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11  We would also like to find out whether a heuristic treatment of intensifiers and detensifiers, the normalization of character repetitions, or the inclusion of some punctuationbased features could further improve classifier performance. 13 For task B, even the extended unigram bag-of-words model by itself, without any additional resources, would have performed quite well as the 9th best constrained system on the Twitter test set (13th best system overall) and the 5th best system on the SMS test set.14 http://www.ark.cs.cmu.edu/TweetNLP/\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 17302615 | paper_num = 2 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 3 paper_sec = 1 \n",
      "14 Given a NI P2 N8 structure, these transt'ornmtions are obtained through corpus-based tuning 1The following symbols are used for syntactic categories: N (nouI0, A (adjective), Av (adverb), V (verb), C (coordinating conjunction), P (pret}osition), and D (detcrminer)\n",
      "====================\n",
      "====================\n",
      "paper_num = 3 paper_sec = 5 \n",
      "13  Only N1 P2 Na t;(;171118 whe, re 1)2 = dc m:e |;real;ed. ]if the v(;rl) is transitive, l;helt the verb forln nlUSI; l)e ;t past t)m;l;i(;il)le (rule Nlw, a(tt()Vl/.('v-])ass), so l;ha, t 1;t1(' , object relation still hol(ls in the vm'iant, if the verl) is intrnnsitive or ergative, then the verl) fornl nms|; l)e active, st) |;h~l; the sut)je(:t; rel~d;ion holds (rule Nhea(ltoVll.ev-A(:tSiml) (resp\n",
      "====================\n",
      "====================\n",
      "paper_num = 3 paper_sec = 8 \n",
      "16 In this section, we ew~luate the variations produced fl'om the two preceding sets of metarules: initial morpho-synt~mti(\" wn'iations (henceforth MS) m~d new wn:i~tions enriched through light; semantics (henceforl;h MS+S). i The wtriald;s \",u:e ot)t~dne(1 Kern a 13.2 millionword (:orpus (:omposed of s(:ientiti(: al)stracl;s in the agricultural dora;fin (in French) ;rod a set of 11,452 terms. :~ The corlms is mlnlyzed through SYLEX, a shallow parser l;h~t buihts limited 1)hrase structures and associ~tes each word with mt unambiguous syntactic (:ategory and a, l(;mma. ~Ibrms are acquired from the out-|;ltl'es a,l:e sele,(:ted nn(t only terms that occur ~l: lea.st three times in the ('ortms m:e retained\n",
      "====================\n",
      "====================\n",
      "paper_num = 3 paper_sec = 8 \n",
      "20  For instance, each of the three MS+S variations Nhea(lToV-Conq), NheadToV-SimI) or NheadtoV-l)rel ) is a refinenmnt of the MS vari~> |ion Nhea(lToV\n",
      "====================\n",
      "====================\n",
      "paper_num = 3 paper_sec = 8 \n",
      "56  These co-occurrences are used to evalm~te the recall wflues of the tiltering metarules.awe arc grateflfl to Xavier Polanco, Jean Royautd and Lmncnt Schmitt (INIST-CNRS) for t)roviding us with this s(:icntitic corpus. abaisser -D -A -I -E abaissement +D -A -I -E absorber -D -A -I -E absorbe'ar +D +A -I -E accorder -D -A -I -E accord +D -A -I -E accumuler -D -A -I -E aceumulateur +D +A -I -E accumulation ÷ D -A -I -E accdl&'er -D -A -I +E\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 3243536 | paper_num = 3 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 4 paper_sec = 0 \n",
      "24 Many Natural Language Processing (NLP) applications demand some basic linguistic processing (Tokenization, Part of Speech (POS) tagging, Named Entity Recognition and Classification (NER), Syntactic Parsing, Coreference Resolution, etc.  to be able to further undertake more complex tasks\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 0 \n",
      "11 Modular: Unlike other NLP toolkits, which often are built in a monolithic architecture, IXA pipeline is built in a data centric architecture so that modules can be picked and changed (even from other NLP toolkits)\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 1 \n",
      "12  Some modules in IXA pipeline provide linguistic annotation based on probabilistic supervised approaches such as POS tagging, NER and Syntactic Parsing\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 1 \n",
      "17  Both Perceptron (Collins, 2002; Collins, 2003) and Maximum Entropy models (Ratnaparkhi, 1999) are adaptable algorithms which have been successfully applied to NLP tasks such as POS tagging, NER and Parsing with state of the art results\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 1 \n",
      "18  To avoid duplication of efforts, IXA pipeline uses the already available open-source Apache OpenNLP API 7 to train POS, NER and parsing probabilistic models using these two approaches.\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 2 \n",
      "21 IXA pipeline currently provides the following linguistic annotations: Sentence segmentation, tokenization, Part of Speech (POS) tagging, Lemmatization, Named Entity Recognition and Classification (NER), Constituent Parsing and Coreference Resolution\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 4 \n",
      "13  This method is convenient whenever lookups on very large dictionaries are required because it reduces the memory footprint to 10% of the memory required for the equivalent plain text dictionary; and (iii) We also provide lemmatization by lookup in WordNet-3.0 (Fellbaum and Miller, 1998) via the JWNL API 12 \n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 5 \n",
      "12 Most of the NER systems nowdays consist of language independent systems (sometimes enriched with gazeteers) based on automatic learning of statistical models. ixa-pipe-nerc provides Named Entity Recognition (NER) for English and Spanish\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 6 \n",
      "11  Maximum Entropy models are trained to build shift reduce bottom up parsers (Ratnaparkhi, 1999) as provided by the Apache OpenNLP API\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 7 \n",
      "12 So far we have evaluated our module on the CoNLL 2011 testset and we are a 5% behind the Stanford's system (52.8 vs 57.6 CoNLL F1), the best on that task (Lee et al., 2013) \n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 8 \n",
      "12  GATE has some capacity for wrapping Apache UIMA components 16 , so should be able to manage distributed NLP components\n",
      "====================\n",
      "====================\n",
      "paper_num = 4 paper_sec = 8 \n",
      "12  IXA pipeline is already being used to do extensive parallel processing in the FP7 European projects OpeNER 18 and NewsReader 19 .\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 3248240 | paper_num = 4 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 5 paper_sec = 0 \n",
      "12 Natural Language Generation (NLG) systems require resources such as templates (in case of template-based NLG) or rules (in case of rulebased NLG)\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 0 \n",
      "11 Input is a corpus of parallel text and data consisting of a set of documents D and an RDF graph G, where D and G are related via a set of entities E where an entity can be described by a document in D and described by data in G\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 0 \n",
      "12  1 The graph pattern GP can be transformed into a SPARQL query Q GP \n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 0 \n",
      "11  G GP can be verbalized as an English (German) sentence S en (S de ) using the sentence pattern SP en (SP de )\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Besides the general idea to allow for nonexperts to assess information encoded in RDF, we envision application of these verbalization templates in three scenarios: (1) In query interfaces to semantic databases, casual users -usually not capable of writing formal queries -specify their information needs using keywords (Lei et al., 2006; Thomas et al., 2007; Wang et al., 2008) , questions in free-text or using a controlled language (Kaufmann et al., 2006; Cimiano et al., 2008; Wendt et al., 2012; Damljanovic et al., 2012) , or forms (Hunter and Odat, 2011 Figure 1 : A template consists of a graph pattern GP and a sentence pattern SP \n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 0 \n",
      "12  The graph pattern GP can be transformed into a SPARQL query Q GP \n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 0 \n",
      "11  This graph can be verbalized as an English sentence S en using the English sentence pattern SP en or as a German sentence S de using the German sentence pattern SP de \n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 2 \n",
      "14  We denote the set of variables within a sentence pattern sp as V ar SP (sp) and the set of variables within a graph pattern gp as V ar GP (gp ∈ U ∪ V, p ∈ U ∪ V, and o ∈ U ∪ L ∪ V\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 3 \n",
      "30  Given an RDF data graph G and a template (sp, gp), a SPARQL SELECT query can be created: SELECT PV WHERE { gp }\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 3 \n",
      "12  Executing a query results in a solution sequence 4 which is a list of solution mappings µ:V →T from a set of variables V to a set of RDF terms T = U∪L. See Fig. 1 for an example of a solution mapping (µ)\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 4 \n",
      "14  4 We adopt the terminology from the SPARQL 1.1 Query Language documentation available at http: //www.w3.org/TR/2013/REC-sparql11-query-20130321/\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 6 \n",
      "37 Algorithm 1 Collect example sentences1: procedure COLLECT EXAMPLE SENTENCES(l, E, D, M ) 2: for each e ∈ E do 3: for each d ∈ document(e, l) ∈ D do 4:for each s ∈ sentences(d, lmin, lmax) do5:for each x ∈ λ(e, l) do6:for each m ∈ Mstring do (left, right, x') = MatchesLabel(s, x, m, \"str\")9:if (left, right, x') = ∅ then return (lef t, right, matched)9: return ∅ In Alg. 2, \\W denotes a non-word character (such as a blank), \\D denotes a non-digit, \\w denotes a word character 5 (such as \"x\"), \\w{a, b} denotes a sequence of at least a word-characters and not more than b word characters, l 0 and l 1 are the minimum and maximum number of word characters that may appear on the left side of the modified string m(x) between this string and a nonword character or the beginning of the sentence (ˆ). r 0 and r 1 are the corresponding numbers regarding the right side of the modified string. $ denotes the end of the sentence\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 10 \n",
      "16 Before we describe the fmSpan algorithm (fmSpan: Frequent Maximal Subgraph PAttern extractioN) we need to introduce our notation: Two graph patterns gp i and gp j are equivalent (gp i =gp j ) if an injective function m:V ar GP (gp i )→V ar GP (gp j ) exists such that when each variable v in gp i is replaced with m(v), the resulting graph pattern gp i is identical to gp j \n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 10 \n",
      "11  Given a set of graph patterns GP ={gp 1 , ..., gp n } and given a graph pattern x, the coverage of x regarding GP is the number of graphs in GP to which x is a subgraph pattern: c(x, GP ) := |{gp i ∈ GP |x ⊆ p gp i }|\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 10 \n",
      "17 Given a set of graph patterns I={gp 1 , ..., gp n }, from the set of all subgraph patterns P =2 gp 1 ∪...∪ 2 gpn a set of graph patterns K={gp i , ..., gp j }⊆P is selected where:1. for each gp k ∈ K: (a) c(gp k , I) ≥ min coverage (b) ¬∃gp l ∈ P : gp k = gp l ∧ gp k ⊆ p gp l ∧ c(gp l , I) ≥ min coverage 2. ¬∃gp l ∈ P : c(gp l , I) ≥ min coverage ∧ (¬∃gp m ∈ P : ¬(gp m , gp l ) ∧ c(gp m , I) ≥ min coverage) ∧ gp l ∈ KThismeans that each member of K is sufficiently frequent (1a) and maximal (2b) and that every maximal graph pattern is contained in K (2).\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 11 \n",
      "30 Algorithm 5 Graph-pattern pruning 1: procedure PRUNEGRAPHPATTERN(k) 2: for each v ∈ V ar GP (k) do 3: T ← {(s, p, o) ∈ k|(s = v ∧ o ∈ V ar GP (k)) ∨ (o = v ∧ s ∈ V ar GP (k))} 4: if |T | > maxt\n",
      "====================\n",
      "====================\n",
      "paper_num = 5 paper_sec = 13 \n",
      "14  We evaluated a random sample of 10 English and 10 German templates using a group of 6 evaluators which are experts in the fields of RDF and SPARQL and that are proficient in both English and German\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 2223737 | paper_num = 5 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 6 paper_sec = 0 \n",
      "12  In our experiments with a manually written broad coverage Definite Clause Grammar (DCG) (Briscoe and Carroll, 1996) , we were only able to recover all parses for Wall Street Journal sentences that were at most 13 tokens long within acceptable time and space bounds on computation\n",
      "====================\n",
      "====================\n",
      "paper_num = 6 paper_sec = 2 \n",
      "12 The (unnormalised) total weight of a parse x, ψ(x), is a function of the k features that are 'active' on a parse:ψ(x) = exp( k i=1 λ i f i (x))(1)The probability of a parse, P (x | M ), is simply the result of normalising the total weight associated with that parse:P (x | M ) = 1 Z ψ(x) (2) Z = y∈Ω ψ(y)(3)The interpretation of this probability depends upon the application of the RFM\n",
      "====================\n",
      "====================\n",
      "paper_num = 6 paper_sec = 6 \n",
      "17 The grammar we model with Random Fields, (called the Tag Sequence Grammar (Briscoe and Carroll, 1996) , or TSG for short) was developed with regard to coverage, and when compiled consists of 455 Definite Clause Grammar (DCG) rules\n",
      "====================\n",
      "====================\n",
      "paper_num = 6 paper_sec = 7 \n",
      "14  The first template creates features that count the number of times a DCG instantiationis present within a parse.3 For example, suppose we parsed the Wall Street Journal AP:1 unimpeded by traffic A parse tree generated by TSG might be as shown in figure 1 \n",
      "====================\n",
      "====================\n",
      "paper_num = 6 paper_sec = 10 \n",
      "12  The informative sample derivable from such a training set was likely to be larger (more representative of Figure 4: Classification Accuracy for Three Models Estimated using a Gaussian Prior and IIS the population) than the informative sample derivabled from a training set using shorter, less syntactically complex sentences\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 488 | paper_num = 6 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 7 paper_sec = 1 \n",
      "11  As a corpus specifically annotated for weasel words, WikiWeasel should be mentioned, which was constructed for the CoNLL-2010 Shared Task (Farkas et al., 2010) and contains Wikipedia paragraphs annotated for weasel words.\n",
      "====================\n",
      "====================\n",
      "paper_num = 7 paper_sec = 2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16  It includes three types of texts from the biomedical domain -namely, radiological reports, biological full papers and abstracts from the GENIA corpus. (15 new full biomedical papers were annotated for hedge cues and their scopes, which served as the evaluation database of the CoNLL-2010 Shared Task (Farkas et al., 2010) , and this dataset will be added to BioScope in the near future.  The annotation was carried out by two students of linguistics supervised by a linguist\n",
      "====================\n",
      "====================\n",
      "paper_num = 7 paper_sec = 4 \n",
      "12  Nested scopes One scope includes another one:These observations (suggest that TNF and PMA do (not lead to NFkappa B activation through induction of changes in the cell redox status))\n",
      "====================\n",
      "====================\n",
      "paper_num = 7 paper_sec = 5 \n",
      "25  In order to see what the main differences are between the corpora, the annotation principles were contrasted: in GENIA Event, no modifier keywords are marked, however, in BioScope, they are; the scope of speculation and negation is explicitly marked in BioScope and it can be extended to various constituents within the clause / sentence though in GENIA Event, it is the event itself that is within the scope; two subtypes of uncertainty are distinguished in GENIA Event: doubtful and probable, however, in BioScope there is one umbrella term for them (speculation)\n",
      "====================\n",
      "====================\n",
      "paper_num = 7 paper_sec = 5 \n",
      "15 The multiplicity of events in GENIA and the maximum scope principle exploited in BioScope (see 3.1) taken together often yields that a GENIA event falls within the scope of a BioScope keyword, however, it should not be seen as a speculated or negated event on its own\n",
      "====================\n",
      "====================\n",
      "paper_num = 7 paper_sec = 5 \n",
      "16  For instance, negated \"investigation\" verbs in Present Perfect are seen as doubtful events in GENIA and as negative events in BioScope:However, a role for NF-kappaB in human CD34(+) bone marrow cells has not been described\n",
      "====================\n",
      "====================\n",
      "paper_num = 7 paper_sec = 5 \n",
      "15 Another difference between the annotation schemes of BioScope and GENIA is that instances of weaseling are annotated as probable events in GENIA, however, in BioScope they are not\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 14323173 | paper_num = 7 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 8 paper_sec = 2 \n",
      "21  The normalized distance score of algorithms which yield similarities (such as the ALINE algorithm) can be calculated by the formula of Downey et al. (2008) :(1) 1 − 2S AB S A + S B ,where S A and S B are the similarity scores of the sequences aligned with themselves, and S AB is the similarity score of the alignment of both sequences\n",
      "====================\n",
      "====================\n",
      "paper_num = 8 paper_sec = 2 \n",
      "11  Thus, given the words German Tochter [tɔxtər] 'daughter' and English daughter [dɔːtər] , the sound class representation of both sequences will be TKTR and TTR, respectively\n",
      "====================\n",
      "====================\n",
      "paper_num = 8 paper_sec = 2 \n",
      "12  Figure 1 contrasts the scores of NED with SCA distance for the alignment of 658 cognate and 658 non-cognate word pairs between English and German (see Sup. Mat. A)\n",
      "====================\n",
      "====================\n",
      "paper_num = 8 paper_sec = 4 \n",
      "12  The input format is a CSV-representation of the way multilingual wordlists are represented in the STARLING software package for lexicostatistical analyses\n",
      "====================\n",
      "====================\n",
      "paper_num = 8 paper_sec = 8 \n",
      "17  Table 5 shows pairwise distances of German, English, Danish, Swedish, Dutch, and Norwegian entries for the item WOMAN taken from the GER dataset (see Sup. Mat\n",
      "====================\n",
      "====================\n",
      "paper_num = 8 paper_sec = 10 \n",
      "12 Bergsma and Kondrak (2007) test their method for automatic cognate detection by calculating the set precision (PRE), the set recall (REC), and the set F-score (FS): The set precision p is the proportion of cognate sets calculated by the method which also occurs in the gold standard\n",
      "====================\n",
      "====================\n",
      "paper_num = 8 paper_sec = 11 \n",
      "12  Based on these scores all words were clustered into cognate sets using the flat cluster variant of UPGMA with a threshold of 0.4 for SCA distances and a threshold of 0.7 for NED, since these both turned out to yield the best results for these approaches\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 15251605 | paper_num = 8 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 9 paper_sec = 1 \n",
      "11 Recently, Yepes et al. (2013) described a system to index Gene Reference Into Function (GeneRIF) sentences that show novel functionality of genes mentioned in Medline\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 8260435 | paper_num = 9 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 10 paper_sec = 4 \n",
      "12 We collected English and French synonym pairs from WordNet 2.1 (Miller, 1995) and WOLF 0.1.4 (Sagot and Fiser, 2008) , respectively\n",
      "====================\n",
      "====================\n",
      "paper_num = 10 paper_sec = 4 \n",
      "16 We compared the word alignment performance of our model with that of GIZA++ 1.03 1 (Vogel et al., 1996; Och and Ney, 2003) , and HMBiTAM (Zhao and Xing, 2008) implemented by us\n",
      "====================\n",
      "====================\n",
      "paper_num = 10 paper_sec = 4 \n",
      "16  GIZA++ is an implementation of IBM-model 4 and HMM, and HM-BiTAM corresponds to ζ = 0 in eq. 7\n",
      "====================\n",
      "====================\n",
      "paper_num = 10 paper_sec = 4 \n",
      "17  To confirm the effect of the synonym pair model with latent topics, we also tested GIZA++ and HMBiTAM with what we call Synonym Replacement Heuristics (SRH) , where all of the synonym pairs in the bilingual training sentences were simply replaced with a representative word\n",
      "====================\n",
      "====================\n",
      "paper_num = 10 paper_sec = 4 \n",
      "11  For instance, the words 'sick' and 'ill' in the bilingual sentences # vocabularies  10k  50k  100k  English  standard  8578  16924  22817  with SRH  5435  7235  13978  French  standard  10791  21872  30294  with SRH  9737  20077  27970   Table 2 : The number of vocabularies in the 10k, 50k and 100k data sets.\n",
      "====================\n",
      "====================\n",
      "paper_num = 10 paper_sec = 5 \n",
      "18  We show the performance of GIZA++ and HMBiTAM with the SRH in the lines entitled \"with SRH\" in Table 1 \n",
      "====================\n",
      "====================\n",
      "paper_num = 10 paper_sec = 5 \n",
      "24  The GIZA++ and HM-BiTAM with the SRH slightly outperformed the standard GIZA++ and HM-BiTAM for the 10k and 100k data sets, but underperformed with the 50k data set\n",
      "====================\n",
      "====================\n",
      "paper_num = 10 paper_sec = 5 \n",
      "15 The proposed method consistently outperformed GIZA++ and HM-BiTAM with the SRH in 10k, 50k and 100k data sets in F-measure\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 14421213 | paper_num = 10 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 11 paper_sec = 0 \n",
      "24  Semantic roles are always between verbs (or nouns derived from verbs) and other constituents (run quickly, went to the store, computer maker), whereas semantic relations can occur between any constituents, for example in complex nominals (malaria mosquito (CAUSE)), genitives (girl's mouth (PART-WHOLE)), prepositional phrases attached to nouns (man at the store (LOCATIVE)), or discourse level (The bus was late\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 0 \n",
      "50 The following NP level constructions are considered here (cf. the classifications provided by (Quirk et al.1985) and (Semmelmeyer and Bolander 1992) ):(1) Compound Nominals consisting of two consecutive nouns (eg night club -a TEMPORAL relation -indicating that club functions at night), (2) Adjective Noun constructions where the adjectival modifier is derived from a noun (eg musical clock -a MAKE/PRODUCE relation), (3) Genitives (eg the door of the car -a PART-WHOLE relation), and (4) Adjective phrases (cf. (Semmelmeyer and Bolander 1992) ) in which the modifier noun is expressed by a prepositional phrase which functions as an adjective (eg toy in the box -a LOCATION relation)\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 0 \n",
      "109 There are several semantic relations at the noun phrase level: (1) Saturday's snowfall is a genitive encoding a TEMPORAL relation, (2) one-day record is a TOPIC noun compound indicating that record is about one-day snowing -an ellipsis here, (3) record in Hartford is an adjective phrase in a LOCATION relation, (4) total of 12.5 inches is an of-genitive that expresses MEASURE, (5) weather service is a noun compound in a TOPIC relation, (6) car which was driven by a college student encodes a THEME semantic role in an adjectival clause, (7) college student is a compound nominal in a PART-WHOLE/MEMBER-OF relation, (8) interstate overpass is a LOCATION noun compound, (9) mountains of Virginia is an of-genitive showing a PART-WHOLE/PLACE-AREA and LOCATION relation, (10) concrete barrier is a noun compound encoding PART-WHOLE/STUFF-OF.\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 1 \n",
      "19  For example, the expression \"Texas city\" contains both a LOCATION as well as a PART-WHOLE relation\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 1 \n",
      "16 Other researchers have identified other sets of semantic relations (Levi 1979) , (Vanderwende 1994) , (Sowa 1994) , (Baker, Fillmore, and Lowe 1998) , (Rosario and Hearst 2001) , (Kingsbury, et al. 2002) , (Blaheta and Charniak 2000) , (Gildea and Jurafsky 2002) , (Gildea and Palmer 2002) \n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 4 \n",
      "21  Most importantly for us, each sequence of nouns, or possibly adjectives and nouns, has a particular meaning as a whole carrying an implicit semantic relation; for example, \"spoon handle\" (PART-WHOLE) or \"musical clock\" (MAKE/PRODUCE)\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 4 \n",
      "40  (1) Sometimes the meaning changes with the head (eg \"musical clock\" MAKE/PRODUCE, \"musical creation\" THEME), other times with the modifier (eg \"GM car\" MAKE/PRODUCE, \"family car\" POSSESSION)\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 4 \n",
      "21  For example, \"USA city\" can be regarded as a LOCATION as well as a PART-WHOLE relation\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 6 \n",
      "187 Definition / Example Relation 1 POSSESSION an animate entity possesses (owns) another entity; (family estate; the girl has a new car. , (Vanderwende 1994 ) 2 KINSHIP an animated entity related by blood, marriage, adoption or strong affinity to another animated entity; (Mary's daughter; my sister); (Levi 1979 ) 3 PROPERTY/ characteristic or quality of an entity/event/state; (red rose; The thunderstorm was awful. ; (Levi 1979 an entity/event/state is part of another entity/event/state (door knob; door of the car), (MERONYMY) (Levi 1979) , (Dolan et al. 1993) , 8 HYPERNYMY an entity/event/state is a subclass of another; (daisy flower; Virginia state; large company, such as Microsoft) (IS-A) (Levi 1979) , (Dolan et al. 1993 ) 9 ENTAIL an event/state is a logical consequence of another; (snoring entails sleeping) 10 CAUSE an event/state makes another event/state to take place; (malaria mosquitoes; to die of hunger; The earthquake generated a Tsunami), (Levi 1979 ) 11 MAKE/PRODUCE an animated entity creates or manufactures another entity; (honey bees; nuclear power plant; GM makes cars) (Levi 1979 ) 12 INSTRUMENT an entity used in an event/action as instrument; (pump drainage; the hammer broke the box) (Levi 1979 ) 13 LOCATION/SPACE spatial relation between two entities or between an event and an entity; includes DIRECTION; (field mouse; street show; I left the keys in the car), (Levi 1979) , (Dolan et al. 1993 the means by which an event is performed or takes place; (bus service; I go to school by bus.  (Quirk et al.1985) 19 ACCOMPANIMENT one/more entities accompanying another entity involved in an event; (meeting with friends; She came with us) (Quirk et al.1985 ) 20 EXPERIENCER an animated entity experiencing a state/feeling; (Mary was in a state of panic. ; (Sowa 1994 ) 21 RECIPIENT an animated entity for which an event is performed; (The eggs are for you) ; includes BENEFICIARY; (Sowa 1994 ) 22 FREQUENCY number of occurrences of an event; (bi-annual meeting; I take the bus every day); (Sowa 1994 expresses the property associated with the subject or the object through the verb; (He feels [sleepy] \n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 6 \n",
      "73  They elected him [treasurer] . ) (Blaheta and Charniak 2000) is considered problematic by linguists because they involve an implicit relation that seems to allow for a large variety of relational interpretations; for example: \"John's car\"-POSSESSOR-POSSESSEE, \"Mary's brother\"-KINSHIP, \"last year's exhibition\"-TEMPORAL, \"a picture of my nice\"-DEPICTION-DEPICTED, and \"the desert's oasis\"-PART-WHOLE/PLACE-AREA\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 6 \n",
      "29  For instance, the preposition \"with\" can encode different semantic relations: (1) It was the girl with blue eyes (MERONYMY), (2) The baby with the red ribbon is cute (POSSESSION), (3) The woman with triplets received a lot of attention (KINSHIP)\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 8 \n",
      "14 We have assembled a corpus from two sources: Wall Street Journal articles from TREC-9, and eXtended WordNet glosses (XWN) (http://xwn.hlt.utdallas.edu)\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 9 \n",
      "23  For instance, in \"owner of car\"-POSSESSION the possessor owner is followed by the possessee car, while in \"car of John\"-POSSESSION/R the order is reversed\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 9 \n",
      "30  For example, the genitive \"city of USA\" was tagged as a PART-WHOLE/PLACE-AREA relation and as a LOCATION relation\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 10 \n",
      "59  The most frequently occurring relations were PART-WHOLE, ATTRIBUTE-HOLDER, POSSESSION, LOCATION, SOURCE, TOPIC, and THEME\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 12 \n",
      "14  Example: \"car manufacturer\" is a kind of manufacturer that MAKES/PRODUCES cars. 2\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 12 \n",
      "22  Example: \"musical clock\" -MAKE/PRODUCE, and \"electric clock\"-INSTRUMENT.\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 16 \n",
      "32  Where have nuclear incidents occurred.  From the question stem word where, we know the question asks for a LOCATION which is found in the complex nominal \"Three Mile Island\"-LOCATION of the sentence \"The Three Mile Island nuclear incident caused a DOE policy crisis\", leading to the correct answer \"Three Mile Island\".Q\n",
      "====================\n",
      "====================\n",
      "paper_num = 11 paper_sec = 16 \n",
      "30  What did the factory in Howell Michigan make.  The verb make tells us to look for a MAKE/PRODUCE relation which is found in the complex nominal \"car factory\"-MAKE/PRODUCE of the text: \"The car factory in Howell Michigan closed on Dec 22, 1991\" which leads to answer car\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 2338256 | paper_num = 11 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 12 paper_sec = 1 \n",
      "15  For any instances i p and i n in RP and RN, they can be in the same output set I of an IR system as:∀i p ∈ RP , ∀i n ∈ RN , ∃I, {i p , i n } ⊂ IPrecision and recall are thus not directly correlated\n",
      "====================\n",
      "====================\n",
      "paper_num = 12 paper_sec = 1 \n",
      "11 In Table 1 , the source Chinese sentence and its English translation in the form of character strings Table 1 : Sample sentences along with the output of two trivial segmenters (T1, T2) and three other segmenters (S1, S2, S3)\n",
      "====================\n",
      "====================\n",
      "paper_num = 12 paper_sec = 1 \n",
      "12  True Positives (TP), Precision (P), recall (R), F1-score (F) and true negative rate (TNR) are calculated respectively.are presented along with the outputs of five handcrafted segmenters\n",
      "====================\n",
      "====================\n",
      "paper_num = 12 paper_sec = 2 \n",
      "36  To put everything together:TNR = TN RN = 1 − FN RN = 1 − PP − TP TW − RP(1)When PP equals TP, we will have a TNR of 1, indicating that a WS system correctly rejects all TN if and only if all the PP are TP\n",
      "====================\n",
      "====================\n",
      "paper_num = 12 paper_sec = 2 \n",
      "14  Since TW is bounded by the input sentence length and RP is bounded by the reference, TNR is negatively correlated to PP as longer segmented word eliminates more TN and generates less FN in general\n",
      "====================\n",
      "====================\n",
      "paper_num = 12 paper_sec = 4 \n",
      "23  P zh R zh F zh TNR zh P en R en F en TNR enFigure 1: Evaluation scores on Chinese (zh) and English (en) in precision (P), recall (P), F1-score (F) and true negative rate (TNR) with different ratios of most probable boundaries λ\n",
      "====================\n",
      "====================\n",
      "paper_num = 12 paper_sec = 4 \n",
      "32  In WS, the values of both RN in the reference as well Score P zh R zh F zh TNR zh P en R en F en TNR enFigure 2: Evaluation scores on Chinese (zh) and English (en) in precision (P), recall (P), F1-score (F) and true negative rate (TNR) with different numbers of training instances N.as PN by the system are drastically greater than the corresponding values of the positives\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 41095560 | paper_num = 12 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "--------------------------------------------------\n",
      "paper_id = 823637 | paper_num = 13 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 14 paper_sec = 0 \n",
      "13  Examples of entity categories are French Female Artistic Gymnasts, Presidents of Ireland and French Senators Of The Second Empire\n",
      "====================\n",
      "====================\n",
      "paper_num = 14 paper_sec = 0 \n",
      "13  It is headquartered in São José dos Campos, São Paulo State.\" 1 The flexibility and richness of natural language allow describing Brazilian aerospace conglomerate both as Brazilian Planemaker 2 or as Aircraft manufacturers of Brazil 3 \n",
      "====================\n",
      "====================\n",
      "paper_num = 14 paper_sec = 1 \n",
      "14  POS-tag TO is converted to IN and NNPS is converted to NNP\n",
      "====================\n",
      "====================\n",
      "paper_num = 14 paper_sec = 4 \n",
      "16 Algorithm 1 Construction 1: input : T : target set of entity categories. 2: output : Σ : a filled graph data model.3: 4: C ← ∅, Z ← ∅, R ← ∅, S ← ∅, E ← ∅ 5: for t ∈ T do 6: c, Z , R , S , E ← graphOf (t) 7:C ← { c} ; and film, indexed in the general specialisation space (Z: also geometric representation)\n",
      "====================\n",
      "====================\n",
      "paper_num = 14 paper_sec = 7 \n",
      "38  Continuous Skip-gram Model (W2V) (Mikolov et al., 2013) : Skip-gram is a vector space model created by deep learning techniques focused on lo-Algorithm 2 Semantic Interpretation Process 1: input : query and Σ = (C, Z, R, S, E) 2: output : Z : related categories and their score.3:4: c, Z q , R q , S q , E q ← graphOf (query) 5: U ← distSearch( c, C) 6: for ( k, h) ∈ K do 7: D ← selectGraphsByCore( k, E) 8:for all D ∈ D do J ← distSearch( o c , O q ) 24:X.append(J)\n",
      "====================\n",
      "====================\n",
      "paper_num = 14 paper_sec = 9 \n",
      "18  For LSA, RI and ESA, we used the SSpace Package (Jurgens and Stevens, 2010), while W2V and GloVe were generated by the code shared by the respective authors\n",
      "====================\n",
      "====================\n",
      "paper_num = 14 paper_sec = 12 \n",
      "13  Finally, expressions such as WWI and USSR should be identified as the paraphrasing of Wold War I and the Soviet Union or even other variations, what is not available in our model\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 13460410 | paper_num = 14 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 15 paper_sec = 1 \n",
      "17  Note that we do not aim to predict the probability of a person being elected, as such effortsWt-c Word Vectors Projection Output Wt Wt-1 Wt+1 Wt+c ...... · · · · · · · · · · · · ...... · · · · · · · · · · · · · · · · · · · · · · · · · V · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · Shared Matrix · · · · · · ...... · · · · · · · · · · · · · · · · · · Wt-c Wt+c · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · Shared Matrix .....\n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 2 \n",
      "11 Bengio et al. (2003) proposed a neural networkbased language model that motivated recent advances in natural language processing (NLP), including two word embedding learning strategies continuous bag-of-word (CBOW) and skip-gram (SG) (Mikolov et al., 2013a) \n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 2 \n",
      "13  The SG model, being a simplified feedforward neural network as well, differs from CBOW in that SG employs an inverse training objective instead for learning word representations (Mikolov et al., 2013a; Mikolov et al., 2013b; Le and Mikolov, 2014) \n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 3 \n",
      "15  Given a training set with emotion categories, we first definek = N (w ∧ E), l = N (w ∧ ¬E), m = N (¬w ∧ E),and n = N (¬w∧¬E), where N (w∧E) denotes the number of documents that contain w and belong to emotion E, N (w∧¬E) denotes the number of documents that contain w but does not belong to emotion E, and so on\n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 4 \n",
      "11  We define publicity score P S i of a person i as:P S i = (P i − N i ) + k j=1,j =i ( N j − P j k − 1 ),(2)where P i and N i denotes the number of documents with positive and negative reader's emotion, respectively\n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 7 \n",
      "14 DEKV is based on embeddings learned from the training set using CBOW with default settings in the toolkit (Řehůřek and Sojka, 2010) article is represented as a weighted average of keywords and classified by linear SVM (Chang and Lin, 2011) \n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 7 \n",
      "11  Next, we include LDA (Blei et al., 2003) as document representation and an SVM classifier (denoted as LDA)\n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 7 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44  The accuracy acc(E k ), macro-average avg M , and micro-average avg µ are defined as follows:acc(E k ) = T P (E k ) + T N (E k ) T P (E k ) + F P (E k ) + T N (E k ) + F N (E k ) , (3) avg M = 1 m m k=1 acc(E k ),(4)avg µ = acc(E k ) × N (E k ) m k=1 N (E k ) ,(5)where T P (E k ) is the set of test documents correctly classified to the emotion E k , F P (E k ) is the set of test documents incorrectly classified to the emotion, F N (E k ) is the set of test documents wrongly rejected, T N (E k ) is the set of test documents correctly rejected, and N (E k ) is the total number of documents in this emotion category\n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 7 \n",
      "12  For example, as stated in the previous section, we observed that keywords related to \"Happy\" (in green) are mostly about sports, including terms such as team names (e.g., \"熱火 (Miami Heat)\" and \"紅襪 (Boston Red Sox)\") and player names (e.g., \"陳偉殷 (Wei-Yin Chen)\", a pitcher for the baseball team Baltimore Orioles)\n",
      "====================\n",
      "====================\n",
      "paper_num = 15 paper_sec = 7 \n",
      "21  W2 W3 W4 W5 W6 W7 W8 W9 W10 W11 W12 W13 W14 W15 W16  PC1 12  29  18  28  14  34  32  37  32  15  18  17  16  15  17  39  PC2 11  27  20  22  16  12  6  27  24  28  27  22  19  24  26  40  PC3 26  28  26  12  12  18  27  12  24  13  11  11  30  14  13 \n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 15416031 | paper_num = 15 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "--------------------------------------------------\n",
      "paper_id = 24957784 | paper_num = 16 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 17 paper_sec = 0 \n",
      "15  For instance, noun phrases such as \"songwriter Dylan\", \"Google founder Page\", or \"rock legend Page\" can be easily mapped to the entities Bob Dylan, Larry Page, and Jimmy Page if their respective types Singer, BusinessPerson, and Guitarist are available (cf\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 3 \n",
      "15  The most common workaround to perform entity classification is a two-stage process: in first applying an online tool for Named-Entity Disambiguation (NED), such as DBpedia Spotlight (Mendes et al., 2011) or AIDA , in order to map the mentions onto canonical entities and subsequently query the knowledge base for their types\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 3 \n",
      "14  In fact, (Ling and Weld, 2012) followed this approach when comparing their entity classification system results against those obtained by an adoption of the Illinois' Named-Entity Linking system (NEL) (Ratinov et al., 2011) and reached the conclusion that while NEL performed decently for prominent entities, it could not scale to cover long tail ones\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 3 \n",
      "11  Later on, we explain the engineering undertaken in order to develop the on-the-fly type classification system HYENA-live (cf. Section 4).3 Type Hierarchy and Feature Set\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 4 \n",
      "16  The YAGO knowledge base (Hoffart et al., 2013 ) is selected to derive the taxonomy from because of its highly precise classification of entities into WordNet classes, which is a result of the accurate mapping YAGO has from Wikipedia Categories to WordNet synsets\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 4 \n",
      "40 We start with five top classes namely PERSON, LOCATION, ORGANIZATION, EVENT and ARTIFACT\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 4 \n",
      "11  While the classes are picked from the YAGO type system, the approach is generic and can be applied to derive type taxonomies from other knowledge bases such as Freebase or DBpedia (Auer et al., 2007) as in (Ling and Weld, 2012) .\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 11 \n",
      "13  We build type-specific classifiers using the SVM software LIBLINEAR (cf. http://liblinear. bwaldvogel.de/)\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 12 \n",
      "11 LIBLINEAR model files are normalized textual files: a header (data about the model and the total number of features), followed by listing the weights assigned to each feature (line number indicates the feature ID)\n",
      "====================\n",
      "====================\n",
      "paper_num = 17 paper_sec = 14 \n",
      "11  Hence, we also provide HYENA-live as a JSON compliant entity classification Web-service\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 442560 | paper_num = 17 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 18 paper_sec = 6 \n",
      "18  Glove (Pennington et al., 2014) , FastText (Bojanowski et al., 2016) , Word2Vec (Mikolov et al., 2013) and Dependency-Based Word2Vec (DBW2V) (Levy and Goldberg, 2014) \n",
      "====================\n",
      "====================\n",
      "paper_num = 18 paper_sec = 7 \n",
      "15 We consider a bit surprising the difference in performance of Glove with respect to knowledgebased (ConceptNet) and Dependency-based (DBW2V) embeddings: 5.9% with respect to ConceptNet and 13.0% with respect to DBW2V\n",
      "====================\n",
      "====================\n",
      "paper_num = 18 paper_sec = 7 \n",
      "22  However, as in the case of DBW2V, they are better than W2V or Glove mainly for syntactic analogies, which probably makes better FastText (and probably DBW2V) for NLP tasks other than DAI, e.g. sentence representation (Arroyo-Fernández et al., 2017) \n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 44112954 | paper_num = 18 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 19 paper_sec = 0 \n",
      "11  A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 0 \n",
      "12  Earlier work (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015) defines verbs as events and uses TimeML-based (Pustejovsky et al., 2003) learning for temporal ordering of events\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 7 \n",
      "11  It covers two major tags: 'EVENT' and 'TIMEX3'\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 7 \n",
      "12  The most notable LINK is TLINK: a Temporal Link representing the temporal relationship between entities (events and time expressions)\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 7 \n",
      "11  The fourth column of Table 1 shows the correspondence between Allen relations and TimeML TLINKs\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 7 \n",
      "17  Indeed, the majority of events which are presumed 'simultaneous' in TimeML annotated corpora are either (1) EVENT-TIMEX relations which are not event-event relations, or (2) wrongly annotated and should be the 'overlapping' relation, e.g., in the following sentence from TimeBank corpus the correct relation for the two events e1 and e2 should be 'overlap': She [listened] e1 to music while [driving] e2 \n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 8 \n",
      "12  Causality is one of the main semantic relationships between events where an event (CAUSE) results in another event (EFFECT) to happen or hold\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18  There is an obvious connection between causal relation and temporal relation: by definition, the CAUSE event starts 'BEFORE' the EFFECT event\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 8 \n",
      "23  This definition is in line with the definition of CAUSE and PRECONDITION presented in the RED annotation guidelines (Ikuta et al., 2014) (to be discussed in Section 6)\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 16 \n",
      "11  Following the other discourse structure annotation tasks such as Rhetorical Structure Theory (RST), we aggregate all the relations captured by all annotators as the annotation object, then labeling 'NONE' as the category for coders who have not captured this relation\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "11 One of the most recent temporal annotation schemas is Temporal Histories of Your Medical Event (THYME) \n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "42  In their work, they combine the TimeML annotation schema with Allen Interval Algebra, identifying the five temporal relations BEFORE, OVERLAP, BEGINS-ON, ENDS-ON, and CONTAINS\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "17  Of note is that they adopt the notion of narrative containers (Pustejovsky and Stubbs, 2011) , which are time slices in which events can take place, such as DOC-TIME (time of the report) and before DOCTIME\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "14  The Penn Discourse Tree Bank (PDTB) corpus (Prasad et al., 2008) addresses the annotation of causal relations, annotating semantic relations that hold between exactly two Abstract Objects (called Arg1 and Arg2), expressed either explicitly via lexical items or implicitly via adjacency in discourse\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "27  Each event pair was annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSE, NO-REL)\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "13 (12) Fuel tanks had leaked and contaminated the soil.-(leaked BEFORE contaminated) -(leaked CAUSED contaminated)\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "17  They mainly introduce 'CLINK', analogous to 'TLINK' in TimeML, to be added to the existing TimeML link tags\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "13  Under this framework, Mirza et al (Mirza and Tonelli, 2014) annotates 318 CLINKs in TempEval-3 TimeBank\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "13  Another relevant work is Richer Event Descriptions (RED) (Ikuta et al., 2014) , which combines event coreference and THYME annotations, and also introduces cause-effect annotation in adjacent sentences to achieve a richer semantic representation of events and their relations\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "31  RED also distinguishes between 'PRECONDITION' and 'CAUSE', similarly to our 'ENABLE' and 'CAUSE' relations\n",
      "====================\n",
      "====================\n",
      "paper_num = 19 paper_sec = 17 \n",
      "31  These can be in the context of BEFORE or OVERLAP, but they do not include PREVENT and CAUSE-TO-END\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 8387007 | paper_num = 19 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n",
      "====================\n",
      "paper_num = 20 paper_sec = 10 \n",
      "14  Hashtags: the number of hashtags in one tweet;  Ill format: the presence of ill format with some characters replacing by *, for example, f**k;  Punctuation: the number of contiguous sequences of exclamation marks, question marks, and both exclamation and question marks; whether the last token contains an exclamation or question mark;  Emoticons: the presence of positive and negative emoticons at any position in the tweet; whether the last token is an emoticon;  OOV: the ratio of words out of vocabulary;  Elongated words: the presence of sentiment words with one character repeated more than two times, for example, 'cooool';  URL: whether the tweet contains a URL\n",
      "====================\n",
      "--------------------------------------------------\n",
      "paper_id = 8983536 | paper_num = 20 \n",
      "[[14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32], [14, 20, 23, 9, 32]]\n"
     ]
    }
   ],
   "source": [
    "for num_art, article in enumerate(all_articles):\n",
    "    sentences_len_up_article = []\n",
    "    if num_art >20:\n",
    "        break\n",
    "    for num_sec, section in enumerate(article['grobid_parse']['body_text']):\n",
    "        sentences_sec = text2sentences(section['text'])\n",
    "#         sentences_sec_len = [len(sentence) for sentence in sentences_sec]\n",
    "        \n",
    "        sentences_sec_len_up = []\n",
    "        for sentence in sentences_sec:\n",
    "            \n",
    "            if len(re.findall(r'[A-Z]',sentence)) > 10:\n",
    "                print(10*'==')\n",
    "                print('paper_num = {0} paper_sec = {1} '.format(num_art,num_sec))\n",
    "                print(len(re.findall(r'[A-Z]',sentence)),sentence)\n",
    "                print(10*'==')\n",
    "            sentences_sec_len_up.append(len(re.findall(r'[A-Z]',sentence)))\n",
    "        sentences_len_up_article.append(sentences_sec_len)\n",
    "    print(50*'-')\n",
    "    print('paper_id = {0} | paper_num = {1} '.format(article['paper_id'],num_art))\n",
    "    print(sentences_len_up_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я считаю, что у нас приемлимое качество\n",
    "\n",
    "Далее можно улучшать"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Создание признаков\n",
    " - подсчитать $\\text{густота ссылок} =  \\frac{\\text{кол-во ссылок}}{\\text{длина абзаца}}$\n",
    " - подсчитать кол-во непрерывных предложений, в которых есть хотя бы 1 ссылка\n",
    " - расположение секции в документе (мб нормализовать)\n",
    " - усреднение позиции in-line ссылок в каждой секции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**подсчитаем кол-во ссылок в предложениях секции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_art = 0\n",
    "num_sect = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "{'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "{'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "{'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "{'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "{'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "{'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "{'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "{'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "{'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "{'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n"
     ]
    }
   ],
   "source": [
    "for cite_span in all_articles[num_art]['grobid_parse']['body_text'][num_sect]['cite_spans']:\n",
    "    print(cite_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources.',\n",
       " '(Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) .',\n",
       " 'In the typical setting of MDS, the input is a set of news documents about the same topic.',\n",
       " \"The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available.\",\n",
       " 'Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 .',\n",
       " 'The content of the original news report talks about some new products based on AI techniques.',\n",
       " 'The news report generally conveys an enthusiastic tone.',\n",
       " 'However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports.',\n",
       " 'Unfortunately, existing MDS approaches cannot handle this issue.',\n",
       " 'We investigate this problem known as reader-aware multi-document summarization (RA-MDS).',\n",
       " 'Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments.',\n",
       " 'Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments.',\n",
       " 'Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions.',\n",
       " 'Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) .',\n",
       " 'However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period.',\n",
       " 'Another challenge is that reader comments are very diverse and noisy.',\n",
       " 'Recently, Li et al.',\n",
       " '(2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy.',\n",
       " 'However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al.',\n",
       " '(2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) .',\n",
       " 'During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts.',\n",
       " 'Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient.',\n",
       " 'Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters.',\n",
       " 'After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS.',\n",
       " 'Existing datasets from DUC 3 and TAC 4 are not appropriate.',\n",
       " 'Therefore, we introduce a new dataset for RA-MDS.',\n",
       " 'We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing.',\n",
       " 'To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS.',\n",
       " 'To our best knowledge, it is the first dataset for RA-MDS.',\n",
       " '(2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments.',\n",
       " '(3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text = nltk.sent_tokenize(all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text'])\n",
    "sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) ',\n",
       " ' In the typical setting of MDS, the input is a set of news documents about the same topic',\n",
       " ' The output summary is a piece of short text document containing several sentences, generated only based on the input original documents',\n",
       " \"With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available\",\n",
       " ' Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 ',\n",
       " ' The content of the original news report talks about some new products based on AI techniques',\n",
       " ' The news report generally conveys an enthusiastic tone',\n",
       " ' However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports',\n",
       " ' Unfortunately, existing MDS approaches cannot handle this issue',\n",
       " ' We investigate this problem known as reader-aware multi-document summarization (RA-MDS)',\n",
       " ' Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries',\n",
       " 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments',\n",
       " ' Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments',\n",
       " ' Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions',\n",
       " ' Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) ',\n",
       " ' However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period',\n",
       " ' Another challenge is that reader comments are very diverse and noisy',\n",
       " ' Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy',\n",
       " ' However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments',\n",
       " 'Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) ',\n",
       " ' During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts',\n",
       " ' Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient',\n",
       " ' Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters',\n",
       " ' After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary',\n",
       " 'There is a lack of high-quality dataset suitable for RA-MDS',\n",
       " ' Existing datasets from DUC 3 and TAC 4 are not appropriate',\n",
       " ' Therefore, we introduce a new dataset for RA-MDS',\n",
       " ' We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing',\n",
       " ' To our best knowledge, this is the first dataset for RA-MDS',\n",
       " 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS',\n",
       " ' To our best knowledge, it is the first dataset for RA-MDS',\n",
       " ' (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments',\n",
       " ' (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_sec = text2sentences(all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text'])\n",
    "sentences_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 31\n"
     ]
    }
   ],
   "source": [
    "if len(sentences_sec) != len(sent_text):\n",
    "    print(len(sentences_sec),len(sent_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**как видим наше разбиение на предложения лучше**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 192,\n",
       "  'end': 216,\n",
       "  'text': '(Goldstein et al., 2000;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF6'},\n",
       " {'start': 217,\n",
       "  'end': 239,\n",
       "  'text': 'Erkan and Radev, 2004;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF4'},\n",
       " {'start': 240,\n",
       "  'end': 257,\n",
       "  'text': 'Wan et al., 2007;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF19'},\n",
       " {'start': 258,\n",
       "  'end': 284,\n",
       "  'text': 'Nenkova and McKeown, 2012;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF16'},\n",
       " {'start': 285,\n",
       "  'end': 302,\n",
       "  'text': 'Min et al., 2012;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF15'},\n",
       " {'start': 303,\n",
       "  'end': 319,\n",
       "  'text': 'Li et al., 2017)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF11'},\n",
       " {'start': 773,\n",
       "  'end': 797,\n",
       "  'text': '(Project Code: 14203414)',\n",
       "  'latex': None,\n",
       "  'ref_id': None},\n",
       " {'start': 2288,\n",
       "  'end': 2305,\n",
       "  'text': '(Hu et al., 2008;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF7'},\n",
       " {'start': 2306,\n",
       "  'end': 2324,\n",
       "  'text': 'Yang et al., 2011)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF22'},\n",
       " {'start': 2582,\n",
       "  'end': 2598,\n",
       "  'text': 'Li et al. (2015)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF2'},\n",
       " {'start': 2911,\n",
       "  'end': 2927,\n",
       "  'text': 'Li et al. (2017)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF11'},\n",
       " {'start': 3069,\n",
       "  'end': 3095,\n",
       "  'text': '(Kingma and Welling, 2014;',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF9'},\n",
       " {'start': 3096,\n",
       "  'end': 3117,\n",
       "  'text': 'Rezende et al., 2014)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF18'}]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_spans_sec = all_articles[num_art]['grobid_parse']['body_text'][num_sect]['cite_spans']\n",
    "cite_spans_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "       {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'},\n",
       "       {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'},\n",
       "       {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'},\n",
       "       {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'},\n",
       "       {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'},\n",
       "       {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None},\n",
       "       {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'},\n",
       "       {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'},\n",
       "       {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'},\n",
       "       {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'},\n",
       "       {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'},\n",
       "       {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "cite_spans_sec_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "[{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "{'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "[{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "{'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "[{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "{'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "[{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "{'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "{'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "[{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "3 With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available\n",
      "{'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "[{'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "14  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) \n",
      "{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "[{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "14  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) \n",
      "{'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "[{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "17  Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy\n",
      "{'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "[{'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "[{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "{'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "[{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "del_bibref = []\n",
    "for num_sent,sentence in enumerate(sentences_sec):\n",
    "    cite_spans_sec_time = [cite_span  for cite_span in  cite_spans_sec_time if not cite_span['ref_id'] in del_bibref] \n",
    "#     del_bibref = []\n",
    "    for num_cite_span,cite_span in enumerate(cite_spans_sec_time):\n",
    "        if len(cite_span['text'])<4:\n",
    "            temp = ' '+cite_span['text']+' '\n",
    "            if temp in sentence:\n",
    "                print(num_sent,sentence)\n",
    "                del_bibref.append(cite_span['ref_id'])\n",
    "                print(cite_span)\n",
    "                print(cite_spans_sec_time)\n",
    "                print('---')\n",
    "#                 del cite_spans_sec_time[num_cite_span]\n",
    "        elif cite_span['text'] in sentence:\n",
    "            print(num_sent,sentence)\n",
    "            del_bibref.append(cite_span['ref_id'])\n",
    "            print(cite_span)\n",
    "            print(cite_spans_sec_time)\n",
    "            print('---')\n",
    "#             del cite_spans_sec_time[num_cite_span]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BIBREF6',\n",
       " 'BIBREF4',\n",
       " 'BIBREF19',\n",
       " 'BIBREF16',\n",
       " 'BIBREF15',\n",
       " 'BIBREF11',\n",
       " None,\n",
       " 'BIBREF7',\n",
       " 'BIBREF22',\n",
       " 'BIBREF2',\n",
       " 'BIBREF9',\n",
       " 'BIBREF18']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_bibref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old version END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расспечатаем из 500 статей: \n",
      "Статьи в которых кол-во найденных ссылок в предложениях секции != кол-ву найденных inline ссылок\n",
      "87 10\n",
      "171 10\n",
      "236 10\n",
      "334 8\n",
      "353 7\n",
      "353 19\n",
      "411 6\n",
      "449 0\n"
     ]
    }
   ],
   "source": [
    "num_art = 0\n",
    "num_sect = 0\n",
    "\n",
    "sentences_sec = text2sentences(all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text'])\n",
    "cite_spans_sec = all_articles[num_art]['grobid_parse']['body_text'][num_sect]['cite_spans']\n",
    "\n",
    "print('Расспечатаем из 500 статей: \\nСтатьи в которых кол-во найденных ссылок в предложениях секции != кол-ву найденных inline ссылок')\n",
    "\n",
    "for num_art,article in enumerate(all_articles):\n",
    "    if num_art > 500:\n",
    "        break \n",
    "    for num_sec,section in enumerate(article['grobid_parse']['body_text']):\n",
    "        \n",
    "        sentences_sec = text2sentences(section['text'])\n",
    "        cite_spans_sec = section['cite_spans']\n",
    "        cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "        sum_prev_sects = 0\n",
    "        sents_num_citations = []\n",
    "        del_bib_start = []\n",
    "        for num_sent,sentence in enumerate(sentences_sec):\n",
    "            cite_spans_sec_time = [cite_span  for cite_span in  cite_spans_sec_time if not cite_span['start'] in del_bib_start]\n",
    "            len_sent = len(sentence)+1\n",
    "            sent_num_cits = 0\n",
    "            for num_cite_span,cite_span in enumerate(cite_spans_sec_time):\n",
    "                if len(cite_span['text'])<4:\n",
    "                    temp = ' '+cite_span['text']+' '\n",
    "                    if (cite_span['start'] >= (sum_prev_sects-2) and (cite_span['end']-3) <=(sum_prev_sects+len_sent)) or (temp in sentence) or (temp.replace('.','') in sentence):\n",
    "                        sent_num_cits+=1\n",
    "                        del_bib_start.append(cite_span['start'])\n",
    "                elif (cite_span['start'] >= (sum_prev_sects-2) and (cite_span['end']-3) <=(sum_prev_sects+len_sent)) or (cite_span['text'] in sentence) or (cite_span['text'].replace('.','') in sentence):\n",
    "                    sent_num_cits+=1\n",
    "                    del_bib_start.append(cite_span['start'])\n",
    "#                     print(cite_span)\n",
    "#                     print(num_sent,sentence)\n",
    "#                     print('---')\n",
    "            sents_num_citations.append(sent_num_cits)\n",
    "            sum_prev_sects += len_sent\n",
    "#         print(num_art,num_sec)\n",
    "        try:\n",
    "            assert sum(sents_num_citations) == len(cite_spans_sec)\n",
    "        except:\n",
    "            print(num_art,num_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_art = 0\n",
    "num_sect = 0\n",
    "article = all_articles[num_art]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "{'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "{'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "{'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "{'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "{'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "{'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "{'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "{'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "{'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "{'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n"
     ]
    }
   ],
   "source": [
    "for cite_span in article['grobid_parse']['body_text'][num_sect]['cite_spans']:\n",
    "    print(cite_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "3 With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available\n",
      "---\n",
      "{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "14  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) \n",
      "---\n",
      "{'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "14  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) \n",
      "---\n",
      "{'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "17  Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy\n",
      "---\n",
      "{'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "---\n",
      "{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "---\n",
      "{'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "---\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for num_sec,section in enumerate(article['grobid_parse']['body_text']):\n",
    "    if num_sec > 0:\n",
    "        break\n",
    "    sentences_sec = text2sentences(section['text'])\n",
    "    cite_spans_sec = section['cite_spans']\n",
    "    cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "    sum_prev_sects = 0\n",
    "    sents_num_citations = []\n",
    "    del_bib_start = []\n",
    "    for num_sent,sentence in enumerate(sentences_sec):\n",
    "        \n",
    "        cite_spans_sec_time = [cite_span  for cite_span in  cite_spans_sec_time if not cite_span['start'] in del_bib_start]\n",
    "        len_sent = len(sentence)+1\n",
    "        sent_num_cits = 0\n",
    "        for num_cite_span,cite_span in enumerate(cite_spans_sec_time):\n",
    "            temp = cite_span['text']\n",
    "            if len(cite_span['text'])<4:\n",
    "                temp = ' '+cite_span['text']+' '\n",
    "                if (cite_span['start'] >= sum_prev_sects and cite_span['end'] <=(sum_prev_sects+len_sent)) or (temp in sentence):\n",
    "                    sent_num_cits+=1\n",
    "                    del_bib_start.append(cite_span['start'])\n",
    "                    print('---')\n",
    "                    print(cite_span)\n",
    "                    print(num_sent,sentence)\n",
    "                    print('---')\n",
    "            elif (cite_span['start'] >= (sum_prev_sects-2) and (cite_span['end']-3) <=(sum_prev_sects+len_sent)) or (temp in sentence) or (temp.replace('.','') in sentence):\n",
    "                sent_num_cits+=1\n",
    "                del_bib_start.append(cite_span['start'])\n",
    "                print(cite_span)\n",
    "                print(num_sent,sentence)\n",
    "                print('---')\n",
    "        sents_num_citations.append(sent_num_cits)\n",
    "        sum_prev_sects += len_sent\n",
    "    try:\n",
    "        assert sum(sents_num_citations) == len(cite_spans_sec)\n",
    "        print(20*'==')\n",
    "    except:\n",
    "        print('!!!!',num_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "{'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "0 The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "---\n",
      "The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) \n",
      "0 0 321\n",
      " In the typical setting of MDS, the input is a set of news documents about the same topic\n",
      "1 321 411\n",
      " The output summary is a piece of short text document containing several sentences, generated only based on the input original documents\n",
      "2 411 548\n",
      "{'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "3 With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available\n",
      "---\n",
      "With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google's big developers' conference content is available\n",
      "3 548 955\n",
      " Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google's big developers' conference\" 2 \n",
      "4 955 1099\n",
      " The content of the original news report talks about some new products based on AI techniques\n",
      "5 1099 1193\n",
      " The news report generally conveys an enthusiastic tone\n",
      "6 1193 1249\n",
      " However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports\n",
      "7 1249 1486\n",
      " Unfortunately, existing MDS approaches cannot handle this issue\n",
      "8 1486 1551\n",
      " We investigate this problem known as reader-aware multi-document summarization (RA-MDS)\n",
      "9 1551 1640\n",
      " Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries\n",
      "10 1640 1760\n",
      "One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments\n",
      "11 1760 1929\n",
      " Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments\n",
      "12 1929 2031\n",
      " Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions\n",
      "13 2031 2157\n",
      "{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "14  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) \n",
      "---\n",
      "{'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "14  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) \n",
      "---\n",
      " Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) \n",
      "14 2157 2326\n",
      " However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period\n",
      "15 2326 2501\n",
      " Another challenge is that reader comments are very diverse and noisy\n",
      "16 2501 2571\n",
      "{'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "17  Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy\n",
      "---\n",
      " Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy\n",
      "17 2571 2755\n",
      " However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments\n",
      "18 2755 2901\n",
      "{'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "---\n",
      "{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "---\n",
      "{'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n",
      "19 Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n",
      "---\n",
      "Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 2901 3119\n",
      " During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts\n",
      "20 3119 3273\n",
      " Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient\n",
      "21 3273 3460\n",
      " Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters\n",
      "22 3460 3775\n",
      " After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary\n",
      "23 3775 3912\n",
      "There is a lack of high-quality dataset suitable for RA-MDS\n",
      "24 3912 3972\n",
      " Existing datasets from DUC 3 and TAC 4 are not appropriate\n",
      "25 3972 4032\n",
      " Therefore, we introduce a new dataset for RA-MDS\n",
      "26 4032 4082\n",
      " We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing\n",
      "27 4082 4212\n",
      " To our best knowledge, this is the first dataset for RA-MDS\n",
      "28 4212 4273\n",
      "Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS\n",
      "29 4273 4399\n",
      " To our best knowledge, it is the first dataset for RA-MDS\n",
      "30 4399 4458\n",
      " (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments\n",
      "31 4458 4580\n",
      " (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "32 4580 4730\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "sentences_sec = text2sentences(article['grobid_parse']['body_text'][0]['text'])\n",
    "cite_spans_sec = article['grobid_parse']['body_text'][0]['cite_spans']\n",
    "\n",
    "cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "sum_prev_sects = 0\n",
    "sents_num_citations = []\n",
    "del_bib_start = []\n",
    "for num_sent,sentence in enumerate(sentences_sec):\n",
    "    cite_spans_sec_time = [cite_span  for cite_span in  cite_spans_sec_time if not cite_span['start'] in del_bib_start]\n",
    "    len_sent = len(sentence)+1\n",
    "    sent_num_cits = 0\n",
    "    for num_cite_span,cite_span in enumerate(cite_spans_sec_time):\n",
    "        temp = cite_span['text']\n",
    "        if len(cite_span['text'])<4:\n",
    "            temp = ' '+cite_span['text']+' '\n",
    "            if (cite_span['start'] >= sum_prev_sects and cite_span['end'] <=(sum_prev_sects+len_sent)) or (temp in sentence):\n",
    "                sent_num_cits+=1\n",
    "                del_bib_start.append(cite_span['start'])\n",
    "                print('---')\n",
    "                print(cite_span)\n",
    "                print(num_sent,sentence)\n",
    "                print('---')\n",
    "        elif (cite_span['start'] >= (sum_prev_sects-2) and (cite_span['end']-3) <=(sum_prev_sects+len_sent)) or (temp in sentence) or (temp.replace('.','') in sentence):\n",
    "            sent_num_cits+=1\n",
    "            del_bib_start.append(cite_span['start'])\n",
    "            print(cite_span)\n",
    "            print(num_sent,sentence)\n",
    "            print('---')\n",
    "    sents_num_citations.append(sent_num_cits)\n",
    "    print(sentence)\n",
    "    print(num_sent,sum_prev_sects,sum_prev_sects+len_sent)\n",
    "    sum_prev_sects += len_sent\n",
    "    \n",
    "print(num_art,num_sec)\n",
    "assert sum(sents_num_citations) == len(cite_spans_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "{'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "{'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}\n",
      "{'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}\n",
      "{'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}\n",
      "{'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "{'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}\n",
      "{'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "{'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}\n",
      "{'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "{'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "{'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "{'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}\n"
     ]
    }
   ],
   "source": [
    "for cite_span in article['grobid_parse']['body_text'][num_sect]['cite_spans']:\n",
    "    print(cite_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы  посмотрим почему есть несоотвествие, то заметим что ошибка в распознавании идентификатора in-line ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для рассчета признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sect_num_citations(section):\n",
    "    sentences_sec = text2sentences(section['text'])\n",
    "    cite_spans_sec = section['cite_spans']\n",
    "    cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "    sum_prev_sects = 0\n",
    "    sents_num_citations = []\n",
    "    del_bib_start = []\n",
    "    for num_sent,sentence in enumerate(sentences_sec):\n",
    "        cite_spans_sec_time = [cite_span  for cite_span in  cite_spans_sec_time if not cite_span['start'] in del_bib_start]\n",
    "        len_sent = len(sentence)+1\n",
    "        sent_num_cits = 0\n",
    "        for num_cite_span,cite_span in enumerate(cite_spans_sec_time):\n",
    "            if len(cite_span['text'])<4:\n",
    "                temp = ' '+cite_span['text']+' '\n",
    "                if (cite_span['start'] >= sum_prev_sects and cite_span['end'] <=(sum_prev_sects+len_sent)) or (temp in sentence) or (temp.replace('.','') in sentence):\n",
    "                    sent_num_cits+=1\n",
    "                    del_bib_start.append(cite_span['start'])\n",
    "            elif (cite_span['start'] >= (sum_prev_sects-2) and (cite_span['end']-3) <=(sum_prev_sects+len_sent)) or (cite_span['text'] in sentence) or (cite_span['text'].replace('.','') in sentence):\n",
    "                sent_num_cits+=1\n",
    "                del_bib_start.append(cite_span['start'])\n",
    "#                 print(cite_span)\n",
    "#                 print(num_sent,sentence)\n",
    "#                 print('---')\n",
    "        sents_num_citations.append(sent_num_cits)\n",
    "        sum_prev_sects += len_sent\n",
    "    # Checking соотвествия кол-ва найденных ссылок в предложениях и кол-ва всех ссылок\n",
    "#     try:\n",
    "#         assert sum(sents_num_citations) == len(cite_spans_sec)\n",
    "#     except:\n",
    "#         print(num_art,num_sec)\n",
    "    return sents_num_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_num_citations(article):\n",
    "    if article['grobid_parse'] and article['grobid_parse']['body_text']:\n",
    "        papers_num_citations = []\n",
    "        for num_sec,section in enumerate(article['grobid_parse']['body_text']):\n",
    "            sents_num_citations = sect_num_citations(section)\n",
    "            papers_num_citations.append(sents_num_citations)\n",
    "        return papers_num_citations\n",
    "    else:\n",
    "        return [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [0, 0, 1, 0, 0],\n",
       " [2,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [2, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       " [0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0],\n",
       " [0, 0, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 1],\n",
       " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 1]]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_num_citations(all_articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_art = 0\n",
    "num_sect = 0\n",
    "all_articles[num_art]['grobid_parse']['body_text'][num_sect]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 57,\n",
       "  'end': 78,\n",
       "  'text': '(Wilson et al., 2013)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF7'},\n",
       " {'start': 369,\n",
       "  'end': 379,\n",
       "  'text': 'Liu (2012)',\n",
       "  'latex': None,\n",
       "  'ref_id': 'BIBREF3'},\n",
       " {'start': 893, 'end': 894, 'text': '1', 'latex': None, 'ref_id': None}]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article['grobid_parse']['body_text'][0]['cite_spans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 369, 893]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "439.6666666666667"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(map(lambda x: int(x['start']),article['grobid_parse']['body_text'][0]['cite_spans'])))\n",
    "np.mean(list(map(lambda x: int(x['start']),article['grobid_parse']['body_text'][0]['cite_spans'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [{'start': 192, 'end': 216, 'text': '(Goldstein et al., 2000;', 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 217, 'end': 239, 'text': 'Erkan and Radev, 2004;', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 240, 'end': 257, 'text': 'Wan et al., 2007;', 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 258, 'end': 284, 'text': 'Nenkova and McKeown, 2012;', 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 285, 'end': 302, 'text': 'Min et al., 2012;', 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 303, 'end': 319, 'text': 'Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 773, 'end': 797, 'text': '(Project Code: 14203414)', 'latex': None, 'ref_id': None}, {'start': 2288, 'end': 2305, 'text': '(Hu et al., 2008;', 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 2306, 'end': 2324, 'text': 'Yang et al., 2011)', 'latex': None, 'ref_id': 'BIBREF22'}, {'start': 2582, 'end': 2598, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 2911, 'end': 2927, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 3069, 'end': 3095, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 3096, 'end': 3117, 'text': 'Rezende et al., 2014)', 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [{'start': 956, 'end': 964, 'text': 'Figure 1', 'latex': None, 'ref_id': 'FIGREF0'}], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.018105849582172703: #citations=13 & sect_len_toks=718|  \n",
      "Number of citations in sentences: [6, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.07142857142857142 : num_sec = 1 len_sects=14\n",
      "Positon aver. citation: 0.3012508743107178 [192, 217, 240, 258, 285, 303, 773, 2288, 2306, 2582, 2911, 3069, 3096] len= 4729\n",
      "====================\n",
      "{'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 451, 'end': 468, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 12, 'end': 20, 'text': 'Figure 2', 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.00684931506849315: #citations=1 & sect_len_toks=146|  \n",
      "Number of citations in sentences: [0, 0, 1, 0, 0]\n",
      "Positon sect: 0.14285714285714285 : num_sec = 2 len_sects=14\n",
      "Positon aver. citation: 0.5130830489192264 [451] len= 879\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [{'start': 32, 'end': 58, 'text': '(Kingma and Welling, 2014;', 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 59, 'end': 79, 'text': 'Rezende et al., 2014', 'latex': None, 'ref_id': 'BIBREF18'}, {'start': 184, 'end': 200, 'text': 'Li et al. (2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2384, 'end': 2401, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 2433, 'end': 2456, 'text': '(Bahdanau et al., 2015;', 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 2457, 'end': 2476, 'text': 'Luong et al., 2015)', 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.0051635111876075735: #citations=6 & sect_len_toks=1162|  \n",
      "Number of citations in sentences: [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.21428571428571427 : num_sec = 3 len_sects=14\n",
      "Positon aver. citation: 0.2179398348634448 [32, 59, 184, 2384, 2433, 2457] len= 5773\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .', 'cite_spans': [{'start': 86, 'end': 102, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 971, 'end': 987, 'text': 'Li et al. (2015)', 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 1133, 'end': 1158, 'text': '(Dantzig and Thapa, 2006)', 'latex': None, 'ref_id': 'BIBREF3'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.013888888888888888: #citations=3 & sect_len_toks=216|  \n",
      "Number of citations in sentences: [2, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "Positon sect: 0.2857142857142857 : num_sec = 4 len_sects=14\n",
      "Positon aver. citation: 0.5983606557377049 [86, 971, 1133] len= 1220\n",
      "====================\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.0: #citations=0 & sect_len_toks=18|  \n",
      "Number of citations in sentences: [0, 0]\n",
      "Positon sect: 0.35714285714285715 : num_sec = 5 len_sects=14\n",
      "Positon aver. citation: 0 [] len= 116\n",
      "====================\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.0: #citations=0 & sect_len_toks=253|  \n",
      "Number of citations in sentences: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.42857142857142855 : num_sec = 6 len_sects=14\n",
      "Positon aver. citation: 0 [] len= 1685\n",
      "====================\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.0: #citations=0 & sect_len_toks=326|  \n",
      "Number of citations in sentences: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.5 : num_sec = 7 len_sects=14\n",
      "Positon aver. citation: 0 [] len= 2028\n",
      "====================\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.0: #citations=0 & sect_len_toks=118|  \n",
      "Number of citations in sentences: [0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.5714285714285714 : num_sec = 8 len_sects=14\n",
      "Positon aver. citation: 0 [] len= 742\n",
      "====================\n",
      "{'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 107, 'end': 118, 'text': '(Lin, 2004)', 'latex': None, 'ref_id': 'BIBREF12'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citing dencity 0.029411764705882353: #citations=1 & sect_len_toks=34|  \n",
      "Number of citations in sentences: [0, 1, 0]\n",
      "Positon sect: 0.6428571428571429 : num_sec = 9 len_sects=14\n",
      "Positon aver. citation: 0.5297029702970297 [107] len= 202\n",
      "====================\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [{'start': 351, 'end': 365, 'text': '(Wasson, 1998)', 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 492, 'end': 512, 'text': '(Radev et al., 2000)', 'latex': None, 'ref_id': 'BIBREF17'}, {'start': 700, 'end': 723, 'text': '(Erkan and Radev, 2004)', 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 737, 'end': 763, 'text': '(Mihalcea and Tarau, 2004)', 'latex': None, 'ref_id': 'BIBREF14'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.021164021164021163: #citations=4 & sect_len_toks=189|  \n",
      "Number of citations in sentences: [0, 0, 1, 1, 2, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.7142857142857143 : num_sec = 10 len_sects=14\n",
      "Positon aver. citation: 0.45346062052505964 [351, 492, 700, 737] len= 1257\n",
      "====================\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .', 'cite_spans': [{'start': 498, 'end': 518, 'text': '(Kingma and Ba, 2014', 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 652, 'end': 674, 'text': '(Bastien et al., 2012)', 'latex': None, 'ref_id': 'BIBREF1'}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.015384615384615385: #citations=2 & sect_len_toks=130|  \n",
      "Number of citations in sentences: [0, 0, 0, 0, 0, 1, 1]\n",
      "Positon sect: 0.7857142857142857 : num_sec = 11 len_sects=14\n",
      "Positon aver. citation: 0.8285302593659942 [498, 652] len= 694\n",
      "====================\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.', 'cite_spans': [{'start': 690, 'end': 707, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 908, 'end': 925, 'text': '(Li et al., 2017)', 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [{'start': 77, 'end': 84, 'text': 'Table 1', 'latex': None, 'ref_id': 'TABREF0'}, {'start': 746, 'end': 753, 'text': 'Table 2', 'latex': None, 'ref_id': 'TABREF1'}, {'start': 1184, 'end': 1191, 'text': 'Table 3', 'latex': None, 'ref_id': 'TABREF2'}], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.008: #citations=2 & sect_len_toks=250|  \n",
      "Number of citations in sentences: [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.8571428571428571 : num_sec = 12 len_sects=14\n",
      "Positon aver. citation: 0.5115236875800256 [690, 908] len= 1562\n",
      "====================\n",
      "{'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".', 'cite_spans': [], 'ref_spans': [{'start': 315, 'end': 322, 'text': 'Table 4', 'latex': None, 'ref_id': 'TABREF3'}], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.0: #citations=0 & sect_len_toks=150|  \n",
      "Number of citations in sentences: [0, 0, 0, 0, 0, 0, 0]\n",
      "Positon sect: 0.9285714285714286 : num_sec = 13 len_sects=14\n",
      "Positon aver. citation: 0 [] len= 924\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ', 'cite_spans': [{'start': 517, 'end': 868, 'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world', 'latex': None, 'ref_id': None}], 'ref_spans': [], 'eq_spans': [], 'section': None}\n",
      "Features: \n",
      "citing dencity 0.008547008547008548: #citations=1 & sect_len_toks=117|  \n",
      "Number of citations in sentences: [0, 0, 0, 0, 1]\n",
      "Positon sect: 1.0 : num_sec = 14 len_sects=14\n",
      "Positon aver. citation: 0.5949367088607594 [517] len= 869\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "grobid_parse_overview = dict()\n",
    "article = all_articles[num_art]\n",
    "len_sects = len(article['grobid_parse']['body_text'])\n",
    "for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "    grobid_parse_overview[num_sec] = sections\n",
    "    print(sections)\n",
    "    cnt_citations = len(grobid_parse_overview[num_sec]['cite_spans'])\n",
    "    sect_len = len(sections['text'])\n",
    "    sect_len_toks = len(sections['text'].split())\n",
    "    cite_spans_sec_start = list(map(lambda x: int(x['start']),grobid_parse_overview[num_sec]['cite_spans']))\n",
    "    cite_spans_sec_start_mean = 0\n",
    "    sents_num_citations = sect_num_citations(sections)\n",
    "    if len(cite_spans_sec_start) >=1:\n",
    "        cite_spans_sec_start_mean = np.mean(cite_spans_sec_start)/sect_len\n",
    "    print('Features: \\nciting dencity {0}: #citations={1} & sect_len_toks={2}|  '.format(cnt_citations/sect_len_toks, cnt_citations, sect_len_toks))\n",
    "    print('Number of citations in sentences: {0}'.format(sents_num_citations))\n",
    "    print('Positon sect: {0} : num_sec = {1} len_sects={2}'.format((num_sec+1)/len_sects,num_sec+1,len_sects))\n",
    "    print('Positon aver. citation: {0} {1} len= {2}'.format(cite_spans_sec_start_mean,cite_spans_sec_start,sect_len))\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'The SemEval-2013 task on \"Sentiment Analysis in Twitter\" (Wilson et al., 2013) focuses on polarity classification, i. e. the problem of determining whether a textual unit, e. g. a document, paragraph, sentence or phrase, expresses a positive, negative or neutral sentiment (for a review of research topics and recent developments in the field of sentiment analysis see Liu (2012) ). There are two subtasks: in task B, \"Message Polarity Classification\", whole messages have to be classified as being of positive, negative or neutral sentiment; in task A, \"Contextual Polarity Disambiguation\", a marked instance of a word or phrase has to be classified in the context of a whole message.The training data for task B consist of approximately 10 200 manually annotated Twitter messages, the training data for task A of approximately 9 500 marked instances in approximately 6 300 Twitter messages. 1 The test data consist of in-domain Twitter messages (3 813 messages for task B and 4 435 marked instances in 2 826 messages for task A) and out-of-domain SMS text messages (2 094 messages for task B, 2 334 marked instances in 1 437 messages for task A). The distribution of messages and marked instances over sentiment categories in the training and test sets is shown in Tab. 1. The main focus of the current paper lies on experimenting with resource-lean and robust methods for task B, the classification of whole messages. We do, however, apply our approach also to task A.',\n",
       "  'cite_spans': [{'start': 57,\n",
       "    'end': 78,\n",
       "    'text': '(Wilson et al., 2013)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 369,\n",
       "    'end': 379,\n",
       "    'text': 'Liu (2012)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF3'},\n",
       "   {'start': 893, 'end': 894, 'text': '1', 'latex': None, 'ref_id': None}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 1: {'text': \"Our general approach is quite simple: we extract feature vectors from the training data (based on the 1 These figures indicate the amount of training data we were actually able to use. Due to Twitter's licensing conditions, the training data could only be made available as a collection of IDs. Even when using the official Twitter API for collecting the actual messages rather than the screen-scraping approach suggested by the task organizers, ca. 10% of the data were not (or no longer) available.original messages and a small number of additional resources) and feed them into fast and robust supervised machine learning algorithms implemented in the Python machine learning library scikit-learn (Pedregosa et al., 2011) . For task B, the features are computed on the basis of the whole message; for task A, we use essentially the same features, but compute them once for the marked word or phrase and once for the rest of the message. All the features we use are described in some more detail in the following subsections.\",\n",
       "  'cite_spans': [{'start': 700,\n",
       "    'end': 724,\n",
       "    'text': '(Pedregosa et al., 2011)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF5'}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 2: {'text': 'We experimented with three different sets of bag-ofwords features: unigrams, unigrams and bigrams, and an extended unigram model that includes a simple treatment of negation. For all three models we simply use the word frequencies as feature weights.Our preprocessing pipeline starts with a simple preliminary tokenization step (lowercasing the whole message and splitting it on whitespace). In the resulting list of tokens, all user IDs and web URLs are replaced with placeholders. 2 Any remaining punctuation is stripped from the tokens and empty tokens are deleted. In the extended unigram model, up to three tokens following a negation marker are then prefixed with not_ (fewer tokens if another negation marker or the end of the message is reached). Finally all words are stemmed using the Snowball stemmer. 3 For a token unigram or bigram to be included in the bag of words models, it has to occur in at least five messages.As an additional feature we include the total number of tokens per message.',\n",
       "  'cite_spans': [{'start': 813,\n",
       "    'end': 814,\n",
       "    'text': '3',\n",
       "    'latex': None,\n",
       "    'ref_id': None}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 3: {'text': 'Widely-used algorithms such as SentiStrength (Thelwall et al., 2010) rely heavily on dictionaries containing sentiment ratings of words and/or phrases. We use features based on an extended version of AFINN-111 (Nielsen, 2011) . 4 The AFINN sentiment dictionary contains sentiment ratings ranging from −5 (very negative) to 5 (very positive) for 2 476 word forms. In order to obtain a better coverage, we extended the dictionary with distributionally similar words. For this purpose, large-vocabulary distributional semantic models (DSM) were constructed from a version of the English Wikipedia 5 and the Google Web 1T 5-Grams database (Brants and Franz, 2006) . The Wikipedia DSM consists of 122 281 case-folded word forms as target terms and 30 484 mid-frequency content words (lemmatised) as feature terms; the Web1T5 DSM of 241 583 case-folded word forms as target terms and 100 063 case-folded word forms as feature terms. Both DSMs use a context window of two words to the left and right, and were reduced to 300 latent dimensions using randomized singular value decomposition (Halko et al., 2009 ).For each AFINN entry, the 30 nearest neighbours according to each DSM were considered as extension candidates. Sentiment ratings for the new candidates were computed by averaging over the 30 nearest neighbours of the respective candidate term (with scores set to 0 for all neighbours not listed in AFINN), and rescaling to the range [−5, 5]. 6 After some initial experiments, only candidates with a computed rating ≤ −2.5 or ≥ 2.5 were retained, resulting in an extended dictionary of 2 820 word forms.As with the bag of words model, we make use of a simple heuristic treatment of negation: following a negation marker, the polarity of the next sentimentcarrying token up to a distance of at most four tokens is multiplied by −1.The sentiment dictionary is used to extract four features: I) the number of tokens that express a positive sentiment, II) the number of tokens that express a negative sentiment, III) the total number of tokens that express a sentiment according to our sentiment dictionary and IV) the arithmetic mean of all the sentiment scores from the sentiment dictionary in the message.',\n",
       "  'cite_spans': [{'start': 45,\n",
       "    'end': 68,\n",
       "    'text': '(Thelwall et al., 2010)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'},\n",
       "   {'start': 200,\n",
       "    'end': 225,\n",
       "    'text': 'AFINN-111 (Nielsen, 2011)',\n",
       "    'latex': None,\n",
       "    'ref_id': None},\n",
       "   {'start': 228, 'end': 229, 'text': '4', 'latex': None, 'ref_id': None},\n",
       "   {'start': 635,\n",
       "    'end': 659,\n",
       "    'text': '(Brants and Franz, 2006)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF0'},\n",
       "   {'start': 1082,\n",
       "    'end': 1101,\n",
       "    'text': '(Halko et al., 2009',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 1446, 'end': 1447, 'text': '6', 'latex': None, 'ref_id': None}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 4: {'text': 'In addition to the sentiment dictionary we use a list of 212 emoticons and 95 internet slang abbreviations from Wikipedia. We manually classified these 307 emotion markers as negative (−1), neutral (0) or positive (1). The extracted features based on this list are similar to the ones based on the sentiment dictionary. We use I) the number of positive emotion markers, II) the number of negative emotion markers, III) the total number of emotion markers and IV) the arithmetic mean of all the emotion markers in the message.',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 5: {'text': \"In this section we evaluate different classifiers (multinomial Naive Bayes, 7 Linear SVM 8 and Maximum Entropy 9 ) and various combinations of features on the gold test sets. We vary the bag-of-words model (bow), the use of AFINN (sent), our extensions to the sentiment dictionary (ext) and the list of emotion markers (emo). To present as clear a picture of the classifiers' performances as possible, we report Fscores for each of the three classes, the weighted average of all three F-scores (F w ), the (unweighted) average of the positive and negative F-scores (F pos+neg ; this is the value shown in the official task results and used for ranking systems), as well as accuracy.Results for submitted systems are typeset in italics, the best results in each column are typeset in bold font.\",\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 6: {'text': \"Experiments with just a simple unigram bag-ofwords model show that for both the Twitter (Tab. 3) and the SMS data (Tab. 4) the Maximum Entropy classifier outperforms multinomial Naive Bayes and Linear SVM by a considerable margin. For comparison, we also include some weak baselines (Tab. 2). The random baselines classify messages randomly, 10 7 We always use the default setting alpha = 1.0. 8 In all experiments, we use the following parameters: penalty = 'l1', dual = False, C = 1.0.9 We use the following parameter settings in our experiments: penalty = 'l1', C = 1.0.10 random uniform assumes a uniform probability distribution (all categories have equal probabilities), random weighted has learned the probability distribution from the training data, the majority baselines simply assign all messages to the most frequent category in the training data. 11 As one would expect, all three learning algorithms are vastly superior to those baselines. Using both unigrams and bigrams in the bag-of-words model improves classifier peformance; so does the extended unigram model with negations.For the Twitter data, adding the sentiment dictionary, the dictionary extensions and the list of emotion markers further improves classifier performance, with the best results being achieved by a combination of all these features with a uni-and bigram bagof-words model. The best combination of features would have been the fourth best system out of 35 constrained systems (sixth best out of all 51 systems), one rank higher than our task submission. 12 For the SMS data, adding the sentiment dictionary and the dictionary extensions seems to improve the official score F pos+neg , but slightly decreases weighted average F-score and accuracy. This might be due to the greater orthographical variation in SMS texts. Emotion markers seem to be a much better sentiment indicator in the SMS data. But while just combining the list of emotion markers with the extended unigram bag-of-words model leads to the best weighted average F-score and accuracy, F pos+neg is best when a combination of all features is used. This is also the system we submitted, being the third best system (out of 44) for that task.\",\n",
       "  'cite_spans': [{'start': 1545,\n",
       "    'end': 1547,\n",
       "    'text': '12',\n",
       "    'latex': None,\n",
       "    'ref_id': None}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 7: {'text': \"The results for task A are similar to those for task B in that Maximum Entropy is the best classifier for the unigram bag-of-words model for both the Twitter (Tab. 5) and the SMS data (Tab. 6). Adding negation treatment to the bag-of-words model increases classifier performance, as do the inclusion of AFINN and the use of emotion markers. Interestingly, extending the sentiment dictionary based on distributional similarity leads to slightly worse results. Therefore, random weighted,binary uses the same probability distribution but classifies messages only as either positive or negative. 11 majority classifies all messages as neutral, as this is the most frequent category in the training data, majority binary does binary classification and thus classifies all messages as positive.12 Evaluation results for all SemEval-2013 we could have improved upon our task submission by excluding the sentiment dictionary extensionshowever, the gains are very small and the system's ranks would still be the same (17/28 for the Twitter data, 16/26 for the SMS data). in (1) and (2) report a negative and positive event, respectively, in a neutral way and should therefore be annotated with neutral sentiment. However, in the test data they are labelled as negative and positive by the human annotators.(1) MT @LccSy #Syria, Deir Ezzor | Marba'eh: Aerial shelling dropped explosive barrels on residential buildings in the town. Tue, 23 October.European Exchanges open with a slight rise: (AGI) Rome, October 24 -European Exchanges opened with a slight ris... http://t.co/mAljf6eTThis problem is probably a major factor in the misclassification of many negative and positive messages as neutral. In order to better reproduce the human annotations, the system would additionally have to decide whether a reported event is of a negative, positive or neutral nature per se -a quite different task that would require external training data and world knowledge.An analysis of mis-classified positive messages further suggests that certain punctuation marks, especially multiple exclamation marks, might be useful as additional features.\",\n",
       "  'cite_spans': [{'start': 819,\n",
       "    'end': 831,\n",
       "    'text': 'SemEval-2013',\n",
       "    'latex': None,\n",
       "    'ref_id': None}],\n",
       "  'ref_spans': [],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 8: {'text': 'The confusion matrix in Tab ',\n",
       "  'cite_spans': [],\n",
       "  'ref_spans': [{'start': 24,\n",
       "    'end': 27,\n",
       "    'text': 'Tab',\n",
       "    'latex': None,\n",
       "    'ref_id': None}],\n",
       "  'eq_spans': [],\n",
       "  'section': None},\n",
       " 9: {'text': 'We use a resource-lean approach, relying only on three external resources: a stemmer, a relatively small sentiment dictionary and an even smaller list of emotion markers. Stemmers are already available for many languages and both kinds of lexical resources can be gathered relatively easily for other languages. The list of emotion markers should apply to most languages. This makes our whole system relatively language-independent, provided that a similar amount of manually labelled training data is available. 13 In fact, the learning curve for our system ( Fig. 1) suggests that even as few as 3 000-3 500 labelled messages might be sufficient. The similar Figure 1: Learning curve of our system for the \"Message Polarity Classification\" task, evaluated on the Twitter data evaluation results for the Twitter and the SMS data show that not relying on Twitter-specific features like hashtags pays off: by making our system as generic as possible, it is robust, not overfitted to the training data, and generalizes well to other types of data. The methods discussed in the current paper are particularly well suited to the \"Message Polarity Classification\" task, our system ranking amongst the best. It turns out, however, that simply applying the same approach to the \"Contextual Polarity Disambiguation\" task yields only mediocre results.In the future, we would like to experiment with a couple of additional features. Determining the nearest neighbors of a message based on Latent Semantic Analysis might be a useful addition, as might be the use of part-of-speech tags created by an in-domain POS tagger (Gimpel et al., 2011) 14 . We would also like to find out whether a heuristic treatment of intensifiers and detensifiers, the normalization of character repetitions, or the inclusion of some punctuationbased features could further improve classifier performance. 13 For task B, even the extended unigram bag-of-words model by itself, without any additional resources, would have performed quite well as the 9th best constrained system on the Twitter test set (13th best system overall) and the 5th best system on the SMS test set.14 http://www.ark.cs.cmu.edu/TweetNLP/',\n",
       "  'cite_spans': [{'start': 513,\n",
       "    'end': 515,\n",
       "    'text': '13',\n",
       "    'latex': None,\n",
       "    'ref_id': None},\n",
       "   {'start': 1610,\n",
       "    'end': 1631,\n",
       "    'text': '(Gimpel et al., 2011)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF1'},\n",
       "   {'start': 1632, 'end': 1634, 'text': '14', 'latex': None, 'ref_id': None},\n",
       "   {'start': 1873, 'end': 1875, 'text': '13', 'latex': None, 'ref_id': None}],\n",
       "  'ref_spans': [{'start': 561,\n",
       "    'end': 568,\n",
       "    'text': 'Fig. 1)',\n",
       "    'latex': None,\n",
       "    'ref_id': None}],\n",
       "  'eq_spans': [],\n",
       "  'section': None}}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grobid_parse_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "для latex_parse:\n",
    " 1. Собираем все абзацы по секциям\n",
    " 2. Рассчёт признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 7\n",
      "{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'cite_spans': [], 'ref_spans': [{'start': 118, 'end': 125, 'text': None, 'latex': None, 'ref_id': 'FIGREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'cite_spans': [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Overview 1\n",
      "{'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.', 'cite_spans': [{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 19, 'end': 26, 'text': None, 'latex': None, 'ref_id': 'FIGREF7'}], 'eq_spans': [{'start': 212, 'end': 223, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 739, 'end': 750, 'text': 'ρ i ', 'latex': '\\\\rho _i', 'ref_id': None}, {'start': 774, 'end': 785, 'text': '𝐱 c i ', 'latex': '\\\\mathbf {x}_c^i', 'ref_id': None}, {'start': 807, 'end': 818, 'text': 'ρ∈ℝ n c  ', 'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}], 'section': 'Overview'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'cite_spans': [{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [], 'eq_spans': [{'start': 472, 'end': 483, 'text': 'p θ (𝐳)=𝒩(0,𝐈)', 'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})', 'ref_id': None}, {'start': 488, 'end': 499, 'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)', 'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})', 'ref_id': None}, {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None}, {'start': 524, 'end': 535, 'text': 'σ', 'latex': '\\\\sigma ', 'ref_id': None}, {'start': 799, 'end': 811, 'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\", 'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n', 'ref_id': 'EQREF9'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [{'start': 50, 'end': 56, 'text': None, 'latex': None, 'ref_id': 'EQREF9'}], 'eq_spans': [{'start': 131, 'end': 142, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 145, 'end': 157, 'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\", 'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ', 'ref_id': 'EQREF10'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': '𝐱', 'latex': '\\\\mathbf {x}', 'ref_id': None}, {'start': 76, 'end': 87, 'text': '𝐱 d ', 'latex': '\\\\mathbf {x}_d', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐱 c ', 'latex': '\\\\mathbf {x}_c', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 364, 'end': 375, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 444, 'end': 456, 'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )', 'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n', 'ref_id': 'EQREF11'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The calculation of INLINEFORM0 will be discussed later.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 19, 'end': 30, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 113, 'end': 124, 'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }', 'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ', 'ref_id': None}, {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 219, 'end': 230, 'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }', 'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ', 'ref_id': None}, {'start': 320, 'end': 331, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 335, 'end': 346, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None}, {'start': 402, 'end': 413, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 416, 'end': 428, 'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )', 'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n', 'ref_id': 'EQREF12'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'cite_spans': [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [], 'eq_spans': [{'start': 304, 'end': 315, 'text': 's h i ', 'latex': 's^i_{h}', 'ref_id': None}, {'start': 366, 'end': 377, 'text': 'h d j ', 'latex': 'h^j_{d}', 'ref_id': None}, {'start': 401, 'end': 412, 'text': 'a d ∈ℝ n d  ', 'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}', 'ref_id': None}, {'start': 472, 'end': 483, 'text': 'h c j ', 'latex': 'h^j_{c}', 'ref_id': None}, {'start': 507, 'end': 518, 'text': 'a c ∈ℝ n c  ', 'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}', 'ref_id': None}, {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 672, 'end': 684, 'text': 'a ˜ c =a c ×ρ', 'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n', 'ref_id': 'EQREF13'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 30, 'end': 41, 'text': 'c d i ', 'latex': 'c_d^i', 'ref_id': None}, {'start': 79, 'end': 90, 'text': 'c c i ', 'latex': 'c_c^i', 'ref_id': None}, {'start': 240, 'end': 252, 'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )', 'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n', 'ref_id': 'EQREF14'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 64, 'end': 75, 'text': 's ˜ h i ', 'latex': '{{\\\\tilde{s}}_h^i}', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': '𝐒 z ', 'latex': '\\\\mathbf {S}_z', 'ref_id': None}, {'start': 15, 'end': 26, 'text': '𝐒 h ', 'latex': '\\\\mathbf {S}_h', 'ref_id': None}, {'start': 33, 'end': 44, 'text': '𝐒 x ', 'latex': '\\\\mathbf {S}_x', 'ref_id': None}, {'start': 220, 'end': 231, 'text': '𝐀 d ∈ℝ n d ×m ', 'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}', 'ref_id': None}, {'start': 297, 'end': 308, 'text': '𝐀 c ∈ℝ n c ×m ', 'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}', 'ref_id': None}, {'start': 569, 'end': 581, 'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )', 'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n', 'ref_id': 'EQREF15'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': 'ℒ(θ,ϕ;𝐱)', 'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})', 'ref_id': None}, {'start': 170, 'end': 182, 'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )', 'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n', 'ref_id': 'EQREF16'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'Θ', 'latex': '\\\\Theta ', 'ref_id': None}, {'start': 110, 'end': 121, 'text': '𝐀 d ', 'latex': '\\\\mathbf {A}_d', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}, {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None}, {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None}, {'start': 434, 'end': 445, 'text': 'R∈ℝ n d ×n c  ', 'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}', 'ref_id': None}, {'start': 450, 'end': 462, 'text': 'R=X d ×X c T ', 'latex': '\\nR = X_d\\\\times X_c^T\\n', 'ref_id': 'EQREF17'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 93, 'end': 105, 'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]', 'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n', 'ref_id': 'EQREF18'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': '(0,1)', 'latex': '(0,1)', 'ref_id': None}, {'start': 84, 'end': 96, 'text': 'ρ=sigmoid(𝐫)', 'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n', 'ref_id': 'EQREF19'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 332, 'end': 343, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 346, 'end': 358, 'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ', 'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n', 'ref_id': 'EQREF20'}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 0\n",
      "{'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'ρ z ', 'latex': '\\\\rho _z', 'ref_id': None}, {'start': 22, 'end': 33, 'text': 'ρ x ', 'latex': '\\\\rho _x', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Reader-Aware Salience Estimation'}\n",
      "====================\n",
      "Summary Construction 2\n",
      "{'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'cite_spans': [{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [{'start': 485, 'end': 497, 'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },', 'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n', 'ref_id': 'EQREF22'}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Summary Construction 4\n",
      "{'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.', 'cite_spans': [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': 'α i ', 'latex': '\\\\alpha _i', 'ref_id': None}, {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None}, {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 126, 'end': 137, 'text': 'α ij ', 'latex': '\\\\alpha _{ij}', 'ref_id': None}, {'start': 142, 'end': 153, 'text': 'R ij ', 'latex': 'R_{ij}', 'ref_id': None}, {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None}, {'start': 234, 'end': 245, 'text': 'P j ', 'latex': 'P_j', 'ref_id': None}], 'section': 'Summary Construction'}\n",
      "====================\n",
      "Data Description 0\n",
      "{'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Description'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'The definition of the terminology related to the dataset is given as follows.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Background 0\n",
      "{'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Background'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Collection 0\n",
      "{'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Data Collection'}\n",
      "====================\n",
      "Data Properties 0\n",
      "{'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.', 'cite_spans': [], 'ref_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}], 'eq_spans': [], 'section': 'Data Properties'}\n",
      "====================\n",
      "Dataset and Metrics 1\n",
      "{'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.', 'cite_spans': [{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}], 'ref_spans': [{'start': 58, 'end': 66, 'text': None, 'latex': None, 'ref_id': 'SECREF28'}], 'eq_spans': [], 'section': 'Dataset and Metrics'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'cite_spans': [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'cite_spans': [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'cite_spans': [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 2\n",
      "{'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 1\n",
      "{'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'cite_spans': [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Comparative Methods 0\n",
      "{'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Comparative Methods'}\n",
      "====================\n",
      "Experimental Settings 2\n",
      "{'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.', 'cite_spans': [{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}], 'ref_spans': [], 'eq_spans': [{'start': 94, 'end': 105, 'text': '|V|', 'latex': '|V|', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None}, {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None}, {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None}, {'start': 360, 'end': 371, 'text': 'm=5', 'latex': 'm = 5', 'ref_id': None}, {'start': 431, 'end': 442, 'text': 'd h =500', 'latex': 'd_h = 500', 'ref_id': None}, {'start': 463, 'end': 474, 'text': 'K=100', 'latex': 'K = 100', 'ref_id': None}, {'start': 495, 'end': 506, 'text': 'λ p ', 'latex': '\\\\lambda _p', 'ref_id': None}, {'start': 538, 'end': 549, 'text': 'λ p =0.2', 'latex': '\\\\lambda _p=0.2', 'ref_id': None}], 'section': 'Experimental Settings'}\n",
      "====================\n",
      "Results on Our Dataset 0\n",
      "{'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.', 'cite_spans': [], 'ref_spans': [{'start': 83, 'end': 91, 'text': None, 'latex': None, 'ref_id': 'TABREF40'}], 'eq_spans': [{'start': 240, 'end': 251, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Results on Our Dataset'}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', 'cite_spans': [{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'TABREF42'}], 'eq_spans': [{'start': 380, 'end': 391, 'text': 'p<0.05', 'latex': 'p<0.05', 'ref_id': None}], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Further Investigation of Our Framework  1\n",
      "{'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\", 'cite_spans': [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], 'ref_spans': [{'start': 305, 'end': 313, 'text': None, 'latex': None, 'ref_id': 'TABREF43'}], 'eq_spans': [], 'section': 'Further Investigation of Our Framework '}\n",
      "====================\n",
      "Case Study 0\n",
      "{'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.', 'cite_spans': [], 'ref_spans': [{'start': 250, 'end': 258, 'text': None, 'latex': None, 'ref_id': 'TABREF45'}], 'eq_spans': [], 'section': 'Case Study'}\n",
      "====================\n",
      "Conclusions 0\n",
      "{'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusions'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "article = all_articles[0]\n",
    "latex_parse_overview = dict()\n",
    "for sections in article['latex_parse']['body_text']:\n",
    "    latex_parse_overview[sections['section']] = sections\n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': [],\n",
       " 'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "   'cite_spans': [{'start': 193,\n",
       "     'end': 200,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF0'},\n",
       "    {'start': 203,\n",
       "     'end': 210,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 213,\n",
       "     'end': 220,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF2'},\n",
       "    {'start': 223,\n",
       "     'end': 230,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF3'},\n",
       "    {'start': 233,\n",
       "     'end': 240,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF4'},\n",
       "    {'start': 243,\n",
       "     'end': 250,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 253,\n",
       "     'end': 260,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 118,\n",
       "     'end': 125,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF2'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "   'cite_spans': [{'start': 527,\n",
       "     'end': 534,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF7'},\n",
       "    {'start': 537,\n",
       "     'end': 544,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF8'},\n",
       "    {'start': 802,\n",
       "     'end': 809,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "   'cite_spans': [{'start': 10,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'},\n",
       "    {'start': 159,\n",
       "     'end': 167,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 170,\n",
       "     'end': 178,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "   'cite_spans': [{'start': 489,\n",
       "     'end': 496,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 19,\n",
       "     'end': 26,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF7'}],\n",
       "   'eq_spans': [{'start': 212,\n",
       "     'end': 223,\n",
       "     'text': 'X d ',\n",
       "     'latex': 'X_d',\n",
       "     'ref_id': None},\n",
       "    {'start': 228, 'end': 239, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None},\n",
       "    {'start': 254, 'end': 265, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None},\n",
       "    {'start': 285, 'end': 296, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None},\n",
       "    {'start': 739,\n",
       "     'end': 750,\n",
       "     'text': 'ρ i ',\n",
       "     'latex': '\\\\rho _i',\n",
       "     'ref_id': None},\n",
       "    {'start': 774,\n",
       "     'end': 785,\n",
       "     'text': '𝐱 c i ',\n",
       "     'latex': '\\\\mathbf {x}_c^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 807,\n",
       "     'end': 818,\n",
       "     'text': 'ρ∈ℝ n c  ',\n",
       "     'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Overview'},\n",
       "  {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 32,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 43,\n",
       "     'end': 51,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'},\n",
       "    {'start': 154,\n",
       "     'end': 161,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 472,\n",
       "     'end': 483,\n",
       "     'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "     'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "     'ref_id': None},\n",
       "    {'start': 488,\n",
       "     'end': 499,\n",
       "     'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "     'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "     'ref_id': None},\n",
       "    {'start': 508, 'end': 519, 'text': 'μ', 'latex': '\\\\mu ', 'ref_id': None},\n",
       "    {'start': 524,\n",
       "     'end': 535,\n",
       "     'text': 'σ',\n",
       "     'latex': '\\\\sigma ',\n",
       "     'ref_id': None},\n",
       "    {'start': 799,\n",
       "     'end': 811,\n",
       "     'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "     'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "     'ref_id': 'EQREF9'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 50,\n",
       "     'end': 56,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'EQREF9'}],\n",
       "   'eq_spans': [{'start': 131,\n",
       "     'end': 142,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 145,\n",
       "     'end': 157,\n",
       "     'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "     'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "     'ref_id': 'EQREF10'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': '𝐱',\n",
       "     'latex': '\\\\mathbf {x}',\n",
       "     'ref_id': None},\n",
       "    {'start': 76,\n",
       "     'end': 87,\n",
       "     'text': '𝐱 d ',\n",
       "     'latex': '\\\\mathbf {x}_d',\n",
       "     'ref_id': None},\n",
       "    {'start': 110,\n",
       "     'end': 121,\n",
       "     'text': '𝐱 c ',\n",
       "     'latex': '\\\\mathbf {x}_c',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 364,\n",
       "     'end': 375,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 430, 'end': 441, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 444,\n",
       "     'end': 456,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "     'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "     'ref_id': 'EQREF11'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 19,\n",
       "     'end': 30,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 113,\n",
       "     'end': 124,\n",
       "     'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "     'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "    {'start': 219,\n",
       "     'end': 230,\n",
       "     'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "     'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 320,\n",
       "     'end': 331,\n",
       "     'text': '𝐒 z ',\n",
       "     'latex': '\\\\mathbf {S}_z',\n",
       "     'ref_id': None},\n",
       "    {'start': 335,\n",
       "     'end': 346,\n",
       "     'text': '𝐒 h ',\n",
       "     'latex': '\\\\mathbf {S}_h',\n",
       "     'ref_id': None},\n",
       "    {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "    {'start': 402,\n",
       "     'end': 413,\n",
       "     'text': '𝐒 x ',\n",
       "     'latex': '\\\\mathbf {S}_x',\n",
       "     'ref_id': None},\n",
       "    {'start': 416,\n",
       "     'end': 428,\n",
       "     'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "     'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "     'ref_id': 'EQREF12'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 7,\n",
       "     'end': 14,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'},\n",
       "    {'start': 46,\n",
       "     'end': 54,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF12'},\n",
       "    {'start': 57,\n",
       "     'end': 65,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF13'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 304,\n",
       "     'end': 315,\n",
       "     'text': 's h i ',\n",
       "     'latex': 's^i_{h}',\n",
       "     'ref_id': None},\n",
       "    {'start': 366,\n",
       "     'end': 377,\n",
       "     'text': 'h d j ',\n",
       "     'latex': 'h^j_{d}',\n",
       "     'ref_id': None},\n",
       "    {'start': 401,\n",
       "     'end': 412,\n",
       "     'text': 'a d ∈ℝ n d  ',\n",
       "     'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "     'ref_id': None},\n",
       "    {'start': 472,\n",
       "     'end': 483,\n",
       "     'text': 'h c j ',\n",
       "     'latex': 'h^j_{c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 507,\n",
       "     'end': 518,\n",
       "     'text': 'a c ∈ℝ n c  ',\n",
       "     'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 616, 'end': 627, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 672,\n",
       "     'end': 684,\n",
       "     'text': 'a ˜ c =a c ×ρ',\n",
       "     'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "     'ref_id': 'EQREF13'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 30,\n",
       "     'end': 41,\n",
       "     'text': 'c d i ',\n",
       "     'latex': 'c_d^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 79,\n",
       "     'end': 90,\n",
       "     'text': 'c c i ',\n",
       "     'latex': 'c_c^i',\n",
       "     'ref_id': None},\n",
       "    {'start': 240,\n",
       "     'end': 252,\n",
       "     'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "     'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "     'ref_id': 'EQREF14'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 64,\n",
       "     'end': 75,\n",
       "     'text': 's ˜ h i ',\n",
       "     'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': '𝐒 z ',\n",
       "     'latex': '\\\\mathbf {S}_z',\n",
       "     'ref_id': None},\n",
       "    {'start': 15,\n",
       "     'end': 26,\n",
       "     'text': '𝐒 h ',\n",
       "     'latex': '\\\\mathbf {S}_h',\n",
       "     'ref_id': None},\n",
       "    {'start': 33,\n",
       "     'end': 44,\n",
       "     'text': '𝐒 x ',\n",
       "     'latex': '\\\\mathbf {S}_x',\n",
       "     'ref_id': None},\n",
       "    {'start': 220,\n",
       "     'end': 231,\n",
       "     'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "     'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "     'ref_id': None},\n",
       "    {'start': 297,\n",
       "     'end': 308,\n",
       "     'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "     'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "     'ref_id': None},\n",
       "    {'start': 569,\n",
       "     'end': 581,\n",
       "     'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "     'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "     'ref_id': 'EQREF15'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 70,\n",
       "     'end': 81,\n",
       "     'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "     'ref_id': None},\n",
       "    {'start': 170,\n",
       "     'end': 182,\n",
       "     'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "     'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "     'ref_id': 'EQREF16'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'Θ',\n",
       "     'latex': '\\\\Theta ',\n",
       "     'ref_id': None},\n",
       "    {'start': 110,\n",
       "     'end': 121,\n",
       "     'text': '𝐀 d ',\n",
       "     'latex': '\\\\mathbf {A}_d',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 94,\n",
       "     'end': 105,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None},\n",
       "    {'start': 199, 'end': 210, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None},\n",
       "    {'start': 348, 'end': 359, 'text': 'X d ', 'latex': 'X_d', 'ref_id': None},\n",
       "    {'start': 390, 'end': 401, 'text': 'X c ', 'latex': 'X_c', 'ref_id': None},\n",
       "    {'start': 434,\n",
       "     'end': 445,\n",
       "     'text': 'R∈ℝ n d ×n c  ',\n",
       "     'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "     'ref_id': None},\n",
       "    {'start': 450,\n",
       "     'end': 462,\n",
       "     'text': 'R=X d ×X c T ',\n",
       "     'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "     'ref_id': 'EQREF17'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 93,\n",
       "     'end': 105,\n",
       "     'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "     'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "     'ref_id': 'EQREF18'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 70,\n",
       "     'end': 81,\n",
       "     'text': '(0,1)',\n",
       "     'latex': '(0,1)',\n",
       "     'ref_id': None},\n",
       "    {'start': 84,\n",
       "     'end': 96,\n",
       "     'text': 'ρ=sigmoid(𝐫)',\n",
       "     'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "     'ref_id': 'EQREF19'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 332,\n",
       "     'end': 343,\n",
       "     'text': 'λ p ',\n",
       "     'latex': '\\\\lambda _p',\n",
       "     'ref_id': None},\n",
       "    {'start': 346,\n",
       "     'end': 358,\n",
       "     'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "     'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "     'ref_id': 'EQREF20'}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'ρ z ',\n",
       "     'latex': '\\\\rho _z',\n",
       "     'ref_id': None},\n",
       "    {'start': 22,\n",
       "     'end': 33,\n",
       "     'text': 'ρ x ',\n",
       "     'latex': '\\\\rho _x',\n",
       "     'ref_id': None},\n",
       "    {'start': 142,\n",
       "     'end': 153,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Reader-Aware Salience Estimation'},\n",
       "  {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "   'cite_spans': [{'start': 82,\n",
       "     'end': 89,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 94,\n",
       "     'end': 101,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 485,\n",
       "     'end': 497,\n",
       "     'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "     'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "     'ref_id': 'EQREF22'}],\n",
       "   'section': 'Summary Construction'},\n",
       "  {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "   'cite_spans': [{'start': 466,\n",
       "     'end': 474,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF14'},\n",
       "    {'start': 477,\n",
       "     'end': 484,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 491,\n",
       "     'end': 498,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'},\n",
       "    {'start': 644,\n",
       "     'end': 652,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF15'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': 'α i ',\n",
       "     'latex': '\\\\alpha _i',\n",
       "     'ref_id': None},\n",
       "    {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "    {'start': 112, 'end': 123, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 126,\n",
       "     'end': 137,\n",
       "     'text': 'α ij ',\n",
       "     'latex': '\\\\alpha _{ij}',\n",
       "     'ref_id': None},\n",
       "    {'start': 142,\n",
       "     'end': 153,\n",
       "     'text': 'R ij ',\n",
       "     'latex': 'R_{ij}',\n",
       "     'ref_id': None},\n",
       "    {'start': 220, 'end': 231, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "    {'start': 234,\n",
       "     'end': 245,\n",
       "     'text': 'P j ',\n",
       "     'latex': 'P_j',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Summary Construction'},\n",
       "  {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Description'},\n",
       "  {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Background'},\n",
       "  {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Collection'},\n",
       "  {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 222,\n",
       "     'end': 229,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF7'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Data Properties'},\n",
       "  {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "   'cite_spans': [{'start': 113,\n",
       "     'end': 121,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF16'}],\n",
       "   'ref_spans': [{'start': 58,\n",
       "     'end': 66,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF28'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Dataset and Metrics'},\n",
       "  {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "   'cite_spans': [{'start': 10,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "   'cite_spans': [{'start': 5,\n",
       "     'end': 13,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF17'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "   'cite_spans': [{'start': 9,\n",
       "     'end': 17,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF18'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "   'cite_spans': [{'start': 8,\n",
       "     'end': 15,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 29,\n",
       "     'end': 37,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF19'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "   'cite_spans': [{'start': 8,\n",
       "     'end': 15,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Comparative Methods'},\n",
       "  {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "   'cite_spans': [{'start': 557,\n",
       "     'end': 565,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF20'},\n",
       "    {'start': 697,\n",
       "     'end': 705,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF21'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 94,\n",
       "     'end': 105,\n",
       "     'text': '|V|',\n",
       "     'latex': '|V|',\n",
       "     'ref_id': None},\n",
       "    {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "    {'start': 194, 'end': 205, 'text': 'n d ', 'latex': 'n_d', 'ref_id': None},\n",
       "    {'start': 210, 'end': 221, 'text': 'n c ', 'latex': 'n_c', 'ref_id': None},\n",
       "    {'start': 360,\n",
       "     'end': 371,\n",
       "     'text': 'm=5',\n",
       "     'latex': 'm = 5',\n",
       "     'ref_id': None},\n",
       "    {'start': 431,\n",
       "     'end': 442,\n",
       "     'text': 'd h =500',\n",
       "     'latex': 'd_h = 500',\n",
       "     'ref_id': None},\n",
       "    {'start': 463,\n",
       "     'end': 474,\n",
       "     'text': 'K=100',\n",
       "     'latex': 'K = 100',\n",
       "     'ref_id': None},\n",
       "    {'start': 495,\n",
       "     'end': 506,\n",
       "     'text': 'λ p ',\n",
       "     'latex': '\\\\lambda _p',\n",
       "     'ref_id': None},\n",
       "    {'start': 538,\n",
       "     'end': 549,\n",
       "     'text': 'λ p =0.2',\n",
       "     'latex': '\\\\lambda _p=0.2',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Experimental Settings'},\n",
       "  {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 83,\n",
       "     'end': 91,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF40'}],\n",
       "   'eq_spans': [{'start': 240,\n",
       "     'end': 251,\n",
       "     'text': 'p<0.05',\n",
       "     'latex': 'p<0.05',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Results on Our Dataset'},\n",
       "  {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "   'cite_spans': [{'start': 208,\n",
       "     'end': 215,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 260,\n",
       "     'end': 268,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF42'}],\n",
       "   'eq_spans': [{'start': 380,\n",
       "     'end': 391,\n",
       "     'text': 'p<0.05',\n",
       "     'latex': 'p<0.05',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Further Investigation of Our Framework '},\n",
       "  {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "   'cite_spans': [{'start': 33,\n",
       "     'end': 40,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   'ref_spans': [{'start': 305,\n",
       "     'end': 313,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF43'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Further Investigation of Our Framework '},\n",
       "  {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 250,\n",
       "     'end': 258,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF45'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Case Study'},\n",
       "  {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Conclusions'}],\n",
       " 'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "   'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "   'ref_id': 'EQREF9',\n",
       "   'type': 'equation'},\n",
       "  'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "   'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "   'ref_id': 'EQREF10',\n",
       "   'type': 'equation'},\n",
       "  'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "   'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "   'ref_id': 'EQREF11',\n",
       "   'type': 'equation'},\n",
       "  'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "   'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "   'ref_id': 'EQREF12',\n",
       "   'type': 'equation'},\n",
       "  'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "   'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "   'ref_id': 'EQREF13',\n",
       "   'type': 'equation'},\n",
       "  'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "   'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "   'ref_id': 'EQREF14',\n",
       "   'type': 'equation'},\n",
       "  'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "   'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "   'ref_id': 'EQREF15',\n",
       "   'type': 'equation'},\n",
       "  'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "   'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "   'ref_id': 'EQREF16',\n",
       "   'type': 'equation'},\n",
       "  'EQREF17': {'text': 'R=X d ×X c T',\n",
       "   'latex': 'R = X_d\\\\times X_c^T',\n",
       "   'ref_id': 'EQREF17',\n",
       "   'type': 'equation'},\n",
       "  'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "   'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "   'ref_id': 'EQREF18',\n",
       "   'type': 'equation'},\n",
       "  'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "   'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "   'ref_id': 'EQREF19',\n",
       "   'type': 'equation'},\n",
       "  'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "   'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "   'ref_id': 'EQREF20',\n",
       "   'type': 'equation'},\n",
       "  'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "   'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "   'ref_id': 'EQREF22',\n",
       "   'type': 'equation'},\n",
       "  'FIGREF2': {'text': '1',\n",
       "   'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "   'latex': None,\n",
       "   'ref_id': 'FIGREF2',\n",
       "   'type': 'figure'},\n",
       "  'FIGREF7': {'text': '2',\n",
       "   'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "   'latex': None,\n",
       "   'ref_id': 'FIGREF7',\n",
       "   'type': 'figure'},\n",
       "  'TABREF40': {'text': '1',\n",
       "   'caption': 'Summarization performance.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF40',\n",
       "   'type': 'table'},\n",
       "  'TABREF42': {'text': '2',\n",
       "   'caption': 'Further investigation of RAVAESum.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF42',\n",
       "   'type': 'table'},\n",
       "  'TABREF43': {'text': '3',\n",
       "   'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF43',\n",
       "   'type': 'table'},\n",
       "  'TABREF45': {'text': '4',\n",
       "   'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF45',\n",
       "   'type': 'table'},\n",
       "  'TABREF46': {'text': '5',\n",
       "   'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF46',\n",
       "   'type': 'table'},\n",
       "  'SECREF1': {'text': 'Introduction',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF1',\n",
       "   'type': 'section'},\n",
       "  'SECREF2': {'text': 'Framework',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF2',\n",
       "   'type': 'section'},\n",
       "  'SECREF6': {'text': 'Conclusions',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF6',\n",
       "   'type': 'section'},\n",
       "  'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF8',\n",
       "   'type': 'section'},\n",
       "  'SECREF21': {'text': 'Summary Construction',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF21',\n",
       "   'type': 'section'},\n",
       "  'SECREF3': {'text': 'Data Description',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF3',\n",
       "   'type': 'section'},\n",
       "  'SECREF24': {'text': 'Background',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF24',\n",
       "   'type': 'section'},\n",
       "  'SECREF26': {'text': 'Data Collection',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF26',\n",
       "   'type': 'section'},\n",
       "  'SECREF28': {'text': 'Data Properties',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF28',\n",
       "   'type': 'section'},\n",
       "  'SECREF4': {'text': 'Experimental Setup',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF4',\n",
       "   'type': 'section'},\n",
       "  'SECREF29': {'text': 'Dataset and Metrics',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF29',\n",
       "   'type': 'section'},\n",
       "  'SECREF31': {'text': 'Comparative Methods',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF31',\n",
       "   'type': 'section'},\n",
       "  'SECREF37': {'text': 'Experimental Settings',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF37',\n",
       "   'type': 'section'},\n",
       "  'SECREF5': {'text': 'Results and Discussions',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF5',\n",
       "   'type': 'section'},\n",
       "  'SECREF39': {'text': 'Results on Our Dataset',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF39',\n",
       "   'type': 'section'},\n",
       "  'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF41',\n",
       "   'type': 'section'},\n",
       "  'SECREF44': {'text': 'Case Study',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF44',\n",
       "   'type': 'section'},\n",
       "  'SECREF7': {'text': 'Topics',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF7',\n",
       "   'type': 'section'}},\n",
       " 'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "   'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "   'authors': [{'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "    {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'ACL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1587--1597',\n",
       "   'other_ids': {},\n",
       "   'links': '8377315'},\n",
       "  'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "   'title': 'Linear programming 1: introduction',\n",
       "   'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "    {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "   'year': 2006,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '53739754'},\n",
       "  'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "   'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "   'authors': [{'first': 'Günes', 'middle': [], 'last': 'Erkan', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "   'year': 2004,\n",
       "   'venue': 'EMNLP',\n",
       "   'volume': '4',\n",
       "   'issn': '',\n",
       "   'pages': '365--371',\n",
       "   'other_ids': {},\n",
       "   'links': '10418456'},\n",
       "  'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "   'title': 'Multi-document summarization by sentence extraction',\n",
       "   'authors': [{'first': 'Jade',\n",
       "     'middle': [],\n",
       "     'last': 'Goldstein',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "    {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "    {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "   'year': 2000,\n",
       "   'venue': 'NAACL-ANLPWorkshop',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '40--48',\n",
       "   'other_ids': {},\n",
       "   'links': '8294822'},\n",
       "  'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "   'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "   'authors': [{'first': 'Meishan', 'middle': [], 'last': 'Hu', 'suffix': ''},\n",
       "    {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "    {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "   'year': 2008,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '291--298',\n",
       "   'other_ids': {},\n",
       "   'links': '13723748'},\n",
       "  'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "   'title': 'Adam: A method for stochastic optimization',\n",
       "   'authors': [{'first': 'Diederik',\n",
       "     'middle': [],\n",
       "     'last': 'Kingma',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '6628106'},\n",
       "  'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "   'title': 'Auto-encoding variational bayes',\n",
       "   'authors': [{'first': 'P', 'middle': [], 'last': 'Diederik', 'suffix': ''},\n",
       "    {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICLR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '15789289'},\n",
       "  'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "   'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'IJCAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1270--1276',\n",
       "   'other_ids': {},\n",
       "   'links': '14777460'},\n",
       "  'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "   'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "    {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "   'year': 2017,\n",
       "   'venue': 'AAAI',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '3497--3503',\n",
       "   'other_ids': {},\n",
       "   'links': '29562039'},\n",
       "  'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "   'title': 'Effective approaches to attention-based neural machine translation',\n",
       "   'authors': [{'first': 'Minh-Thang',\n",
       "     'middle': [],\n",
       "     'last': 'Luong',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "    {'first': 'Christopher D', 'middle': [], 'last': 'Manning', 'suffix': ''}],\n",
       "   'year': 2015,\n",
       "   'venue': 'EMNLP',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1412--1421',\n",
       "   'other_ids': {},\n",
       "   'links': '1998416'},\n",
       "  'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "   'title': 'Textrank: Bringing order into texts',\n",
       "   'authors': [{'first': 'Rada',\n",
       "     'middle': [],\n",
       "     'last': 'Mihalcea',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "   'year': 2004,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': '577937'},\n",
       "  'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "   'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "   'authors': [{'first': 'Yen',\n",
       "     'middle': ['Kan'],\n",
       "     'last': 'Ziheng Lin Min',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'COLING',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '2093--2108',\n",
       "   'other_ids': {},\n",
       "   'links': '6317274'},\n",
       "  'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "   'title': 'A survey of text summarization techniques',\n",
       "   'authors': [{'first': 'Ani', 'middle': [], 'last': 'Nenkova', 'suffix': ''},\n",
       "    {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'Mining Text Data',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '43--76',\n",
       "   'other_ids': {},\n",
       "   'links': '556431'},\n",
       "  'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "   'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "   'authors': [{'first': 'Hongyan',\n",
       "     'middle': [],\n",
       "     'last': 'Dragomir R Radev',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "    {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "   'year': 2000,\n",
       "   'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '21--30',\n",
       "   'other_ids': {},\n",
       "   'links': '1320'},\n",
       "  'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "   'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "   'authors': [{'first': 'Danilo',\n",
       "     'middle': [],\n",
       "     'last': 'Jimenez Rezende',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "    {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "   'year': 2014,\n",
       "   'venue': 'ICML',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1278--1286',\n",
       "   'other_ids': {},\n",
       "   'links': '16895865'},\n",
       "  'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "   'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "   'authors': [{'first': 'Mark',\n",
       "     'middle': [],\n",
       "     'last': 'Wasson',\n",
       "     'suffix': ''}],\n",
       "   'year': 1998,\n",
       "   'venue': 'ACL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1364--1368',\n",
       "   'other_ids': {},\n",
       "   'links': '12681629'},\n",
       "  'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "   'title': 'Multiple aspect summarization using integer linear programming',\n",
       "   'authors': [{'first': 'Kristian',\n",
       "     'middle': [],\n",
       "     'last': 'Woodsend',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "   'year': 2012,\n",
       "   'venue': 'EMNLP-CNLL',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '233--243',\n",
       "   'other_ids': {},\n",
       "   'links': '17497992'},\n",
       "  'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "   'title': 'Social context summarization',\n",
       "   'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "    {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "    {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "    {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "    {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "    {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "   'year': 2011,\n",
       "   'venue': 'SIGIR',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '255--264',\n",
       "   'other_ids': {},\n",
       "   'links': '704517'}}}"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article['latex_parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 6 [7, 0, 3, 3, 0, 0]\n",
      "{'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.', \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\", 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.', 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.', 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.', 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'], 'cite_spans': [[{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}, {'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], [], [{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}], [], []], 'cite_span_lens': [7, 0, 3, 3, 0, 0], 'cite_spans_start': [[193, 203, 213, 223, 233, 243, 253], [], [527, 537, 802], [10, 159, 170], [], []], 'section': ['Introduction', 'Introduction', 'Introduction', 'Introduction', 'Introduction', 'Introduction']}\n",
      "====================\n",
      "Overview 1 [1]\n",
      "{'text': ['As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.'], 'cite_spans': [[{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}]], 'cite_span_lens': [1], 'cite_spans_start': [[489]], 'section': ['Overview']}\n",
      "====================\n",
      "Reader-Aware Salience Estimation 17 [3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ', 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ', 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .', 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ', 'The calculation of INLINEFORM0 will be discussed later.', 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ', 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ', 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ', 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.', ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ', 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ', 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.', 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ', 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ', 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ', 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ', 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.'], 'cite_spans': [[{'start': 32, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 43, 'end': 51, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}, {'start': 154, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], [], [], [], [], [], [{'start': 7, 'end': 14, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 46, 'end': 54, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 57, 'end': 65, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], [], [], [], [], [], [], [], [], [], []], 'cite_span_lens': [3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'cite_spans_start': [[32, 43, 154], [], [], [], [], [], [7, 46, 57], [], [], [], [], [], [], [], [], [], []], 'section': ['Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation', 'Reader-Aware Salience Estimation']}\n",
      "====================\n",
      "Summary Construction 2 [2, 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ', 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.'], 'cite_spans': [[{'start': 82, 'end': 89, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 94, 'end': 101, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], [{'start': 466, 'end': 474, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 477, 'end': 484, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 491, 'end': 498, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 644, 'end': 652, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}]], 'cite_span_lens': [2, 4], 'cite_spans_start': [[82, 94], [466, 477, 491, 644]], 'section': ['Summary Construction', 'Summary Construction']}\n",
      "====================\n",
      "Data Description 1 [0]\n",
      "{'text': ['In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.'], 'cite_spans': [[]], 'cite_span_lens': [0], 'cite_spans_start': [[]], 'section': ['Data Description']}\n",
      "====================\n",
      "Background 7 [0, 0, 0, 0, 0, 0, 0]\n",
      "{'text': ['The definition of the terminology related to the dataset is given as follows.', 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.', 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.', 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).', 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.', 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.', 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.'], 'cite_spans': [[], [], [], [], [], [], []], 'cite_span_lens': [0, 0, 0, 0, 0, 0, 0], 'cite_spans_start': [[], [], [], [], [], [], []], 'section': ['Background', 'Background', 'Background', 'Background', 'Background', 'Background', 'Background']}\n",
      "====================\n",
      "Data Collection 4 [0, 0, 0, 0]\n",
      "{'text': ['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .', 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.', 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.', 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.'], 'cite_spans': [[], [], [], []], 'cite_span_lens': [0, 0, 0, 0], 'cite_spans_start': [[], [], [], []], 'section': ['Data Collection', 'Data Collection', 'Data Collection', 'Data Collection']}\n",
      "====================\n",
      "Data Properties 1 [0]\n",
      "{'text': ['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.'], 'cite_spans': [[]], 'cite_span_lens': [0], 'cite_spans_start': [[]], 'section': ['Data Properties']}\n",
      "====================\n",
      "Dataset and Metrics 1 [1]\n",
      "{'text': ['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.'], 'cite_spans': [[{'start': 113, 'end': 121, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}]], 'cite_span_lens': [1], 'cite_spans_start': [[113]], 'section': ['Dataset and Metrics']}\n",
      "====================\n",
      "Comparative Methods 7 [0, 1, 1, 1, 2, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:', 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.', 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.', 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.', 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.', 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.', 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.'], 'cite_spans': [[], [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], [{'start': 5, 'end': 13, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}], [{'start': 9, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 29, 'end': 37, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], [{'start': 8, 'end': 15, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], []], 'cite_span_lens': [0, 1, 1, 1, 2, 1, 0], 'cite_spans_start': [[], [10], [5], [9], [8, 29], [8], []], 'section': ['Comparative Methods', 'Comparative Methods', 'Comparative Methods', 'Comparative Methods', 'Comparative Methods', 'Comparative Methods', 'Comparative Methods']}\n",
      "====================\n",
      "Experimental Settings 1 [2]\n",
      "{'text': ['The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.'], 'cite_spans': [[{'start': 557, 'end': 565, 'text': None, 'latex': None, 'ref_id': 'BIBREF20'}, {'start': 697, 'end': 705, 'text': None, 'latex': None, 'ref_id': 'BIBREF21'}]], 'cite_span_lens': [2], 'cite_spans_start': [[557, 697]], 'section': ['Experimental Settings']}\n",
      "====================\n",
      "Results on Our Dataset 1 [0]\n",
      "{'text': ['The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.'], 'cite_spans': [[]], 'cite_span_lens': [0], 'cite_spans_start': [[]], 'section': ['Results on Our Dataset']}\n",
      "====================\n",
      "Further Investigation of Our Framework  2 [1, 1]\n",
      "{'text': ['To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).', \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\"], 'cite_spans': [[{'start': 208, 'end': 215, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}], [{'start': 33, 'end': 40, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}]], 'cite_span_lens': [1, 1], 'cite_spans_start': [[208], [33]], 'section': ['Further Investigation of Our Framework ', 'Further Investigation of Our Framework ']}\n",
      "====================\n",
      "Case Study 1 [0]\n",
      "{'text': ['Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.'], 'cite_spans': [[]], 'cite_span_lens': [0], 'cite_spans_start': [[]], 'section': ['Case Study']}\n",
      "====================\n",
      "Conclusions 1 [0]\n",
      "{'text': ['We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.'], 'cite_spans': [[]], 'cite_span_lens': [0], 'cite_spans_start': [[]], 'section': ['Conclusions']}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "latex_parse_overview = dict()\n",
    "for sections in article['latex_parse']['body_text']:\n",
    "    if sections['section'] in latex_parse_overview:\n",
    "        # если есть дублирование, такое бывает у первых часте\n",
    "        if latex_parse_overview[sections['section']] == sections:\n",
    "            continue\n",
    "        else:\n",
    "            latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "            latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "            latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "            latex_parse_overview[sections['section']]['cite_spans_start'].append(list(map(lambda x: int(x['start']),sections['cite_spans'])))\n",
    "            latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "#             latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "    else:\n",
    "        latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                      'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                      'cite_spans_start':[list(map(lambda x: int(x['start']),sections['cite_spans']))],\n",
    "                                                      'section':[sections['section']]}#, \n",
    "                                                      #'bib_entries':article['latex_parse']}\n",
    "       \n",
    "    \n",
    "for sections in latex_parse_overview:\n",
    "    sections_dict = latex_parse_overview[sections]\n",
    "    print(sections,len(sections_dict['cite_spans']),sections_dict['cite_span_lens'])\n",
    "    print(sections_dict)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Introduction\n",
      "sentence The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources\n",
      "final len= 0 7\n",
      "sentence  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}\n",
      "1  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "---\n",
      "{'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}\n",
      "1  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "---\n",
      "{'start': 213, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}\n",
      "1  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "---\n",
      "{'start': 223, 'end': 230, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}\n",
      "1  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "---\n",
      "{'start': 233, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}\n",
      "1  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "---\n",
      "{'start': 243, 'end': 250, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}\n",
      "1  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "---\n",
      "{'start': 253, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "1  BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 \n",
      "---\n",
      "final len= 7 7\n",
      "sentence  In the typical setting of MDS, the input is a set of news documents about the same topic\n",
      "final len= 7 7\n",
      "sentence  The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.\n",
      "final len= 7 7\n",
      "========================================\n",
      "sentence With the development of social media and mobile equipments, more and more user generated content is available\n",
      "final len= 0 0\n",
      "sentence  Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”\n",
      "final len= 0 0\n",
      "sentence  The content of the original news report talks about some new products based on AI techniques\n",
      "final len= 0 0\n",
      "sentence  The news report generally conveys an enthusiastic tone\n",
      "final len= 0 0\n",
      "sentence  However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports\n",
      "final len= 0 0\n",
      "sentence  Unfortunately, existing MDS approaches cannot handle this issue\n",
      "final len= 0 0\n",
      "sentence  We investigate this problem known as reader-aware multi-document summarization (RA-MDS)\n",
      "final len= 0 0\n",
      "sentence  Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\n",
      "final len= 0 0\n",
      "========================================\n",
      "sentence One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments\n",
      "final len= 0 3\n",
      "sentence  Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments\n",
      "final len= 0 3\n",
      "sentence  Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions\n",
      "final len= 0 3\n",
      "sentence  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 \n",
      "{'start': 527, 'end': 534, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}\n",
      "3  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 \n",
      "---\n",
      "{'start': 537, 'end': 544, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}\n",
      "3  Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 \n",
      "---\n",
      "final len= 2 3\n",
      "sentence  However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period\n",
      "final len= 2 3\n",
      "sentence  Another challenge is that reader comments are very diverse and noisy\n",
      "final len= 2 3\n",
      "sentence  Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy\n",
      "{'start': 802, 'end': 809, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}\n",
      "6  Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy\n",
      "---\n",
      "final len= 3 3\n",
      "sentence  However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.\n",
      "final len= 3 3\n",
      "========================================\n",
      "sentence Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 \n",
      "{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "0 Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 \n",
      "---\n",
      "{'start': 159, 'end': 167, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}\n",
      "0 Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 \n",
      "---\n",
      "{'start': 170, 'end': 178, 'text': None, 'latex': None, 'ref_id': 'BIBREF11'}\n",
      "0 Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 \n",
      "---\n",
      "final len= 3 3\n",
      "sentence  During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts\n",
      "final len= 3 3\n",
      "sentence  Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient\n",
      "final len= 3 3\n",
      "sentence  Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters\n",
      "final len= 3 3\n",
      "sentence  After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.\n",
      "final len= 3 3\n",
      "========================================\n",
      "sentence There is a lack of high-quality dataset suitable for RA-MDS\n",
      "final len= 0 0\n",
      "sentence  Existing datasets from DUC and TAC are not appropriate\n",
      "final len= 0 0\n",
      "sentence  Therefore, we introduce a new dataset for RA-MDS\n",
      "final len= 0 0\n",
      "sentence  We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing\n",
      "final len= 0 0\n",
      "sentence  To our best knowledge, this is the first dataset for RA-MDS.\n",
      "final len= 0 0\n",
      "========================================\n",
      "sentence Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS\n",
      "final len= 0 0\n",
      "sentence  To our best knowledge, it is the first dataset for RA-MDS\n",
      "final len= 0 0\n",
      "sentence  (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments\n",
      "final len= 0 0\n",
      "sentence  (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.\n",
      "final len= 0 0\n",
      "========================================\n",
      "[[0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "========================================\n",
      "1 Overview\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction\n",
      "final len= 0 1\n",
      "sentence  Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors\n",
      "final len= 0 1\n",
      "sentence  Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments\n",
      "{'start': 489, 'end': 496, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}\n",
      "2  Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments\n",
      "---\n",
      "final len= 1 1\n",
      "sentence  One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 \n",
      "final len= 1 1\n",
      "sentence  The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.\n",
      "final len= 1 1\n",
      "========================================\n",
      "[[0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0]]\n",
      "========================================\n",
      "2 Reader-Aware Salience Estimation\n"
     ]
    }
   ],
   "source": [
    "sents_num_citations_sections = []\n",
    "for num_key,keys in enumerate(latex_parse_overview.keys()):\n",
    "    print(num_key,keys)\n",
    "    if num_key>=2:\n",
    "        break\n",
    "    sents_num_citations_section = []\n",
    "    for num_text, text in enumerate(latex_parse_overview[keys]['text']):\n",
    "        sentences_sec = text2sentences(text)\n",
    "        cite_spans_sec = latex_parse_overview[keys]['cite_spans'][num_text]\n",
    "        cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "        sum_prev_sects = 0\n",
    "        sents_num_citations = []\n",
    "        del_bib_start = []\n",
    "        for num_sent,sentence in enumerate(sentences_sec):\n",
    "            cite_spans_sec_time = [cite_span  for cite_span in  cite_spans_sec_time if not cite_span['start'] in del_bib_start]\n",
    "            print('sentence',sentence)\n",
    "#             print(cite_spans_sec_time)\n",
    "            len_sent = len(sentence)+1\n",
    "            sent_num_cits = 0\n",
    "            for num_cite_span,cite_span in enumerate(cite_spans_sec_time):\n",
    "#                 print('cite_span',cite_span,)\n",
    "                if  cite_span['text']!=None:\n",
    "                    temp = cite_span['text']\n",
    "                elif cite_span['ref_id']:\n",
    "                    temp = cite_span['ref_id']\n",
    "                else:\n",
    "                    print('ERROR')\n",
    "                    break\n",
    "                if len(temp)<4:\n",
    "                    temp = ' '+temp+' '\n",
    "                    if (cite_span['start'] >= sum_prev_sects and cite_span['end'] <=(sum_prev_sects+len_sent)) or (temp in sentence):\n",
    "                        sent_num_cits+=1\n",
    "                        del_bib_start.append(cite_span['start'])\n",
    "                        print('---')\n",
    "                        print(cite_span)\n",
    "                        print(num_sent,sentence)\n",
    "                        print('---')\n",
    "                elif (cite_span['start'] >= (sum_prev_sects-2) and (cite_span['end']-3) <=(sum_prev_sects+len_sent)) or (temp in sentence) or (temp.replace('.','') in sentence):\n",
    "                    sent_num_cits+=1\n",
    "                    del_bib_start.append(cite_span['start'])\n",
    "                    print(cite_span)\n",
    "                    print(num_sent,sentence)\n",
    "                    print('---')\n",
    "            sents_num_citations.append(sent_num_cits)\n",
    "            sum_prev_sects += len_sent\n",
    "            print('final len=',sum(sents_num_citations), len(cite_spans_sec))\n",
    "        sents_num_citations_section += sents_num_citations\n",
    "        \n",
    "        try:\n",
    "            assert sum(sents_num_citations) == len(cite_spans_sec)\n",
    "            print(20*'==')\n",
    "        except:\n",
    "            print('!!!!',num_sec)\n",
    "    sents_num_citations_sections.append(sents_num_citations_section)\n",
    "    print(sents_num_citations_sections)\n",
    "    print(20*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sect_num_citations_latex(latex_parse_overview_key):\n",
    "#     for num_key,keys in enumerate(latex_parse_overview.keys()):\n",
    "#     print(num_key,keys)\n",
    "    sents_num_citations_section = []\n",
    "    for num_text, text in enumerate(latex_parse_overview_key['text']):\n",
    "        sentences_sec = text2sentences(text)\n",
    "        cite_spans_sec = latex_parse_overview_key['cite_spans'][num_text]\n",
    "        cite_spans_sec_time = np.array(cite_spans_sec.copy())\n",
    "        sum_prev_sects = 0\n",
    "        sents_num_citations = []\n",
    "        del_bib_start = []\n",
    "        for num_sent,sentence in enumerate(sentences_sec):\n",
    "            cite_spans_sec_time = [cite_span  for cite_span in  cite_spans_sec_time if not cite_span['start'] in del_bib_start]\n",
    "            len_sent = len(sentence)+1\n",
    "            sent_num_cits = 0\n",
    "            for num_cite_span,cite_span in enumerate(cite_spans_sec_time):\n",
    "                if  cite_span['text']!=None:\n",
    "                    temp = cite_span['text']\n",
    "                elif cite_span['ref_id']:\n",
    "                    temp = cite_span['ref_id']\n",
    "                else:\n",
    "                    print('ERROR')\n",
    "                    break\n",
    "                if len(temp)<4:\n",
    "                    temp = ' '+temp+' '\n",
    "                    if (cite_span['start'] >= sum_prev_sects and cite_span['end'] <=(sum_prev_sects+len_sent)) or (temp in sentence) or (temp.replace('.','') in sentence):\n",
    "                        sent_num_cits+=1\n",
    "                        del_bib_start.append(cite_span['start'])\n",
    "                elif (cite_span['start'] >= (sum_prev_sects-2) and (cite_span['end']-3) <=(sum_prev_sects+len_sent)) or (temp in sentence) or (temp.replace('.','') in sentence):\n",
    "                    sent_num_cits+=1\n",
    "                    del_bib_start.append(cite_span['start'])\n",
    "    #                 print(cite_span)\n",
    "    #                 print(num_sent,sentence)\n",
    "    #                 print('---')\n",
    "            sents_num_citations.append(sent_num_cits)\n",
    "            sum_prev_sects += len_sent\n",
    "        sents_num_citations_section += sents_num_citations\n",
    "        # Checking соотвествия кол-ва найденных ссылок в предложениях и кол-ва всех ссылок\n",
    "        try:\n",
    "            assert sum(sents_num_citations) == len(cite_spans_sec)\n",
    "        except:\n",
    "            print(num_art,num_sec)\n",
    "    return sents_num_citations_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 7,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sect_num_citations_latex(latex_parse_overview['Introduction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_latex_overview(article):\n",
    "    latex_parse_overview = dict()\n",
    "    for sections in article['latex_parse']['body_text']:\n",
    "        if sections['section'] == None:\n",
    "            sections['section'] = 'other'\n",
    "        if sections['section'] in latex_parse_overview:\n",
    "            # если есть дублирование, такое бывает у первых часте\n",
    "            if latex_parse_overview[sections['section']] == sections:\n",
    "                continue\n",
    "            else:\n",
    "                latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "                latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "                latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "                latex_parse_overview[sections['section']]['cite_spans_start'].append(list(map(lambda x: int(x['start']),sections['cite_spans'])))\n",
    "                latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "    #             latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "        else:\n",
    "            latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                          'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                          'cite_spans_start':[list(map(lambda x: int(x['start']),sections['cite_spans']))],\n",
    "                                                          'section':[sections['section']]}#, \n",
    "                                                          #'latex_parse':article['latex_parse']}\n",
    "\n",
    "    return latex_parse_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'other': {'text': [' '],\n",
       "  'cite_spans': [[]],\n",
       "  'cite_span_lens': [0],\n",
       "  'cite_spans_start': [[]],\n",
       "  'section': ['other']}}"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_latex_overview(all_articles[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_num_citations_latex(article):\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        latex_parse_overview = make_latex_overview(article)\n",
    "        papers_num_citations = []\n",
    "#         for num_sec,section in enumerate(article['latex_parse']['body_text']):\n",
    "        for num_key,keys in enumerate(latex_parse_overview.keys()):\n",
    "            sents_num_citations = sect_num_citations_latex(latex_parse_overview[keys])\n",
    "            papers_num_citations.append(sents_num_citations)\n",
    "        return papers_num_citations\n",
    "    else:\n",
    "        return [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[]]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_num_citations_latex(all_articles[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[489, 918, 1112, 973, 357, 456]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(text) for text in latex_parse_overview['Introduction']['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[193, 203, 213, 223, 233, 243, 253],\n",
       " [],\n",
       " [527, 537, 802],\n",
       " [10, 159, 170],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_parse_overview['Introduction']['cite_spans_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 193\n",
      "203 203\n",
      "213 213\n",
      "223 223\n",
      "233 233\n",
      "243 243\n",
      "253 253\n",
      "==============================\n",
      "==============================\n",
      "527 1934\n",
      "537 1944\n",
      "802 2209\n",
      "==============================\n",
      "10 2529\n",
      "159 2678\n",
      "170 2689\n",
      "==============================\n",
      "==============================\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "len_blocks = [len(text) for text in latex_parse_overview['Introduction']['text']]\n",
    "start_by_all = []\n",
    "for num_block,start_block in enumerate(latex_parse_overview['Introduction']['cite_spans_start']):\n",
    "    for start in start_block:\n",
    "        new_start = start\n",
    "        new_start += sum(len_blocks[:num_block])\n",
    "        start_by_all.append(new_start)\n",
    "        print(start,new_start)\n",
    "    print(10*'===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upgrade_start_span_latex(latex_parse_overview_key):\n",
    "    len_blocks = [len(text) for text in latex_parse_overview_key['text']]\n",
    "    start_by_all = []\n",
    "    for num_block,start_block in enumerate(latex_parse_overview_key['cite_spans_start']):\n",
    "        for start in start_block:\n",
    "            new_start = start\n",
    "            new_start += sum(len_blocks[:num_block])\n",
    "            start_by_all.append(new_start)\n",
    "\n",
    "    return start_by_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[193, 203, 213, 223, 233, 243, 253, 1934, 1944, 2209, 2529, 2678, 2689]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upgrade_start_span_latex(latex_parse_overview['Introduction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(latex_parse_overview['Introduction']['text'])[2529:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 3, 3, 0, 0]"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_parse_overview['Introduction']['cite_span_lens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_paper_latex(article):\n",
    "\n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "        if len(article['latex_parse']['body_text']) <= 2:\n",
    "            if len(article['latex_parse']['body_text'][0]['text'].split()) < 1 :\n",
    "                #or not article['latex_parse']['body_text'][0]['section']\n",
    "                # не использую условие выше тк иначе уменьшится выборка (навряд ли в ней есть RW)\n",
    "                return [-1]\n",
    "        latex_parse_overview = make_latex_overview(article)\n",
    "        section_num_citations = []\n",
    "        section_names = []\n",
    "        section_cite_dencite = []\n",
    "        section_position = []\n",
    "        section_av_cit_pos = []\n",
    "        \n",
    "#         for num_sec,section in enumerate(article['latex_parse']['body_text']):\n",
    "\n",
    "        len_sects = len(latex_parse_overview)\n",
    "        for num_key,keys in enumerate(latex_parse_overview.keys()):\n",
    "            \n",
    "            section_names.append(keys)\n",
    "            all_text = ''.join(latex_parse_overview[keys]['text'])\n",
    "            \n",
    "            cnt_citations = sum(latex_parse_overview[keys]['cite_span_lens'])\n",
    "            sect_len = len(all_text)\n",
    "            sect_len_toks = len(all_text.split())\n",
    "            \n",
    "            cite_spans_sec_start = upgrade_start_span_latex(latex_parse_overview[keys])\n",
    "            cite_spans_sec_start_mean = 0\n",
    "            if len(cite_spans_sec_start) >=1:\n",
    "                cite_spans_sec_start_mean = np.mean(cite_spans_sec_start)/sect_len\n",
    "                \n",
    "            sents_num_citations = sect_num_citations_latex(latex_parse_overview[keys])\n",
    "            \n",
    "            section_num_citations.append(sents_num_citations)\n",
    "            section_cite_dencite.append(cnt_citations/sect_len_toks)\n",
    "            section_position.append((num_key+1)/len_sects)\n",
    "            section_av_cit_pos.append(cite_spans_sec_start_mean)\n",
    "            print(20*'==')\n",
    "            print('Features: \\nciting dencity {0}: #citations={1} & sect_len_toks={2}|  '.format(cnt_citations/sect_len_toks, cnt_citations, sect_len_toks))\n",
    "            print('Number of citations in sentences: {0}'.format(sents_num_citations))\n",
    "            print('Positon sect: {0} : num_sec = {1} len_sects={2}'.format((num_key+1)/len_sects,num_key+1,len_sects))\n",
    "            print('Positon aver. citation: {0} {1} len= {2}'.format(cite_spans_sec_start_mean,cite_spans_sec_start,sect_len))\n",
    "            print('Section name:',keys)\n",
    "        papers_features = {'cite_dencity':section_cite_dencite,'num_cits':section_num_citations,\n",
    "                           'sec_pos':section_position,'sec_av_cit_pos':section_av_cit_pos,'sec_name':section_names}\n",
    "        return papers_features\n",
    "    else:\n",
    "        return [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 5\n",
      "{'text': 'Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation BIBREF0 , BIBREF1 and relation extraction BIBREF2 , and has also been shown useful in the development of reading aids, e.g., for people with dyslexia BIBREF3 or non-native speakers BIBREF4 .', 'cite_spans': [{'start': 193, 'end': 200, 'text': None, 'latex': None, 'ref_id': 'BIBREF0'}, {'start': 203, 'end': 210, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}, {'start': 235, 'end': 242, 'text': None, 'latex': None, 'ref_id': 'BIBREF2'}, {'start': 343, 'end': 350, 'text': None, 'latex': None, 'ref_id': 'BIBREF3'}, {'start': 374, 'end': 381, 'text': None, 'latex': None, 'ref_id': 'BIBREF4'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 6\n",
      "{'text': 'The task has attracted much attention in the past decade BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations (e.g., deletion, splitting and substitution). BIBREF10 has recently made considerable progress towards that goal, and proposed to tackle it both by using an improved reference-based measure, named SARI, and by increasing the number of references. However, their research focused on lexical, rather than structural simplification, which provides a complementary view of TS quality as this paper will show.', 'cite_spans': [{'start': 57, 'end': 64, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 67, 'end': 74, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 77, 'end': 84, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 87, 'end': 94, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'}, {'start': 97, 'end': 104, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 405, 'end': 413, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'This paper focuses on the evaluation of the structural aspects of the task. We introduce the semantic measure SAMSA (Simplification Automatic evaluation Measure through Semantic Annotation), the first structure-aware measure for TS in general, and the first to use semantic structure in this context in particular. SAMSA stipulates that an optimal split of the input is one where each predicate-argument structure is assigned its own sentence, and measures to what extent this assertion holds for the input-output pair in question, by using semantic structure. SAMSA focuses on the core semantic components of the sentence, and is tolerant towards the deletion of other units.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 0\n",
      "{'text': 'For example, SAMSA will assign a high score to the output split “John got home. John gave Mary a call.” for the input sentence “John got home and gave Mary a call.”, as it splits each of its predicate-argument structures to a different sentence. Splits that alter predicate-argument relations such as “John got home and gave. Mary called.” are penalized by SAMSA.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 3\n",
      "{'text': \"SAMSA's use of semantic structures for TS evaluation has several motivations. First, it provides means to measure the extent to which the meaning of the source is preserved in the output. Second, it provides means for measuring whether the input sentence was split to semantic units of the right granularity. Third, defining a semantic measure that does not require references avoids the difficulties incurred by their non-uniqueness, and the difficulty in collecting high quality references, as reported by BIBREF12 and by BIBREF9 with respect to the Parallel Wikipedia Corpus BIBREF5 . SAMSA is further motivated by its use of semantic annotation only on the source side, which allows to evaluate multiple systems using same source-side annotation, and avoids the need to parse system outputs, which can be garbled.\", 'cite_spans': [{'start': 508, 'end': 516, 'text': None, 'latex': None, 'ref_id': 'BIBREF12'}, {'start': 524, 'end': 531, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 578, 'end': 585, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 6\n",
      "{'text': 'In this paper we use the UCCA scheme for defining semantic structure BIBREF13 . UCCA has been shown to be preserved remarkably well across translations BIBREF14 and has also been successfully used for machine translation evaluation BIBREF15 (Section SECREF2 ). We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR BIBREF16 or Discourse Representation Structures BIBREF17 , as used by BIBREF9 .', 'cite_spans': [{'start': 69, 'end': 77, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}, {'start': 152, 'end': 160, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}, {'start': 232, 'end': 240, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}, {'start': 394, 'end': 402, 'text': None, 'latex': None, 'ref_id': 'BIBREF16'}, {'start': 442, 'end': 450, 'text': None, 'latex': None, 'ref_id': 'BIBREF17'}, {'start': 464, 'end': 471, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}], 'ref_spans': [{'start': 250, 'end': 257, 'text': None, 'latex': None, 'ref_id': 'SECREF2'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Introduction 1\n",
      "{'text': \"We experiment with SAMSA both where semantic annotation is carried out manually, and where it is carried out by a parser. See Section SECREF4 . We conduct human rating experiments and compare the resulting system rankings with those predicted by SAMSA. We find that SAMSA's rankings obtain high correlations with human rankings, and compare favorably to existing reference-based measures for TS. Moreover, our results show that existing measures, which mainly target lexical simplification, are ill-suited to predict human judgments where structural simplification is involved. Finally, we apply SAMSA to the dataset of the QATS shared task on simplification evaluation BIBREF18 . We find that SAMSA obtains comparative correlation with human judgments on the task, despite operating in a more restricted setting, as it does not use human ratings as training data and focuses only on structural aspects of simplicity. Section SECREF2 presents previous work. Section SECREF3 discusses UCCA. Section SECREF4 presents SAMSA. Section SECREF5 details the collection of human judgments. Our experimental setup for comparing our human and automatic rankings is given in Section SECREF6 , and results are given in Section SECREF7 , showing superior results for SAMSA. A discussion on the results is presented in Section SECREF8 . Section SECREF9 presents experiments with SAMSA on the QATS evaluation benchmark.\", 'cite_spans': [{'start': 670, 'end': 678, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [{'start': 134, 'end': 141, 'text': None, 'latex': None, 'ref_id': 'SECREF4'}, {'start': 926, 'end': 933, 'text': None, 'latex': None, 'ref_id': 'SECREF2'}, {'start': 966, 'end': 973, 'text': None, 'latex': None, 'ref_id': 'SECREF3'}, {'start': 998, 'end': 1005, 'text': None, 'latex': None, 'ref_id': 'SECREF4'}, {'start': 1030, 'end': 1037, 'text': None, 'latex': None, 'ref_id': 'SECREF5'}, {'start': 1171, 'end': 1178, 'text': None, 'latex': None, 'ref_id': 'SECREF6'}, {'start': 1214, 'end': 1221, 'text': None, 'latex': None, 'ref_id': 'SECREF7'}, {'start': 1312, 'end': 1319, 'text': None, 'latex': None, 'ref_id': 'SECREF8'}, {'start': 1330, 'end': 1337, 'text': None, 'latex': None, 'ref_id': 'SECREF9'}], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Introduction'}\n",
      "====================\n",
      "Related Work 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Related Work'}\n",
      "====================\n",
      "UCCA's Semantic Structures 7\n",
      "{'text': ' In this section we will briefly describe the UCCA scheme, focusing on the concepts of Scenes and Centers which are key in the definition of SAMSA. UCCA BIBREF13 is a semantic annotation scheme based on typological BIBREF35 , BIBREF36 , BIBREF37 and cognitive BIBREF38 theories which aims to represent the main semantic phenomena in the text, abstracting away from syntactic detail. UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph (including the words of the text) or to several elements jointly viewed as a single entity according to some semantic or cognitive consideration. Unlike AMR, UCCA semantic units are directly anchored in the text BIBREF39 , BIBREF15 , which allows easy inclusion of a word-to-word alignment in the metric model (Section SECREF4 ).', 'cite_spans': [{'start': 153, 'end': 161, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}, {'start': 215, 'end': 223, 'text': None, 'latex': None, 'ref_id': 'BIBREF35'}, {'start': 226, 'end': 234, 'text': None, 'latex': None, 'ref_id': 'BIBREF36'}, {'start': 237, 'end': 245, 'text': None, 'latex': None, 'ref_id': 'BIBREF37'}, {'start': 260, 'end': 268, 'text': None, 'latex': None, 'ref_id': 'BIBREF38'}, {'start': 707, 'end': 715, 'text': None, 'latex': None, 'ref_id': 'BIBREF39'}, {'start': 718, 'end': 726, 'text': None, 'latex': None, 'ref_id': 'BIBREF15'}], 'ref_spans': [{'start': 814, 'end': 821, 'text': None, 'latex': None, 'ref_id': 'SECREF4'}], 'eq_spans': [], 'section': \"UCCA's Semantic Structures\"}\n",
      "====================\n",
      "UCCA's Semantic Structures 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': \"UCCA's Semantic Structures\"}\n",
      "====================\n",
      "The SAMSA Metric 0\n",
      "{'text': \"SAMSA's main premise is that a structurally correct simplification is one where: (1) each sentence contains a single event from the input (UCCA Scene), (2) the main relation of each of the events and their participants are retained in the output.\", 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'The SAMSA Metric'}\n",
      "====================\n",
      "The SAMSA Metric 0\n",
      "{'text': 'For example, consider “John wrote a book. I read that book.” as a simplification of “I read the book that John wrote.”. Each output sentence contains one Scene, which has the same Scene elements as the source, and would thus be deemed correct by SAMSA. On the other hand, the output “John wrote. I read the book.” is an incorrect split of that sentence, since a participant of the “writing” Scene, namely “the book” is absent in the split sentence. SAMSA would indeed penalize such a case.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'The SAMSA Metric'}\n",
      "====================\n",
      "The SAMSA Metric 0\n",
      "{'text': 'Similarly, Scenes which have elements across several sentences receive a zero score by SAMSA. As an example, consider the sentence “The combination of new weapons and tactics marks this battle as the end of chivalry”, and erroneous split “The combination of new weapons and tactics. It is the end of chivalry.” (adapted from the output of a recent system on the PWKP corpus), which does not preserve the original meaning.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'The SAMSA Metric'}\n",
      "====================\n",
      "The SAMSA Metric 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'The SAMSA Metric'}\n",
      "====================\n",
      "Matching Scenes to Sentences 1\n",
      "{'text': 'SAMSA is based on two external linguistic resources. One is a semantic annotation (UCCA in our experiments) of the source side, which can be carried out either manually or automatically, using the TUPA parser BIBREF40 for UCCA. UCCA decomposes each sentence INLINEFORM0 into a set of Scenes INLINEFORM1 , where each scene INLINEFORM2 contains a main relation INLINEFORM3 (sub-span of INLINEFORM4 ) and a set of zero or more participants INLINEFORM5 .', 'cite_spans': [{'start': 209, 'end': 217, 'text': None, 'latex': None, 'ref_id': 'BIBREF40'}], 'ref_spans': [], 'eq_spans': [{'start': 258, 'end': 269, 'text': 's', 'latex': 's', 'ref_id': None}, {'start': 291, 'end': 302, 'text': '{sc 1 ,sc 2 ,..,sc n }', 'latex': '\\\\lbrace sc_1,sc_2,..,sc_n\\\\rbrace ', 'ref_id': None}, {'start': 322, 'end': 333, 'text': 'sc i ', 'latex': 'sc_i', 'ref_id': None}, {'start': 359, 'end': 370, 'text': 'mr i ', 'latex': 'mr_i', 'ref_id': None}, {'start': 384, 'end': 395, 'text': 'sc i ', 'latex': 'sc_i', 'ref_id': None}, {'start': 437, 'end': 448, 'text': 'A i ', 'latex': 'A_i', 'ref_id': None}], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n",
      "{'text': 'The second resource is a word-to-word alignment INLINEFORM0 between the words in the input and one or zero words in the output. The monolingual alignment thus permits SAMSA not to penalize outputs that involve lexical substitutions (e.g., “commence” might be aligned with “start”). We denote by INLINEFORM1 the number of UCCA Scenes in the input sentence and by INLINEFORM2 the number of sentences in the output.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 48, 'end': 59, 'text': 'A', 'latex': 'A', 'ref_id': None}, {'start': 295, 'end': 306, 'text': 'n inp ', 'latex': 'n_{inp}', 'ref_id': None}, {'start': 362, 'end': 373, 'text': 'n out ', 'latex': 'n_{out}', 'ref_id': None}], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n",
      "{'text': \"Given an input sentence's UCCA Scenes INLINEFORM0 , a non-annotated output of a simplification system split into sentences INLINEFORM1 , and their word alignment INLINEFORM2 , we distinguish between two cases:\", 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 38, 'end': 49, 'text': 'sc 1 ,...,sc n inp  ', 'latex': 'sc_1,\\\\ldots , sc_{n_{inp}}', 'ref_id': None}, {'start': 123, 'end': 134, 'text': 's 1 ,...,s n out  ', 'latex': 's_1,\\\\ldots ,s_{n_{out}}', 'ref_id': None}, {'start': 162, 'end': 173, 'text': 'A', 'latex': 'A', 'ref_id': None}], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ' INLINEFORM0 : in this case, we compute the maximal Many-to-1 correspondence between Scenes and sentences. A Scene is matched to a sentence in the following way. We say that a leaf INLINEFORM1 in a Scene INLINEFORM2 is consistent in a Scene-sentence mapping INLINEFORM3 which maps INLINEFORM4 to a sentence INLINEFORM5 , if there is a word INLINEFORM6 which INLINEFORM7 aligns to (according to the word alignment INLINEFORM8 ). The score of matching a Scene INLINEFORM9 to a sentence INLINEFORM10 is then defined to be the total number of consistent leaves in INLINEFORM11 . We traverse the Scenes in their order of occurrence in the text, selecting for each the sentence that maximizes the score. If INLINEFORM12 , once a sentence is matched to a Scene, it cannot be matched to another one. Ties between sentences are broken towards the sentence that appeared first in the output.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': 'n inp ≥n out ', 'latex': 'n_{inp} \\\\ge n_{out}', 'ref_id': None}, {'start': 181, 'end': 192, 'text': 'l', 'latex': 'l', 'ref_id': None}, {'start': 204, 'end': 215, 'text': 'sc', 'latex': 'sc', 'ref_id': None}, {'start': 258, 'end': 269, 'text': 'M', 'latex': 'M', 'ref_id': None}, {'start': 281, 'end': 292, 'text': 'sc', 'latex': 'sc', 'ref_id': None}, {'start': 307, 'end': 318, 'text': 's', 'latex': 's', 'ref_id': None}, {'start': 340, 'end': 351, 'text': 'w∈s', 'latex': 'w \\\\in s', 'ref_id': None}, {'start': 358, 'end': 369, 'text': 'l', 'latex': 'l', 'ref_id': None}, {'start': 413, 'end': 424, 'text': 'A', 'latex': 'A', 'ref_id': None}, {'start': 458, 'end': 469, 'text': 'sc', 'latex': 'sc', 'ref_id': None}, {'start': 484, 'end': 496, 'text': 's', 'latex': 's', 'ref_id': None}, {'start': 560, 'end': 572, 'text': 'sc', 'latex': 'sc', 'ref_id': None}, {'start': 701, 'end': 713, 'text': 'n inp =n out ', 'latex': 'n_{inp} = n_{out}', 'ref_id': None}], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n",
      "{'text': ' INLINEFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': 'M * (sc i )= argmax  s score(sc i ,s)', 'latex': ' \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ {\\\\rm M^*}(sc_i) = {\\\\rm argmax}_{s} score(sc_i,s)', 'ref_id': None}], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n",
      "{'text': ' INLINEFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': 's.t.s∉{M * (sc 1 ),⋯,M * (sc i-1 )} if n inp =n out ', 'latex': '{\\\\rm s.t.}\\\\,\\\\,\\\\, s \\\\notin \\\\lbrace M^*(sc_1),\\\\dots ,M^*(sc_{i-1})\\\\rbrace \\\\ \\\\ {\\\\rm if} \\\\ n_{inp} = n_{out}', 'ref_id': None}], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n",
      "{'text': ' INLINEFORM0 : In this case, a Scene will necessarily be split across several sentences. As this is an undesired result, we assign this instance a score of zero.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': 'n inp <n out ', 'latex': 'n_{inp}<n_{out}', 'ref_id': None}], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Matching Scenes to Sentences 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Matching Scenes to Sentences'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': \"The minimal center of a UCCA unit INLINEFORM0 is UCCA's notion of a semantic head word, defined through recursive rules not unlike the head propagation rules used for converting constituency structures to dependency structures. More formally, we define the minimal center of a UCCA unit INLINEFORM1 (here a Participant or a Main Relation) to be the UCCA graph's leaf reached by starting from INLINEFORM2 and iteratively selecting the child tagged as Center. If a Participant (or a Center inside a Participant) is a Scene, its center is the main relation (Process or State) of the Scene.\", 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 34, 'end': 45, 'text': 'u', 'latex': 'u', 'ref_id': None}, {'start': 287, 'end': 298, 'text': 'u', 'latex': 'u', 'ref_id': None}, {'start': 392, 'end': 403, 'text': 'u', 'latex': 'u', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': 'For example, the center of the unit “The previous president of the commission” ( INLINEFORM0 ) is “president of the commission”. The center of the latter is “president”, which is a leaf in the graph. So the minimal center of INLINEFORM1 is “president”.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 81, 'end': 92, 'text': 'u 1 ', 'latex': 'u_1', 'ref_id': None}, {'start': 225, 'end': 236, 'text': 'u 1 ', 'latex': 'u_1', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': 'Given the input sentence Scenes INLINEFORM0 , the output sentences INLINEFORM1 , and a mapping between them INLINEFORM2 , SAMSA is defined as:', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 32, 'end': 43, 'text': '{sc 1 ,...,sc n inp  }', 'latex': '\\\\lbrace sc_1,...,sc_{n_{inp}}\\\\rbrace ', 'ref_id': None}, {'start': 67, 'end': 78, 'text': '{s 1 ,...,s n out  }', 'latex': '\\\\lbrace s_1,...,s_{n_{out}}\\\\rbrace ', 'ref_id': None}, {'start': 108, 'end': 119, 'text': 'M * ', 'latex': 'M^*', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': ' INLINEFORM0 ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 1, 'end': 12, 'text': 'n out  n inp 1 2n inp ∑ sc i  1 M * (sc i )  (MR i ) + 1 k i  ∑ j=1 k i   1 M * (sc i )  ( Par  i (j) )', 'latex': '\\n\\\\dfrac{n_{out}}{n_{inp}} \\\\dfrac{1}{2n_{inp}} \\\\sum _{sc_i} \\\\big [{1}_{M^*(sc_i)}(MR_i) + \\\\dfrac{1}{k_i} \\\\sum _{j=1}^{k_i} {1}_{M^*(sc_i)}({\\\\rm Par}^{(j)}_{i}) \\\\big ]\\n', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': 'where INLINEFORM0 is the minimal center of the main relation (Process or State) of INLINEFORM1 , and INLINEFORM2 ( INLINEFORM3 ) are the minimal centers of the Participants of INLINEFORM4 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 6, 'end': 17, 'text': ' MR  i ', 'latex': '{\\\\rm MR_i}', 'ref_id': None}, {'start': 83, 'end': 94, 'text': 'sc i ', 'latex': 'sc_i', 'ref_id': None}, {'start': 101, 'end': 112, 'text': ' Par  i (j) ', 'latex': '{\\\\rm Par^{(j)}_i}', 'ref_id': None}, {'start': 115, 'end': 126, 'text': 'j=1,⋯,k i ', 'latex': 'j=1,\\\\dots ,k_i', 'ref_id': None}, {'start': 176, 'end': 187, 'text': 'sc i ', 'latex': 'sc_i', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': 'For an output sentence INLINEFORM0 , INLINEFORM1 is a function from the input units to INLINEFORM2 , which returns 1 iff INLINEFORM3 is aligned (according to INLINEFORM4 ) with a word in INLINEFORM5 .', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 23, 'end': 34, 'text': 's', 'latex': 's', 'ref_id': None}, {'start': 37, 'end': 48, 'text': '1 s (u)', 'latex': '{1}_s(u)', 'ref_id': None}, {'start': 87, 'end': 98, 'text': '{0,1}', 'latex': '\\\\lbrace 0,1\\\\rbrace ', 'ref_id': None}, {'start': 121, 'end': 132, 'text': 'u', 'latex': 'u', 'ref_id': None}, {'start': 158, 'end': 169, 'text': 'A', 'latex': 'A', 'ref_id': None}, {'start': 187, 'end': 198, 'text': 's', 'latex': 's', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The role of the non-splitting penalty term INLINEFORM0 in the SAMSA formula is to penalize cases where the number of sentences in the output is smaller than the number of Scenes. In order to isolate the effect of the non-splitting penalty, we experiment with an additional metric SAMSA INLINEFORM1 (reads “SAMSA ablated”), which is identical to SAMSA but does not take this term into account. Corpus-level SAMSA and SAMSA INLINEFORM2 scores are obtained by averaging their sentence scores.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 43, 'end': 54, 'text': 'n out /n inp ', 'latex': 'n_{out}/n_{inp}', 'ref_id': None}, {'start': 286, 'end': 297, 'text': ' abl ', 'latex': '_{abl}', 'ref_id': None}, {'start': 422, 'end': 433, 'text': ' abl ', 'latex': '_{abl}', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 1\n",
      "{'text': 'In the case of implicit units i.e. omitted units that do not appear explicitly in the text BIBREF13 , since the unit preservation cannot be directly captured, the score INLINEFORM0 for the relevant unit will be set to INLINEFORM1 . For example, in the Scene “traveling is fun”, the people who are traveling correspond to an implicit Participant. As implicit units are not covered by TUPA, this will only be relevant for the semi-automatic implementation of the metric (see Section SECREF6 ).', 'cite_spans': [{'start': 91, 'end': 99, 'text': None, 'latex': None, 'ref_id': 'BIBREF13'}], 'ref_spans': [{'start': 481, 'end': 488, 'text': None, 'latex': None, 'ref_id': 'SECREF6'}], 'eq_spans': [{'start': 169, 'end': 180, 'text': 't', 'latex': 't', 'ref_id': None}, {'start': 218, 'end': 229, 'text': '0.5', 'latex': '0.5', 'ref_id': None}], 'section': 'Score Computation'}\n",
      "====================\n",
      "Score Computation 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Score Computation'}\n",
      "====================\n",
      "Evaluation Protocol 6\n",
      "{'text': 'For testing the automatic metric, we first build a human evaluation benchmark, using 100 sentences from the complex part of the PWKP corpus and the outputs of six recent simplification systems for these sentences: (1) TSM BIBREF5 using Tree-Based SMT, (2) RevILP BIBREF6 using Quasi-Synchronous Grammars, (3) PBMT-R BIBREF7 using Phrase-Based SMT, (4) Hybrid BIBREF9 , a supervised system using DRS, (5) UNSUP BIBREF19 , an unsupervised system using DRS, and (6) Split-Deletion BIBREF19 , the unsupervised system with only structural operations.', 'cite_spans': [{'start': 222, 'end': 229, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 263, 'end': 270, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'}, {'start': 316, 'end': 323, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'}, {'start': 359, 'end': 366, 'text': None, 'latex': None, 'ref_id': 'BIBREF9'}, {'start': 410, 'end': 418, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}, {'start': 478, 'end': 486, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation Protocol'}\n",
      "====================\n",
      "Evaluation Protocol 0\n",
      "{'text': 'All these systems explicitly address at least one type of structural simplification operation. The last system, Split-Deletion, performs only structural (i.e., no lexical) operations. It is thus an interesting test case for SAMSA since here the aligner can be replaced by a simple match between identical words. In total we obtain 600 system outputs from the six systems, as well as 100 sentences from the simple Wikipedia side of the corpus, which serve as references. Five in-house annotators with high proficiency in English evaluated the resulting 700 input-output pairs by answering the questions in Table TABREF20 .', 'cite_spans': [], 'ref_spans': [{'start': 611, 'end': 619, 'text': None, 'latex': None, 'ref_id': 'TABREF20'}], 'eq_spans': [], 'section': 'Evaluation Protocol'}\n",
      "====================\n",
      "Evaluation Protocol 2\n",
      "{'text': 'Qa addresses grammaticality, Qb and Qc capture two complementary aspects of meaning preservation (the addition and the removal of information) and Qd addresses structural simplicity. Possible answers are: 1 (“no”), 2 (“maybe”) and 3 (“yes”). Following BIBREF34 , we used a 3 point Likert scale, which has recently been shown to be preferable over a 5 point scale through human studies on sentence compression BIBREF30 .', 'cite_spans': [{'start': 252, 'end': 260, 'text': None, 'latex': None, 'ref_id': 'BIBREF34'}, {'start': 409, 'end': 417, 'text': None, 'latex': None, 'ref_id': 'BIBREF30'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation Protocol'}\n",
      "====================\n",
      "Evaluation Protocol 0\n",
      "{'text': 'Question Qd was accompanied by a negative example showing a case of lexical simplification, where a complex word is replaced by a simple one. A positive example was not included so as not to bias the annotators by revealing the nature of the operations our experiments focus on (i.e., splitting and deletion).', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation Protocol'}\n",
      "====================\n",
      "Evaluation Protocol 2\n",
      "{'text': \"The PWKP test corpus BIBREF5 was selected for our experiments over the development and test sets used in BIBREF10 , as the latter's selection process was explicitly biased towards input-output pairs that mainly contain lexical simplifications.\", 'cite_spans': [{'start': 21, 'end': 28, 'text': None, 'latex': None, 'ref_id': 'BIBREF5'}, {'start': 105, 'end': 113, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation Protocol'}\n",
      "====================\n",
      "Evaluation Protocol 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation Protocol'}\n",
      "====================\n",
      "Evaluation Protocol 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation Protocol'}\n",
      "====================\n",
      "Human Score Computation 0\n",
      "{'text': \"Given the annotator's answers, we consider the following scores. First, the grammaticality score INLINEFORM0 is the answer to Qa. By inverting (changing 1 to 3 and 3 to 1) the answer for Qb, we obtain a Non-Addition score indicating to which extent no additional information has been added. Similarly, inverting the answer to Qc yields the Non-Removal score. Averaging these two scores, we obtain the meaning preservation score INLINEFORM1 . Finally, the structural simplicity score INLINEFORM2 is the answer to Qd. Each of these scores is averaged over the five annotators. We further compute an average human score: INLINEFORM3 \", 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 97, 'end': 108, 'text': '𝒢', 'latex': '\\\\mathcal {G}', 'ref_id': None}, {'start': 428, 'end': 439, 'text': '𝒫', 'latex': '\\\\mathcal {P}', 'ref_id': None}, {'start': 483, 'end': 494, 'text': '𝒮', 'latex': '\\\\mathcal {S}', 'ref_id': None}, {'start': 618, 'end': 629, 'text': ' AvgHuman =1 3(𝒢+𝒫+𝒮)', 'latex': '{\\\\rm AvgHuman} = \\\\frac{1}{3} (\\\\mathcal {G} + \\\\mathcal {P} + \\\\mathcal {S})', 'ref_id': None}], 'section': 'Human Score Computation'}\n",
      "====================\n",
      "Human Score Computation 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Human Score Computation'}\n",
      "====================\n",
      "Human Score Computation 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Human Score Computation'}\n",
      "====================\n",
      "Inter-annotator Agreement 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Inter-annotator agreement rates are computed in two ways. Table TABREF23 presents the absolute agreement and Cohen's quadratic weighted INLINEFORM0 BIBREF41 . Table TABREF24 presents Spearman's correlation ( INLINEFORM1 ) between the human ratings of the input-output pairs (top row), and between the resulting system scores (bottom row). In both cases, the agreement between the five annotators is computed as the average agreement over the 10 annotator pairs.\", 'cite_spans': [{'start': 148, 'end': 156, 'text': None, 'latex': None, 'ref_id': 'BIBREF41'}], 'ref_spans': [{'start': 64, 'end': 72, 'text': None, 'latex': None, 'ref_id': 'TABREF23'}, {'start': 165, 'end': 173, 'text': None, 'latex': None, 'ref_id': 'TABREF24'}], 'eq_spans': [{'start': 136, 'end': 147, 'text': 'κ', 'latex': '\\\\kappa ', 'ref_id': None}, {'start': 208, 'end': 219, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Inter-annotator Agreement'}\n",
      "====================\n",
      "Inter-annotator Agreement 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Inter-annotator Agreement'}\n",
      "====================\n",
      "Inter-annotator Agreement 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Inter-annotator Agreement'}\n",
      "====================\n",
      "Inter-annotator Agreement 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Inter-annotator Agreement'}\n",
      "====================\n",
      "Experimental Setup 4\n",
      "{'text': 'We further compute SAMSA for the 100 sentences of the PWKP test set and the corresponding system outputs. Experiments are conducted in two settings: (1) a semi-automatic setting where UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software BIBREF42 , and according to the standard annotation guidelines; (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser BIBREF40 . Sentence segmentation of the outputs was carried out using the NLTK package BIBREF43 . For word alignments, we used the aligner of BIBREF44 .', 'cite_spans': [{'start': 297, 'end': 305, 'text': None, 'latex': None, 'ref_id': 'BIBREF42'}, {'start': 447, 'end': 455, 'text': None, 'latex': None, 'ref_id': 'BIBREF40'}, {'start': 534, 'end': 542, 'text': None, 'latex': None, 'ref_id': 'BIBREF43'}, {'start': 589, 'end': 597, 'text': None, 'latex': None, 'ref_id': 'BIBREF44'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Experimental Setup'}\n",
      "====================\n",
      "Experimental Setup 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Experimental Setup'}\n",
      "====================\n",
      "Correlation with Human Evaluation 0\n",
      "{'text': 'We compare the system rankings obtained by SAMSA and by the four human parameters. We find that the two leading systems according to AvgHuman and SAMSA turn out to be the same: Split-Deletion and RevILP. This is the case both for the semi-automatic and the automatic implementations of the metric. A Spearman INLINEFORM0 correlation between the human and SAMSA scores (comparing their rankings) is presented in Table TABREF27 .', 'cite_spans': [], 'ref_spans': [{'start': 417, 'end': 425, 'text': None, 'latex': None, 'ref_id': 'TABREF27'}], 'eq_spans': [{'start': 309, 'end': 320, 'text': 'ρ', 'latex': '\\\\rho ', 'ref_id': None}], 'section': 'Correlation with Human Evaluation'}\n",
      "====================\n",
      "Correlation with Human Evaluation 2\n",
      "{'text': 'We compare SAMSA and SAMSA INLINEFORM0 to the reference-based measures SARI BIBREF10 and BLEU, as well as to the negative Levenshtein distance to the reference (-LD INLINEFORM1 ). We use the only available reference for this corpus, in accordance with the standard practice. SARI is a reference-based measure, based on n-gram overlap between the source, output and reference, and focuses on lexical (rather than structural) simplification. For completeness, we include the other two measures reported in BIBREF19 , which are measures of similarity to the input (i.e., they quantify the tendency of the systems to introduce changes to the input): the negative Levenshtein distances between the output and input compared to the original complex corpus (-LD INLINEFORM2 ), and the number of sentences split by each of the systems.', 'cite_spans': [{'start': 76, 'end': 84, 'text': None, 'latex': None, 'ref_id': 'BIBREF10'}, {'start': 504, 'end': 512, 'text': None, 'latex': None, 'ref_id': 'BIBREF19'}], 'ref_spans': [], 'eq_spans': [{'start': 27, 'end': 38, 'text': ' abl ', 'latex': '_{abl}', 'ref_id': None}, {'start': 165, 'end': 176, 'text': '  SR  ', 'latex': '_{{\\\\rm SR}}', 'ref_id': None}, {'start': 755, 'end': 766, 'text': '  SC  ', 'latex': '_{{\\\\rm SC}}', 'ref_id': None}], 'section': 'Correlation with Human Evaluation'}\n",
      "====================\n",
      "Correlation with Human Evaluation 0\n",
      "{'text': 'The highest correlation with AvgHuman and grammaticality is obtained by semi-automatic SAMSA (0.58 and 0.54), a high correlation especially in comparison to the inter-annotator agreement on AvgHuman (0.64, Table TABREF24 ). The automatic version obtains high correlation with human judgments in these settings, where for structural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although when restricted to multi-Scene sentences, the correlation for SAMSA (semi-automatic) is higher (0.89, INLINEFORM0 and 0.77, INLINEFORM1 ).', 'cite_spans': [], 'ref_spans': [{'start': 212, 'end': 220, 'text': None, 'latex': None, 'ref_id': 'TABREF24'}], 'eq_spans': [{'start': 690, 'end': 701, 'text': 'p=0.009', 'latex': 'p=0.009', 'ref_id': None}, {'start': 712, 'end': 723, 'text': 'p=0.04', 'latex': 'p=0.04', 'ref_id': None}], 'section': 'Correlation with Human Evaluation'}\n",
      "====================\n",
      "Correlation with Human Evaluation 1\n",
      "{'text': 'The highest correlation for meaning preservation is obtained by SAMSA INLINEFORM0 which provides further evidence that the retainment of semantic structures is a strong predictor of meaning preservation BIBREF14 . SAMSA in itself does not correlate with meaning preservation, probably due to its penalization of under-splitting sentences.', 'cite_spans': [{'start': 203, 'end': 211, 'text': None, 'latex': None, 'ref_id': 'BIBREF14'}], 'ref_spans': [], 'eq_spans': [{'start': 70, 'end': 81, 'text': ' abl ', 'latex': '_{abl}', 'ref_id': None}], 'section': 'Correlation with Human Evaluation'}\n",
      "====================\n",
      "Correlation with Human Evaluation 0\n",
      "{'text': 'Note that the standard reference-based measures for simplification, BLEU and SARI, obtain low and often negative correlation with human ratings. We believe that this is the case because SARI and BLEU admittedly focus on lexical simplification, and are difficult to use to rank systems which also perform structural simplification.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Correlation with Human Evaluation'}\n",
      "====================\n",
      "Correlation with Human Evaluation 0\n",
      "{'text': 'Our results thus suggest that SAMSA provides additional value in predicting the quality of a simplification system and should be reported in tandem with more lexically-oriented measures.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Correlation with Human Evaluation'}\n",
      "====================\n",
      "Correlation with Human Evaluation 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Correlation with Human Evaluation'}\n",
      "====================\n",
      "Discussion 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Discussion'}\n",
      "====================\n",
      "Evaluation on the QATS Benchmark 1\n",
      "{'text': \" In order to provide further validation for SAMSA predictive value for quality of simplification systems, we report SAMSA's correlation with a recently proposed benchmark, used for the QATS (Quality Assessment for Text Simplification) shared task BIBREF18 .\", 'cite_spans': [{'start': 247, 'end': 255, 'text': None, 'latex': None, 'ref_id': 'BIBREF18'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation on the QATS Benchmark'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Evaluation on the QATS Benchmark 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Evaluation on the QATS Benchmark'}\n",
      "====================\n",
      "Conclusion 0\n",
      "{'text': ' We presented the first structure-aware metric for text simplification, SAMSA, and the first evaluation experiments that directly target the structural simplification component, separately from the lexical component. We argue that the structural and lexical dimensions of simplification are loosely related, and that TS evaluation protocols should assess both. We empirically demonstrate that strong measures that assess lexical simplification quality (notably SARI), fail to correlate with human judgments when structural simplification is performed by the evaluated systems. Our experiments show that SAMSA correlates well with human judgments in such settings, which demonstrates its usefulness for evaluating and tuning statistical simplification systems, and shows that structural evaluation provides a complementary perspective on simplification quality.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusion'}\n",
      "====================\n",
      "Conclusion 0\n",
      "{'text': '', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusion'}\n",
      "====================\n",
      "Acknowledgments 0\n",
      "{'text': \" We would like to thank Zhemin Zhu and Sander Wubben for sharing their data, as well as the annotators for participating in our evaluation and UCCA annotation experiments. We also thank Daniel Hershcovich and the anonymous reviewers for their helpful comments. This work was partially supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and by the Israel Science Foundation (grant No. 929/17), as well as by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister's Office.\", 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgments'}\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "article = all_articles[1007]\n",
    "latex_parse_overview = dict()\n",
    "for sections in article['latex_parse']['body_text']:\n",
    "    latex_parse_overview[sections['section']] = sections\n",
    "    print(sections['section'],len(sections['cite_spans']))\n",
    "    print(sections)\n",
    "    print(10*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': [],\n",
       " 'body_text': [{'text': 'Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation BIBREF0 , BIBREF1 and relation extraction BIBREF2 , and has also been shown useful in the development of reading aids, e.g., for people with dyslexia BIBREF3 or non-native speakers BIBREF4 .',\n",
       "   'cite_spans': [{'start': 193,\n",
       "     'end': 200,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF0'},\n",
       "    {'start': 203,\n",
       "     'end': 210,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 235,\n",
       "     'end': 242,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF2'},\n",
       "    {'start': 343,\n",
       "     'end': 350,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF3'},\n",
       "    {'start': 374,\n",
       "     'end': 381,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF4'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'The task has attracted much attention in the past decade BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations (e.g., deletion, splitting and substitution). BIBREF10 has recently made considerable progress towards that goal, and proposed to tackle it both by using an improved reference-based measure, named SARI, and by increasing the number of references. However, their research focused on lexical, rather than structural simplification, which provides a complementary view of TS quality as this paper will show.',\n",
       "   'cite_spans': [{'start': 57,\n",
       "     'end': 64,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 67, 'end': 74, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "    {'start': 77, 'end': 84, 'text': None, 'latex': None, 'ref_id': 'BIBREF7'},\n",
       "    {'start': 87, 'end': 94, 'text': None, 'latex': None, 'ref_id': 'BIBREF8'},\n",
       "    {'start': 97,\n",
       "     'end': 104,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'},\n",
       "    {'start': 405,\n",
       "     'end': 413,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'This paper focuses on the evaluation of the structural aspects of the task. We introduce the semantic measure SAMSA (Simplification Automatic evaluation Measure through Semantic Annotation), the first structure-aware measure for TS in general, and the first to use semantic structure in this context in particular. SAMSA stipulates that an optimal split of the input is one where each predicate-argument structure is assigned its own sentence, and measures to what extent this assertion holds for the input-output pair in question, by using semantic structure. SAMSA focuses on the core semantic components of the sentence, and is tolerant towards the deletion of other units.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'For example, SAMSA will assign a high score to the output split “John got home. John gave Mary a call.” for the input sentence “John got home and gave Mary a call.”, as it splits each of its predicate-argument structures to a different sentence. Splits that alter predicate-argument relations such as “John got home and gave. Mary called.” are penalized by SAMSA.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': \"SAMSA's use of semantic structures for TS evaluation has several motivations. First, it provides means to measure the extent to which the meaning of the source is preserved in the output. Second, it provides means for measuring whether the input sentence was split to semantic units of the right granularity. Third, defining a semantic measure that does not require references avoids the difficulties incurred by their non-uniqueness, and the difficulty in collecting high quality references, as reported by BIBREF12 and by BIBREF9 with respect to the Parallel Wikipedia Corpus BIBREF5 . SAMSA is further motivated by its use of semantic annotation only on the source side, which allows to evaluate multiple systems using same source-side annotation, and avoids the need to parse system outputs, which can be garbled.\",\n",
       "   'cite_spans': [{'start': 508,\n",
       "     'end': 516,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF12'},\n",
       "    {'start': 524,\n",
       "     'end': 531,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'},\n",
       "    {'start': 578,\n",
       "     'end': 585,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': 'In this paper we use the UCCA scheme for defining semantic structure BIBREF13 . UCCA has been shown to be preserved remarkably well across translations BIBREF14 and has also been successfully used for machine translation evaluation BIBREF15 (Section SECREF2 ). We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR BIBREF16 or Discourse Representation Structures BIBREF17 , as used by BIBREF9 .',\n",
       "   'cite_spans': [{'start': 69,\n",
       "     'end': 77,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF13'},\n",
       "    {'start': 152,\n",
       "     'end': 160,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF14'},\n",
       "    {'start': 232,\n",
       "     'end': 240,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF15'},\n",
       "    {'start': 394,\n",
       "     'end': 402,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF16'},\n",
       "    {'start': 442,\n",
       "     'end': 450,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF17'},\n",
       "    {'start': 464,\n",
       "     'end': 471,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   'ref_spans': [{'start': 250,\n",
       "     'end': 257,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF2'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': \"We experiment with SAMSA both where semantic annotation is carried out manually, and where it is carried out by a parser. See Section SECREF4 . We conduct human rating experiments and compare the resulting system rankings with those predicted by SAMSA. We find that SAMSA's rankings obtain high correlations with human rankings, and compare favorably to existing reference-based measures for TS. Moreover, our results show that existing measures, which mainly target lexical simplification, are ill-suited to predict human judgments where structural simplification is involved. Finally, we apply SAMSA to the dataset of the QATS shared task on simplification evaluation BIBREF18 . We find that SAMSA obtains comparative correlation with human judgments on the task, despite operating in a more restricted setting, as it does not use human ratings as training data and focuses only on structural aspects of simplicity. Section SECREF2 presents previous work. Section SECREF3 discusses UCCA. Section SECREF4 presents SAMSA. Section SECREF5 details the collection of human judgments. Our experimental setup for comparing our human and automatic rankings is given in Section SECREF6 , and results are given in Section SECREF7 , showing superior results for SAMSA. A discussion on the results is presented in Section SECREF8 . Section SECREF9 presents experiments with SAMSA on the QATS evaluation benchmark.\",\n",
       "   'cite_spans': [{'start': 670,\n",
       "     'end': 678,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF18'}],\n",
       "   'ref_spans': [{'start': 134,\n",
       "     'end': 141,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF4'},\n",
       "    {'start': 926,\n",
       "     'end': 933,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF2'},\n",
       "    {'start': 966,\n",
       "     'end': 973,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF3'},\n",
       "    {'start': 998,\n",
       "     'end': 1005,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF4'},\n",
       "    {'start': 1030,\n",
       "     'end': 1037,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF5'},\n",
       "    {'start': 1171,\n",
       "     'end': 1178,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF6'},\n",
       "    {'start': 1214,\n",
       "     'end': 1221,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF7'},\n",
       "    {'start': 1312,\n",
       "     'end': 1319,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF8'},\n",
       "    {'start': 1330,\n",
       "     'end': 1337,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF9'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Introduction'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Related Work'},\n",
       "  {'text': ' In this section we will briefly describe the UCCA scheme, focusing on the concepts of Scenes and Centers which are key in the definition of SAMSA. UCCA BIBREF13 is a semantic annotation scheme based on typological BIBREF35 , BIBREF36 , BIBREF37 and cognitive BIBREF38 theories which aims to represent the main semantic phenomena in the text, abstracting away from syntactic detail. UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph (including the words of the text) or to several elements jointly viewed as a single entity according to some semantic or cognitive consideration. Unlike AMR, UCCA semantic units are directly anchored in the text BIBREF39 , BIBREF15 , which allows easy inclusion of a word-to-word alignment in the metric model (Section SECREF4 ).',\n",
       "   'cite_spans': [{'start': 153,\n",
       "     'end': 161,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF13'},\n",
       "    {'start': 215,\n",
       "     'end': 223,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF35'},\n",
       "    {'start': 226,\n",
       "     'end': 234,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF36'},\n",
       "    {'start': 237,\n",
       "     'end': 245,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF37'},\n",
       "    {'start': 260,\n",
       "     'end': 268,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF38'},\n",
       "    {'start': 707,\n",
       "     'end': 715,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF39'},\n",
       "    {'start': 718,\n",
       "     'end': 726,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF15'}],\n",
       "   'ref_spans': [{'start': 814,\n",
       "     'end': 821,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF4'}],\n",
       "   'eq_spans': [],\n",
       "   'section': \"UCCA's Semantic Structures\"},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': \"UCCA's Semantic Structures\"},\n",
       "  {'text': \"SAMSA's main premise is that a structurally correct simplification is one where: (1) each sentence contains a single event from the input (UCCA Scene), (2) the main relation of each of the events and their participants are retained in the output.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'The SAMSA Metric'},\n",
       "  {'text': 'For example, consider “John wrote a book. I read that book.” as a simplification of “I read the book that John wrote.”. Each output sentence contains one Scene, which has the same Scene elements as the source, and would thus be deemed correct by SAMSA. On the other hand, the output “John wrote. I read the book.” is an incorrect split of that sentence, since a participant of the “writing” Scene, namely “the book” is absent in the split sentence. SAMSA would indeed penalize such a case.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'The SAMSA Metric'},\n",
       "  {'text': 'Similarly, Scenes which have elements across several sentences receive a zero score by SAMSA. As an example, consider the sentence “The combination of new weapons and tactics marks this battle as the end of chivalry”, and erroneous split “The combination of new weapons and tactics. It is the end of chivalry.” (adapted from the output of a recent system on the PWKP corpus), which does not preserve the original meaning.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'The SAMSA Metric'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'The SAMSA Metric'},\n",
       "  {'text': 'SAMSA is based on two external linguistic resources. One is a semantic annotation (UCCA in our experiments) of the source side, which can be carried out either manually or automatically, using the TUPA parser BIBREF40 for UCCA. UCCA decomposes each sentence INLINEFORM0 into a set of Scenes INLINEFORM1 , where each scene INLINEFORM2 contains a main relation INLINEFORM3 (sub-span of INLINEFORM4 ) and a set of zero or more participants INLINEFORM5 .',\n",
       "   'cite_spans': [{'start': 209,\n",
       "     'end': 217,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF40'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 258,\n",
       "     'end': 269,\n",
       "     'text': 's',\n",
       "     'latex': 's',\n",
       "     'ref_id': None},\n",
       "    {'start': 291,\n",
       "     'end': 302,\n",
       "     'text': '{sc 1 ,sc 2 ,..,sc n }',\n",
       "     'latex': '\\\\lbrace sc_1,sc_2,..,sc_n\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 322,\n",
       "     'end': 333,\n",
       "     'text': 'sc i ',\n",
       "     'latex': 'sc_i',\n",
       "     'ref_id': None},\n",
       "    {'start': 359,\n",
       "     'end': 370,\n",
       "     'text': 'mr i ',\n",
       "     'latex': 'mr_i',\n",
       "     'ref_id': None},\n",
       "    {'start': 384,\n",
       "     'end': 395,\n",
       "     'text': 'sc i ',\n",
       "     'latex': 'sc_i',\n",
       "     'ref_id': None},\n",
       "    {'start': 437,\n",
       "     'end': 448,\n",
       "     'text': 'A i ',\n",
       "     'latex': 'A_i',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': 'The second resource is a word-to-word alignment INLINEFORM0 between the words in the input and one or zero words in the output. The monolingual alignment thus permits SAMSA not to penalize outputs that involve lexical substitutions (e.g., “commence” might be aligned with “start”). We denote by INLINEFORM1 the number of UCCA Scenes in the input sentence and by INLINEFORM2 the number of sentences in the output.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 48,\n",
       "     'end': 59,\n",
       "     'text': 'A',\n",
       "     'latex': 'A',\n",
       "     'ref_id': None},\n",
       "    {'start': 295,\n",
       "     'end': 306,\n",
       "     'text': 'n inp ',\n",
       "     'latex': 'n_{inp}',\n",
       "     'ref_id': None},\n",
       "    {'start': 362,\n",
       "     'end': 373,\n",
       "     'text': 'n out ',\n",
       "     'latex': 'n_{out}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': \"Given an input sentence's UCCA Scenes INLINEFORM0 , a non-annotated output of a simplification system split into sentences INLINEFORM1 , and their word alignment INLINEFORM2 , we distinguish between two cases:\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 38,\n",
       "     'end': 49,\n",
       "     'text': 'sc 1 ,...,sc n inp  ',\n",
       "     'latex': 'sc_1,\\\\ldots , sc_{n_{inp}}',\n",
       "     'ref_id': None},\n",
       "    {'start': 123,\n",
       "     'end': 134,\n",
       "     'text': 's 1 ,...,s n out  ',\n",
       "     'latex': 's_1,\\\\ldots ,s_{n_{out}}',\n",
       "     'ref_id': None},\n",
       "    {'start': 162, 'end': 173, 'text': 'A', 'latex': 'A', 'ref_id': None}],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': ' INLINEFORM0 : in this case, we compute the maximal Many-to-1 correspondence between Scenes and sentences. A Scene is matched to a sentence in the following way. We say that a leaf INLINEFORM1 in a Scene INLINEFORM2 is consistent in a Scene-sentence mapping INLINEFORM3 which maps INLINEFORM4 to a sentence INLINEFORM5 , if there is a word INLINEFORM6 which INLINEFORM7 aligns to (according to the word alignment INLINEFORM8 ). The score of matching a Scene INLINEFORM9 to a sentence INLINEFORM10 is then defined to be the total number of consistent leaves in INLINEFORM11 . We traverse the Scenes in their order of occurrence in the text, selecting for each the sentence that maximizes the score. If INLINEFORM12 , once a sentence is matched to a Scene, it cannot be matched to another one. Ties between sentences are broken towards the sentence that appeared first in the output.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': 'n inp ≥n out ',\n",
       "     'latex': 'n_{inp} \\\\ge n_{out}',\n",
       "     'ref_id': None},\n",
       "    {'start': 181, 'end': 192, 'text': 'l', 'latex': 'l', 'ref_id': None},\n",
       "    {'start': 204, 'end': 215, 'text': 'sc', 'latex': 'sc', 'ref_id': None},\n",
       "    {'start': 258, 'end': 269, 'text': 'M', 'latex': 'M', 'ref_id': None},\n",
       "    {'start': 281, 'end': 292, 'text': 'sc', 'latex': 'sc', 'ref_id': None},\n",
       "    {'start': 307, 'end': 318, 'text': 's', 'latex': 's', 'ref_id': None},\n",
       "    {'start': 340,\n",
       "     'end': 351,\n",
       "     'text': 'w∈s',\n",
       "     'latex': 'w \\\\in s',\n",
       "     'ref_id': None},\n",
       "    {'start': 358, 'end': 369, 'text': 'l', 'latex': 'l', 'ref_id': None},\n",
       "    {'start': 413, 'end': 424, 'text': 'A', 'latex': 'A', 'ref_id': None},\n",
       "    {'start': 458, 'end': 469, 'text': 'sc', 'latex': 'sc', 'ref_id': None},\n",
       "    {'start': 484, 'end': 496, 'text': 's', 'latex': 's', 'ref_id': None},\n",
       "    {'start': 560, 'end': 572, 'text': 'sc', 'latex': 'sc', 'ref_id': None},\n",
       "    {'start': 701,\n",
       "     'end': 713,\n",
       "     'text': 'n inp =n out ',\n",
       "     'latex': 'n_{inp} = n_{out}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': ' INLINEFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': 'M * (sc i )= argmax  s score(sc i ,s)',\n",
       "     'latex': ' \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ {\\\\rm M^*}(sc_i) = {\\\\rm argmax}_{s} score(sc_i,s)',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': ' INLINEFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': 's.t.s∉{M * (sc 1 ),⋯,M * (sc i-1 )} if n inp =n out ',\n",
       "     'latex': '{\\\\rm s.t.}\\\\,\\\\,\\\\, s \\\\notin \\\\lbrace M^*(sc_1),\\\\dots ,M^*(sc_{i-1})\\\\rbrace \\\\ \\\\ {\\\\rm if} \\\\ n_{inp} = n_{out}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': ' INLINEFORM0 : In this case, a Scene will necessarily be split across several sentences. As this is an undesired result, we assign this instance a score of zero.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': 'n inp <n out ',\n",
       "     'latex': 'n_{inp}<n_{out}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Matching Scenes to Sentences'},\n",
       "  {'text': \"The minimal center of a UCCA unit INLINEFORM0 is UCCA's notion of a semantic head word, defined through recursive rules not unlike the head propagation rules used for converting constituency structures to dependency structures. More formally, we define the minimal center of a UCCA unit INLINEFORM1 (here a Participant or a Main Relation) to be the UCCA graph's leaf reached by starting from INLINEFORM2 and iteratively selecting the child tagged as Center. If a Participant (or a Center inside a Participant) is a Scene, its center is the main relation (Process or State) of the Scene.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 34,\n",
       "     'end': 45,\n",
       "     'text': 'u',\n",
       "     'latex': 'u',\n",
       "     'ref_id': None},\n",
       "    {'start': 287, 'end': 298, 'text': 'u', 'latex': 'u', 'ref_id': None},\n",
       "    {'start': 392, 'end': 403, 'text': 'u', 'latex': 'u', 'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': 'For example, the center of the unit “The previous president of the commission” ( INLINEFORM0 ) is “president of the commission”. The center of the latter is “president”, which is a leaf in the graph. So the minimal center of INLINEFORM1 is “president”.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 81,\n",
       "     'end': 92,\n",
       "     'text': 'u 1 ',\n",
       "     'latex': 'u_1',\n",
       "     'ref_id': None},\n",
       "    {'start': 225,\n",
       "     'end': 236,\n",
       "     'text': 'u 1 ',\n",
       "     'latex': 'u_1',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': 'Given the input sentence Scenes INLINEFORM0 , the output sentences INLINEFORM1 , and a mapping between them INLINEFORM2 , SAMSA is defined as:',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 32,\n",
       "     'end': 43,\n",
       "     'text': '{sc 1 ,...,sc n inp  }',\n",
       "     'latex': '\\\\lbrace sc_1,...,sc_{n_{inp}}\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 67,\n",
       "     'end': 78,\n",
       "     'text': '{s 1 ,...,s n out  }',\n",
       "     'latex': '\\\\lbrace s_1,...,s_{n_{out}}\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 108,\n",
       "     'end': 119,\n",
       "     'text': 'M * ',\n",
       "     'latex': 'M^*',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': ' INLINEFORM0 ',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 1,\n",
       "     'end': 12,\n",
       "     'text': 'n out  n inp 1 2n inp ∑ sc i  1 M * (sc i )  (MR i ) + 1 k i  ∑ j=1 k i   1 M * (sc i )  ( Par  i (j) )',\n",
       "     'latex': '\\n\\\\dfrac{n_{out}}{n_{inp}} \\\\dfrac{1}{2n_{inp}} \\\\sum _{sc_i} \\\\big [{1}_{M^*(sc_i)}(MR_i) + \\\\dfrac{1}{k_i} \\\\sum _{j=1}^{k_i} {1}_{M^*(sc_i)}({\\\\rm Par}^{(j)}_{i}) \\\\big ]\\n',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': 'where INLINEFORM0 is the minimal center of the main relation (Process or State) of INLINEFORM1 , and INLINEFORM2 ( INLINEFORM3 ) are the minimal centers of the Participants of INLINEFORM4 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 6,\n",
       "     'end': 17,\n",
       "     'text': ' MR  i ',\n",
       "     'latex': '{\\\\rm MR_i}',\n",
       "     'ref_id': None},\n",
       "    {'start': 83, 'end': 94, 'text': 'sc i ', 'latex': 'sc_i', 'ref_id': None},\n",
       "    {'start': 101,\n",
       "     'end': 112,\n",
       "     'text': ' Par  i (j) ',\n",
       "     'latex': '{\\\\rm Par^{(j)}_i}',\n",
       "     'ref_id': None},\n",
       "    {'start': 115,\n",
       "     'end': 126,\n",
       "     'text': 'j=1,⋯,k i ',\n",
       "     'latex': 'j=1,\\\\dots ,k_i',\n",
       "     'ref_id': None},\n",
       "    {'start': 176,\n",
       "     'end': 187,\n",
       "     'text': 'sc i ',\n",
       "     'latex': 'sc_i',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': 'For an output sentence INLINEFORM0 , INLINEFORM1 is a function from the input units to INLINEFORM2 , which returns 1 iff INLINEFORM3 is aligned (according to INLINEFORM4 ) with a word in INLINEFORM5 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 23,\n",
       "     'end': 34,\n",
       "     'text': 's',\n",
       "     'latex': 's',\n",
       "     'ref_id': None},\n",
       "    {'start': 37,\n",
       "     'end': 48,\n",
       "     'text': '1 s (u)',\n",
       "     'latex': '{1}_s(u)',\n",
       "     'ref_id': None},\n",
       "    {'start': 87,\n",
       "     'end': 98,\n",
       "     'text': '{0,1}',\n",
       "     'latex': '\\\\lbrace 0,1\\\\rbrace ',\n",
       "     'ref_id': None},\n",
       "    {'start': 121, 'end': 132, 'text': 'u', 'latex': 'u', 'ref_id': None},\n",
       "    {'start': 158, 'end': 169, 'text': 'A', 'latex': 'A', 'ref_id': None},\n",
       "    {'start': 187, 'end': 198, 'text': 's', 'latex': 's', 'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': 'The role of the non-splitting penalty term INLINEFORM0 in the SAMSA formula is to penalize cases where the number of sentences in the output is smaller than the number of Scenes. In order to isolate the effect of the non-splitting penalty, we experiment with an additional metric SAMSA INLINEFORM1 (reads “SAMSA ablated”), which is identical to SAMSA but does not take this term into account. Corpus-level SAMSA and SAMSA INLINEFORM2 scores are obtained by averaging their sentence scores.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 43,\n",
       "     'end': 54,\n",
       "     'text': 'n out /n inp ',\n",
       "     'latex': 'n_{out}/n_{inp}',\n",
       "     'ref_id': None},\n",
       "    {'start': 286,\n",
       "     'end': 297,\n",
       "     'text': ' abl ',\n",
       "     'latex': '_{abl}',\n",
       "     'ref_id': None},\n",
       "    {'start': 422,\n",
       "     'end': 433,\n",
       "     'text': ' abl ',\n",
       "     'latex': '_{abl}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': 'In the case of implicit units i.e. omitted units that do not appear explicitly in the text BIBREF13 , since the unit preservation cannot be directly captured, the score INLINEFORM0 for the relevant unit will be set to INLINEFORM1 . For example, in the Scene “traveling is fun”, the people who are traveling correspond to an implicit Participant. As implicit units are not covered by TUPA, this will only be relevant for the semi-automatic implementation of the metric (see Section SECREF6 ).',\n",
       "   'cite_spans': [{'start': 91,\n",
       "     'end': 99,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF13'}],\n",
       "   'ref_spans': [{'start': 481,\n",
       "     'end': 488,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF6'}],\n",
       "   'eq_spans': [{'start': 169,\n",
       "     'end': 180,\n",
       "     'text': 't',\n",
       "     'latex': 't',\n",
       "     'ref_id': None},\n",
       "    {'start': 218, 'end': 229, 'text': '0.5', 'latex': '0.5', 'ref_id': None}],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Score Computation'},\n",
       "  {'text': 'For testing the automatic metric, we first build a human evaluation benchmark, using 100 sentences from the complex part of the PWKP corpus and the outputs of six recent simplification systems for these sentences: (1) TSM BIBREF5 using Tree-Based SMT, (2) RevILP BIBREF6 using Quasi-Synchronous Grammars, (3) PBMT-R BIBREF7 using Phrase-Based SMT, (4) Hybrid BIBREF9 , a supervised system using DRS, (5) UNSUP BIBREF19 , an unsupervised system using DRS, and (6) Split-Deletion BIBREF19 , the unsupervised system with only structural operations.',\n",
       "   'cite_spans': [{'start': 222,\n",
       "     'end': 229,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 263,\n",
       "     'end': 270,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'},\n",
       "    {'start': 316,\n",
       "     'end': 323,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF7'},\n",
       "    {'start': 359,\n",
       "     'end': 366,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'},\n",
       "    {'start': 410,\n",
       "     'end': 418,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF19'},\n",
       "    {'start': 478,\n",
       "     'end': 486,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF19'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation Protocol'},\n",
       "  {'text': 'All these systems explicitly address at least one type of structural simplification operation. The last system, Split-Deletion, performs only structural (i.e., no lexical) operations. It is thus an interesting test case for SAMSA since here the aligner can be replaced by a simple match between identical words. In total we obtain 600 system outputs from the six systems, as well as 100 sentences from the simple Wikipedia side of the corpus, which serve as references. Five in-house annotators with high proficiency in English evaluated the resulting 700 input-output pairs by answering the questions in Table TABREF20 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 611,\n",
       "     'end': 619,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF20'}],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation Protocol'},\n",
       "  {'text': 'Qa addresses grammaticality, Qb and Qc capture two complementary aspects of meaning preservation (the addition and the removal of information) and Qd addresses structural simplicity. Possible answers are: 1 (“no”), 2 (“maybe”) and 3 (“yes”). Following BIBREF34 , we used a 3 point Likert scale, which has recently been shown to be preferable over a 5 point scale through human studies on sentence compression BIBREF30 .',\n",
       "   'cite_spans': [{'start': 252,\n",
       "     'end': 260,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF34'},\n",
       "    {'start': 409,\n",
       "     'end': 417,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF30'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation Protocol'},\n",
       "  {'text': 'Question Qd was accompanied by a negative example showing a case of lexical simplification, where a complex word is replaced by a simple one. A positive example was not included so as not to bias the annotators by revealing the nature of the operations our experiments focus on (i.e., splitting and deletion).',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation Protocol'},\n",
       "  {'text': \"The PWKP test corpus BIBREF5 was selected for our experiments over the development and test sets used in BIBREF10 , as the latter's selection process was explicitly biased towards input-output pairs that mainly contain lexical simplifications.\",\n",
       "   'cite_spans': [{'start': 21,\n",
       "     'end': 28,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 105,\n",
       "     'end': 113,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation Protocol'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation Protocol'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation Protocol'},\n",
       "  {'text': \"Given the annotator's answers, we consider the following scores. First, the grammaticality score INLINEFORM0 is the answer to Qa. By inverting (changing 1 to 3 and 3 to 1) the answer for Qb, we obtain a Non-Addition score indicating to which extent no additional information has been added. Similarly, inverting the answer to Qc yields the Non-Removal score. Averaging these two scores, we obtain the meaning preservation score INLINEFORM1 . Finally, the structural simplicity score INLINEFORM2 is the answer to Qd. Each of these scores is averaged over the five annotators. We further compute an average human score: INLINEFORM3 \",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 97,\n",
       "     'end': 108,\n",
       "     'text': '𝒢',\n",
       "     'latex': '\\\\mathcal {G}',\n",
       "     'ref_id': None},\n",
       "    {'start': 428,\n",
       "     'end': 439,\n",
       "     'text': '𝒫',\n",
       "     'latex': '\\\\mathcal {P}',\n",
       "     'ref_id': None},\n",
       "    {'start': 483,\n",
       "     'end': 494,\n",
       "     'text': '𝒮',\n",
       "     'latex': '\\\\mathcal {S}',\n",
       "     'ref_id': None},\n",
       "    {'start': 618,\n",
       "     'end': 629,\n",
       "     'text': ' AvgHuman =1 3(𝒢+𝒫+𝒮)',\n",
       "     'latex': '{\\\\rm AvgHuman} = \\\\frac{1}{3} (\\\\mathcal {G} + \\\\mathcal {P} + \\\\mathcal {S})',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Human Score Computation'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Human Score Computation'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Human Score Computation'},\n",
       "  {'text': \"Inter-annotator agreement rates are computed in two ways. Table TABREF23 presents the absolute agreement and Cohen's quadratic weighted INLINEFORM0 BIBREF41 . Table TABREF24 presents Spearman's correlation ( INLINEFORM1 ) between the human ratings of the input-output pairs (top row), and between the resulting system scores (bottom row). In both cases, the agreement between the five annotators is computed as the average agreement over the 10 annotator pairs.\",\n",
       "   'cite_spans': [{'start': 148,\n",
       "     'end': 156,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF41'}],\n",
       "   'ref_spans': [{'start': 64,\n",
       "     'end': 72,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF23'},\n",
       "    {'start': 165,\n",
       "     'end': 173,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF24'}],\n",
       "   'eq_spans': [{'start': 136,\n",
       "     'end': 147,\n",
       "     'text': 'κ',\n",
       "     'latex': '\\\\kappa ',\n",
       "     'ref_id': None},\n",
       "    {'start': 208,\n",
       "     'end': 219,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Inter-annotator Agreement'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Inter-annotator Agreement'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Inter-annotator Agreement'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Inter-annotator Agreement'},\n",
       "  {'text': 'We further compute SAMSA for the 100 sentences of the PWKP test set and the corresponding system outputs. Experiments are conducted in two settings: (1) a semi-automatic setting where UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software BIBREF42 , and according to the standard annotation guidelines; (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser BIBREF40 . Sentence segmentation of the outputs was carried out using the NLTK package BIBREF43 . For word alignments, we used the aligner of BIBREF44 .',\n",
       "   'cite_spans': [{'start': 297,\n",
       "     'end': 305,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF42'},\n",
       "    {'start': 447,\n",
       "     'end': 455,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF40'},\n",
       "    {'start': 534,\n",
       "     'end': 542,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF43'},\n",
       "    {'start': 589,\n",
       "     'end': 597,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF44'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Experimental Setup'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Experimental Setup'},\n",
       "  {'text': 'We compare the system rankings obtained by SAMSA and by the four human parameters. We find that the two leading systems according to AvgHuman and SAMSA turn out to be the same: Split-Deletion and RevILP. This is the case both for the semi-automatic and the automatic implementations of the metric. A Spearman INLINEFORM0 correlation between the human and SAMSA scores (comparing their rankings) is presented in Table TABREF27 .',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 417,\n",
       "     'end': 425,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF27'}],\n",
       "   'eq_spans': [{'start': 309,\n",
       "     'end': 320,\n",
       "     'text': 'ρ',\n",
       "     'latex': '\\\\rho ',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Correlation with Human Evaluation'},\n",
       "  {'text': 'We compare SAMSA and SAMSA INLINEFORM0 to the reference-based measures SARI BIBREF10 and BLEU, as well as to the negative Levenshtein distance to the reference (-LD INLINEFORM1 ). We use the only available reference for this corpus, in accordance with the standard practice. SARI is a reference-based measure, based on n-gram overlap between the source, output and reference, and focuses on lexical (rather than structural) simplification. For completeness, we include the other two measures reported in BIBREF19 , which are measures of similarity to the input (i.e., they quantify the tendency of the systems to introduce changes to the input): the negative Levenshtein distances between the output and input compared to the original complex corpus (-LD INLINEFORM2 ), and the number of sentences split by each of the systems.',\n",
       "   'cite_spans': [{'start': 76,\n",
       "     'end': 84,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 504,\n",
       "     'end': 512,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF19'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 27,\n",
       "     'end': 38,\n",
       "     'text': ' abl ',\n",
       "     'latex': '_{abl}',\n",
       "     'ref_id': None},\n",
       "    {'start': 165,\n",
       "     'end': 176,\n",
       "     'text': '  SR  ',\n",
       "     'latex': '_{{\\\\rm SR}}',\n",
       "     'ref_id': None},\n",
       "    {'start': 755,\n",
       "     'end': 766,\n",
       "     'text': '  SC  ',\n",
       "     'latex': '_{{\\\\rm SC}}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Correlation with Human Evaluation'},\n",
       "  {'text': 'The highest correlation with AvgHuman and grammaticality is obtained by semi-automatic SAMSA (0.58 and 0.54), a high correlation especially in comparison to the inter-annotator agreement on AvgHuman (0.64, Table TABREF24 ). The automatic version obtains high correlation with human judgments in these settings, where for structural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although when restricted to multi-Scene sentences, the correlation for SAMSA (semi-automatic) is higher (0.89, INLINEFORM0 and 0.77, INLINEFORM1 ).',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [{'start': 212,\n",
       "     'end': 220,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'TABREF24'}],\n",
       "   'eq_spans': [{'start': 690,\n",
       "     'end': 701,\n",
       "     'text': 'p=0.009',\n",
       "     'latex': 'p=0.009',\n",
       "     'ref_id': None},\n",
       "    {'start': 712,\n",
       "     'end': 723,\n",
       "     'text': 'p=0.04',\n",
       "     'latex': 'p=0.04',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Correlation with Human Evaluation'},\n",
       "  {'text': 'The highest correlation for meaning preservation is obtained by SAMSA INLINEFORM0 which provides further evidence that the retainment of semantic structures is a strong predictor of meaning preservation BIBREF14 . SAMSA in itself does not correlate with meaning preservation, probably due to its penalization of under-splitting sentences.',\n",
       "   'cite_spans': [{'start': 203,\n",
       "     'end': 211,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF14'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [{'start': 70,\n",
       "     'end': 81,\n",
       "     'text': ' abl ',\n",
       "     'latex': '_{abl}',\n",
       "     'ref_id': None}],\n",
       "   'section': 'Correlation with Human Evaluation'},\n",
       "  {'text': 'Note that the standard reference-based measures for simplification, BLEU and SARI, obtain low and often negative correlation with human ratings. We believe that this is the case because SARI and BLEU admittedly focus on lexical simplification, and are difficult to use to rank systems which also perform structural simplification.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Correlation with Human Evaluation'},\n",
       "  {'text': 'Our results thus suggest that SAMSA provides additional value in predicting the quality of a simplification system and should be reported in tandem with more lexically-oriented measures.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Correlation with Human Evaluation'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Correlation with Human Evaluation'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Discussion'},\n",
       "  {'text': \" In order to provide further validation for SAMSA predictive value for quality of simplification systems, we report SAMSA's correlation with a recently proposed benchmark, used for the QATS (Quality Assessment for Text Simplification) shared task BIBREF18 .\",\n",
       "   'cite_spans': [{'start': 247,\n",
       "     'end': 255,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF18'}],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation on the QATS Benchmark'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Evaluation on the QATS Benchmark'},\n",
       "  {'text': ' We presented the first structure-aware metric for text simplification, SAMSA, and the first evaluation experiments that directly target the structural simplification component, separately from the lexical component. We argue that the structural and lexical dimensions of simplification are loosely related, and that TS evaluation protocols should assess both. We empirically demonstrate that strong measures that assess lexical simplification quality (notably SARI), fail to correlate with human judgments when structural simplification is performed by the evaluated systems. Our experiments show that SAMSA correlates well with human judgments in such settings, which demonstrates its usefulness for evaluating and tuning statistical simplification systems, and shows that structural evaluation provides a complementary perspective on simplification quality.',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Conclusion'},\n",
       "  {'text': '',\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Conclusion'},\n",
       "  {'text': \" We would like to thank Zhemin Zhu and Sander Wubben for sharing their data, as well as the annotators for participating in our evaluation and UCCA annotation experiments. We also thank Daniel Hershcovich and the anonymous reviewers for their helpful comments. This work was partially supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and by the Israel Science Foundation (grant No. 929/17), as well as by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister's Office.\",\n",
       "   'cite_spans': [],\n",
       "   'ref_spans': [],\n",
       "   'eq_spans': [],\n",
       "   'section': 'Acknowledgments'}],\n",
       " 'ref_entries': {'FIGREF31': {'text': '1',\n",
       "   'caption': 'Joint distribution of the automatic SAMSA and the AvgHuman scores at the sentence level. Each point in the graph corresponds to a single source sentence. In addition to the scatter plot, a least-squares regression line is presented.',\n",
       "   'latex': 'SAMSA_AvgHuman',\n",
       "   'ref_id': 'FIGREF31',\n",
       "   'type': 'figure'},\n",
       "  'TABREF20': {'text': '1',\n",
       "   'caption': 'Questions for the human evaluation',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF20',\n",
       "   'type': 'table'},\n",
       "  'TABREF23': {'text': '2',\n",
       "   'caption': 'Inter-annotator absolute agreement (and quadratic weighted κ), averaged over the 10 annotator pairs. Rows correspond to systems, columns to questions. The top “Total” row refers to the concatenation of the outputs of all 6 systems together with the reference sentences.',\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF23',\n",
       "   'type': 'table'},\n",
       "  'TABREF24': {'text': '3',\n",
       "   'caption': \"Spearman's correlation (and p-values) of the system-level (top row) and sentence-level (bottom row) ratings of the five annotators. * p<10 -5 , ** p=0.002.\",\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF24',\n",
       "   'type': 'table'},\n",
       "  'TABREF27': {'text': '4',\n",
       "   'caption': \"Spearman's correlation of system scores i.e. Pearson's correlation of system rankings (and p-values), between evaluation measures (columns) and human judgments (rows). The ranking is between the six simplification systems experimented with. The left block of columns corresponds to the SAMSA and SAMSA abl measures, in their semi-automatic and automatic forms. The middle block of columns corresponds to the reference-based measures SARI and BLEU, as well as -LD SR , which is the negative Levenshtein distances of the system output from the reference. The right block corresponds to measures of conservatism, and reflect how well the tendency of the systems to introduce changes to the input correlates with the human rankings. The block includes -LD SC , the negative Levenshtein distance from the source sentence, and the number of input sentences split by each of the systems. Levenshtein distances are taken as negative in order to capture similarity between the output and source/reference. The measure with the highest correlation in each row is boldfaced.\",\n",
       "   'latex': [],\n",
       "   'ref_id': 'TABREF27',\n",
       "   'type': 'table'},\n",
       "  'SECREF1': {'text': 'Introduction',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF1',\n",
       "   'type': 'section'},\n",
       "  'SECREF2': {'text': 'Related Work',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF2',\n",
       "   'type': 'section'},\n",
       "  'SECREF3': {'text': \"UCCA's Semantic Structures\",\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF3',\n",
       "   'type': 'section'},\n",
       "  'SECREF4': {'text': 'The SAMSA Metric',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF4',\n",
       "   'type': 'section'},\n",
       "  'SECREF8': {'text': 'Discussion',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF8',\n",
       "   'type': 'section'},\n",
       "  'SECREF12': {'text': 'Score Computation',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF12',\n",
       "   'type': 'section'},\n",
       "  'SECREF5': {'text': 'Human Evaluation Benchmark',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF5',\n",
       "   'type': 'section'},\n",
       "  'SECREF16': {'text': 'Evaluation Protocol',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF16',\n",
       "   'type': 'section'},\n",
       "  'SECREF21': {'text': 'Human Score Computation',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF21',\n",
       "   'type': 'section'},\n",
       "  'SECREF22': {'text': 'Inter-annotator Agreement',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF22',\n",
       "   'type': 'section'},\n",
       "  'SECREF6': {'text': 'Experimental Setup',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF6',\n",
       "   'type': 'section'},\n",
       "  'SECREF7': {'text': 'Correlation with Human Evaluation',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF7',\n",
       "   'type': 'section'},\n",
       "  'SECREF9': {'text': 'Evaluation on the QATS Benchmark',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF9',\n",
       "   'type': 'section'},\n",
       "  'SECREF10': {'text': 'Conclusion',\n",
       "   'latex': None,\n",
       "   'ref_id': 'SECREF10',\n",
       "   'type': 'section'}},\n",
       " 'bib_entries': {'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "   'title': 'Syntactic simplification of text',\n",
       "   'authors': [{'first': 'Yvonne',\n",
       "     'middle': ['Margaret'],\n",
       "     'last': 'Canning',\n",
       "     'suffix': ''}],\n",
       "   'year': 2002,\n",
       "   'venue': '',\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '',\n",
       "   'other_ids': {},\n",
       "   'links': None},\n",
       "  'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "   'title': 'Motivations and methods for sentence simplification',\n",
       "   'authors': [{'first': 'Raman',\n",
       "     'middle': [],\n",
       "     'last': 'Chandrasekar',\n",
       "     'suffix': ''},\n",
       "    {'first': 'Christine', 'middle': [], 'last': 'Doran', 'suffix': ''},\n",
       "    {'first': 'Bangalore', 'middle': [], 'last': 'Srinivas', 'suffix': ''}],\n",
       "   'year': 1996,\n",
       "   'venue': \"Proc. of COLING'96\",\n",
       "   'volume': '',\n",
       "   'issn': '',\n",
       "   'pages': '1041--1044',\n",
       "   'other_ids': {},\n",
       "   'links': None}}}"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article['latex_parse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3868"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latex_nums  = [num_art  for num_art,article in enumerate(all_articles) if article['latex_parse'] and article['latex_parse']['body_text']]\n",
    "len(latex_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "[{'text': 'We thank Kyle Richardson, Vivek Srikumar and the anonymous reviewers for their constructive feedback. This work was completed in partial fulfillment for the PhD degree of the first author. Herzig was supported by a Google PhD fellowship. This research was partially supported by The Israel Science Foundation grant 942/16 and The Blavatnik Computer Science Research Fund.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgments'}]\n",
      "========================================\n",
      "Delete them\n",
      "70\n",
      "[{'text': ' ', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'other'}]\n",
      "========================================\n",
      "Delete them\n",
      "96\n",
      "[{'text': 'This work was partially supported by Berkeley AI Research, the NSF and DARPA XAI. DF is supported by a Tencent AI Lab Fellowship.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "160\n",
      "[{'text': 'We thank all the anonymous reviewers for their insightful comments on this paper. This work was partially supported by National Natural Science Foundation of China (61572049 and 61333018). The correspondence author of this paper is Sujian Li.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgments'}]\n",
      "========================================\n",
      "201\n",
      "[{'text': 'The authors would like to thank Kevin Clark for answering all of our questions regarding deep-coref. We would also like to thank the three anonymous reviewers for their thoughtful comments. This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany. The first author has been supported by a Heidelberg Institute for Theoretical Studies PhD. scholarship.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgments'}]\n",
      "========================================\n",
      "Delete them\n",
      "218\n",
      "[{'text': 'compat=1.14', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "225\n",
      "[{'text': 'The authors would like to thank the anonymous reviewers for their thoughtful comments. This research was sponsored in part by the Army Research Laboratory under cooperative agreements W911NF09-2-0053 and NSF IIS 1528175. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgment'}]\n",
      "========================================\n",
      "233\n",
      "[{'text': \"We would like to thank Charles Sutton, anonymous reviewers, and all members of Noah's ARK for helpful discussions and feedback. This work was made possible by a University of Washington Innovation award and computing resources provided by XSEDE.\", 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgments'}]\n",
      "========================================\n",
      "249\n",
      "[{'text': 'We thank Sara Patterson and Susan Mockus for guidance on precision oncology knowledge curation and CKB data, Hai Wang for help in running experiments with deep probabilistic logic, and Tristan Naumann, Rajesh Rao, Peng Qi, John Hewitt, and the anonymous reviewers for their helpful comments.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgements'}]\n",
      "========================================\n",
      "Delete them\n",
      "302\n",
      "[{'text': 'We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "Delete them\n",
      "321\n",
      "[{'text': 'same', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "Delete them\n",
      "339\n",
      "[{'text': 'Dealing with the co mplex word forms in morphologically rich languages is an open problem in language processing, and is particularly important in translation. In contrast to most modern neural systems of translation, which discard the identity for rare words, in this paper we propose several architectures for learning word representations from character and morpheme level word decompositions. We incorporate these representations in a novel machine translation model which jointly learns word alignments and translations via a hard attention mechanism. Evaluating on translating from several morphologically rich languages into English, we show consistent improvements over strong baseline methods, of between 1 and 1.5 BLEU points.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "347\n",
      "[{'text': 'This work was supported by the collaborative NSF Grant iis-1409287 (umd) and iis-1409739 (byu). Boyd-Graber is also supported by nsf grant iis-1822494 and iis-1748663. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgements'}]\n",
      "========================================\n",
      "355\n",
      "[{'text': 'Special thanks to Xiang Long for his help during the writing of this paper.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgements'}]\n",
      "========================================\n",
      "375\n",
      "[{'text': \"This paper introduces ZSDG, dealing with neural dialog systems' domain generalization ability. We formalize the ZSDG problem and propose an Action Matching framework that discovers cross-domain latent actions. We present a new simulated multi-domain dialog dataset, SimDial, to benchmark the ZSDG models. Our assessment validates the AM framework's effectiveness and the AM encoder decoders perform well in the ZSDG setting.\", 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusion and Future Work'}, {'text': 'ZSDG provides promising future research questions. How can we reduce the annotation cost of learning the latent alignment between actions in different domains? How can we create ZSDG for new domains where the discourse-level patterns are significantly different? What are other potential domain description formats? In summary, solving ZSDG is an important step for future general-purpose conversational agents.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusion and Future Work'}]\n",
      "========================================\n",
      "409\n",
      "[{'text': 'RF kindly acknowledges funding by the Netherlands Organisation for Scientific Research (NWO), under VIDI grant 276-89-008, Asymmetry in Conversation.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgements'}]\n",
      "========================================\n",
      "475\n",
      "[{'text': 'In this paper, we explained the importance of temporal considerations when working with language related to mental health conditions. We introduced RSDD-Time, a novel dataset of manually annotated self-reported depression diagnosis posts from Reddit. Our dataset includes extensive temporal information about the diagnosis, including when the diagnosis occurred, whether the condition is still current, and exact temporal spans. Using RSDD-Time, we applied rule-based and machine learning methods to automatically extract these temporal cues and predict temporal aspects of a diagnosis. While encouraging, the experiments and dataset allow much room for further exploration.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusion'}]\n",
      "========================================\n",
      "Delete them\n",
      "532\n",
      "[{'text': 'section§§§ section§§§', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Seen in Table TABREF2 is a list of hyperparameters for our deep entity resolution models. We use the same hyperparameters regardless of scenario and dataset. We initialize the 300 dimensional word embeddings by the character-based pretrained fastText vectors publicly available.', 'cite_spans': [], 'ref_spans': [{'start': 14, 'end': 21, 'text': None, 'latex': None, 'ref_id': 'TABREF2'}], 'eq_spans': [], 'section': 'Deep ER Hyperparameters'}, {'text': 'Magellan BIBREF1 is an open-source package that provides state-of-the-art learning-based algorithms for ER. We use the package to run the following 6 learning algorithms for baselines: Decision Tree, SVM, Random Forest, Naive Bayes, Logistic Regression, and Linear Regression. For each attribute in the schema, we apply the following similarity functions: q-gram jaccard, cosine distance, Levenshtein disntance, Levenshtein similairty, Monge-Elkan measure, and exact matching.', 'cite_spans': [{'start': 9, 'end': 16, 'text': None, 'latex': None, 'ref_id': 'BIBREF1'}], 'ref_spans': [], 'eq_spans': [], 'section': 'Non-DL Learning Algorithms'}]\n",
      "========================================\n",
      "Delete them\n",
      "566\n",
      "[{'text': ' This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 8cm', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}, {'text': 'Learning social media content is the basis of many real-world applications, including information retrieval and recommendation systems, among others. In contrast with previous works that focus mainly on single modal or bi-modal learning, we propose to learn social media content by fusing jointly textual, acoustic, and visual information (JTAV). Effective strategies are proposed to extract fine-grained features of each modality, that is, attBiGRU and DCRNN. We also introduce cross-modal fusion and attentive pooling techniques to integrate multi-modal information comprehensively. Extensive experimental evaluation conducted on real-world datasets demonstrates our proposed model outperforms the state-of-the-art approaches by a large margin.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "Delete them\n",
      "592\n",
      "[{'text': 'A growing number of state-of-the-art transfer learning methods employ language models pretrained on large generic corpora. In this paper we present a conceptually simple and effective transfer learning approach that addresses the problem of catastrophic forgetting. Specifically, we combine the task-specific optimization function with an auxiliary language model objective, which is adjusted during the training process. This preserves language regularities captured by language models, while enabling sufficient adaptation for solving the target task. Our method does not require pretraining or finetuning separate components of the network and we train our models end-to-end in a single step. We present results on a variety of challenging affective and text classification tasks, surpassing well established transfer learning methods with greater level of complexity.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "Delete them\n",
      "627\n",
      "[{'text': 'We introduce a new approach to tackle the problem of offensive language in online social media. Our approach uses unsupervised text style transfer to translate offensive sentences into non-offensive ones. We propose a new method for training encoder-decoders using non-parallel data that combines a collaborative classifier, attention and the cycle consistency loss. Experimental results on data from Twitter and Reddit show that our method outperforms a state-of-the-art text style transfer system in two out of three quantitative metrics and produces reliable non-offensive transferred sentences.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "Delete them\n",
      "651\n",
      "[{'text': 'This work is partially supported by USC Graduate Fellowship, NSF IIS-1065243, 1451412, 1513966/1632803/1833137, 1208500, CCF-1139148, a Google Research Award, an Alfred P. Sloan Research Fellowship, gifts from Facebook and Netflix, and ARO# W911NF-12-1-0241 and W911NF-15-1-0484.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "Delete them\n",
      "682\n",
      "[{'text': 'We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixed-length hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "Delete them\n",
      "713\n",
      "[{'text': 'This paper presents a new task, the grounding of spatio-temporal identifying descriptions in videos. Previous work suggests potential bias in existing datasets and emphasizes the need for a new data creation schema to better model linguistic structure. We introduce a new data collection scheme based on grammatical constraints for surface realization to enable us to investigate the problem of grounding spatio-temporal identifying descriptions in videos. We then propose a two-stream modular attention network that learns and grounds spatio-temporal identifying descriptions based on appearance and motion. We show that motion modules help to ground motion-related words and also help to learn in appearance modules because modular neural networks resolve task interference between modules. Finally, we propose a future challenge and a need for a robust system arising from replacing ground truth visual annotations with automatic video object detector and temporal event localization.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': None}]\n",
      "========================================\n",
      "725\n",
      "[{'text': 'The perceptron weights INLINEFORM0 from the INLINEFORM1 are visualized in the attached weights_full.png, and for INLINEFORM2 in weights_Py_only.png.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [{'start': 23, 'end': 34, 'text': '𝐯', 'latex': '\\\\mathbf {v}', 'ref_id': None}, {'start': 44, 'end': 55, 'text': 'φ(x,c)=[𝐡 1 𝐡 2 P(y)]', 'latex': '\\\\phi (x,c) = [\\\\mathbf {h}_1 \\\\mathbf {h}_2 P(y)]', 'ref_id': None}, {'start': 113, 'end': 124, 'text': 'φ(,x)=[P(y)]', 'latex': '\\\\phi (,x) = [P(y)]', 'ref_id': None}], 'section': 'Visualization of perceptron weights.'}]\n",
      "========================================\n",
      "811\n",
      "[{'text': 'The author appreciates the valuable feedback from the anonymous reviewers and would like to thank Massimo Poesio for sharing the ARRAU corpus.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgments'}]\n",
      "========================================\n",
      "945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': ' In this paper, we explored the task of describing the visual relationship between two images. We collected the Image Editing Request dataset, which contains image pairs and human annotated editing instructions. We designed novel relational speaker models and evaluate them on our collected and other public existing dataset. Based on automatic and human evaluations, our relational speaker model improves the ability to capture visual relationships. For future work, we are going to further explore the possibility to merge the three datasets by either learning a joint image representation or by transferring domain-specific knowledge. We are also aiming to enlarge our Image Editing Request dataset with newly-released posts on Reddit and Zhopped.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Conclusion'}, {'text': 'We thank the reviewers for their helpful comments and Nham Le for helping with the initial data collection. This work was supported by Adobe, ARO-YIP Award #W911NF-18-1-0336, and faculty awards from Google, Facebook, and Salesforce. The views, opinions, and/or findings contained in this article are those of the authors and should not be interpreted as representing the official views or policies, either expressed or implied, of the funding agency.', 'cite_spans': [], 'ref_spans': [], 'eq_spans': [], 'section': 'Acknowledgments'}]\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for num_l in latex_nums:\n",
    "    if num_l > 1000:\n",
    "        break\n",
    "    if len(all_articles[num_l]['latex_parse']['body_text']) <= 2:\n",
    "        if len(all_articles[num_l]['latex_parse']['body_text'][0]['text'].split()) < 1 or not all_articles[num_l]['latex_parse']['body_text'][0]['section']:\n",
    "            print('Delete them')\n",
    "            print(num_l)\n",
    "            print(all_articles[num_l]['latex_parse']['body_text'])\n",
    "            print(20*'==')\n",
    "        else:\n",
    "            print(num_l)\n",
    "            print(all_articles[num_l]['latex_parse']['body_text'])\n",
    "            print(20*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_features_paper_latex(all_articles[70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features_all_papers_latex(all_articles,verbose=False,with_empty=False):\n",
    "    section_num_citations = []\n",
    "    section_names = []\n",
    "    section_cite_dencite = []\n",
    "    section_position = []\n",
    "    section_av_cit_pos = []\n",
    "    article_paper_ids = []\n",
    "    for num_artic,article in enumerate(all_articles):\n",
    "        if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "            if len(article['latex_parse']['body_text']) <= 2:\n",
    "                if len(article['latex_parse']['body_text'][0]['text'].split()) < 1 :\n",
    "                    #or not article['latex_parse']['body_text'][0]['section']\n",
    "                    # не использую условие выше тк иначе уменьшится выборка (навряд ли в ней есть RW)\n",
    "                    if with_empty:\n",
    "                        article_paper_ids.append(article['paper_id'])\n",
    "                        section_names.append(keys)\n",
    "                        section_num_citations.append(-1)\n",
    "                        section_cite_dencite.append(-1)\n",
    "                        section_position.append(-1)\n",
    "                        section_av_cit_pos.append(-1)\n",
    "                    else:\n",
    "                        continue\n",
    "            print('---\\n{}'.format(num_artic))\n",
    "            \n",
    "            latex_parse_overview = make_latex_overview(article)\n",
    "            \n",
    "            len_sects = len(latex_parse_overview)\n",
    "            for num_key,keys in enumerate(latex_parse_overview.keys()):\n",
    "                article_paper_ids.append(article['paper_id'])\n",
    "                section_names.append(keys)\n",
    "                all_text = ''.join(latex_parse_overview[keys]['text'])\n",
    "\n",
    "                cnt_citations = sum(latex_parse_overview[keys]['cite_span_lens'])\n",
    "                sect_len = len(all_text)\n",
    "                sect_len_toks = len(all_text.split())\n",
    "                # если у нас мусор в тексте\n",
    "                if sect_len_toks == 0:\n",
    "                    cnt_citations = 0\n",
    "                    sect_len_toks = 1 \n",
    "                cite_spans_sec_start = upgrade_start_span_latex(latex_parse_overview[keys])\n",
    "                cite_spans_sec_start_mean = 0\n",
    "                if len(cite_spans_sec_start) >=1:\n",
    "                    cite_spans_sec_start_mean = np.mean(cite_spans_sec_start)/sect_len\n",
    "\n",
    "                sents_num_citations = sect_num_citations_latex(latex_parse_overview[keys])\n",
    "\n",
    "                section_num_citations.append(sents_num_citations)\n",
    "                section_cite_dencite.append(cnt_citations/sect_len_toks)\n",
    "                section_position.append((num_key+1)/len_sects)\n",
    "                section_av_cit_pos.append(cite_spans_sec_start_mean)\n",
    "                if verbose:\n",
    "                    print(20*'==')\n",
    "                    print('Features: \\nciting dencity {0}: #citations={1} & sect_len_toks={2}|  '.format(cnt_citations/sect_len_toks, cnt_citations, sect_len_toks))\n",
    "                    print('Number of citations in sentences: {0}'.format(sents_num_citations))\n",
    "                    print('Positon sect: {0} : num_sec = {1} len_sects={2}'.format((num_key+1)/len_sects,num_key+1,len_sects))\n",
    "                    print('Positon aver. citation: {0} {1} len= {2}'.format(cite_spans_sec_start_mean,cite_spans_sec_start,sect_len))\n",
    "                    print('Section name:',keys)\n",
    "        else:\n",
    "            if with_empty:\n",
    "                article_paper_ids.append(article['paper_id'])\n",
    "                section_names.append(keys)\n",
    "                section_num_citations.append(-1)\n",
    "                section_cite_dencite.append(-1)\n",
    "                section_position.append(-1)\n",
    "                section_av_cit_pos.append(-1)\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "    papers_features = {'paper_id':article_paper_ids,'sec_name':section_names ,'cite_dencity':section_cite_dencite,\n",
    "                       'num_cits':section_num_citations,'sec_pos':section_position,'sec_av_cit_pos':section_av_cit_pos}\n",
    "    return papers_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "0\n",
      "---\n",
      "6\n",
      "---\n",
      "37\n",
      "---\n",
      "38\n",
      "---\n",
      "53\n",
      "---\n",
      "71\n",
      "---\n",
      "72\n",
      "---\n",
      "76\n",
      "---\n",
      "96\n",
      "---\n",
      "99\n",
      "---\n",
      "100\n",
      "---\n",
      "104\n",
      "---\n",
      "109\n",
      "---\n",
      "123\n",
      "---\n",
      "134\n",
      "---\n",
      "135\n",
      "---\n",
      "155\n",
      "---\n",
      "160\n",
      "---\n",
      "163\n",
      "---\n",
      "179\n",
      "---\n",
      "201\n",
      "---\n",
      "218\n",
      "---\n",
      "223\n",
      "---\n",
      "225\n",
      "---\n",
      "233\n",
      "---\n",
      "239\n",
      "---\n",
      "249\n",
      "---\n",
      "253\n",
      "---\n",
      "257\n",
      "---\n",
      "267\n",
      "---\n",
      "273\n",
      "---\n",
      "291\n",
      "---\n",
      "302\n",
      "---\n",
      "304\n",
      "---\n",
      "308\n",
      "---\n",
      "309\n",
      "---\n",
      "317\n",
      "---\n",
      "321\n",
      "---\n",
      "329\n",
      "---\n",
      "331\n",
      "---\n",
      "335\n",
      "---\n",
      "337\n",
      "---\n",
      "339\n",
      "---\n",
      "346\n",
      "---\n",
      "347\n",
      "---\n",
      "348\n",
      "---\n",
      "349\n",
      "---\n",
      "355\n",
      "---\n",
      "373\n",
      "---\n",
      "375\n",
      "---\n",
      "399\n",
      "---\n",
      "404\n",
      "---\n",
      "406\n",
      "---\n",
      "409\n",
      "---\n",
      "454\n",
      "---\n",
      "458\n",
      "---\n",
      "461\n",
      "---\n",
      "463\n",
      "---\n",
      "475\n",
      "---\n",
      "480\n",
      "---\n",
      "485\n",
      "---\n",
      "496\n",
      "---\n",
      "504\n",
      "---\n",
      "520\n",
      "---\n",
      "528\n",
      "---\n",
      "529\n",
      "0 13\n",
      "---\n",
      "532\n",
      "---\n",
      "533\n",
      "---\n",
      "537\n",
      "---\n",
      "545\n",
      "---\n",
      "549\n",
      "---\n",
      "552\n",
      "---\n",
      "564\n",
      "---\n",
      "566\n",
      "---\n",
      "592\n",
      "---\n",
      "598\n",
      "---\n",
      "601\n",
      "---\n",
      "624\n",
      "---\n",
      "627\n",
      "---\n",
      "644\n",
      "---\n",
      "651\n",
      "---\n",
      "652\n",
      "---\n",
      "653\n",
      "---\n",
      "662\n",
      "---\n",
      "669\n",
      "---\n",
      "682\n",
      "---\n",
      "685\n",
      "---\n",
      "690\n",
      "---\n",
      "693\n",
      "---\n",
      "696\n",
      "---\n",
      "700\n",
      "---\n",
      "713\n",
      "---\n",
      "725\n",
      "---\n",
      "735\n",
      "---\n",
      "752\n",
      "---\n",
      "757\n",
      "---\n",
      "759\n",
      "---\n",
      "773\n",
      "---\n",
      "797\n",
      "---\n",
      "811\n",
      "---\n",
      "823\n",
      "---\n",
      "827\n",
      "---\n",
      "828\n",
      "---\n",
      "834\n",
      "---\n",
      "846\n",
      "---\n",
      "869\n",
      "---\n",
      "886\n",
      "---\n",
      "892\n",
      "---\n",
      "911\n",
      "---\n",
      "920\n",
      "---\n",
      "945\n",
      "---\n",
      "983\n",
      "---\n",
      "989\n",
      "---\n",
      "996\n",
      "---\n",
      "1007\n",
      "---\n",
      "1012\n",
      "---\n",
      "1031\n",
      "---\n",
      "1035\n",
      "---\n",
      "1045\n",
      "---\n",
      "1047\n",
      "---\n",
      "1051\n",
      "---\n",
      "1062\n",
      "---\n",
      "1063\n",
      "---\n",
      "1070\n",
      "---\n",
      "1092\n",
      "---\n",
      "1097\n",
      "---\n",
      "1109\n",
      "---\n",
      "1119\n",
      "0 13\n",
      "---\n",
      "1141\n",
      "---\n",
      "1147\n",
      "---\n",
      "1167\n",
      "---\n",
      "1172\n",
      "---\n",
      "1183\n",
      "---\n",
      "1190\n",
      "---\n",
      "1215\n",
      "---\n",
      "1222\n",
      "---\n",
      "1232\n",
      "---\n",
      "1235\n",
      "---\n",
      "1239\n",
      "---\n",
      "1271\n",
      "---\n",
      "1300\n",
      "---\n",
      "1302\n",
      "---\n",
      "1314\n",
      "---\n",
      "1323\n",
      "---\n",
      "1324\n",
      "---\n",
      "1335\n",
      "---\n",
      "1341\n",
      "---\n",
      "1351\n",
      "---\n",
      "1355\n",
      "---\n",
      "1368\n",
      "---\n",
      "1379\n",
      "---\n",
      "1385\n",
      "---\n",
      "1392\n",
      "---\n",
      "1397\n",
      "---\n",
      "1422\n",
      "---\n",
      "1426\n",
      "---\n",
      "1432\n",
      "---\n",
      "1446\n",
      "---\n",
      "1452\n",
      "---\n",
      "1456\n",
      "---\n",
      "1461\n",
      "---\n",
      "1463\n",
      "---\n",
      "1473\n",
      "---\n",
      "1481\n",
      "---\n",
      "1488\n",
      "---\n",
      "1506\n",
      "---\n",
      "1513\n",
      "---\n",
      "1516\n",
      "---\n",
      "1517\n",
      "---\n",
      "1523\n",
      "---\n",
      "1536\n",
      "---\n",
      "1546\n",
      "---\n",
      "1547\n",
      "---\n",
      "1556\n",
      "---\n",
      "1563\n",
      "---\n",
      "1596\n",
      "---\n",
      "1601\n",
      "---\n",
      "1628\n",
      "---\n",
      "1634\n",
      "---\n",
      "1635\n",
      "---\n",
      "1669\n",
      "---\n",
      "1680\n",
      "---\n",
      "1688\n",
      "---\n",
      "1694\n",
      "---\n",
      "1701\n",
      "---\n",
      "1705\n",
      "---\n",
      "1713\n",
      "---\n",
      "1734\n",
      "---\n",
      "1742\n",
      "---\n",
      "1743\n",
      "---\n",
      "1744\n",
      "---\n",
      "1758\n",
      "---\n",
      "1768\n",
      "---\n",
      "1809\n",
      "---\n",
      "1811\n",
      "---\n",
      "1818\n",
      "---\n",
      "1846\n",
      "---\n",
      "1856\n",
      "---\n",
      "1859\n",
      "---\n",
      "1866\n",
      "---\n",
      "1881\n",
      "---\n",
      "1889\n",
      "---\n",
      "1926\n",
      "---\n",
      "1962\n",
      "---\n",
      "1970\n",
      "---\n",
      "2000\n",
      "---\n",
      "2011\n",
      "---\n",
      "2027\n",
      "---\n",
      "2028\n",
      "---\n",
      "2051\n",
      "---\n",
      "2052\n",
      "---\n",
      "2062\n",
      "---\n",
      "2077\n",
      "---\n",
      "2082\n",
      "---\n",
      "2100\n",
      "---\n",
      "2103\n",
      "---\n",
      "2109\n",
      "---\n",
      "2125\n",
      "---\n",
      "2138\n",
      "---\n",
      "2151\n",
      "---\n",
      "2152\n",
      "---\n",
      "2157\n",
      "---\n",
      "2160\n",
      "---\n",
      "2166\n",
      "---\n",
      "2168\n",
      "---\n",
      "2170\n",
      "---\n",
      "2189\n",
      "---\n",
      "2227\n",
      "---\n",
      "2236\n",
      "---\n",
      "2255\n",
      "---\n",
      "2257\n",
      "---\n",
      "2263\n",
      "---\n",
      "2265\n",
      "---\n",
      "2267\n",
      "---\n",
      "2271\n",
      "---\n",
      "2272\n",
      "---\n",
      "2286\n",
      "---\n",
      "2287\n",
      "---\n",
      "2299\n",
      "---\n",
      "2339\n",
      "---\n",
      "2345\n",
      "---\n",
      "2351\n",
      "---\n",
      "2370\n",
      "---\n",
      "2384\n",
      "---\n",
      "2423\n",
      "---\n",
      "2430\n",
      "---\n",
      "2451\n",
      "---\n",
      "2459\n",
      "---\n",
      "2471\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "---\n",
      "2476\n",
      "---\n",
      "2480\n",
      "---\n",
      "2492\n",
      "---\n",
      "2495\n",
      "---\n",
      "2513\n",
      "---\n",
      "2520\n",
      "---\n",
      "2534\n",
      "---\n",
      "2536\n",
      "---\n",
      "2546\n",
      "---\n",
      "2548\n",
      "---\n",
      "2551\n",
      "---\n",
      "2553\n",
      "---\n",
      "2566\n",
      "---\n",
      "2570\n",
      "---\n",
      "2580\n",
      "---\n",
      "2586\n",
      "---\n",
      "2595\n",
      "---\n",
      "2610\n",
      "---\n",
      "2614\n",
      "---\n",
      "2630\n",
      "---\n",
      "2648\n",
      "---\n",
      "2666\n",
      "---\n",
      "2680\n",
      "---\n",
      "2691\n",
      "---\n",
      "2697\n",
      "---\n",
      "2722\n",
      "---\n",
      "2724\n",
      "---\n",
      "2728\n",
      "---\n",
      "2745\n",
      "---\n",
      "2754\n",
      "---\n",
      "2762\n",
      "---\n",
      "2786\n",
      "---\n",
      "2818\n",
      "---\n",
      "2826\n",
      "---\n",
      "2837\n",
      "---\n",
      "2839\n",
      "---\n",
      "2858\n",
      "---\n",
      "2863\n",
      "---\n",
      "2873\n",
      "---\n",
      "2875\n",
      "---\n",
      "2879\n",
      "---\n",
      "2891\n",
      "---\n",
      "2955\n",
      "---\n",
      "2967\n",
      "---\n",
      "2990\n",
      "---\n",
      "3000\n",
      "---\n",
      "3005\n",
      "---\n",
      "3006\n",
      "---\n",
      "3011\n",
      "---\n",
      "3012\n",
      "---\n",
      "3018\n",
      "---\n",
      "3019\n",
      "---\n",
      "3022\n",
      "---\n",
      "3032\n",
      "---\n",
      "3034\n",
      "---\n",
      "3045\n",
      "---\n",
      "3055\n",
      "---\n",
      "3063\n",
      "---\n",
      "3074\n",
      "---\n",
      "3077\n",
      "---\n",
      "3082\n",
      "---\n",
      "3087\n",
      "---\n",
      "3094\n",
      "---\n",
      "3095\n",
      "---\n",
      "3118\n",
      "---\n",
      "3120\n",
      "---\n",
      "3144\n",
      "---\n",
      "3163\n",
      "---\n",
      "3170\n",
      "---\n",
      "3179\n",
      "---\n",
      "3180\n",
      "---\n",
      "3187\n",
      "---\n",
      "3193\n",
      "---\n",
      "3201\n",
      "---\n",
      "3205\n",
      "---\n",
      "3217\n",
      "---\n",
      "3218\n",
      "---\n",
      "3220\n",
      "---\n",
      "3221\n",
      "---\n",
      "3222\n",
      "---\n",
      "3225\n",
      "---\n",
      "3230\n",
      "---\n",
      "3237\n",
      "---\n",
      "3250\n",
      "---\n",
      "3260\n",
      "---\n",
      "3264\n",
      "---\n",
      "3267\n",
      "---\n",
      "3278\n",
      "---\n",
      "3279\n",
      "---\n",
      "3296\n",
      "---\n",
      "3305\n",
      "---\n",
      "3317\n",
      "---\n",
      "3326\n",
      "---\n",
      "3336\n",
      "---\n",
      "3346\n",
      "---\n",
      "3355\n",
      "---\n",
      "3356\n",
      "---\n",
      "3357\n",
      "---\n",
      "3371\n",
      "---\n",
      "3399\n",
      "---\n",
      "3402\n",
      "---\n",
      "3414\n",
      "---\n",
      "3416\n",
      "---\n",
      "3422\n",
      "---\n",
      "3436\n",
      "---\n",
      "3440\n",
      "---\n",
      "3497\n",
      "---\n",
      "3513\n",
      "---\n",
      "3516\n",
      "---\n",
      "3518\n",
      "---\n",
      "3523\n",
      "---\n",
      "3543\n",
      "---\n",
      "3546\n",
      "---\n",
      "3556\n",
      "---\n",
      "3567\n",
      "---\n",
      "3586\n",
      "---\n",
      "3588\n",
      "---\n",
      "3595\n",
      "---\n",
      "3596\n",
      "---\n",
      "3597\n",
      "---\n",
      "3598\n",
      "---\n",
      "3611\n",
      "---\n",
      "3622\n",
      "---\n",
      "3627\n",
      "---\n",
      "3643\n",
      "---\n",
      "3645\n",
      "---\n",
      "3655\n",
      "---\n",
      "3656\n",
      "---\n",
      "3660\n",
      "---\n",
      "3678\n",
      "---\n",
      "3689\n",
      "---\n",
      "3700\n",
      "---\n",
      "3708\n",
      "---\n",
      "3735\n",
      "---\n",
      "3750\n",
      "---\n",
      "3765\n",
      "---\n",
      "3772\n",
      "---\n",
      "3780\n",
      "---\n",
      "3797\n",
      "---\n",
      "3800\n",
      "---\n",
      "3801\n",
      "---\n",
      "3805\n",
      "---\n",
      "3808\n",
      "---\n",
      "3814\n",
      "---\n",
      "3834\n",
      "---\n",
      "3844\n",
      "---\n",
      "3856\n",
      "---\n",
      "3858\n",
      "---\n",
      "3859\n",
      "---\n",
      "3861\n",
      "---\n",
      "3865\n",
      "---\n",
      "3866\n",
      "---\n",
      "3874\n",
      "---\n",
      "3882\n",
      "---\n",
      "3894\n",
      "---\n",
      "3899\n",
      "---\n",
      "3903\n",
      "---\n",
      "3916\n",
      "---\n",
      "3917\n",
      "---\n",
      "3954\n",
      "---\n",
      "3965\n",
      "---\n",
      "3971\n",
      "---\n",
      "3972\n",
      "---\n",
      "3976\n",
      "---\n",
      "3977\n",
      "---\n",
      "3979\n",
      "---\n",
      "3987\n",
      "---\n",
      "3989\n",
      "---\n",
      "3996\n",
      "---\n",
      "4006\n",
      "---\n",
      "4009\n",
      "---\n",
      "4011\n",
      "---\n",
      "4015\n",
      "---\n",
      "4030\n",
      "---\n",
      "4043\n",
      "---\n",
      "4065\n",
      "---\n",
      "4077\n",
      "0 13\n",
      "---\n",
      "4083\n",
      "---\n",
      "4084\n",
      "---\n",
      "4086\n",
      "---\n",
      "4091\n",
      "---\n",
      "4107\n",
      "---\n",
      "4108\n",
      "---\n",
      "4112\n",
      "---\n",
      "4114\n",
      "---\n",
      "4125\n",
      "---\n",
      "4129\n",
      "---\n",
      "4135\n",
      "---\n",
      "4137\n",
      "---\n",
      "4147\n",
      "---\n",
      "4163\n",
      "---\n",
      "4191\n",
      "---\n",
      "4194\n",
      "---\n",
      "4196\n",
      "---\n",
      "4199\n",
      "---\n",
      "4205\n",
      "---\n",
      "4212\n",
      "---\n",
      "4214\n",
      "---\n",
      "4216\n",
      "---\n",
      "4227\n",
      "---\n",
      "4246\n",
      "---\n",
      "4250\n",
      "---\n",
      "4252\n",
      "---\n",
      "4257\n",
      "---\n",
      "4264\n",
      "---\n",
      "4273\n",
      "---\n",
      "4275\n",
      "---\n",
      "4278\n",
      "---\n",
      "4290\n",
      "---\n",
      "4292\n",
      "---\n",
      "4294\n",
      "---\n",
      "4304\n",
      "---\n",
      "4308\n",
      "---\n",
      "4320\n",
      "---\n",
      "4342\n",
      "---\n",
      "4345\n",
      "---\n",
      "4351\n",
      "---\n",
      "4357\n",
      "---\n",
      "4370\n",
      "---\n",
      "4393\n",
      "---\n",
      "4395\n",
      "---\n",
      "4421\n",
      "---\n",
      "4425\n",
      "---\n",
      "4430\n",
      "---\n",
      "4443\n",
      "---\n",
      "4454\n",
      "---\n",
      "4458\n",
      "0 13\n",
      "---\n",
      "4459\n",
      "---\n",
      "4470\n",
      "---\n",
      "4471\n",
      "---\n",
      "4476\n",
      "---\n",
      "4479\n",
      "---\n",
      "4480\n",
      "---\n",
      "4489\n",
      "---\n",
      "4505\n",
      "---\n",
      "4513\n",
      "---\n",
      "4521\n",
      "---\n",
      "4526\n",
      "---\n",
      "4536\n",
      "---\n",
      "4537\n",
      "---\n",
      "4546\n",
      "---\n",
      "4554\n",
      "---\n",
      "4567\n",
      "---\n",
      "4580\n",
      "---\n",
      "4585\n",
      "---\n",
      "4588\n",
      "---\n",
      "4590\n",
      "---\n",
      "4592\n",
      "---\n",
      "4595\n",
      "---\n",
      "4596\n",
      "---\n",
      "4610\n",
      "---\n",
      "4628\n",
      "---\n",
      "4635\n",
      "---\n",
      "4636\n",
      "---\n",
      "4657\n",
      "---\n",
      "4695\n",
      "---\n",
      "4717\n",
      "---\n",
      "4755\n",
      "---\n",
      "4756\n",
      "---\n",
      "4759\n",
      "---\n",
      "4776\n",
      "---\n",
      "4790\n",
      "---\n",
      "4814\n",
      "---\n",
      "4817\n",
      "---\n",
      "4831\n",
      "---\n",
      "4840\n",
      "---\n",
      "4843\n",
      "---\n",
      "4848\n",
      "---\n",
      "4867\n",
      "---\n",
      "4871\n",
      "---\n",
      "4873\n",
      "---\n",
      "4881\n",
      "---\n",
      "4888\n",
      "---\n",
      "4893\n",
      "---\n",
      "4895\n",
      "---\n",
      "4900\n",
      "---\n",
      "4911\n",
      "---\n",
      "4913\n",
      "---\n",
      "4925\n",
      "---\n",
      "4935\n",
      "---\n",
      "4944\n",
      "---\n",
      "4958\n",
      "---\n",
      "4970\n",
      "---\n",
      "4994\n",
      "---\n",
      "4997\n",
      "---\n",
      "5000\n",
      "---\n",
      "5002\n",
      "---\n",
      "5011\n",
      "---\n",
      "5017\n",
      "---\n",
      "5035\n",
      "---\n",
      "5037\n",
      "---\n",
      "5043\n",
      "---\n",
      "5044\n",
      "---\n",
      "5066\n",
      "---\n",
      "5067\n",
      "---\n",
      "5068\n",
      "---\n",
      "5070\n",
      "---\n",
      "5071\n",
      "---\n",
      "5083\n",
      "---\n",
      "5097\n",
      "---\n",
      "5124\n",
      "---\n",
      "5137\n",
      "---\n",
      "5147\n",
      "---\n",
      "5150\n",
      "---\n",
      "5162\n",
      "---\n",
      "5164\n",
      "---\n",
      "5167\n",
      "---\n",
      "5173\n",
      "---\n",
      "5174\n",
      "---\n",
      "5176\n",
      "---\n",
      "5182\n",
      "---\n",
      "5187\n",
      "---\n",
      "5188\n",
      "---\n",
      "5265\n",
      "---\n",
      "5273\n",
      "---\n",
      "5274\n",
      "---\n",
      "5276\n",
      "---\n",
      "5279\n",
      "---\n",
      "5282\n",
      "---\n",
      "5293\n",
      "---\n",
      "5295\n",
      "---\n",
      "5300\n",
      "---\n",
      "5303\n",
      "---\n",
      "5319\n",
      "---\n",
      "5323\n",
      "---\n",
      "5330\n",
      "---\n",
      "5338\n",
      "---\n",
      "5339\n",
      "---\n",
      "5346\n",
      "---\n",
      "5364\n",
      "---\n",
      "5367\n",
      "---\n",
      "5368\n",
      "---\n",
      "5369\n",
      "---\n",
      "5387\n",
      "---\n",
      "5403\n",
      "---\n",
      "5405\n",
      "---\n",
      "5414\n",
      "---\n",
      "5418\n",
      "---\n",
      "5438\n",
      "---\n",
      "5447\n",
      "---\n",
      "5459\n",
      "---\n",
      "5468\n",
      "---\n",
      "5472\n",
      "---\n",
      "5474\n",
      "---\n",
      "5476\n",
      "---\n",
      "5483\n",
      "---\n",
      "5486\n",
      "---\n",
      "5507\n",
      "---\n",
      "5512\n",
      "---\n",
      "5536\n",
      "---\n",
      "5541\n",
      "---\n",
      "5543\n",
      "---\n",
      "5556\n",
      "---\n",
      "5559\n",
      "---\n",
      "5561\n",
      "---\n",
      "5570\n",
      "---\n",
      "5576\n",
      "---\n",
      "5584\n",
      "---\n",
      "5586\n",
      "---\n",
      "5587\n",
      "---\n",
      "5611\n",
      "---\n",
      "5614\n",
      "---\n",
      "5643\n",
      "---\n",
      "5653\n",
      "---\n",
      "5658\n",
      "---\n",
      "5665\n",
      "---\n",
      "5684\n",
      "---\n",
      "5687\n",
      "---\n",
      "5689\n",
      "---\n",
      "5691\n",
      "---\n",
      "5692\n",
      "---\n",
      "5693\n",
      "---\n",
      "5714\n",
      "---\n",
      "5722\n",
      "---\n",
      "5726\n",
      "---\n",
      "5763\n",
      "---\n",
      "5769\n",
      "---\n",
      "5780\n",
      "---\n",
      "5790\n",
      "---\n",
      "5814\n",
      "---\n",
      "5815\n",
      "---\n",
      "5825\n",
      "---\n",
      "5841\n",
      "---\n",
      "5850\n",
      "---\n",
      "5860\n",
      "---\n",
      "5897\n",
      "---\n",
      "5905\n",
      "---\n",
      "5912\n",
      "---\n",
      "5916\n",
      "---\n",
      "5924\n",
      "---\n",
      "5945\n",
      "---\n",
      "5947\n",
      "---\n",
      "5984\n",
      "---\n",
      "5987\n",
      "---\n",
      "6028\n",
      "---\n",
      "6063\n",
      "---\n",
      "6075\n",
      "---\n",
      "6104\n",
      "---\n",
      "6111\n",
      "---\n",
      "6113\n",
      "---\n",
      "6115\n",
      "---\n",
      "6119\n",
      "---\n",
      "6120\n",
      "---\n",
      "6124\n",
      "---\n",
      "6127\n",
      "---\n",
      "6128\n",
      "---\n",
      "6129\n",
      "---\n",
      "6135\n",
      "---\n",
      "6162\n",
      "---\n",
      "6168\n",
      "---\n",
      "6175\n",
      "---\n",
      "6179\n",
      "---\n",
      "6182\n",
      "---\n",
      "6186\n",
      "---\n",
      "6195\n",
      "---\n",
      "6199\n",
      "---\n",
      "6201\n",
      "---\n",
      "6202\n",
      "---\n",
      "6218\n",
      "---\n",
      "6220\n",
      "---\n",
      "6245\n",
      "---\n",
      "6247\n",
      "---\n",
      "6288\n",
      "---\n",
      "6303\n",
      "---\n",
      "6304\n",
      "---\n",
      "6313\n",
      "---\n",
      "6314\n",
      "---\n",
      "6316\n",
      "---\n",
      "6318\n",
      "---\n",
      "6322\n",
      "---\n",
      "6326\n",
      "---\n",
      "6332\n",
      "---\n",
      "6342\n",
      "---\n",
      "6359\n",
      "---\n",
      "6366\n",
      "---\n",
      "6376\n",
      "---\n",
      "6377\n",
      "---\n",
      "6384\n",
      "---\n",
      "6392\n",
      "---\n",
      "6398\n",
      "---\n",
      "6408\n",
      "---\n",
      "6414\n",
      "---\n",
      "6424\n",
      "---\n",
      "6425\n",
      "---\n",
      "6456\n",
      "---\n",
      "6463\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "---\n",
      "6474\n",
      "---\n",
      "6508\n",
      "---\n",
      "6527\n",
      "---\n",
      "6529\n",
      "---\n",
      "6543\n",
      "---\n",
      "6562\n",
      "---\n",
      "6576\n",
      "---\n",
      "6591\n",
      "---\n",
      "6594\n",
      "---\n",
      "6600\n",
      "---\n",
      "6603\n",
      "---\n",
      "6625\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "---\n",
      "6647\n",
      "---\n",
      "6670\n",
      "---\n",
      "6674\n",
      "---\n",
      "6689\n",
      "---\n",
      "6695\n",
      "---\n",
      "6698\n",
      "---\n",
      "6700\n",
      "---\n",
      "6705\n",
      "---\n",
      "6713\n",
      "---\n",
      "6721\n",
      "---\n",
      "6724\n",
      "---\n",
      "6725\n",
      "---\n",
      "6750\n",
      "---\n",
      "6752\n",
      "---\n",
      "6756\n",
      "---\n",
      "6769\n",
      "---\n",
      "6778\n",
      "---\n",
      "6823\n",
      "---\n",
      "6827\n",
      "---\n",
      "6837\n",
      "---\n",
      "6847\n",
      "---\n",
      "6857\n",
      "---\n",
      "6893\n",
      "---\n",
      "6907\n",
      "---\n",
      "6908\n",
      "---\n",
      "6939\n",
      "---\n",
      "6995\n",
      "---\n",
      "7007\n",
      "---\n",
      "7008\n",
      "---\n",
      "7014\n",
      "---\n",
      "7016\n",
      "---\n",
      "7023\n",
      "---\n",
      "7025\n",
      "---\n",
      "7028\n",
      "---\n",
      "7033\n",
      "---\n",
      "7065\n",
      "---\n",
      "7081\n",
      "---\n",
      "7096\n",
      "---\n",
      "7100\n",
      "---\n",
      "7103\n",
      "---\n",
      "7105\n",
      "---\n",
      "7106\n",
      "---\n",
      "7116\n",
      "---\n",
      "7120\n",
      "---\n",
      "7157\n",
      "---\n",
      "7168\n",
      "---\n",
      "7171\n",
      "---\n",
      "7174\n",
      "---\n",
      "7178\n",
      "---\n",
      "7182\n",
      "---\n",
      "7188\n",
      "---\n",
      "7212\n",
      "---\n",
      "7233\n",
      "---\n",
      "7235\n",
      "---\n",
      "7243\n",
      "0 13\n",
      "---\n",
      "7245\n",
      "---\n",
      "7246\n",
      "---\n",
      "7256\n",
      "---\n",
      "7258\n",
      "---\n",
      "7266\n",
      "---\n",
      "7282\n",
      "---\n",
      "7286\n",
      "---\n",
      "7298\n",
      "---\n",
      "7313\n",
      "---\n",
      "7321\n",
      "---\n",
      "7329\n",
      "---\n",
      "7334\n",
      "---\n",
      "7335\n",
      "---\n",
      "7339\n",
      "---\n",
      "7352\n",
      "---\n",
      "7398\n",
      "---\n",
      "7438\n",
      "---\n",
      "7445\n",
      "---\n",
      "7452\n",
      "---\n",
      "7462\n",
      "---\n",
      "7467\n",
      "---\n",
      "7475\n",
      "---\n",
      "7487\n",
      "---\n",
      "7546\n",
      "---\n",
      "7550\n",
      "---\n",
      "7564\n",
      "---\n",
      "7596\n",
      "---\n",
      "7599\n",
      "---\n",
      "7600\n",
      "---\n",
      "7609\n",
      "---\n",
      "7628\n",
      "---\n",
      "7661\n",
      "---\n",
      "7662\n",
      "---\n",
      "7670\n",
      "---\n",
      "7691\n",
      "---\n",
      "7710\n",
      "---\n",
      "7713\n",
      "---\n",
      "7717\n",
      "---\n",
      "7725\n",
      "---\n",
      "7745\n",
      "---\n",
      "7746\n",
      "---\n",
      "7747\n",
      "---\n",
      "7761\n",
      "---\n",
      "7790\n",
      "---\n",
      "7801\n",
      "---\n",
      "7803\n",
      "---\n",
      "7809\n",
      "---\n",
      "7818\n",
      "---\n",
      "7854\n",
      "---\n",
      "7855\n",
      "---\n",
      "7863\n",
      "---\n",
      "7881\n",
      "---\n",
      "7885\n",
      "---\n",
      "7888\n",
      "---\n",
      "7898\n",
      "---\n",
      "7903\n",
      "---\n",
      "7912\n",
      "---\n",
      "7939\n",
      "---\n",
      "7941\n",
      "---\n",
      "7948\n",
      "---\n",
      "7958\n",
      "---\n",
      "7961\n",
      "---\n",
      "7974\n",
      "---\n",
      "7983\n",
      "---\n",
      "8000\n",
      "---\n",
      "8017\n",
      "---\n",
      "8024\n",
      "---\n",
      "8025\n",
      "---\n",
      "8028\n",
      "---\n",
      "8031\n",
      "---\n",
      "8042\n",
      "---\n",
      "8060\n",
      "---\n",
      "8065\n",
      "---\n",
      "8090\n",
      "---\n",
      "8092\n",
      "---\n",
      "8132\n",
      "0 13\n",
      "---\n",
      "8154\n",
      "---\n",
      "8156\n",
      "---\n",
      "8168\n",
      "---\n",
      "8174\n",
      "---\n",
      "8185\n",
      "---\n",
      "8193\n",
      "---\n",
      "8207\n",
      "---\n",
      "8209\n",
      "---\n",
      "8214\n",
      "---\n",
      "8217\n",
      "---\n",
      "8246\n",
      "---\n",
      "8273\n",
      "---\n",
      "8274\n",
      "---\n",
      "8284\n",
      "---\n",
      "8286\n",
      "---\n",
      "8288\n",
      "---\n",
      "8303\n",
      "---\n",
      "8306\n",
      "---\n",
      "8318\n",
      "---\n",
      "8321\n",
      "---\n",
      "8359\n",
      "---\n",
      "8366\n",
      "---\n",
      "8367\n",
      "---\n",
      "8372\n",
      "---\n",
      "8375\n",
      "---\n",
      "8385\n",
      "---\n",
      "8391\n",
      "---\n",
      "8449\n",
      "---\n",
      "8496\n",
      "---\n",
      "8508\n",
      "---\n",
      "8518\n",
      "---\n",
      "8530\n",
      "---\n",
      "8540\n",
      "---\n",
      "8568\n",
      "---\n",
      "8570\n",
      "---\n",
      "8583\n",
      "---\n",
      "8586\n",
      "---\n",
      "8616\n",
      "---\n",
      "8618\n",
      "---\n",
      "8622\n",
      "---\n",
      "8646\n",
      "---\n",
      "8656\n",
      "---\n",
      "8685\n",
      "---\n",
      "8704\n",
      "---\n",
      "8708\n",
      "---\n",
      "8722\n",
      "---\n",
      "8726\n",
      "---\n",
      "8737\n",
      "---\n",
      "8748\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8751\n",
      "---\n",
      "8752\n",
      "---\n",
      "8755\n",
      "---\n",
      "8757\n",
      "---\n",
      "8758\n",
      "---\n",
      "8766\n",
      "---\n",
      "8777\n",
      "---\n",
      "8792\n",
      "---\n",
      "8794\n",
      "---\n",
      "8802\n",
      "---\n",
      "8814\n",
      "---\n",
      "8815\n",
      "---\n",
      "8836\n",
      "---\n",
      "8837\n",
      "---\n",
      "8841\n",
      "---\n",
      "8842\n",
      "---\n",
      "8856\n",
      "---\n",
      "8871\n",
      "---\n",
      "8874\n",
      "---\n",
      "8878\n",
      "---\n",
      "8879\n",
      "---\n",
      "8880\n",
      "---\n",
      "8885\n",
      "---\n",
      "8895\n",
      "---\n",
      "8896\n",
      "---\n",
      "8898\n",
      "---\n",
      "8913\n",
      "---\n",
      "8925\n",
      "---\n",
      "8944\n",
      "---\n",
      "8955\n",
      "---\n",
      "8957\n",
      "---\n",
      "8987\n",
      "---\n",
      "9003\n",
      "---\n",
      "9004\n",
      "---\n",
      "9014\n",
      "---\n",
      "9025\n",
      "---\n",
      "9041\n",
      "---\n",
      "9049\n",
      "---\n",
      "9059\n",
      "---\n",
      "9067\n",
      "---\n",
      "9070\n",
      "---\n",
      "9079\n",
      "---\n",
      "9110\n",
      "---\n",
      "9150\n",
      "---\n",
      "9157\n",
      "---\n",
      "9180\n",
      "---\n",
      "9183\n",
      "---\n",
      "9212\n",
      "---\n",
      "9217\n",
      "---\n",
      "9234\n",
      "---\n",
      "9238\n",
      "---\n",
      "9251\n",
      "---\n",
      "9299\n",
      "---\n",
      "9306\n",
      "---\n",
      "9313\n",
      "---\n",
      "9321\n",
      "---\n",
      "9326\n",
      "---\n",
      "9329\n",
      "---\n",
      "9331\n",
      "---\n",
      "9338\n",
      "---\n",
      "9357\n",
      "---\n",
      "9435\n",
      "---\n",
      "9442\n",
      "---\n",
      "9460\n",
      "---\n",
      "9478\n",
      "---\n",
      "9489\n",
      "---\n",
      "9501\n",
      "---\n",
      "9504\n",
      "---\n",
      "9506\n",
      "---\n",
      "9511\n",
      "---\n",
      "9518\n",
      "---\n",
      "9536\n",
      "---\n",
      "9543\n",
      "---\n",
      "9562\n",
      "---\n",
      "9563\n",
      "---\n",
      "9571\n",
      "---\n",
      "9572\n",
      "---\n",
      "9578\n",
      "---\n",
      "9609\n",
      "---\n",
      "9649\n",
      "---\n",
      "9657\n",
      "---\n",
      "9658\n",
      "---\n",
      "9683\n",
      "---\n",
      "9690\n",
      "---\n",
      "9706\n",
      "---\n",
      "9707\n",
      "---\n",
      "9714\n",
      "---\n",
      "9722\n",
      "---\n",
      "9727\n",
      "---\n",
      "9739\n",
      "---\n",
      "9741\n",
      "---\n",
      "9742\n",
      "---\n",
      "9770\n",
      "---\n",
      "9782\n",
      "---\n",
      "9839\n",
      "---\n",
      "9845\n",
      "---\n",
      "9847\n",
      "---\n",
      "9851\n",
      "---\n",
      "9855\n",
      "---\n",
      "9887\n",
      "---\n",
      "9897\n",
      "---\n",
      "9903\n",
      "---\n",
      "9915\n",
      "---\n",
      "9943\n",
      "---\n",
      "9966\n",
      "---\n",
      "9969\n",
      "---\n",
      "9988\n",
      "---\n",
      "10006\n",
      "---\n",
      "10024\n",
      "---\n",
      "10025\n",
      "---\n",
      "10030\n",
      "---\n",
      "10033\n",
      "---\n",
      "10043\n",
      "---\n",
      "10049\n",
      "---\n",
      "10056\n",
      "---\n",
      "10072\n",
      "---\n",
      "10080\n",
      "---\n",
      "10116\n",
      "---\n",
      "10120\n",
      "---\n",
      "10128\n",
      "---\n",
      "10141\n",
      "---\n",
      "10154\n",
      "---\n",
      "10155\n",
      "---\n",
      "10160\n",
      "---\n",
      "10163\n",
      "---\n",
      "10167\n",
      "---\n",
      "10171\n",
      "---\n",
      "10192\n",
      "---\n",
      "10206\n",
      "---\n",
      "10227\n",
      "---\n",
      "10238\n",
      "---\n",
      "10239\n",
      "---\n",
      "10242\n",
      "---\n",
      "10244\n",
      "---\n",
      "10249\n",
      "---\n",
      "10262\n",
      "---\n",
      "10272\n",
      "---\n",
      "10280\n",
      "---\n",
      "10306\n",
      "---\n",
      "10316\n",
      "---\n",
      "10319\n",
      "---\n",
      "10322\n",
      "---\n",
      "10327\n",
      "---\n",
      "10336\n",
      "---\n",
      "10353\n",
      "---\n",
      "10360\n",
      "---\n",
      "10364\n",
      "---\n",
      "10372\n",
      "---\n",
      "10374\n",
      "---\n",
      "10383\n",
      "---\n",
      "10384\n",
      "---\n",
      "10394\n",
      "---\n",
      "10399\n",
      "---\n",
      "10419\n",
      "---\n",
      "10422\n",
      "---\n",
      "10424\n",
      "---\n",
      "10432\n",
      "---\n",
      "10434\n",
      "---\n",
      "10435\n",
      "---\n",
      "10436\n",
      "---\n",
      "10445\n",
      "---\n",
      "10454\n",
      "---\n",
      "10459\n",
      "---\n",
      "10499\n",
      "---\n",
      "10504\n",
      "---\n",
      "10516\n",
      "---\n",
      "10517\n",
      "---\n",
      "10522\n",
      "---\n",
      "10527\n",
      "---\n",
      "10542\n",
      "---\n",
      "10547\n",
      "---\n",
      "10559\n",
      "---\n",
      "10561\n",
      "---\n",
      "10602\n",
      "---\n",
      "10608\n",
      "---\n",
      "10622\n",
      "---\n",
      "10627\n",
      "---\n",
      "10638\n",
      "---\n",
      "10640\n",
      "---\n",
      "10641\n",
      "---\n",
      "10644\n",
      "---\n",
      "10646\n",
      "---\n",
      "10657\n",
      "---\n",
      "10687\n",
      "---\n",
      "10689\n",
      "---\n",
      "10692\n",
      "---\n",
      "10696\n",
      "---\n",
      "10717\n",
      "---\n",
      "10719\n",
      "---\n",
      "10721\n",
      "---\n",
      "10726\n",
      "---\n",
      "10733\n",
      "---\n",
      "10758\n",
      "---\n",
      "10785\n",
      "---\n",
      "10796\n",
      "---\n",
      "10801\n",
      "---\n",
      "10808\n",
      "---\n",
      "10822\n",
      "---\n",
      "10827\n",
      "---\n",
      "10838\n",
      "---\n",
      "10841\n",
      "---\n",
      "10853\n",
      "---\n",
      "10860\n",
      "---\n",
      "10862\n",
      "---\n",
      "10871\n",
      "---\n",
      "10883\n",
      "---\n",
      "10889\n",
      "---\n",
      "10890\n",
      "---\n",
      "10893\n",
      "---\n",
      "10895\n",
      "---\n",
      "10897\n",
      "---\n",
      "10905\n",
      "---\n",
      "10925\n",
      "---\n",
      "10942\n",
      "---\n",
      "10994\n",
      "---\n",
      "11000\n",
      "---\n",
      "11013\n",
      "---\n",
      "11034\n",
      "---\n",
      "11047\n",
      "---\n",
      "11061\n",
      "---\n",
      "11067\n",
      "---\n",
      "11088\n",
      "---\n",
      "11093\n",
      "---\n",
      "11096\n",
      "---\n",
      "11098\n",
      "---\n",
      "11107\n",
      "---\n",
      "11113\n",
      "---\n",
      "11133\n",
      "---\n",
      "11141\n",
      "---\n",
      "11142\n",
      "---\n",
      "11145\n",
      "---\n",
      "11154\n",
      "---\n",
      "11176\n",
      "---\n",
      "11177\n",
      "---\n",
      "11185\n",
      "---\n",
      "11187\n",
      "---\n",
      "11194\n",
      "---\n",
      "11200\n",
      "---\n",
      "11219\n",
      "---\n",
      "11223\n",
      "---\n",
      "11224\n",
      "---\n",
      "11230\n",
      "---\n",
      "11240\n",
      "---\n",
      "11257\n",
      "---\n",
      "11258\n",
      "---\n",
      "11267\n",
      "---\n",
      "11275\n",
      "---\n",
      "11277\n",
      "---\n",
      "11285\n",
      "---\n",
      "11287\n",
      "---\n",
      "11290\n",
      "---\n",
      "11304\n",
      "---\n",
      "11318\n",
      "---\n",
      "11319\n",
      "---\n",
      "11342\n",
      "---\n",
      "11365\n",
      "---\n",
      "11370\n",
      "---\n",
      "11371\n",
      "---\n",
      "11374\n",
      "---\n",
      "11383\n",
      "---\n",
      "11388\n",
      "---\n",
      "11393\n",
      "---\n",
      "11399\n",
      "---\n",
      "11400\n",
      "---\n",
      "11404\n",
      "---\n",
      "11423\n",
      "---\n",
      "11439\n",
      "---\n",
      "11444\n",
      "---\n",
      "11459\n",
      "---\n",
      "11473\n",
      "---\n",
      "11481\n",
      "---\n",
      "11484\n",
      "---\n",
      "11503\n",
      "---\n",
      "11518\n",
      "---\n",
      "11522\n",
      "---\n",
      "11524\n",
      "---\n",
      "11551\n",
      "---\n",
      "11559\n",
      "---\n",
      "11563\n",
      "---\n",
      "11568\n",
      "---\n",
      "11571\n",
      "---\n",
      "11613\n",
      "---\n",
      "11643\n",
      "---\n",
      "11658\n",
      "---\n",
      "11716\n",
      "---\n",
      "11724\n",
      "---\n",
      "11730\n",
      "0 13\n",
      "---\n",
      "11734\n",
      "---\n",
      "11753\n",
      "---\n",
      "11780\n",
      "---\n",
      "11799\n",
      "---\n",
      "11811\n",
      "---\n",
      "11815\n",
      "---\n",
      "11826\n",
      "---\n",
      "11878\n",
      "---\n",
      "11880\n",
      "---\n",
      "11884\n",
      "---\n",
      "11898\n",
      "---\n",
      "11923\n",
      "---\n",
      "11935\n",
      "---\n",
      "11938\n",
      "---\n",
      "11941\n",
      "---\n",
      "11951\n",
      "---\n",
      "11955\n",
      "0 13\n",
      "---\n",
      "11969\n",
      "---\n",
      "11974\n",
      "---\n",
      "12010\n",
      "---\n",
      "12029\n",
      "---\n",
      "12053\n",
      "---\n",
      "12058\n",
      "---\n",
      "12066\n",
      "---\n",
      "12072\n",
      "---\n",
      "12078\n",
      "---\n",
      "12079\n",
      "---\n",
      "12082\n",
      "---\n",
      "12086\n",
      "---\n",
      "12088\n",
      "---\n",
      "12102\n",
      "---\n",
      "12105\n",
      "---\n",
      "12106\n",
      "---\n",
      "12122\n",
      "---\n",
      "12139\n",
      "---\n",
      "12152\n",
      "---\n",
      "12153\n",
      "---\n",
      "12155\n",
      "---\n",
      "12159\n",
      "---\n",
      "12163\n",
      "---\n",
      "12181\n",
      "---\n",
      "12183\n",
      "---\n",
      "12189\n",
      "---\n",
      "12210\n",
      "---\n",
      "12212\n",
      "---\n",
      "12218\n",
      "---\n",
      "12226\n",
      "---\n",
      "12241\n",
      "---\n",
      "12259\n",
      "---\n",
      "12283\n",
      "---\n",
      "12291\n",
      "---\n",
      "12298\n",
      "---\n",
      "12319\n",
      "---\n",
      "12327\n",
      "---\n",
      "12341\n",
      "---\n",
      "12344\n",
      "---\n",
      "12355\n",
      "---\n",
      "12367\n",
      "---\n",
      "12371\n",
      "---\n",
      "12375\n",
      "---\n",
      "12378\n",
      "---\n",
      "12379\n",
      "---\n",
      "12424\n",
      "---\n",
      "12429\n",
      "---\n",
      "12433\n",
      "---\n",
      "12436\n",
      "---\n",
      "12437\n",
      "---\n",
      "12444\n",
      "---\n",
      "12445\n",
      "---\n",
      "12447\n",
      "---\n",
      "12462\n",
      "---\n",
      "12477\n",
      "---\n",
      "12479\n",
      "---\n",
      "12493\n",
      "---\n",
      "12494\n",
      "---\n",
      "12496\n",
      "---\n",
      "12497\n",
      "---\n",
      "12513\n",
      "---\n",
      "12517\n",
      "---\n",
      "12553\n",
      "---\n",
      "12560\n",
      "---\n",
      "12578\n",
      "---\n",
      "12584\n",
      "---\n",
      "12588\n",
      "---\n",
      "12592\n",
      "---\n",
      "12598\n",
      "---\n",
      "12599\n",
      "---\n",
      "12621\n",
      "---\n",
      "12622\n",
      "---\n",
      "12629\n",
      "---\n",
      "12642\n",
      "---\n",
      "12655\n",
      "---\n",
      "12670\n",
      "---\n",
      "12696\n",
      "---\n",
      "12698\n",
      "---\n",
      "12699\n",
      "---\n",
      "12706\n",
      "---\n",
      "12717\n",
      "---\n",
      "12734\n",
      "---\n",
      "12744\n",
      "---\n",
      "12753\n",
      "---\n",
      "12766\n",
      "---\n",
      "12774\n",
      "---\n",
      "12787\n",
      "---\n",
      "12801\n",
      "---\n",
      "12803\n",
      "---\n",
      "12835\n",
      "---\n",
      "12844\n",
      "---\n",
      "12847\n",
      "---\n",
      "12850\n",
      "---\n",
      "12878\n",
      "---\n",
      "12916\n",
      "0 13\n",
      "0 13\n",
      "---\n",
      "12923\n",
      "---\n",
      "12930\n",
      "---\n",
      "12931\n",
      "0 13\n",
      "---\n",
      "12947\n",
      "---\n",
      "12950\n",
      "---\n",
      "12955\n",
      "---\n",
      "12966\n",
      "---\n",
      "12968\n",
      "---\n",
      "12987\n",
      "---\n",
      "12993\n",
      "---\n",
      "12998\n",
      "---\n",
      "13003\n",
      "---\n",
      "13011\n",
      "---\n",
      "13012\n",
      "---\n",
      "13026\n",
      "---\n",
      "13034\n",
      "---\n",
      "13037\n",
      "---\n",
      "13064\n",
      "---\n",
      "13070\n",
      "---\n",
      "13072\n",
      "---\n",
      "13077\n",
      "---\n",
      "13084\n",
      "---\n",
      "13088\n",
      "---\n",
      "13112\n",
      "---\n",
      "13140\n",
      "---\n",
      "13142\n",
      "---\n",
      "13145\n",
      "---\n",
      "13146\n",
      "---\n",
      "13151\n",
      "---\n",
      "13183\n",
      "---\n",
      "13186\n",
      "---\n",
      "13198\n",
      "---\n",
      "13218\n",
      "---\n",
      "13219\n",
      "---\n",
      "13226\n",
      "---\n",
      "13228\n",
      "---\n",
      "13237\n",
      "---\n",
      "13239\n",
      "---\n",
      "13240\n",
      "---\n",
      "13241\n",
      "---\n",
      "13246\n",
      "---\n",
      "13247\n",
      "---\n",
      "13248\n",
      "---\n",
      "13249\n",
      "---\n",
      "13261\n",
      "---\n",
      "13272\n",
      "---\n",
      "13309\n",
      "---\n",
      "13317\n",
      "---\n",
      "13372\n",
      "---\n",
      "13375\n",
      "---\n",
      "13378\n",
      "---\n",
      "13381\n",
      "---\n",
      "13388\n",
      "---\n",
      "13393\n",
      "---\n",
      "13411\n",
      "---\n",
      "13441\n",
      "---\n",
      "13442\n",
      "---\n",
      "13447\n",
      "---\n",
      "13451\n",
      "---\n",
      "13471\n",
      "---\n",
      "13477\n",
      "---\n",
      "13482\n",
      "---\n",
      "13487\n",
      "---\n",
      "13489\n",
      "---\n",
      "13498\n",
      "---\n",
      "13501\n",
      "---\n",
      "13502\n",
      "---\n",
      "13509\n",
      "---\n",
      "13512\n",
      "---\n",
      "13516\n",
      "---\n",
      "13522\n",
      "---\n",
      "13554\n",
      "---\n",
      "13558\n",
      "---\n",
      "13572\n",
      "---\n",
      "13582\n",
      "---\n",
      "13585\n",
      "---\n",
      "13607\n",
      "---\n",
      "13618\n",
      "---\n",
      "13627\n",
      "---\n",
      "13629\n",
      "---\n",
      "13632\n",
      "---\n",
      "13643\n",
      "---\n",
      "13659\n",
      "---\n",
      "13666\n",
      "---\n",
      "13668\n",
      "---\n",
      "13686\n",
      "---\n",
      "13689\n",
      "---\n",
      "13690\n",
      "---\n",
      "13715\n",
      "---\n",
      "13728\n",
      "---\n",
      "13732\n",
      "---\n",
      "13735\n",
      "---\n",
      "13739\n",
      "---\n",
      "13754\n",
      "---\n",
      "13780\n",
      "0 13\n",
      "---\n",
      "13790\n",
      "---\n",
      "13793\n",
      "---\n",
      "13811\n",
      "---\n",
      "13852\n",
      "---\n",
      "13855\n",
      "---\n",
      "13876\n",
      "---\n",
      "13903\n",
      "---\n",
      "13904\n",
      "---\n",
      "13905\n",
      "---\n",
      "13932\n",
      "---\n",
      "13940\n",
      "---\n",
      "13942\n",
      "---\n",
      "13948\n",
      "---\n",
      "13949\n",
      "---\n",
      "13976\n",
      "---\n",
      "13977\n",
      "---\n",
      "14014\n",
      "---\n",
      "14017\n",
      "---\n",
      "14024\n",
      "---\n",
      "14035\n",
      "---\n",
      "14036\n",
      "---\n",
      "14049\n",
      "---\n",
      "14063\n",
      "---\n",
      "14064\n",
      "---\n",
      "14071\n",
      "---\n",
      "14092\n",
      "---\n",
      "14096\n",
      "---\n",
      "14103\n",
      "---\n",
      "14112\n",
      "---\n",
      "14151\n",
      "---\n",
      "14171\n",
      "---\n",
      "14193\n",
      "---\n",
      "14195\n",
      "---\n",
      "14197\n",
      "---\n",
      "14198\n",
      "---\n",
      "14237\n",
      "---\n",
      "14238\n",
      "---\n",
      "14240\n",
      "---\n",
      "14258\n",
      "---\n",
      "14264\n",
      "---\n",
      "14272\n",
      "---\n",
      "14281\n",
      "---\n",
      "14284\n",
      "---\n",
      "14310\n",
      "---\n",
      "14324\n",
      "---\n",
      "14325\n",
      "---\n",
      "14328\n",
      "---\n",
      "14332\n",
      "---\n",
      "14340\n",
      "---\n",
      "14356\n",
      "---\n",
      "14367\n",
      "---\n",
      "14372\n",
      "---\n",
      "14375\n",
      "---\n",
      "14380\n",
      "---\n",
      "14393\n",
      "---\n",
      "14396\n",
      "---\n",
      "14399\n",
      "---\n",
      "14401\n",
      "---\n",
      "14408\n",
      "---\n",
      "14416\n",
      "---\n",
      "14444\n",
      "---\n",
      "14464\n",
      "---\n",
      "14469\n",
      "---\n",
      "14482\n",
      "---\n",
      "14490\n",
      "---\n",
      "14549\n",
      "---\n",
      "14557\n",
      "---\n",
      "14559\n",
      "---\n",
      "14560\n",
      "---\n",
      "14561\n",
      "---\n",
      "14571\n",
      "---\n",
      "14602\n",
      "---\n",
      "14607\n",
      "---\n",
      "14619\n",
      "---\n",
      "14625\n",
      "---\n",
      "14636\n",
      "---\n",
      "14642\n",
      "---\n",
      "14654\n",
      "---\n",
      "14656\n",
      "---\n",
      "14689\n",
      "---\n",
      "14694\n",
      "---\n",
      "14695\n",
      "---\n",
      "14701\n",
      "---\n",
      "14703\n",
      "---\n",
      "14705\n",
      "---\n",
      "14707\n",
      "---\n",
      "14709\n",
      "---\n",
      "14716\n",
      "---\n",
      "14718\n",
      "---\n",
      "14725\n",
      "---\n",
      "14728\n",
      "---\n",
      "14742\n",
      "---\n",
      "14761\n",
      "---\n",
      "14776\n",
      "---\n",
      "14783\n",
      "---\n",
      "14792\n",
      "---\n",
      "14800\n",
      "---\n",
      "14825\n",
      "---\n",
      "14832\n",
      "---\n",
      "14833\n",
      "---\n",
      "14849\n",
      "---\n",
      "14851\n",
      "---\n",
      "14860\n",
      "---\n",
      "14864\n",
      "---\n",
      "14869\n",
      "---\n",
      "14900\n",
      "---\n",
      "14904\n",
      "---\n",
      "14914\n",
      "---\n",
      "14926\n",
      "---\n",
      "14929\n",
      "---\n",
      "14936\n",
      "---\n",
      "14937\n",
      "---\n",
      "14938\n",
      "---\n",
      "14941\n",
      "---\n",
      "14971\n",
      "---\n",
      "14975\n",
      "---\n",
      "14986\n",
      "---\n",
      "14989\n",
      "---\n",
      "14993\n",
      "---\n",
      "15014\n",
      "---\n",
      "15019\n",
      "---\n",
      "15028\n",
      "---\n",
      "15039\n",
      "---\n",
      "15044\n",
      "---\n",
      "15045\n",
      "---\n",
      "15056\n",
      "---\n",
      "15068\n",
      "---\n",
      "15083\n",
      "---\n",
      "15087\n",
      "---\n",
      "15088\n",
      "---\n",
      "15090\n",
      "---\n",
      "15103\n",
      "---\n",
      "15121\n",
      "---\n",
      "15124\n",
      "---\n",
      "15133\n",
      "---\n",
      "15137\n",
      "---\n",
      "15162\n",
      "---\n",
      "15164\n",
      "---\n",
      "15187\n",
      "---\n",
      "15192\n",
      "---\n",
      "15196\n",
      "---\n",
      "15203\n",
      "---\n",
      "15212\n",
      "---\n",
      "15214\n",
      "---\n",
      "15218\n",
      "---\n",
      "15225\n",
      "---\n",
      "15232\n",
      "---\n",
      "15238\n",
      "---\n",
      "15242\n",
      "---\n",
      "15265\n",
      "---\n",
      "15287\n",
      "0 13\n",
      "---\n",
      "15302\n",
      "---\n",
      "15313\n",
      "---\n",
      "15326\n",
      "---\n",
      "15332\n",
      "---\n",
      "15365\n",
      "---\n",
      "15378\n",
      "---\n",
      "15381\n",
      "---\n",
      "15397\n",
      "---\n",
      "15401\n",
      "---\n",
      "15402\n",
      "---\n",
      "15406\n",
      "---\n",
      "15407\n",
      "---\n",
      "15429\n",
      "---\n",
      "15439\n",
      "---\n",
      "15449\n",
      "---\n",
      "15475\n",
      "---\n",
      "15477\n",
      "---\n",
      "15483\n",
      "---\n",
      "15495\n",
      "---\n",
      "15499\n",
      "---\n",
      "15531\n",
      "---\n",
      "15546\n",
      "---\n",
      "15548\n",
      "---\n",
      "15575\n",
      "---\n",
      "15590\n",
      "---\n",
      "15599\n",
      "---\n",
      "15604\n",
      "---\n",
      "15606\n",
      "---\n",
      "15607\n",
      "---\n",
      "15617\n",
      "---\n",
      "15631\n",
      "---\n",
      "15642\n",
      "---\n",
      "15643\n",
      "---\n",
      "15666\n",
      "---\n",
      "15674\n",
      "---\n",
      "15690\n",
      "---\n",
      "15695\n",
      "---\n",
      "15699\n",
      "---\n",
      "15717\n",
      "---\n",
      "15747\n",
      "---\n",
      "15751\n",
      "---\n",
      "15753\n",
      "---\n",
      "15761\n",
      "---\n",
      "15764\n",
      "---\n",
      "15781\n",
      "---\n",
      "15787\n",
      "---\n",
      "15793\n",
      "---\n",
      "15820\n",
      "---\n",
      "15825\n",
      "---\n",
      "15841\n",
      "0 13\n",
      "---\n",
      "15844\n",
      "---\n",
      "15857\n",
      "---\n",
      "15903\n",
      "---\n",
      "15909\n",
      "---\n",
      "15910\n",
      "---\n",
      "15915\n",
      "---\n",
      "15923\n",
      "---\n",
      "15929\n",
      "---\n",
      "15932\n",
      "---\n",
      "15943\n",
      "---\n",
      "15974\n",
      "---\n",
      "15987\n",
      "---\n",
      "15993\n",
      "---\n",
      "15995\n",
      "---\n",
      "15996\n",
      "---\n",
      "16006\n",
      "---\n",
      "16016\n",
      "---\n",
      "16019\n",
      "---\n",
      "16021\n",
      "---\n",
      "16029\n",
      "---\n",
      "16031\n",
      "---\n",
      "16033\n",
      "---\n",
      "16035\n",
      "---\n",
      "16045\n",
      "---\n",
      "16054\n",
      "---\n",
      "16056\n",
      "---\n",
      "16091\n",
      "---\n",
      "16111\n",
      "---\n",
      "16114\n",
      "---\n",
      "16119\n",
      "---\n",
      "16164\n",
      "---\n",
      "16179\n",
      "---\n",
      "16207\n",
      "---\n",
      "16219\n",
      "---\n",
      "16230\n",
      "---\n",
      "16250\n",
      "---\n",
      "16256\n",
      "---\n",
      "16261\n",
      "---\n",
      "16278\n",
      "0 13\n",
      "---\n",
      "16281\n",
      "---\n",
      "16294\n",
      "---\n",
      "16323\n",
      "---\n",
      "16326\n",
      "---\n",
      "16371\n",
      "---\n",
      "16373\n",
      "---\n",
      "16383\n",
      "---\n",
      "16385\n",
      "---\n",
      "16395\n",
      "---\n",
      "16417\n",
      "---\n",
      "16428\n",
      "---\n",
      "16432\n",
      "---\n",
      "16449\n",
      "---\n",
      "16462\n",
      "---\n",
      "16474\n",
      "---\n",
      "16484\n",
      "---\n",
      "16496\n",
      "---\n",
      "16511\n",
      "---\n",
      "16519\n",
      "---\n",
      "16526\n",
      "---\n",
      "16538\n",
      "---\n",
      "16540\n",
      "0 13\n",
      "---\n",
      "16606\n",
      "---\n",
      "16609\n",
      "---\n",
      "16616\n",
      "---\n",
      "16645\n",
      "---\n",
      "16652\n",
      "---\n",
      "16657\n",
      "---\n",
      "16660\n",
      "---\n",
      "16667\n",
      "---\n",
      "16700\n",
      "---\n",
      "16708\n",
      "---\n",
      "16709\n",
      "---\n",
      "16716\n",
      "---\n",
      "16723\n",
      "---\n",
      "16748\n",
      "---\n",
      "16749\n",
      "---\n",
      "16760\n",
      "---\n",
      "16775\n",
      "---\n",
      "16778\n",
      "---\n",
      "16781\n",
      "---\n",
      "16786\n",
      "---\n",
      "16789\n",
      "---\n",
      "16792\n",
      "---\n",
      "16793\n",
      "---\n",
      "16794\n",
      "---\n",
      "16804\n",
      "---\n",
      "16814\n",
      "---\n",
      "16818\n",
      "---\n",
      "16821\n",
      "---\n",
      "16832\n",
      "---\n",
      "16869\n",
      "---\n",
      "16873\n",
      "---\n",
      "16877\n",
      "---\n",
      "16879\n",
      "---\n",
      "16884\n",
      "---\n",
      "16894\n",
      "---\n",
      "16904\n",
      "---\n",
      "16907\n",
      "---\n",
      "16919\n",
      "---\n",
      "16926\n",
      "---\n",
      "16931\n",
      "---\n",
      "16936\n",
      "---\n",
      "16942\n",
      "---\n",
      "16970\n",
      "---\n",
      "16998\n",
      "---\n",
      "17013\n",
      "---\n",
      "17023\n",
      "---\n",
      "17043\n",
      "---\n",
      "17050\n",
      "---\n",
      "17066\n",
      "---\n",
      "17076\n",
      "---\n",
      "17082\n",
      "---\n",
      "17093\n",
      "---\n",
      "17095\n",
      "---\n",
      "17116\n",
      "---\n",
      "17117\n",
      "---\n",
      "17122\n",
      "---\n",
      "17144\n",
      "---\n",
      "17167\n",
      "---\n",
      "17174\n",
      "---\n",
      "17185\n",
      "---\n",
      "17207\n",
      "---\n",
      "17208\n",
      "---\n",
      "17215\n",
      "---\n",
      "17223\n",
      "---\n",
      "17224\n",
      "---\n",
      "17245\n",
      "---\n",
      "17247\n",
      "---\n",
      "17257\n",
      "---\n",
      "17275\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17298\n",
      "---\n",
      "17305\n",
      "---\n",
      "17316\n",
      "---\n",
      "17337\n",
      "---\n",
      "17338\n",
      "---\n",
      "17344\n",
      "---\n",
      "17346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "21431\n",
      "---\n",
      "21435\n",
      "---\n",
      "21436\n",
      "---\n",
      "21441\n",
      "---\n",
      "21447\n",
      "---\n",
      "21458\n",
      "---\n",
      "21464\n",
      "---\n",
      "21468\n",
      "---\n",
      "21480\n",
      "---\n",
      "21481\n",
      "---\n",
      "21483\n",
      "---\n",
      "21500\n",
      "---\n",
      "21525\n",
      "---\n",
      "21534\n",
      "---\n",
      "21540\n",
      "---\n",
      "21555\n",
      "---\n",
      "21564\n",
      "---\n",
      "21566\n",
      "---\n",
      "21573\n",
      "---\n",
      "21592\n",
      "---\n",
      "21593\n",
      "---\n",
      "21623\n",
      "---\n",
      "21627\n",
      "---\n",
      "21629\n",
      "---\n",
      "21651\n",
      "---\n",
      "21663\n",
      "---\n",
      "21669\n",
      "---\n",
      "21677\n",
      "---\n",
      "21680\n",
      "---\n",
      "21682\n",
      "---\n",
      "21686\n",
      "---\n",
      "21690\n",
      "---\n",
      "21712\n",
      "---\n",
      "21720\n",
      "---\n",
      "21738\n",
      "---\n",
      "21739\n",
      "---\n",
      "21744\n",
      "---\n",
      "21746\n",
      "---\n",
      "21755\n",
      "---\n",
      "21758\n",
      "---\n",
      "21774\n",
      "---\n",
      "21788\n",
      "---\n",
      "21797\n",
      "---\n",
      "21823\n",
      "---\n",
      "21824\n",
      "---\n",
      "21828\n",
      "---\n",
      "21830\n",
      "---\n",
      "21832\n",
      "---\n",
      "21836\n",
      "---\n",
      "21861\n",
      "---\n",
      "21864\n",
      "---\n",
      "21872\n",
      "---\n",
      "21900\n",
      "---\n",
      "21920\n",
      "---\n",
      "21943\n",
      "---\n",
      "21964\n",
      "---\n",
      "21974\n",
      "---\n",
      "21978\n",
      "---\n",
      "21994\n",
      "---\n",
      "22024\n",
      "---\n",
      "22030\n",
      "---\n",
      "22039\n",
      "---\n",
      "22045\n",
      "---\n",
      "22046\n",
      "---\n",
      "22052\n",
      "---\n",
      "22053\n",
      "---\n",
      "22055\n",
      "---\n",
      "22082\n",
      "---\n",
      "22145\n",
      "---\n",
      "22151\n",
      "---\n",
      "22160\n",
      "---\n",
      "22163\n",
      "---\n",
      "22175\n",
      "---\n",
      "22177\n",
      "---\n",
      "22178\n",
      "---\n",
      "22184\n",
      "---\n",
      "22189\n",
      "---\n",
      "22201\n",
      "---\n",
      "22215\n",
      "---\n",
      "22218\n",
      "---\n",
      "22221\n",
      "---\n",
      "22232\n",
      "---\n",
      "22236\n",
      "---\n",
      "22245\n",
      "---\n",
      "22283\n",
      "---\n",
      "22312\n",
      "---\n",
      "22323\n",
      "---\n",
      "22328\n",
      "---\n",
      "22343\n",
      "---\n",
      "22355\n",
      "---\n",
      "22358\n",
      "---\n",
      "22376\n",
      "---\n",
      "22403\n",
      "---\n",
      "22404\n",
      "---\n",
      "22408\n",
      "---\n",
      "22411\n",
      "---\n",
      "22412\n",
      "---\n",
      "22415\n",
      "---\n",
      "22418\n",
      "---\n",
      "22420\n",
      "---\n",
      "22424\n",
      "---\n",
      "22427\n",
      "---\n",
      "22439\n",
      "---\n",
      "22450\n",
      "---\n",
      "22460\n",
      "---\n",
      "22468\n",
      "---\n",
      "22472\n",
      "---\n",
      "22474\n",
      "---\n",
      "22476\n",
      "---\n",
      "22483\n",
      "---\n",
      "22496\n",
      "---\n",
      "22507\n",
      "---\n",
      "22523\n",
      "---\n",
      "22533\n",
      "---\n",
      "22535\n",
      "---\n",
      "22538\n",
      "---\n",
      "22555\n",
      "---\n",
      "22564\n",
      "---\n",
      "22576\n",
      "---\n",
      "22582\n",
      "---\n",
      "22583\n",
      "---\n",
      "22590\n",
      "---\n",
      "22596\n",
      "---\n",
      "22613\n",
      "---\n",
      "22626\n",
      "---\n",
      "22642\n",
      "---\n",
      "22650\n",
      "---\n",
      "22671\n",
      "---\n",
      "22675\n",
      "---\n",
      "22678\n",
      "---\n",
      "22679\n",
      "---\n",
      "22686\n",
      "---\n",
      "22710\n",
      "---\n",
      "22715\n",
      "---\n",
      "22716\n",
      "---\n",
      "22726\n",
      "---\n",
      "22730\n",
      "---\n",
      "22741\n",
      "---\n",
      "22745\n",
      "---\n",
      "22762\n",
      "---\n",
      "22766\n",
      "---\n",
      "22796\n",
      "---\n",
      "22803\n",
      "---\n",
      "22807\n",
      "---\n",
      "22808\n",
      "---\n",
      "22816\n",
      "---\n",
      "22821\n",
      "---\n",
      "22831\n",
      "0 13\n",
      "---\n",
      "22834\n",
      "---\n",
      "22839\n",
      "---\n",
      "22840\n",
      "---\n",
      "22852\n",
      "---\n",
      "22876\n",
      "---\n",
      "22881\n",
      "---\n",
      "22885\n",
      "---\n",
      "22908\n",
      "---\n",
      "22916\n",
      "---\n",
      "22918\n",
      "---\n",
      "22934\n",
      "---\n",
      "22943\n",
      "---\n",
      "22956\n",
      "---\n",
      "22968\n",
      "---\n",
      "22969\n",
      "---\n",
      "22972\n",
      "---\n",
      "22979\n",
      "---\n",
      "22982\n",
      "---\n",
      "22983\n",
      "---\n",
      "23006\n",
      "---\n",
      "23010\n",
      "---\n",
      "23013\n",
      "---\n",
      "23019\n",
      "---\n",
      "23028\n",
      "---\n",
      "23029\n",
      "---\n",
      "23036\n",
      "---\n",
      "23038\n",
      "---\n",
      "23041\n",
      "---\n",
      "23042\n",
      "---\n",
      "23067\n",
      "---\n",
      "23080\n",
      "---\n",
      "23085\n",
      "---\n",
      "23090\n",
      "---\n",
      "23092\n",
      "---\n",
      "23109\n",
      "---\n",
      "23125\n",
      "---\n",
      "23151\n",
      "---\n",
      "23155\n",
      "---\n",
      "23165\n",
      "---\n",
      "23190\n",
      "---\n",
      "23191\n",
      "---\n",
      "23195\n",
      "---\n",
      "23198\n",
      "---\n",
      "23209\n",
      "---\n",
      "23224\n",
      "---\n",
      "23225\n",
      "---\n",
      "23226\n",
      "---\n",
      "23233\n",
      "---\n",
      "23262\n",
      "---\n",
      "23263\n",
      "---\n",
      "23265\n",
      "---\n",
      "23284\n",
      "---\n",
      "23294\n",
      "---\n",
      "23303\n",
      "---\n",
      "23309\n",
      "---\n",
      "23321\n",
      "---\n",
      "23337\n",
      "---\n",
      "23344\n",
      "---\n",
      "23346\n",
      "---\n",
      "23350\n",
      "---\n",
      "23356\n",
      "---\n",
      "23368\n",
      "---\n",
      "23369\n",
      "---\n",
      "23418\n",
      "---\n",
      "23449\n",
      "---\n",
      "23452\n",
      "---\n",
      "23453\n",
      "---\n",
      "23464\n",
      "---\n",
      "23490\n",
      "---\n",
      "23494\n",
      "---\n",
      "23501\n",
      "---\n",
      "23523\n",
      "---\n",
      "23535\n",
      "---\n",
      "23545\n",
      "---\n",
      "23565\n",
      "---\n",
      "23573\n",
      "---\n",
      "23579\n",
      "---\n",
      "23612\n",
      "---\n",
      "23630\n",
      "---\n",
      "23650\n",
      "---\n",
      "23654\n",
      "---\n",
      "23668\n",
      "---\n",
      "23678\n",
      "---\n",
      "23679\n",
      "---\n",
      "23687\n",
      "---\n",
      "23696\n",
      "---\n",
      "23700\n",
      "---\n",
      "23712\n",
      "---\n",
      "23750\n",
      "---\n",
      "23755\n",
      "---\n",
      "23771\n",
      "---\n",
      "23778\n",
      "---\n",
      "23784\n",
      "---\n",
      "23802\n",
      "---\n",
      "23820\n",
      "---\n",
      "23821\n",
      "---\n",
      "23825\n",
      "---\n",
      "23832\n",
      "---\n",
      "23833\n",
      "---\n",
      "23848\n",
      "---\n",
      "23851\n",
      "---\n",
      "23853\n",
      "---\n",
      "23859\n",
      "---\n",
      "23863\n",
      "---\n",
      "23871\n",
      "---\n",
      "23875\n",
      "---\n",
      "23878\n",
      "---\n",
      "23879\n",
      "---\n",
      "23893\n",
      "---\n",
      "23900\n",
      "---\n",
      "23911\n",
      "---\n",
      "23922\n",
      "---\n",
      "23924\n",
      "---\n",
      "23925\n",
      "---\n",
      "23955\n",
      "---\n",
      "23966\n",
      "---\n",
      "23996\n",
      "---\n",
      "24021\n",
      "---\n",
      "24032\n",
      "---\n",
      "24062\n",
      "---\n",
      "24082\n",
      "---\n",
      "24087\n",
      "---\n",
      "24106\n",
      "---\n",
      "24108\n",
      "---\n",
      "24112\n",
      "---\n",
      "24126\n",
      "---\n",
      "24133\n",
      "---\n",
      "24136\n",
      "---\n",
      "24151\n",
      "---\n",
      "24167\n",
      "---\n",
      "24180\n",
      "---\n",
      "24191\n",
      "---\n",
      "24196\n",
      "---\n",
      "24198\n",
      "---\n",
      "24202\n",
      "---\n",
      "24231\n",
      "---\n",
      "24245\n",
      "---\n",
      "24246\n",
      "---\n",
      "24253\n",
      "---\n",
      "24258\n",
      "---\n",
      "24271\n",
      "---\n",
      "24304\n",
      "---\n",
      "24308\n",
      "---\n",
      "24320\n",
      "---\n",
      "24340\n",
      "---\n",
      "24349\n",
      "---\n",
      "24352\n",
      "---\n",
      "24354\n",
      "---\n",
      "24360\n",
      "---\n",
      "24362\n",
      "---\n",
      "24371\n",
      "---\n",
      "24384\n",
      "---\n",
      "24388\n",
      "---\n",
      "24396\n",
      "---\n",
      "24412\n",
      "---\n",
      "24415\n",
      "---\n",
      "24416\n",
      "---\n",
      "24420\n",
      "---\n",
      "24438\n",
      "---\n",
      "24439\n",
      "---\n",
      "24454\n",
      "---\n",
      "24458\n",
      "---\n",
      "24480\n",
      "---\n",
      "24488\n",
      "---\n",
      "24514\n",
      "---\n",
      "24523\n",
      "---\n",
      "24528\n",
      "---\n",
      "24531\n",
      "---\n",
      "24534\n",
      "---\n",
      "24548\n",
      "---\n",
      "24552\n",
      "---\n",
      "24553\n",
      "---\n",
      "24554\n",
      "---\n",
      "24556\n",
      "---\n",
      "24557\n",
      "---\n",
      "24565\n",
      "---\n",
      "24573\n",
      "---\n",
      "24578\n",
      "---\n",
      "24580\n",
      "---\n",
      "24585\n",
      "---\n",
      "24588\n",
      "---\n",
      "24589\n",
      "---\n",
      "24592\n",
      "---\n",
      "24619\n",
      "0 13\n",
      "---\n",
      "24622\n",
      "---\n",
      "24648\n",
      "---\n",
      "24663\n",
      "---\n",
      "24664\n",
      "---\n",
      "24680\n",
      "---\n",
      "24692\n",
      "---\n",
      "24700\n",
      "---\n",
      "24701\n",
      "---\n",
      "24705\n",
      "---\n",
      "24708\n",
      "---\n",
      "24718\n",
      "---\n",
      "24729\n",
      "---\n",
      "24735\n",
      "---\n",
      "24738\n",
      "---\n",
      "24759\n",
      "---\n",
      "24763\n",
      "---\n",
      "24780\n",
      "---\n",
      "24786\n",
      "---\n",
      "24803\n",
      "---\n",
      "24807\n",
      "---\n",
      "24816\n",
      "---\n",
      "24834\n",
      "---\n",
      "24849\n",
      "---\n",
      "24858\n",
      "---\n",
      "24872\n",
      "---\n",
      "24877\n",
      "---\n",
      "24881\n",
      "---\n",
      "24886\n",
      "---\n",
      "24899\n",
      "---\n",
      "24903\n",
      "---\n",
      "24919\n",
      "---\n",
      "24922\n",
      "---\n",
      "24934\n",
      "---\n",
      "24939\n",
      "---\n",
      "24943\n",
      "---\n",
      "24968\n",
      "---\n",
      "24980\n",
      "---\n",
      "24985\n",
      "---\n",
      "24990\n",
      "---\n",
      "24996\n",
      "---\n",
      "25010\n",
      "---\n",
      "25015\n",
      "---\n",
      "25020\n",
      "---\n",
      "25021\n",
      "---\n",
      "25026\n",
      "---\n",
      "25029\n",
      "---\n",
      "25043\n",
      "---\n",
      "25048\n",
      "---\n",
      "25050\n",
      "---\n",
      "25055\n",
      "---\n",
      "25068\n",
      "---\n",
      "25089\n",
      "---\n",
      "25126\n",
      "---\n",
      "25147\n",
      "---\n",
      "25150\n",
      "---\n",
      "25164\n",
      "---\n",
      "25169\n",
      "---\n",
      "25179\n",
      "---\n",
      "25204\n",
      "---\n",
      "25208\n",
      "---\n",
      "25228\n",
      "---\n",
      "25229\n",
      "---\n",
      "25240\n",
      "---\n",
      "25243\n",
      "---\n",
      "25245\n",
      "---\n",
      "25260\n",
      "---\n",
      "25269\n",
      "---\n",
      "25274\n",
      "---\n",
      "25281\n",
      "---\n",
      "25292\n",
      "---\n",
      "25312\n",
      "---\n",
      "25314\n",
      "---\n",
      "25316\n",
      "---\n",
      "25327\n",
      "---\n",
      "25351\n",
      "---\n",
      "25357\n",
      "---\n",
      "25360\n",
      "---\n",
      "25375\n",
      "---\n",
      "25379\n",
      "---\n",
      "25385\n",
      "---\n",
      "25392\n",
      "---\n",
      "25410\n",
      "---\n",
      "25412\n",
      "---\n",
      "25421\n",
      "---\n",
      "25436\n",
      "---\n",
      "25445\n",
      "---\n",
      "25476\n",
      "---\n",
      "25522\n",
      "---\n",
      "25550\n",
      "---\n",
      "25554\n",
      "---\n",
      "25556\n",
      "---\n",
      "25573\n",
      "---\n",
      "25586\n",
      "---\n",
      "25608\n",
      "---\n",
      "25616\n",
      "---\n",
      "25622\n",
      "---\n",
      "25626\n",
      "---\n",
      "25648\n",
      "---\n",
      "25649\n",
      "---\n",
      "25661\n",
      "---\n",
      "25675\n",
      "---\n",
      "25693\n",
      "---\n",
      "25696\n",
      "---\n",
      "25710\n",
      "---\n",
      "25736\n",
      "---\n",
      "25760\n",
      "---\n",
      "25762\n",
      "---\n",
      "25776\n",
      "---\n",
      "25792\n",
      "---\n",
      "25798\n",
      "---\n",
      "25807\n",
      "---\n",
      "25817\n",
      "---\n",
      "25818\n",
      "---\n",
      "25830\n",
      "---\n",
      "25835\n",
      "---\n",
      "25837\n",
      "---\n",
      "25840\n",
      "---\n",
      "25846\n",
      "---\n",
      "25859\n",
      "---\n",
      "25878\n",
      "---\n",
      "25882\n",
      "---\n",
      "25886\n",
      "---\n",
      "25893\n",
      "---\n",
      "25910\n",
      "---\n",
      "25920\n",
      "---\n",
      "25922\n",
      "---\n",
      "25940\n",
      "---\n",
      "25941\n",
      "---\n",
      "25951\n",
      "---\n",
      "25952\n",
      "---\n",
      "25959\n",
      "---\n",
      "25964\n",
      "---\n",
      "25970\n",
      "---\n",
      "25982\n",
      "---\n",
      "25989\n",
      "---\n",
      "26035\n",
      "---\n",
      "26062\n",
      "---\n",
      "26078\n",
      "---\n",
      "26086\n",
      "---\n",
      "26091\n",
      "---\n",
      "26103\n",
      "---\n",
      "26109\n",
      "---\n",
      "26125\n",
      "---\n",
      "26127\n",
      "---\n",
      "26140\n",
      "---\n",
      "26141\n",
      "---\n",
      "26143\n",
      "---\n",
      "26156\n",
      "---\n",
      "26167\n",
      "---\n",
      "26168\n",
      "---\n",
      "26171\n",
      "---\n",
      "26173\n",
      "---\n",
      "26174\n",
      "---\n",
      "26175\n",
      "---\n",
      "26181\n",
      "---\n",
      "26186\n",
      "---\n",
      "26192\n",
      "---\n",
      "26203\n",
      "---\n",
      "26204\n",
      "---\n",
      "26217\n",
      "---\n",
      "26227\n",
      "---\n",
      "26231\n",
      "---\n",
      "26257\n",
      "---\n",
      "26265\n",
      "---\n",
      "26267\n",
      "---\n",
      "26270\n",
      "---\n",
      "26271\n",
      "---\n",
      "26288\n",
      "---\n",
      "26302\n",
      "---\n",
      "26303\n",
      "---\n",
      "26335\n",
      "---\n",
      "26355\n",
      "---\n",
      "26361\n",
      "---\n",
      "26376\n",
      "---\n",
      "26377\n",
      "---\n",
      "26382\n",
      "---\n",
      "26386\n",
      "---\n",
      "26391\n",
      "---\n",
      "26396\n",
      "---\n",
      "26416\n",
      "---\n",
      "26455\n",
      "---\n",
      "26481\n",
      "---\n",
      "26482\n",
      "---\n",
      "26489\n",
      "---\n",
      "26491\n",
      "---\n",
      "26495\n",
      "---\n",
      "26496\n",
      "---\n",
      "26500\n",
      "---\n",
      "26519\n",
      "---\n",
      "26535\n",
      "---\n",
      "26554\n",
      "---\n",
      "26560\n",
      "---\n",
      "26565\n",
      "---\n",
      "26594\n",
      "---\n",
      "26599\n",
      "---\n",
      "26602\n",
      "---\n",
      "26606\n",
      "---\n",
      "26648\n",
      "---\n",
      "26651\n",
      "---\n",
      "26654\n",
      "---\n",
      "26670\n",
      "---\n",
      "26689\n",
      "---\n",
      "26697\n",
      "---\n",
      "26703\n",
      "---\n",
      "26707\n",
      "---\n",
      "26714\n",
      "---\n",
      "26727\n",
      "---\n",
      "26744\n",
      "---\n",
      "26748\n",
      "---\n",
      "26760\n",
      "---\n",
      "26773\n",
      "---\n",
      "26778\n",
      "---\n",
      "26782\n",
      "---\n",
      "26798\n",
      "---\n",
      "26799\n",
      "---\n",
      "26823\n",
      "---\n",
      "26825\n",
      "---\n",
      "26836\n",
      "---\n",
      "26837\n",
      "---\n",
      "26840\n",
      "---\n",
      "26868\n",
      "---\n",
      "26881\n",
      "---\n",
      "26882\n",
      "---\n",
      "26886\n",
      "---\n",
      "26889\n",
      "---\n",
      "26895\n",
      "---\n",
      "26897\n",
      "---\n",
      "26908\n",
      "---\n",
      "26913\n",
      "---\n",
      "26924\n",
      "---\n",
      "26933\n",
      "---\n",
      "26963\n",
      "---\n",
      "26982\n",
      "---\n",
      "26985\n",
      "---\n",
      "26996\n",
      "---\n",
      "27000\n",
      "---\n",
      "27011\n",
      "---\n",
      "27024\n",
      "---\n",
      "27027\n",
      "---\n",
      "27029\n",
      "---\n",
      "27036\n",
      "---\n",
      "27065\n",
      "---\n",
      "27076\n",
      "---\n",
      "27088\n",
      "---\n",
      "27094\n",
      "---\n",
      "27099\n",
      "---\n",
      "27105\n",
      "---\n",
      "27123\n",
      "---\n",
      "27125\n",
      "---\n",
      "27127\n",
      "---\n",
      "27132\n",
      "---\n",
      "27175\n",
      "---\n",
      "27192\n",
      "---\n",
      "27194\n",
      "---\n",
      "27197\n",
      "---\n",
      "27220\n",
      "---\n",
      "27232\n",
      "---\n",
      "27233\n",
      "---\n",
      "27271\n",
      "---\n",
      "27282\n",
      "---\n",
      "27287\n",
      "---\n",
      "27298\n",
      "---\n",
      "27303\n",
      "---\n",
      "27304\n",
      "---\n",
      "27317\n",
      "---\n",
      "27322\n",
      "---\n",
      "27353\n",
      "---\n",
      "27356\n",
      "---\n",
      "27365\n",
      "---\n",
      "27369\n",
      "---\n",
      "27379\n",
      "---\n",
      "27394\n",
      "---\n",
      "27426\n",
      "---\n",
      "27432\n",
      "---\n",
      "27433\n",
      "---\n",
      "27449\n",
      "---\n",
      "27454\n",
      "---\n",
      "27458\n",
      "---\n",
      "27459\n",
      "---\n",
      "27463\n",
      "---\n",
      "27473\n",
      "---\n",
      "27484\n",
      "---\n",
      "27485\n",
      "---\n",
      "27500\n",
      "---\n",
      "27516\n",
      "---\n",
      "27534\n",
      "---\n",
      "27548\n",
      "---\n",
      "27553\n",
      "---\n",
      "27555\n",
      "---\n",
      "27582\n",
      "---\n",
      "27583\n",
      "---\n",
      "27591\n",
      "---\n",
      "27602\n",
      "---\n",
      "27615\n",
      "---\n",
      "27624\n",
      "---\n",
      "27631\n",
      "---\n",
      "27647\n",
      "---\n",
      "27663\n",
      "---\n",
      "27664\n",
      "---\n",
      "27689\n",
      "---\n",
      "27692\n",
      "---\n",
      "27709\n",
      "---\n",
      "27710\n",
      "---\n",
      "27714\n",
      "---\n",
      "27726\n",
      "---\n",
      "27730\n",
      "---\n",
      "27750\n",
      "---\n",
      "27770\n",
      "---\n",
      "27777\n",
      "---\n",
      "27786\n",
      "---\n",
      "27791\n",
      "---\n",
      "27794\n",
      "---\n",
      "27806\n",
      "---\n",
      "27807\n",
      "---\n",
      "27832\n",
      "---\n",
      "27845\n",
      "---\n",
      "27851\n",
      "---\n",
      "27855\n",
      "---\n",
      "27866\n",
      "---\n",
      "27874\n",
      "---\n",
      "27897\n",
      "---\n",
      "27901\n",
      "---\n",
      "27903\n",
      "---\n",
      "27912\n",
      "---\n",
      "27913\n",
      "---\n",
      "27917\n",
      "---\n",
      "27919\n",
      "---\n",
      "27925\n",
      "---\n",
      "27940\n",
      "---\n",
      "27942\n",
      "---\n",
      "27944\n",
      "---\n",
      "27952\n",
      "---\n",
      "27963\n",
      "---\n",
      "27966\n",
      "---\n",
      "27989\n",
      "---\n",
      "27990\n",
      "---\n",
      "27999\n",
      "---\n",
      "28003\n",
      "---\n",
      "28030\n",
      "---\n",
      "28040\n",
      "---\n",
      "28048\n",
      "---\n",
      "28060\n",
      "---\n",
      "28062\n",
      "---\n",
      "28067\n",
      "---\n",
      "28079\n",
      "---\n",
      "28080\n",
      "---\n",
      "28081\n",
      "---\n",
      "28082\n",
      "---\n",
      "28087\n",
      "---\n",
      "28091\n",
      "---\n",
      "28103\n",
      "---\n",
      "28114\n",
      "---\n",
      "28119\n",
      "---\n",
      "28121\n",
      "---\n",
      "28125\n",
      "---\n",
      "28131\n",
      "---\n",
      "28154\n",
      "---\n",
      "28156\n",
      "---\n",
      "28159\n",
      "---\n",
      "28165\n",
      "---\n",
      "28188\n",
      "---\n",
      "28192\n",
      "---\n",
      "28208\n",
      "---\n",
      "28215\n",
      "---\n",
      "28220\n",
      "---\n",
      "28223\n",
      "---\n",
      "28231\n",
      "---\n",
      "28252\n",
      "---\n",
      "28265\n",
      "---\n",
      "28276\n",
      "---\n",
      "28317\n",
      "---\n",
      "28323\n",
      "---\n",
      "28330\n",
      "---\n",
      "28356\n",
      "---\n",
      "28358\n",
      "---\n",
      "28359\n",
      "---\n",
      "28375\n",
      "---\n",
      "28391\n",
      "---\n",
      "28426\n",
      "---\n",
      "28432\n",
      "---\n",
      "28454\n",
      "---\n",
      "28458\n",
      "---\n",
      "28461\n",
      "---\n",
      "28466\n",
      "---\n",
      "28469\n",
      "---\n",
      "28472\n",
      "---\n",
      "28473\n",
      "---\n",
      "28476\n",
      "---\n",
      "28516\n",
      "---\n",
      "28537\n",
      "---\n",
      "28540\n",
      "---\n",
      "28558\n",
      "---\n",
      "28578\n",
      "---\n",
      "28609\n",
      "---\n",
      "28642\n",
      "---\n",
      "28645\n",
      "---\n",
      "28656\n",
      "---\n",
      "28673\n",
      "---\n",
      "28676\n",
      "---\n",
      "28685\n",
      "---\n",
      "28692\n",
      "---\n",
      "28694\n",
      "---\n",
      "28705\n",
      "---\n",
      "28719\n",
      "---\n",
      "28721\n",
      "---\n",
      "28726\n",
      "---\n",
      "28727\n",
      "---\n",
      "28748\n",
      "---\n",
      "28756\n",
      "---\n",
      "28770\n",
      "---\n",
      "28792\n",
      "---\n",
      "28830\n",
      "---\n",
      "28839\n",
      "---\n",
      "28853\n",
      "---\n",
      "28855\n",
      "---\n",
      "28870\n",
      "---\n",
      "28881\n",
      "---\n",
      "28911\n",
      "---\n",
      "28913\n",
      "---\n",
      "28917\n",
      "---\n",
      "28918\n",
      "---\n",
      "28921\n",
      "---\n",
      "28924\n",
      "---\n",
      "28966\n",
      "---\n",
      "28990\n",
      "---\n",
      "29005\n",
      "---\n",
      "29016\n",
      "---\n",
      "29020\n",
      "---\n",
      "29029\n",
      "---\n",
      "29069\n",
      "---\n",
      "29071\n",
      "---\n",
      "29075\n",
      "---\n",
      "29084\n",
      "---\n",
      "29092\n",
      "---\n",
      "29096\n",
      "---\n",
      "29113\n",
      "---\n",
      "29117\n",
      "---\n",
      "29121\n",
      "---\n",
      "29133\n",
      "---\n",
      "29151\n",
      "---\n",
      "29158\n",
      "---\n",
      "29168\n",
      "---\n",
      "29177\n",
      "---\n",
      "29179\n",
      "---\n",
      "29181\n",
      "---\n",
      "29185\n",
      "---\n",
      "29189\n",
      "---\n",
      "29204\n",
      "---\n",
      "29209\n",
      "---\n",
      "29219\n",
      "---\n",
      "29249\n",
      "---\n",
      "29250\n",
      "---\n",
      "29252\n",
      "---\n",
      "29255\n",
      "---\n",
      "29257\n",
      "---\n",
      "29263\n",
      "---\n",
      "29275\n",
      "---\n",
      "29292\n",
      "---\n",
      "29294\n",
      "---\n",
      "29295\n",
      "---\n",
      "29298\n",
      "---\n",
      "29301\n",
      "---\n",
      "29306\n",
      "---\n",
      "29309\n",
      "---\n",
      "29316\n",
      "---\n",
      "29344\n",
      "---\n",
      "29351\n",
      "---\n",
      "29372\n",
      "---\n",
      "29375\n",
      "---\n",
      "29378\n",
      "0 13\n",
      "0 13\n",
      "0 13\n",
      "---\n",
      "29392\n",
      "---\n",
      "29399\n",
      "---\n",
      "29410\n",
      "---\n",
      "29435\n",
      "---\n",
      "29438\n",
      "---\n",
      "29447\n",
      "---\n",
      "29449\n",
      "---\n",
      "29450\n",
      "---\n",
      "29466\n",
      "---\n",
      "29479\n",
      "---\n",
      "29480\n",
      "---\n",
      "29509\n",
      "---\n",
      "29528\n",
      "---\n",
      "29534\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29537\n",
      "---\n",
      "29555\n",
      "---\n",
      "29560\n",
      "---\n",
      "29565\n",
      "---\n",
      "29572\n",
      "---\n",
      "29574\n",
      "---\n",
      "29585\n",
      "---\n",
      "29586\n",
      "---\n",
      "29604\n",
      "---\n",
      "29609\n",
      "---\n",
      "29619\n",
      "---\n",
      "29631\n",
      "---\n",
      "29642\n",
      "---\n",
      "29648\n",
      "---\n",
      "29660\n",
      "---\n",
      "29673\n",
      "---\n",
      "29676\n",
      "---\n",
      "29690\n",
      "---\n",
      "29699\n",
      "---\n",
      "29712\n",
      "---\n",
      "29715\n",
      "---\n",
      "29735\n",
      "---\n",
      "29747\n",
      "---\n",
      "29757\n",
      "---\n",
      "29773\n",
      "---\n",
      "29776\n",
      "---\n",
      "29777\n",
      "---\n",
      "29788\n",
      "---\n",
      "29790\n",
      "---\n",
      "29797\n",
      "---\n",
      "29809\n",
      "---\n",
      "29832\n",
      "---\n",
      "29882\n",
      "---\n",
      "29892\n",
      "---\n",
      "29920\n",
      "---\n",
      "29946\n",
      "---\n",
      "29985\n",
      "---\n",
      "29995\n",
      "---\n",
      "30002\n",
      "---\n",
      "30007\n",
      "---\n",
      "30013\n",
      "---\n",
      "30043\n",
      "---\n",
      "30076\n",
      "---\n",
      "30089\n",
      "---\n",
      "30091\n",
      "---\n",
      "30092\n",
      "---\n",
      "30125\n",
      "---\n",
      "30141\n",
      "---\n",
      "30144\n",
      "---\n",
      "30167\n",
      "---\n",
      "30187\n",
      "---\n",
      "30188\n",
      "---\n",
      "30190\n",
      "---\n",
      "30194\n",
      "---\n",
      "30206\n",
      "---\n",
      "30207\n",
      "---\n",
      "30213\n",
      "---\n",
      "30219\n",
      "---\n",
      "30239\n",
      "---\n",
      "30253\n",
      "---\n",
      "30280\n",
      "---\n",
      "30281\n",
      "---\n",
      "30287\n",
      "---\n",
      "30306\n",
      "---\n",
      "30311\n",
      "---\n",
      "30312\n",
      "---\n",
      "30314\n",
      "---\n",
      "30325\n",
      "---\n",
      "30338\n",
      "---\n",
      "30351\n",
      "---\n",
      "30356\n",
      "---\n",
      "30374\n",
      "---\n",
      "30390\n",
      "---\n",
      "30415\n",
      "---\n",
      "30423\n",
      "---\n",
      "30446\n",
      "---\n",
      "30512\n",
      "---\n",
      "30523\n",
      "---\n",
      "30536\n",
      "---\n",
      "30537\n",
      "---\n",
      "30557\n",
      "---\n",
      "30565\n",
      "---\n",
      "30580\n",
      "---\n",
      "30584\n",
      "---\n",
      "30600\n",
      "---\n",
      "30601\n",
      "---\n",
      "30611\n",
      "---\n",
      "30617\n",
      "---\n",
      "30642\n",
      "---\n",
      "30643\n",
      "---\n",
      "30644\n",
      "---\n",
      "30654\n",
      "---\n",
      "30671\n",
      "---\n",
      "30673\n",
      "---\n",
      "30681\n",
      "---\n",
      "30729\n",
      "---\n",
      "30733\n",
      "---\n",
      "30738\n",
      "---\n",
      "30747\n",
      "---\n",
      "30750\n",
      "---\n",
      "30759\n",
      "---\n",
      "30771\n",
      "---\n",
      "30773\n",
      "---\n",
      "30779\n",
      "---\n",
      "30780\n",
      "---\n",
      "30790\n",
      "---\n",
      "30795\n",
      "---\n",
      "30801\n",
      "---\n",
      "30803\n",
      "---\n",
      "30807\n",
      "---\n",
      "30809\n",
      "---\n",
      "30839\n",
      "---\n",
      "30851\n",
      "---\n",
      "30852\n",
      "---\n",
      "30875\n",
      "---\n",
      "30894\n",
      "---\n",
      "30895\n",
      "---\n",
      "30900\n",
      "---\n",
      "30902\n",
      "---\n",
      "30904\n",
      "---\n",
      "30908\n",
      "---\n",
      "30914\n",
      "---\n",
      "30926\n",
      "---\n",
      "30927\n",
      "---\n",
      "30937\n",
      "---\n",
      "30943\n",
      "---\n",
      "30961\n",
      "---\n",
      "30977\n",
      "---\n",
      "30979\n",
      "---\n",
      "31028\n",
      "---\n",
      "31029\n",
      "---\n",
      "31030\n",
      "---\n",
      "31049\n",
      "---\n",
      "31058\n",
      "---\n",
      "31076\n",
      "---\n",
      "31078\n",
      "---\n",
      "31092\n",
      "---\n",
      "31095\n",
      "---\n",
      "31113\n",
      "---\n",
      "31122\n",
      "---\n",
      "31131\n",
      "---\n",
      "31141\n",
      "---\n",
      "31155\n",
      "---\n",
      "31161\n",
      "---\n",
      "31178\n",
      "---\n",
      "31180\n",
      "---\n",
      "31187\n",
      "---\n",
      "31191\n",
      "---\n",
      "31195\n",
      "---\n",
      "31196\n",
      "---\n",
      "31199\n",
      "---\n",
      "31243\n",
      "---\n",
      "31246\n",
      "0 13\n",
      "---\n",
      "31263\n",
      "---\n",
      "31279\n",
      "---\n",
      "31296\n",
      "---\n",
      "31300\n",
      "---\n",
      "31323\n",
      "---\n",
      "31332\n",
      "---\n",
      "31344\n",
      "---\n",
      "31347\n",
      "---\n",
      "31376\n",
      "---\n",
      "31378\n",
      "---\n",
      "31385\n",
      "---\n",
      "31398\n",
      "---\n",
      "31400\n",
      "---\n",
      "31401\n",
      "---\n",
      "31406\n",
      "---\n",
      "31431\n",
      "---\n",
      "31469\n",
      "---\n",
      "31488\n",
      "---\n",
      "31494\n",
      "---\n",
      "31495\n",
      "---\n",
      "31497\n",
      "---\n",
      "31503\n",
      "---\n",
      "31508\n",
      "---\n",
      "31512\n",
      "---\n",
      "31515\n",
      "---\n",
      "31524\n",
      "---\n",
      "31530\n",
      "---\n",
      "31532\n",
      "---\n",
      "31540\n",
      "---\n",
      "31544\n",
      "---\n",
      "31570\n",
      "---\n",
      "31588\n",
      "---\n",
      "31593\n",
      "---\n",
      "31594\n",
      "---\n",
      "31617\n",
      "---\n",
      "31620\n",
      "---\n",
      "31629\n",
      "---\n",
      "31640\n",
      "---\n",
      "31654\n",
      "---\n",
      "31658\n",
      "---\n",
      "31660\n",
      "---\n",
      "31673\n",
      "---\n",
      "31686\n",
      "---\n",
      "31708\n",
      "---\n",
      "31714\n",
      "---\n",
      "31728\n",
      "---\n",
      "31735\n",
      "---\n",
      "31736\n",
      "---\n",
      "31740\n",
      "---\n",
      "31748\n",
      "---\n",
      "31757\n",
      "---\n",
      "31762\n",
      "---\n",
      "31776\n",
      "---\n",
      "31801\n",
      "---\n",
      "31815\n",
      "---\n",
      "31817\n",
      "---\n",
      "31819\n",
      "---\n",
      "31840\n",
      "---\n",
      "31845\n",
      "---\n",
      "31846\n",
      "---\n",
      "31856\n",
      "---\n",
      "31866\n",
      "---\n",
      "31872\n",
      "---\n",
      "31883\n",
      "---\n",
      "31885\n",
      "---\n",
      "31886\n",
      "---\n",
      "31900\n",
      "---\n",
      "31917\n",
      "---\n",
      "31925\n",
      "---\n",
      "31927\n",
      "---\n",
      "31929\n",
      "---\n",
      "31930\n",
      "---\n",
      "31939\n",
      "---\n",
      "31943\n",
      "---\n",
      "31945\n",
      "---\n",
      "31975\n",
      "---\n",
      "31990\n",
      "---\n",
      "32006\n",
      "---\n",
      "32012\n",
      "---\n",
      "32015\n",
      "---\n",
      "32025\n",
      "---\n",
      "32047\n",
      "---\n",
      "32048\n",
      "---\n",
      "32054\n",
      "---\n",
      "32058\n",
      "---\n",
      "32064\n",
      "---\n",
      "32077\n",
      "---\n",
      "32090\n",
      "---\n",
      "32091\n",
      "---\n",
      "32095\n",
      "---\n",
      "32097\n",
      "---\n",
      "32140\n",
      "---\n",
      "32146\n",
      "---\n",
      "32147\n",
      "---\n",
      "32156\n",
      "---\n",
      "32169\n",
      "---\n",
      "32170\n",
      "---\n",
      "32173\n",
      "---\n",
      "32185\n",
      "---\n",
      "32187\n",
      "---\n",
      "32191\n",
      "---\n",
      "32201\n",
      "---\n",
      "32203\n",
      "---\n",
      "32219\n",
      "---\n",
      "32225\n",
      "---\n",
      "32251\n",
      "---\n",
      "32258\n",
      "---\n",
      "32270\n",
      "---\n",
      "32273\n",
      "---\n",
      "32277\n",
      "---\n",
      "32279\n",
      "---\n",
      "32281\n",
      "---\n",
      "32312\n",
      "---\n",
      "32316\n",
      "---\n",
      "32320\n",
      "---\n",
      "32323\n",
      "---\n",
      "32324\n",
      "---\n",
      "32328\n",
      "---\n",
      "32335\n",
      "---\n",
      "32338\n",
      "---\n",
      "32350\n",
      "---\n",
      "32360\n",
      "---\n",
      "32363\n",
      "---\n",
      "32375\n",
      "---\n",
      "32376\n",
      "---\n",
      "32387\n",
      "---\n",
      "32389\n",
      "---\n",
      "32400\n",
      "---\n",
      "32426\n",
      "---\n",
      "32438\n",
      "---\n",
      "32439\n",
      "---\n",
      "32442\n",
      "---\n",
      "32462\n",
      "---\n",
      "32498\n",
      "---\n",
      "32500\n",
      "---\n",
      "32504\n",
      "---\n",
      "32506\n",
      "---\n",
      "32513\n",
      "---\n",
      "32525\n",
      "---\n",
      "32550\n",
      "---\n",
      "32572\n",
      "---\n",
      "32575\n",
      "---\n",
      "32584\n",
      "---\n",
      "32587\n",
      "---\n",
      "32590\n",
      "---\n",
      "32593\n",
      "---\n",
      "32594\n",
      "---\n",
      "32602\n",
      "---\n",
      "32614\n",
      "---\n",
      "32624\n",
      "---\n",
      "32628\n",
      "---\n",
      "32630\n",
      "---\n",
      "32676\n",
      "---\n",
      "32704\n",
      "---\n",
      "32741\n",
      "---\n",
      "32749\n",
      "---\n",
      "32765\n",
      "---\n",
      "32770\n",
      "---\n",
      "32783\n",
      "---\n",
      "32812\n",
      "---\n",
      "32815\n",
      "---\n",
      "32873\n",
      "---\n",
      "32881\n",
      "---\n",
      "32892\n",
      "---\n",
      "32909\n",
      "---\n",
      "32920\n",
      "---\n",
      "32946\n",
      "---\n",
      "32949\n",
      "---\n",
      "32965\n",
      "---\n",
      "32971\n",
      "---\n",
      "32991\n",
      "---\n",
      "32994\n",
      "---\n",
      "32996\n",
      "---\n",
      "33013\n",
      "---\n",
      "33019\n",
      "---\n",
      "33023\n",
      "---\n",
      "33024\n",
      "---\n",
      "33040\n",
      "---\n",
      "33043\n",
      "---\n",
      "33063\n",
      "---\n",
      "33074\n",
      "---\n",
      "33075\n",
      "---\n",
      "33078\n",
      "---\n",
      "33094\n",
      "---\n",
      "33111\n",
      "---\n",
      "33116\n",
      "---\n",
      "33117\n",
      "---\n",
      "33122\n",
      "---\n",
      "33126\n",
      "---\n",
      "33140\n",
      "---\n",
      "33148\n",
      "---\n",
      "33149\n",
      "---\n",
      "33174\n",
      "---\n",
      "33175\n",
      "---\n",
      "33208\n",
      "---\n",
      "33255\n",
      "---\n",
      "33274\n",
      "---\n",
      "33282\n",
      "---\n",
      "33291\n",
      "---\n",
      "33293\n",
      "---\n",
      "33331\n",
      "---\n",
      "33338\n",
      "---\n",
      "33340\n",
      "---\n",
      "33341\n",
      "---\n",
      "33379\n",
      "---\n",
      "33388\n",
      "---\n",
      "33391\n",
      "---\n",
      "33413\n",
      "---\n",
      "33417\n",
      "---\n",
      "33418\n",
      "---\n",
      "33427\n",
      "---\n",
      "33445\n",
      "---\n",
      "33464\n",
      "---\n",
      "33467\n",
      "---\n",
      "33480\n",
      "0 13\n",
      "---\n",
      "33491\n",
      "---\n",
      "33493\n",
      "---\n",
      "33494\n",
      "---\n",
      "33503\n",
      "---\n",
      "33506\n",
      "---\n",
      "33513\n",
      "---\n",
      "33515\n",
      "---\n",
      "33516\n",
      "---\n",
      "33549\n",
      "---\n",
      "33561\n",
      "0 13\n",
      "---\n",
      "33571\n",
      "---\n",
      "33583\n",
      "---\n",
      "33590\n",
      "---\n",
      "33602\n",
      "---\n",
      "33604\n",
      "---\n",
      "33608\n",
      "---\n",
      "33611\n",
      "---\n",
      "33624\n",
      "---\n",
      "33631\n",
      "---\n",
      "33645\n",
      "---\n",
      "33655\n",
      "---\n",
      "33661\n",
      "---\n",
      "33671\n",
      "---\n",
      "33682\n",
      "---\n",
      "33683\n",
      "---\n",
      "33707\n",
      "---\n",
      "33711\n",
      "---\n",
      "33718\n",
      "---\n",
      "33720\n",
      "---\n",
      "33721\n",
      "---\n",
      "33733\n",
      "---\n",
      "33734\n",
      "---\n",
      "33738\n",
      "---\n",
      "33745\n",
      "---\n",
      "33753\n",
      "---\n",
      "33774\n",
      "---\n",
      "33811\n",
      "---\n",
      "33812\n",
      "---\n",
      "33827\n",
      "---\n",
      "33831\n",
      "---\n",
      "33850\n",
      "---\n",
      "33853\n",
      "---\n",
      "33864\n",
      "---\n",
      "33868\n",
      "---\n",
      "33872\n",
      "---\n",
      "33877\n",
      "---\n",
      "33880\n",
      "---\n",
      "33894\n",
      "---\n",
      "33895\n",
      "---\n",
      "33896\n",
      "---\n",
      "33905\n",
      "---\n",
      "33914\n",
      "---\n",
      "33917\n",
      "---\n",
      "33927\n",
      "---\n",
      "33931\n",
      "---\n",
      "33940\n",
      "---\n",
      "33968\n",
      "---\n",
      "33987\n",
      "---\n",
      "33990\n",
      "---\n",
      "33992\n",
      "---\n",
      "33996\n",
      "---\n",
      "34017\n",
      "---\n",
      "34035\n",
      "---\n",
      "34045\n",
      "0 13\n",
      "---\n",
      "34056\n",
      "---\n",
      "34058\n",
      "---\n",
      "34061\n",
      "---\n",
      "34064\n",
      "---\n",
      "34070\n",
      "---\n",
      "34076\n",
      "---\n",
      "34093\n",
      "---\n",
      "34103\n",
      "---\n",
      "34131\n",
      "---\n",
      "34135\n",
      "---\n",
      "34139\n",
      "---\n",
      "34142\n",
      "---\n",
      "34185\n",
      "---\n",
      "34189\n",
      "---\n",
      "34194\n",
      "---\n",
      "34196\n",
      "---\n",
      "34200\n",
      "---\n",
      "34214\n",
      "---\n",
      "34221\n",
      "---\n",
      "34223\n",
      "---\n",
      "34247\n",
      "---\n",
      "34250\n",
      "---\n",
      "34262\n",
      "---\n",
      "34267\n",
      "---\n",
      "34288\n",
      "---\n",
      "34289\n",
      "---\n",
      "34292\n",
      "---\n",
      "34294\n",
      "---\n",
      "34296\n",
      "---\n",
      "34309\n",
      "---\n",
      "34312\n",
      "---\n",
      "34318\n",
      "---\n",
      "34322\n",
      "---\n",
      "34331\n",
      "---\n",
      "34340\n",
      "---\n",
      "34355\n",
      "---\n",
      "34356\n",
      "---\n",
      "34357\n",
      "---\n",
      "34358\n",
      "---\n",
      "34363\n",
      "---\n",
      "34418\n",
      "---\n",
      "34421\n",
      "---\n",
      "34426\n",
      "---\n",
      "34433\n",
      "---\n",
      "34439\n",
      "---\n",
      "34449\n",
      "---\n",
      "34450\n",
      "---\n",
      "34463\n",
      "---\n",
      "34468\n",
      "---\n",
      "34476\n",
      "0 13\n",
      "---\n",
      "34479\n",
      "---\n",
      "34501\n",
      "---\n",
      "34503\n",
      "---\n",
      "34533\n",
      "---\n",
      "34563\n",
      "---\n",
      "34577\n",
      "---\n",
      "34578\n",
      "---\n",
      "34613\n",
      "---\n",
      "34614\n",
      "---\n",
      "34657\n",
      "---\n",
      "34660\n",
      "---\n",
      "34662\n",
      "---\n",
      "34695\n",
      "---\n",
      "34701\n",
      "---\n",
      "34703\n",
      "---\n",
      "34706\n",
      "---\n",
      "34711\n",
      "---\n",
      "34721\n",
      "---\n",
      "34731\n",
      "---\n",
      "34737\n",
      "---\n",
      "34755\n",
      "---\n",
      "34767\n",
      "---\n",
      "34774\n",
      "---\n",
      "34776\n",
      "---\n",
      "34792\n",
      "---\n",
      "34819\n",
      "---\n",
      "34826\n",
      "---\n",
      "34833\n",
      "---\n",
      "34849\n",
      "---\n",
      "34851\n",
      "---\n",
      "34852\n",
      "---\n",
      "34869\n",
      "---\n",
      "34879\n",
      "---\n",
      "34883\n",
      "---\n",
      "34892\n",
      "---\n",
      "34895\n",
      "---\n",
      "34898\n",
      "---\n",
      "34901\n",
      "---\n",
      "34906\n",
      "---\n",
      "34907\n",
      "---\n",
      "34946\n",
      "---\n",
      "34957\n",
      "---\n",
      "34970\n",
      "---\n",
      "34973\n",
      "---\n",
      "34980\n",
      "---\n",
      "34981\n",
      "---\n",
      "34986\n",
      "---\n",
      "34988\n",
      "---\n",
      "34991\n",
      "---\n",
      "34993\n",
      "---\n",
      "34994\n",
      "---\n",
      "34999\n",
      "---\n",
      "35003\n",
      "---\n",
      "35005\n",
      "---\n",
      "35023\n",
      "---\n",
      "35024\n",
      "---\n",
      "35034\n",
      "---\n",
      "35036\n",
      "---\n",
      "35042\n",
      "---\n",
      "35057\n",
      "---\n",
      "35060\n",
      "---\n",
      "35067\n",
      "---\n",
      "35077\n",
      "---\n",
      "35108\n",
      "---\n",
      "35118\n",
      "---\n",
      "35130\n",
      "---\n",
      "35155\n",
      "---\n",
      "35194\n",
      "---\n",
      "35203\n",
      "---\n",
      "35215\n",
      "---\n",
      "35223\n",
      "---\n",
      "35226\n",
      "---\n",
      "35241\n",
      "---\n",
      "35244\n",
      "---\n",
      "35247\n",
      "---\n",
      "35258\n",
      "---\n",
      "35276\n",
      "---\n",
      "35281\n",
      "---\n",
      "35295\n",
      "---\n",
      "35313\n",
      "---\n",
      "35331\n",
      "---\n",
      "35339\n",
      "---\n",
      "35342\n",
      "---\n",
      "35348\n",
      "---\n",
      "35349\n",
      "---\n",
      "35355\n",
      "---\n",
      "35389\n",
      "---\n",
      "35394\n",
      "---\n",
      "35411\n",
      "---\n",
      "35424\n",
      "---\n",
      "35454\n",
      "---\n",
      "35459\n",
      "---\n",
      "35469\n",
      "---\n",
      "35471\n",
      "---\n",
      "35479\n",
      "---\n",
      "35480\n",
      "---\n",
      "35493\n",
      "---\n",
      "35507\n",
      "---\n",
      "35512\n",
      "---\n",
      "35524\n",
      "---\n",
      "35528\n",
      "---\n",
      "35536\n",
      "---\n",
      "35552\n",
      "---\n",
      "35568\n",
      "---\n",
      "35569\n",
      "---\n",
      "35570\n",
      "---\n",
      "35573\n",
      "---\n",
      "35575\n",
      "---\n",
      "35579\n",
      "---\n",
      "35581\n",
      "---\n",
      "35583\n",
      "---\n",
      "35592\n",
      "---\n",
      "35617\n",
      "---\n",
      "35625\n",
      "---\n",
      "35634\n",
      "---\n",
      "35636\n",
      "---\n",
      "35641\n",
      "---\n",
      "35650\n",
      "---\n",
      "35651\n",
      "---\n",
      "35657\n",
      "---\n",
      "35661\n",
      "---\n",
      "35667\n",
      "---\n",
      "35687\n",
      "---\n",
      "35693\n",
      "---\n",
      "35697\n",
      "---\n",
      "35699\n",
      "---\n",
      "35711\n",
      "---\n",
      "35715\n",
      "---\n",
      "35723\n",
      "---\n",
      "35756\n",
      "---\n",
      "35757\n",
      "---\n",
      "35767\n",
      "---\n",
      "35770\n",
      "---\n",
      "35771\n",
      "---\n",
      "35780\n",
      "---\n",
      "35785\n",
      "---\n",
      "35788\n",
      "---\n",
      "35803\n",
      "---\n",
      "35820\n",
      "---\n",
      "35821\n",
      "---\n",
      "35825\n",
      "---\n",
      "35831\n",
      "---\n",
      "35835\n",
      "---\n",
      "35837\n",
      "---\n",
      "35842\n",
      "---\n",
      "35847\n",
      "---\n",
      "35854\n",
      "---\n",
      "35857\n",
      "---\n",
      "35858\n",
      "---\n",
      "35862\n",
      "---\n",
      "35872\n",
      "---\n",
      "35874\n",
      "---\n",
      "35875\n",
      "---\n",
      "35877\n",
      "---\n",
      "35878\n",
      "---\n",
      "35884\n",
      "---\n",
      "35886\n",
      "---\n",
      "35895\n",
      "---\n",
      "35905\n",
      "---\n",
      "35906\n",
      "---\n",
      "35909\n",
      "---\n",
      "35914\n",
      "---\n",
      "35929\n",
      "---\n",
      "35938\n",
      "---\n",
      "35944\n",
      "---\n",
      "35955\n",
      "---\n",
      "35958\n",
      "---\n",
      "35963\n",
      "---\n",
      "35971\n",
      "---\n",
      "35973\n",
      "---\n",
      "35983\n",
      "---\n",
      "35991\n",
      "---\n",
      "36012\n",
      "---\n",
      "36022\n",
      "---\n",
      "36028\n",
      "---\n",
      "36034\n",
      "---\n",
      "36035\n",
      "---\n",
      "36045\n",
      "---\n",
      "36046\n",
      "---\n",
      "36060\n",
      "---\n",
      "36061\n",
      "---\n",
      "36083\n",
      "---\n",
      "36099\n",
      "---\n",
      "36103\n",
      "---\n",
      "36114\n",
      "---\n",
      "36121\n",
      "---\n",
      "36127\n",
      "---\n",
      "36131\n",
      "---\n",
      "36137\n",
      "---\n",
      "36154\n",
      "---\n",
      "36170\n",
      "---\n",
      "36179\n",
      "---\n",
      "36186\n",
      "---\n",
      "36200\n",
      "---\n",
      "36203\n",
      "---\n",
      "36208\n",
      "---\n",
      "36209\n",
      "---\n",
      "36215\n",
      "---\n",
      "36228\n",
      "---\n",
      "36231\n",
      "---\n",
      "36233\n",
      "---\n",
      "36242\n",
      "---\n",
      "36261\n",
      "---\n",
      "36263\n",
      "---\n",
      "36279\n",
      "---\n",
      "36320\n",
      "---\n",
      "36324\n",
      "---\n",
      "36331\n",
      "---\n",
      "36335\n",
      "---\n",
      "36384\n",
      "---\n",
      "36393\n",
      "---\n",
      "36398\n",
      "---\n",
      "36405\n",
      "---\n",
      "36436\n",
      "---\n",
      "36437\n",
      "---\n",
      "36440\n",
      "---\n",
      "36448\n",
      "---\n",
      "36455\n",
      "---\n",
      "36478\n",
      "0 13\n",
      "---\n",
      "36479\n",
      "---\n",
      "36483\n",
      "---\n",
      "36498\n",
      "---\n",
      "36503\n",
      "---\n",
      "36513\n",
      "---\n",
      "36518\n",
      "---\n",
      "36539\n",
      "---\n",
      "36550\n",
      "---\n",
      "36551\n",
      "---\n",
      "36554\n",
      "---\n",
      "36562\n",
      "---\n",
      "36564\n",
      "---\n",
      "36582\n",
      "---\n",
      "36593\n",
      "---\n",
      "36620\n",
      "---\n",
      "36622\n",
      "---\n",
      "36628\n",
      "---\n",
      "36638\n",
      "---\n",
      "36645\n",
      "---\n",
      "36646\n",
      "---\n",
      "36651\n",
      "---\n",
      "36652\n",
      "---\n",
      "36653\n",
      "---\n",
      "36655\n",
      "---\n",
      "36664\n",
      "---\n",
      "36676\n",
      "---\n",
      "36699\n",
      "---\n",
      "36732\n",
      "---\n",
      "36735\n",
      "---\n",
      "36739\n",
      "---\n",
      "36741\n",
      "---\n",
      "36743\n",
      "---\n",
      "36756\n",
      "---\n",
      "36768\n",
      "---\n",
      "36786\n",
      "---\n",
      "36821\n",
      "---\n",
      "36828\n",
      "---\n",
      "36841\n",
      "---\n",
      "36848\n",
      "---\n",
      "36855\n",
      "---\n",
      "36857\n",
      "---\n",
      "36862\n",
      "---\n",
      "36874\n",
      "---\n",
      "36878\n",
      "---\n",
      "36883\n",
      "---\n",
      "36888\n",
      "---\n",
      "36891\n",
      "---\n",
      "36895\n",
      "---\n",
      "36899\n",
      "---\n",
      "36929\n",
      "---\n",
      "36937\n",
      "---\n",
      "36939\n",
      "---\n",
      "36967\n",
      "---\n",
      "36969\n",
      "---\n",
      "36974\n",
      "---\n",
      "37009\n",
      "---\n",
      "37026\n",
      "---\n",
      "37045\n",
      "---\n",
      "37046\n",
      "---\n",
      "37052\n",
      "---\n",
      "37060\n",
      "---\n",
      "37076\n",
      "---\n",
      "37083\n",
      "---\n",
      "37107\n",
      "---\n",
      "37122\n",
      "---\n",
      "37144\n",
      "---\n",
      "37145\n",
      "---\n",
      "37153\n",
      "---\n",
      "37160\n",
      "---\n",
      "37164\n",
      "---\n",
      "37170\n",
      "---\n",
      "37174\n",
      "---\n",
      "37180\n",
      "---\n",
      "37181\n",
      "---\n",
      "37185\n",
      "---\n",
      "37188\n",
      "---\n",
      "37209\n",
      "---\n",
      "37213\n",
      "---\n",
      "37221\n",
      "---\n",
      "37231\n",
      "---\n",
      "37232\n",
      "---\n",
      "37242\n",
      "---\n",
      "37253\n",
      "0 13\n",
      "---\n",
      "37265\n",
      "---\n",
      "37269\n",
      "---\n",
      "37274\n",
      "---\n",
      "37283\n",
      "---\n",
      "37294\n",
      "---\n",
      "37300\n",
      "---\n",
      "37312\n",
      "---\n",
      "37316\n",
      "---\n",
      "37318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_features = make_features_all_papers_latex(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>sec_name</th>\n",
       "      <th>cite_dencity</th>\n",
       "      <th>num_cits</th>\n",
       "      <th>sec_pos</th>\n",
       "      <th>sec_av_cit_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>0.020093</td>\n",
       "      <td>[0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.277745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Overview</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>[0, 0, 1, 0, 0]</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.529221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Reader-Aware Salience Estimation</td>\n",
       "      <td>0.007557</td>\n",
       "      <td>[2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.201558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Summary Construction</td>\n",
       "      <td>0.032967</td>\n",
       "      <td>[2, 0, 0, 0, 0, 0, 3, 0, 1, 0]</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.585332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Data Description</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Background</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Data Collection</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Data Properties</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Dataset and Metrics</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.556650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Comparative Methods</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>[0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 0]</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.442059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Experimental Settings</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1]</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Results on Our Dataset</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Further Investigation of Our Framework</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>[0, 1, 0, 1, 0, 0, 0, 0]</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.294527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Case Study</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10164018</td>\n",
       "      <td>Conclusions</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>488</td>\n",
       "      <td>Introduction</td>\n",
       "      <td>0.008119</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.310827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>488</td>\n",
       "      <td>Random Field Models</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.603874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>488</td>\n",
       "      <td>RFM Estimation and Selection of the Informativ...</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.906590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>488</td>\n",
       "      <td>The Grammar</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>[1, 0, 0, 0]</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.101928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>488</td>\n",
       "      <td>Modelling the Grammar</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    paper_id                                           sec_name  cite_dencity  \\\n",
       "0   10164018                                       Introduction      0.020093   \n",
       "1   10164018                                           Overview      0.007519   \n",
       "2   10164018                   Reader-Aware Salience Estimation      0.007557   \n",
       "3   10164018                               Summary Construction      0.032967   \n",
       "4   10164018                                   Data Description      0.000000   \n",
       "5   10164018                                         Background      0.000000   \n",
       "6   10164018                                    Data Collection      0.000000   \n",
       "7   10164018                                    Data Properties      0.000000   \n",
       "8   10164018                                Dataset and Metrics      0.031250   \n",
       "9   10164018                                Comparative Methods      0.034091   \n",
       "10  10164018                              Experimental Settings      0.018018   \n",
       "11  10164018                             Results on Our Dataset      0.000000   \n",
       "12  10164018            Further Investigation of Our Framework       0.011834   \n",
       "13  10164018                                         Case Study      0.000000   \n",
       "14  10164018                                        Conclusions      0.000000   \n",
       "15       488                                       Introduction      0.008119   \n",
       "16       488                                Random Field Models      0.002364   \n",
       "17       488  RFM Estimation and Selection of the Informativ...      0.007491   \n",
       "18       488                                        The Grammar      0.008850   \n",
       "19       488                             Modelling the Grammar       0.000000   \n",
       "\n",
       "                                             num_cits   sec_pos  \\\n",
       "0   [0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.066667   \n",
       "1                                     [0, 0, 1, 0, 0]  0.133333   \n",
       "2   [2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.200000   \n",
       "3                      [2, 0, 0, 0, 0, 0, 3, 0, 1, 0]  0.266667   \n",
       "4                                              [0, 0]  0.333333   \n",
       "5             [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  0.400000   \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.466667   \n",
       "7                            [0, 0, 0, 0, 0, 0, 0, 0]  0.533333   \n",
       "8                                           [0, 1, 0]  0.600000   \n",
       "9                   [0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 0]  0.666667   \n",
       "10                           [0, 0, 0, 0, 0, 0, 1, 1]  0.733333   \n",
       "11                                       [0, 0, 0, 0]  0.800000   \n",
       "12                           [0, 1, 0, 1, 0, 0, 0, 0]  0.866667   \n",
       "13                              [0, 0, 0, 0, 0, 0, 0]  0.933333   \n",
       "14                                       [0, 0, 0, 0]  1.000000   \n",
       "15  [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, ...  0.100000   \n",
       "16  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  0.200000   \n",
       "17  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.300000   \n",
       "18                                       [1, 0, 0, 0]  0.400000   \n",
       "19  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.500000   \n",
       "\n",
       "    sec_av_cit_pos  \n",
       "0         0.277745  \n",
       "1         0.529221  \n",
       "2         0.201558  \n",
       "3         0.585332  \n",
       "4         0.000000  \n",
       "5         0.000000  \n",
       "6         0.000000  \n",
       "7         0.000000  \n",
       "8         0.556650  \n",
       "9         0.442059  \n",
       "10        0.868421  \n",
       "11        0.000000  \n",
       "12        0.294527  \n",
       "13        0.000000  \n",
       "14        0.000000  \n",
       "15        0.310827  \n",
       "16        0.603874  \n",
       "17        0.906590  \n",
       "18        0.101928  \n",
       "19        0.000000  "
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_latex_features = pd.DataFrame(latex_features)\n",
    "df_latex_features.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### мы должны выделить только обзорную часть из текста, а все остальноё сохранить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers[all_articles[0]['paper_id']]['grobid_parse'] = {'abstract':all_articles[0]['grobid_parse']['abstract'],\n",
    "                                                                'overview_text':grobid_parse_overview,  \n",
    "                                                                'bib_entries':all_articles[0]['grobid_parse']['bib_entries']}\n",
    "overview_papers[all_articles[0]['paper_id']]['latex_parse'] =  {'abstract':all_articles[0]['latex_parse']['abstract'],\n",
    "                                                                'overview_text':latex_parse_overview,  \n",
    "                                                                'bib_entries':all_articles[0]['latex_parse']['bib_entries']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10164018': {'paper_id': '10164018',\n",
       "  'metadata': {'title': 'Reader-Aware Multi-Document Summarization: An Enhanced Model and The First Dataset',\n",
       "   'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "    {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "    {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''}],\n",
       "   'abstract': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online.',\n",
       "   'year': '2017',\n",
       "   'arxiv_id': '1708.01065',\n",
       "   'acl_id': 'W17-4512',\n",
       "   'pmc_id': None,\n",
       "   'pubmed_id': None,\n",
       "   'doi': '10.18653/v1/w17-4512',\n",
       "   'venue': 'ArXiv',\n",
       "   'journal': 'ArXiv'},\n",
       "  's2_pdf_hash': '326c27877d7ed9425547c4e40093d423911d6e5e',\n",
       "  'grobid_parse': {'abstract': [{'text': 'AbstractWe investigate the problem of readeraware multi-document summarization (RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. To conduct evaluation for summarization performance, we prepare a new dataset. We describe the methods for data collection, aspect annotation, and summary writing as well as scrutinizing by experts. Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the proposed dataset. The annotated dataset for RA-MDS is available online 1 .',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Abstract'}],\n",
       "   'overview_text': {0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "     'cite_spans': [{'start': 192,\n",
       "       'end': 216,\n",
       "       'text': '(Goldstein et al., 2000;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 217,\n",
       "       'end': 239,\n",
       "       'text': 'Erkan and Radev, 2004;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 240,\n",
       "       'end': 257,\n",
       "       'text': 'Wan et al., 2007;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF19'},\n",
       "      {'start': 258,\n",
       "       'end': 284,\n",
       "       'text': 'Nenkova and McKeown, 2012;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF16'},\n",
       "      {'start': 285,\n",
       "       'end': 302,\n",
       "       'text': 'Min et al., 2012;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF15'},\n",
       "      {'start': 303,\n",
       "       'end': 319,\n",
       "       'text': 'Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 773,\n",
       "       'end': 797,\n",
       "       'text': '(Project Code: 14203414)',\n",
       "       'latex': None,\n",
       "       'ref_id': None},\n",
       "      {'start': 2288,\n",
       "       'end': 2305,\n",
       "       'text': '(Hu et al., 2008;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF7'},\n",
       "      {'start': 2306,\n",
       "       'end': 2324,\n",
       "       'text': 'Yang et al., 2011)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF22'},\n",
       "      {'start': 2582,\n",
       "       'end': 2598,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 2911,\n",
       "       'end': 2927,\n",
       "       'text': 'Li et al. (2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 3069,\n",
       "       'end': 3095,\n",
       "       'text': '(Kingma and Welling, 2014;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 3096,\n",
       "       'end': 3117,\n",
       "       'text': 'Rezende et al., 2014)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'}],\n",
       "     'ref_spans': [{'start': 956,\n",
       "       'end': 964,\n",
       "       'text': 'Figure 1',\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF0'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    1: {'text': 'As shown in Figure 2 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset X d and X c consisting of n d news sentences and n c comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum (Li et al., 2017) , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value ρ i for a comment sentence x i c . The comment weight ρ ∈ R nc is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "     'cite_spans': [{'start': 451,\n",
       "       'end': 468,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     'ref_spans': [{'start': 12,\n",
       "       'end': 20,\n",
       "       'text': 'Figure 2',\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF2'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    2: {'text': 'Variational Autoencoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014 ) is a generative model based on neural networks which can be used to conduct latent semantic modeling. Li et al. (2017) employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., p θ (z) = N (0, I) and q φ (z|x) = N (z; µ, σ 2 I), where µ and σ denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are de- picted as follows:h enc = relu(W xh x + b xh ) µ = W hµ h enc + b hµ log(σ 2 ) = W hσ h enc + b hσ ε ∼ N (0, I), z = µ + σ ⊗ ε h dec = relu(W zh z + b zh ) x = sigmoid(W hx h dec + b hx )(1)Based on the reparameterization trick in Equation 1, we can get the analytical representation of the variational lower bound L(θ, ϕ; x):log p(x|z) = |V | i=1 xi log x i + (1 − xi) · log(1 − x i ) −DKL[qϕ(z|x) p θ (z)]= 1 2 K i=1 (1 + log(σ 2 i ) − µ 2 i − σ 2 i )where x denotes a general sentence, and it can be a news sentence x d or a comment sentnece x c .By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound L(θ, ϕ; x) into two parts and fuse them using the comment weight ρ:L(θ, ϕ; x) = L(θ, ϕ; x d ) + ρ × L(θ, ϕ; x c ) (2)The calculation of ρ will be discussed later.The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that S z = {s 1 z , s 2 z , · · · , s m z } are m latent aspect vectors used for reconstructing all the latent semantic vectors Z = {z 1 , z 2 , · · · , z n }. Thereafter, the variationaldecoding progress of VAEs can map the latent aspect vector S z to S h , and then produce m new aspect term vectors S x :s h = relu(W zh s z + b zh ) s x = sigmoid(W hx s h + b hx )(3)VAESum (Li et al., 2017) employs an alignment mechanism (Bahdanau et al., 2015; Luong et al., 2015) to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state s i h , we align it with each news encoder hidden state h j dby an alignment vector a d ∈ R n d . We also align it with each comments encoder hidden state h j c by an alignment vector a c ∈ R nc . In order to filter the noisy information from the comments, we again employ the comment weight ρ to adjust the alignment vector of comments:a c = a c × ρ(4)The news-based context vector c i d and the comment-based context vector c i c can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors:s i h = tanh(W h dh c i d + W h ch c i c + W a hh s i h )(5)Then we can generate the updated output aspect vectors based ons i h . We add a similar alignment mechanism into the output layer.S z , S h , and S x can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let A d ∈ R n d ×m be the reconstruction coefficient matrix for news sentences, and A c ∈ R nc×m be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively:L A = ( Z d − A d S z 2 2 + H d − A d S h 2 2 + X d − A d S x 2 2 ) + ρ × ( Z c − A c S z 2 2 + H c − A c S h 2 2 + X c − A c S x 2 2 ) (6)This objective is integrated with the variational lower bound of VAEs L(θ, ϕ; x) and optimized in a multi-task learning fashion. Then the new optimization objective is:J = min Θ (−L(θ, ϕ; x)+L A )(7)where Θ is a set of all the parameters related to this task. We define the magnitude of each row of A d as the salience scores for the corresponding news sentences.We should note that the most important variable in our framework is the comment weight vector ρ, which appears in all the three components of our framework. The basic idea for calculating ρ is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences X d and all the comment sentences X c , calculate the relation matrix R ∈ R n d ×nc by:R = X d × X T c (8)Then we add an average pooling layer to get the coefficient value for each comment sentence:r = 1 n c nc i=1 R[i, :](9)Finally, we add a sigmoid function to adjust the coefficient value to (0, 1):ρ = sigmoid(r)(10)Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter λ p :ρ = λ p × ρ z + (1 − λ p ) × ρ x(11)where ρ z and ρ x are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard ρ as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "     'cite_spans': [{'start': 32,\n",
       "       'end': 58,\n",
       "       'text': '(Kingma and Welling, 2014;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 59,\n",
       "       'end': 79,\n",
       "       'text': 'Rezende et al., 2014',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'},\n",
       "      {'start': 184,\n",
       "       'end': 200,\n",
       "       'text': 'Li et al. (2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 2384,\n",
       "       'end': 2401,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 2433,\n",
       "       'end': 2456,\n",
       "       'text': '(Bahdanau et al., 2015;',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF0'},\n",
       "      {'start': 2457,\n",
       "       'end': 2476,\n",
       "       'text': 'Luong et al., 2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF13'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    3: {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in and Li et al. (2015) , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem:max{ i α i S i − i<j α ij (S i + S j )R ij },(12)where α i is the selection indicator for the phrase P i , S i is the salience scores of P i , α ij and R ij is co-occurrence indicator and the similarity a pair of phrases (P i , P j ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to Woodsend and Lapata (2012), , and Li et al. (2015) . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms (Dantzig and Thapa, 2006) . In the implementation, we use a package called lp solve 5 .',\n",
       "     'cite_spans': [{'start': 86,\n",
       "       'end': 102,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 971,\n",
       "       'end': 987,\n",
       "       'text': 'Li et al. (2015)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 1133,\n",
       "       'end': 1158,\n",
       "       'text': '(Dantzig and Thapa, 2006)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF3'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    4: {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    5: {'text': 'The definition of the terminology related to the dataset is given as follows. 6 Topic: A topic refers to an event and it is composed of a set of news documents from different sources. Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days. Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other). Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category \"Accidents and Natural Disasters\", the aspects are \"WHAT\", \"WHEN\", \"WHERE\", \"WHY\", \"WHO AFFECTED\", \"DAMAGES\", and \"COUNTERMEASURES\". Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic \"Malaysia Airlines Disappearance\" as an example, facets for the aspect \"WHAT\" include \"missing Malaysia Airlines Flight 370\", \"two passengers used passports stolen in Thailand from an Austrian and an Italian.\" etc. Facets for the aspect \"WHEN\" are \" Saturday morning\", 5 http://lpsolve.sourceforge.net/5.5/ 6 In fact, for the core terminology, namely, topic, document, category, and aspect, we follow the MDS task in TAC (https://tac.nist.gov/ /2011/Summarization/Guided-Summ.2011. guidelines.html).\"about an hour into its flight from Kuala Lumpur\", etc. Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    6: {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 7 .For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    7: {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are \"Malaysia Airlines Disappearance\", \"Flappy Bird\", \"Bitcoin Mt. Gox\", etc. All the topics and categories are listed in Appendix A. Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    8: {'text': 'The properties of our own dataset are depicted in Section 3.3. We use ROUGE score as our evaluation metric (Lin, 2004) with standard options 8 . Fmeasures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "     'cite_spans': [{'start': 107,\n",
       "       'end': 118,\n",
       "       'text': '(Lin, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF12'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    9: {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:• RA-Sparse : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.• Lead (Wasson, 1998) : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.• Centroid (Radev et al., 2000) : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.• LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.• Concept : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "     'cite_spans': [{'start': 351,\n",
       "       'end': 365,\n",
       "       'text': '(Wasson, 1998)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF20'},\n",
       "      {'start': 492,\n",
       "       'end': 512,\n",
       "       'text': '(Radev et al., 2000)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF17'},\n",
       "      {'start': 700,\n",
       "       'end': 723,\n",
       "       'text': '(Erkan and Radev, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 737,\n",
       "       'end': 763,\n",
       "       'text': '(Mihalcea and Tarau, 2004)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF14'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    10: {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension |V |. The dictionary V is created using unigrams, bigrams and named entity terms. n d and n c are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let m = 5. For the neural network framework, we set the hidden size d h = 500 and the latent size K = 100. For the parameter λ p used in comment weight, we let λ p = 0.2. Adam (Kingma and Ba, 2014 ) is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano (Bastien et al., 2012) on a single GPU 9 .',\n",
       "     'cite_spans': [{'start': 498,\n",
       "       'end': 518,\n",
       "       'text': '(Kingma and Ba, 2014',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF8'},\n",
       "      {'start': 652,\n",
       "       'end': 674,\n",
       "       'text': '(Bastien et al., 2012)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    11: {'text': 'The results of our framework as well as the baseline methods are depicted in Table 1 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly (p < 0.05), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments. To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum (Li et al., 2017) . The evaluation results are shown in Table 2 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly(p < 0.05). Moreover, as mentioned in VAESum (Li et al., 2017) , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event \"Sony Virtual Reality PS4\", and \"\\'Bitcoin Mt. Gox Offlile\"\\' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table 3 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic \"Sony Virtual Reality PS4\", many readers talked about the product of \"Oculus\", hence the word \"oculus\" is assigned a high salience by our model.',\n",
       "     'cite_spans': [{'start': 690,\n",
       "       'end': 707,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 908,\n",
       "       'end': 925,\n",
       "       'text': '(Li et al., 2017)',\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     'ref_spans': [{'start': 77,\n",
       "       'end': 84,\n",
       "       'text': 'Table 1',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF0'},\n",
       "      {'start': 746,\n",
       "       'end': 753,\n",
       "       'text': 'Table 2',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF1'},\n",
       "      {'start': 1184,\n",
       "       'end': 1191,\n",
       "       'text': 'Table 3',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF2'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    12: {'text': 'Based on the news and comments of the topic \"Sony Virtual Reality PS4\", we generate two summaries with our model considering comments (RAVAESum) and ignoring comments 9 Tesla K80, 1 Kepler GK210 is used, 2496 Cuda cores, 12G GDDR5 memory.(RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table 4 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about \"Oculus\", the other product with virtual reality techniques. This issue is well identified by our model and select the sentence \"Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.\".',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 315,\n",
       "       'end': 322,\n",
       "       'text': 'Table 4',\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF3'}],\n",
       "     'eq_spans': [],\n",
       "     'section': None},\n",
       "    13: {'text': 'We investigate the problem of reader-aware multidocument summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset. Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world ',\n",
       "     'cite_spans': [{'start': 517,\n",
       "       'end': 868,\n",
       "       'text': 'Sony, headset, game, virtual, morpheus, reality, vr, project, playstation, Yoshida +C Sony, game, vr, virtual, headset, reality, morpheus, oculus, project, playstation \"Bitcoin Mt. Gox Offlile\" −C bitcoin, gox, exchange, mt., currency, Gox, virtual, company, money, price +C bitcoin, currency, money, exchange, gox, mt., virtual, company, price, world',\n",
       "       'latex': None,\n",
       "       'ref_id': None}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': None}},\n",
       "   'bib_entries': {'BIBREF0': {'ref_id': 'b0',\n",
       "     'title': 'Neural machine translation by jointly learning to align and translate',\n",
       "     'authors': [{'first': 'Dzmitry',\n",
       "       'middle': [],\n",
       "       'last': 'Bahdanau',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kyunghyun', 'middle': [], 'last': 'Cho', 'suffix': ''},\n",
       "      {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '11212020'},\n",
       "    'BIBREF1': {'ref_id': 'b1',\n",
       "     'title': 'Theano: new features and speed improvements',\n",
       "     'authors': [{'first': 'Frédéric',\n",
       "       'middle': [],\n",
       "       'last': 'Bastien',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Pascal', 'middle': [], 'last': 'Lamblin', 'suffix': ''},\n",
       "      {'first': 'Razvan', 'middle': [], 'last': 'Pascanu', 'suffix': ''},\n",
       "      {'first': 'James', 'middle': [], 'last': 'Bergstra', 'suffix': ''},\n",
       "      {'first': 'Ian', 'middle': [], 'last': 'Goodfellow', 'suffix': ''},\n",
       "      {'first': 'Arnaud', 'middle': [], 'last': 'Bergeron', 'suffix': ''},\n",
       "      {'first': 'Nicolas', 'middle': [], 'last': 'Bouchard', 'suffix': ''},\n",
       "      {'first': 'David', 'middle': [], 'last': 'Warde-Farley', 'suffix': ''},\n",
       "      {'first': 'Yoshua', 'middle': [], 'last': 'Bengio', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {'arXiv': ['arXiv:1211.5590']},\n",
       "     'links': '8180128'},\n",
       "    'BIBREF2': {'ref_id': 'b2',\n",
       "     'title': 'Abstractive multidocument summarization via phrase selection and merging',\n",
       "     'authors': [{'first': 'Lidong',\n",
       "       'middle': [],\n",
       "       'last': 'Bing',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "      {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1587--1597',\n",
       "     'other_ids': {},\n",
       "     'links': '8377315'},\n",
       "    'BIBREF3': {'ref_id': 'b3',\n",
       "     'title': 'Linear programming 1: introduction',\n",
       "     'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "      {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "     'year': 2006,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '53739754'},\n",
       "    'BIBREF4': {'ref_id': 'b4',\n",
       "     'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "     'authors': [{'first': 'Günes',\n",
       "       'middle': [],\n",
       "       'last': 'Erkan',\n",
       "       'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '4',\n",
       "     'issn': '',\n",
       "     'pages': '365--371',\n",
       "     'other_ids': {},\n",
       "     'links': '10418456'},\n",
       "    'BIBREF5': {'ref_id': 'b5',\n",
       "     'title': \"A virtual reality headset that's coming to the PlayStation 4. Sony showed off a prototype device V called Project Morpheus V that can be worn to create a virtual reality experience when playing games on its new PlayStation 4 console. Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve\",\n",
       "     'authors': [{'first': 'Shuhei',\n",
       "       'middle': [],\n",
       "       'last': 'Yoshida',\n",
       "       'suffix': ''}],\n",
       "     'year': None,\n",
       "     'venue': 'president of Sony Computer Entertainment, revealed a prototype of Morpheus at the Game Developers Conference in San Francisco on Tuesday',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': None},\n",
       "    'BIBREF6': {'ref_id': 'b6',\n",
       "     'title': 'Multi-document summarization by sentence extraction',\n",
       "     'authors': [{'first': 'Jade',\n",
       "       'middle': [],\n",
       "       'last': 'Goldstein',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "      {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "      {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'NAACLANLPWorkshop',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '40--48',\n",
       "     'other_ids': {},\n",
       "     'links': '8294822'},\n",
       "    'BIBREF7': {'ref_id': 'b7',\n",
       "     'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "     'authors': [{'first': 'Meishan',\n",
       "       'middle': [],\n",
       "       'last': 'Hu',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "      {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "     'year': 2008,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '291--298',\n",
       "     'other_ids': {},\n",
       "     'links': '13723748'},\n",
       "    'BIBREF8': {'ref_id': 'b8',\n",
       "     'title': 'Adam: A method for stochastic optimization',\n",
       "     'authors': [{'first': 'Diederik',\n",
       "       'middle': [],\n",
       "       'last': 'Kingma',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '6628106'},\n",
       "    'BIBREF9': {'ref_id': 'b9',\n",
       "     'title': 'Autoencoding variational bayes',\n",
       "     'authors': [{'first': 'P',\n",
       "       'middle': [],\n",
       "       'last': 'Diederik',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': None},\n",
       "    'BIBREF10': {'ref_id': 'b10',\n",
       "     'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1270--1276',\n",
       "     'other_ids': {},\n",
       "     'links': '14777460'},\n",
       "    'BIBREF11': {'ref_id': 'b11',\n",
       "     'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "     'year': 2017,\n",
       "     'venue': 'AAAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '3497--3503',\n",
       "     'other_ids': {},\n",
       "     'links': '29562039'},\n",
       "    'BIBREF12': {'ref_id': 'b12',\n",
       "     'title': 'Rouge: A package for automatic evaluation of summaries',\n",
       "     'authors': [{'first': 'Chin-Yew',\n",
       "       'middle': [],\n",
       "       'last': 'Lin',\n",
       "       'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'Text summarization branches out: Proceedings of the ACL-04 workshop',\n",
       "     'volume': '8',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '964287'},\n",
       "    'BIBREF13': {'ref_id': 'b13',\n",
       "     'title': 'Effective approaches to attentionbased neural machine translation',\n",
       "     'authors': [{'first': 'Minh-Thang',\n",
       "       'middle': [],\n",
       "       'last': 'Luong',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "      {'first': 'Christopher D',\n",
       "       'middle': [],\n",
       "       'last': 'Manning',\n",
       "       'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1412--1421',\n",
       "     'other_ids': {},\n",
       "     'links': '1998416'},\n",
       "    'BIBREF14': {'ref_id': 'b14',\n",
       "     'title': 'Textrank: Bringing order into texts',\n",
       "     'authors': [{'first': 'Rada',\n",
       "       'middle': [],\n",
       "       'last': 'Mihalcea',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '577937'},\n",
       "    'BIBREF15': {'ref_id': 'b15',\n",
       "     'title': 'Exploiting category-specific information for multidocument summarization',\n",
       "     'authors': [{'first': 'Yen',\n",
       "       'middle': ['Kan'],\n",
       "       'last': 'Ziheng Lin Min',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'COLING',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '2093--2108',\n",
       "     'other_ids': {},\n",
       "     'links': '6317274'},\n",
       "    'BIBREF16': {'ref_id': 'b16',\n",
       "     'title': 'A survey of text summarization techniques',\n",
       "     'authors': [{'first': 'Ani',\n",
       "       'middle': [],\n",
       "       'last': 'Nenkova',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'Mining Text Data',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '43--76',\n",
       "     'other_ids': {},\n",
       "     'links': '556431'},\n",
       "    'BIBREF17': {'ref_id': 'b17',\n",
       "     'title': 'Centroid-based summarization of multiple documents: sentence extraction, utilitybased evaluation, and user studies',\n",
       "     'authors': [{'first': 'Hongyan',\n",
       "       'middle': [],\n",
       "       'last': 'Dragomir R Radev',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '21--30',\n",
       "     'other_ids': {},\n",
       "     'links': '1320'},\n",
       "    'BIBREF18': {'ref_id': 'b18',\n",
       "     'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "     'authors': [{'first': 'Danilo',\n",
       "       'middle': [],\n",
       "       'last': 'Jimenez Rezende',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "      {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICML',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1278--1286',\n",
       "     'other_ids': {},\n",
       "     'links': '16895865'},\n",
       "    'BIBREF19': {'ref_id': 'b19',\n",
       "     'title': 'Manifold-ranking based topic-focused multidocument summarization',\n",
       "     'authors': [{'first': 'Xiaojun',\n",
       "       'middle': [],\n",
       "       'last': 'Wan',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jianwu', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Jianguo', 'middle': [], 'last': 'Xiao', 'suffix': ''}],\n",
       "     'year': 2007,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '7',\n",
       "     'issn': '',\n",
       "     'pages': '2903--2908',\n",
       "     'other_ids': {},\n",
       "     'links': '532313'},\n",
       "    'BIBREF20': {'ref_id': 'b20',\n",
       "     'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "     'authors': [{'first': 'Mark',\n",
       "       'middle': [],\n",
       "       'last': 'Wasson',\n",
       "       'suffix': ''}],\n",
       "     'year': 1998,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1364--1368',\n",
       "     'other_ids': {},\n",
       "     'links': '12681629'},\n",
       "    'BIBREF21': {'ref_id': 'b21',\n",
       "     'title': 'Multiple aspect summarization using integer linear programming',\n",
       "     'authors': [{'first': 'Kristian',\n",
       "       'middle': [],\n",
       "       'last': 'Woodsend',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'EMNLP-CNLL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '233--243',\n",
       "     'other_ids': {},\n",
       "     'links': '17497992'},\n",
       "    'BIBREF22': {'ref_id': 'b22',\n",
       "     'title': 'Social context summarization',\n",
       "     'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "      {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "      {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "      {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "      {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "     'year': 2011,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '255--264',\n",
       "     'other_ids': {},\n",
       "     'links': '704517'}}},\n",
       "  'latex_parse': {'abstract': [],\n",
       "   'overview_text': {'Introduction': {'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "      \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "      'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "      'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "      'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "      'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       "     'cite_spans': [[{'start': 193,\n",
       "        'end': 200,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF0'},\n",
       "       {'start': 203,\n",
       "        'end': 210,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF1'},\n",
       "       {'start': 213,\n",
       "        'end': 220,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF2'},\n",
       "       {'start': 223,\n",
       "        'end': 230,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF3'},\n",
       "       {'start': 233,\n",
       "        'end': 240,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF4'},\n",
       "       {'start': 243,\n",
       "        'end': 250,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 253,\n",
       "        'end': 260,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [],\n",
       "      [{'start': 527,\n",
       "        'end': 534,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF7'},\n",
       "       {'start': 537,\n",
       "        'end': 544,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF8'},\n",
       "       {'start': 802,\n",
       "        'end': 809,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 10,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'},\n",
       "       {'start': 159,\n",
       "        'end': 167,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF10'},\n",
       "       {'start': 170,\n",
       "        'end': 178,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF11'}],\n",
       "      [],\n",
       "      []],\n",
       "     'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       "     'section': ['Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction',\n",
       "      'Introduction'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Overview': {'text': ['As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.'],\n",
       "     'cite_spans': [[{'start': 489,\n",
       "        'end': 496,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}]],\n",
       "     'cite_span_lens': [1],\n",
       "     'section': ['Overview'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Reader-Aware Salience Estimation': {'text': ['Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "      'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "      'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "      'The calculation of INLINEFORM0 will be discussed later.',\n",
       "      'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "      'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "      'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "      'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "      ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "      'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "      'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "      'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "      'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.'],\n",
       "     'cite_spans': [[{'start': 32,\n",
       "        'end': 40,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF10'},\n",
       "       {'start': 43,\n",
       "        'end': 51,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF11'},\n",
       "       {'start': 154,\n",
       "        'end': 161,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [{'start': 7,\n",
       "        'end': 14,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'},\n",
       "       {'start': 46,\n",
       "        'end': 54,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF12'},\n",
       "       {'start': 57,\n",
       "        'end': 65,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF13'}],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      [],\n",
       "      []],\n",
       "     'cite_span_lens': [3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "     'section': ['Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation',\n",
       "      'Reader-Aware Salience Estimation'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Summary Construction': {'text': ['In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "      'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.'],\n",
       "     'cite_spans': [[{'start': 82,\n",
       "        'end': 89,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 94,\n",
       "        'end': 101,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 466,\n",
       "        'end': 474,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF14'},\n",
       "       {'start': 477,\n",
       "        'end': 484,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'},\n",
       "       {'start': 491,\n",
       "        'end': 498,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'},\n",
       "       {'start': 644,\n",
       "        'end': 652,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF15'}]],\n",
       "     'cite_span_lens': [2, 4],\n",
       "     'section': ['Summary Construction', 'Summary Construction'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Description': {'text': ['In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Data Description'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Background': {'text': ['The definition of the terminology related to the dataset is given as follows.',\n",
       "      'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "      'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "      'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "      'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "      'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "      'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.'],\n",
       "     'cite_spans': [[], [], [], [], [], [], []],\n",
       "     'cite_span_lens': [0, 0, 0, 0, 0, 0, 0],\n",
       "     'section': ['Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background',\n",
       "      'Background'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Collection': {'text': ['The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "      'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "      'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "      'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.'],\n",
       "     'cite_spans': [[], [], [], []],\n",
       "     'cite_span_lens': [0, 0, 0, 0],\n",
       "     'section': ['Data Collection',\n",
       "      'Data Collection',\n",
       "      'Data Collection',\n",
       "      'Data Collection'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Data Properties': {'text': ['The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Data Properties'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Dataset and Metrics': {'text': ['The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.'],\n",
       "     'cite_spans': [[{'start': 113,\n",
       "        'end': 121,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF16'}]],\n",
       "     'cite_span_lens': [1],\n",
       "     'section': ['Dataset and Metrics'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Comparative Methods': {'text': ['To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "      'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "      'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "      'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "      'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "      'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "      'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.'],\n",
       "     'cite_spans': [[],\n",
       "      [{'start': 10,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF9'}],\n",
       "      [{'start': 5,\n",
       "        'end': 13,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF17'}],\n",
       "      [{'start': 9,\n",
       "        'end': 17,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF18'}],\n",
       "      [{'start': 8,\n",
       "        'end': 15,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF1'},\n",
       "       {'start': 29,\n",
       "        'end': 37,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF19'}],\n",
       "      [{'start': 8,\n",
       "        'end': 15,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF5'}],\n",
       "      []],\n",
       "     'cite_span_lens': [0, 1, 1, 1, 2, 1, 0],\n",
       "     'section': ['Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods',\n",
       "      'Comparative Methods'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Experimental Settings': {'text': ['The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.'],\n",
       "     'cite_spans': [[{'start': 557,\n",
       "        'end': 565,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF20'},\n",
       "       {'start': 697,\n",
       "        'end': 705,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF21'}]],\n",
       "     'cite_span_lens': [2],\n",
       "     'section': ['Experimental Settings'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Results on Our Dataset': {'text': ['The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Results on Our Dataset'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Further Investigation of Our Framework ': {'text': ['To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "      \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\"],\n",
       "     'cite_spans': [[{'start': 208,\n",
       "        'end': 215,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}],\n",
       "      [{'start': 33,\n",
       "        'end': 40,\n",
       "        'text': None,\n",
       "        'latex': None,\n",
       "        'ref_id': 'BIBREF6'}]],\n",
       "     'cite_span_lens': [1, 1],\n",
       "     'section': ['Further Investigation of Our Framework ',\n",
       "      'Further Investigation of Our Framework '],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Case Study': {'text': ['Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Case Study'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}},\n",
       "    'Conclusions': {'text': ['We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.'],\n",
       "     'cite_spans': [[]],\n",
       "     'cite_span_lens': [0],\n",
       "     'section': ['Conclusions'],\n",
       "     'bib_entries': {'abstract': [],\n",
       "      'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "        'cite_spans': [{'start': 193,\n",
       "          'end': 200,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF0'},\n",
       "         {'start': 203,\n",
       "          'end': 210,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 213,\n",
       "          'end': 220,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF2'},\n",
       "         {'start': 223,\n",
       "          'end': 230,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF3'},\n",
       "         {'start': 233,\n",
       "          'end': 240,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF4'},\n",
       "         {'start': 243,\n",
       "          'end': 250,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 253,\n",
       "          'end': 260,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 118,\n",
       "          'end': 125,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF2'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "        'cite_spans': [{'start': 527,\n",
       "          'end': 534,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF7'},\n",
       "         {'start': 537,\n",
       "          'end': 544,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF8'},\n",
       "         {'start': 802,\n",
       "          'end': 809,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 159,\n",
       "          'end': 167,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 170,\n",
       "          'end': 178,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Introduction'},\n",
       "       {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "        'cite_spans': [{'start': 489,\n",
       "          'end': 496,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 19,\n",
       "          'end': 26,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'FIGREF7'}],\n",
       "        'eq_spans': [{'start': 212,\n",
       "          'end': 223,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 228,\n",
       "          'end': 239,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 254,\n",
       "          'end': 265,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 285,\n",
       "          'end': 296,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 739,\n",
       "          'end': 750,\n",
       "          'text': 'ρ i ',\n",
       "          'latex': '\\\\rho _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 774,\n",
       "          'end': 785,\n",
       "          'text': '𝐱 c i ',\n",
       "          'latex': '\\\\mathbf {x}_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 807,\n",
       "          'end': 818,\n",
       "          'text': 'ρ∈ℝ n c  ',\n",
       "          'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Overview'},\n",
       "       {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 32,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF10'},\n",
       "         {'start': 43,\n",
       "          'end': 51,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF11'},\n",
       "         {'start': 154,\n",
       "          'end': 161,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "          'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 488,\n",
       "          'end': 499,\n",
       "          'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "          'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "          'ref_id': None},\n",
       "         {'start': 508,\n",
       "          'end': 519,\n",
       "          'text': 'μ',\n",
       "          'latex': '\\\\mu ',\n",
       "          'ref_id': None},\n",
       "         {'start': 524,\n",
       "          'end': 535,\n",
       "          'text': 'σ',\n",
       "          'latex': '\\\\sigma ',\n",
       "          'ref_id': None},\n",
       "         {'start': 799,\n",
       "          'end': 811,\n",
       "          'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 50,\n",
       "          'end': 56,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'EQREF9'}],\n",
       "        'eq_spans': [{'start': 131,\n",
       "          'end': 142,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 145,\n",
       "          'end': 157,\n",
       "          'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "          'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "          'ref_id': 'EQREF10'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': '𝐱',\n",
       "          'latex': '\\\\mathbf {x}',\n",
       "          'ref_id': None},\n",
       "         {'start': 76,\n",
       "          'end': 87,\n",
       "          'text': '𝐱 d ',\n",
       "          'latex': '\\\\mathbf {x}_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐱 c ',\n",
       "          'latex': '\\\\mathbf {x}_c',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 364,\n",
       "          'end': 375,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 430,\n",
       "          'end': 441,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 444,\n",
       "          'end': 456,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "          'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "          'ref_id': 'EQREF11'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 19,\n",
       "          'end': 30,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 113,\n",
       "          'end': 124,\n",
       "          'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "          'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 219,\n",
       "          'end': 230,\n",
       "          'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "          'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "          'ref_id': None},\n",
       "         {'start': 320,\n",
       "          'end': 331,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 335,\n",
       "          'end': 346,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "         {'start': 402,\n",
       "          'end': 413,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 416,\n",
       "          'end': 428,\n",
       "          'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "          'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "          'ref_id': 'EQREF12'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 7,\n",
       "          'end': 14,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'},\n",
       "         {'start': 46,\n",
       "          'end': 54,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF12'},\n",
       "         {'start': 57,\n",
       "          'end': 65,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF13'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 304,\n",
       "          'end': 315,\n",
       "          'text': 's h i ',\n",
       "          'latex': 's^i_{h}',\n",
       "          'ref_id': None},\n",
       "         {'start': 366,\n",
       "          'end': 377,\n",
       "          'text': 'h d j ',\n",
       "          'latex': 'h^j_{d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 401,\n",
       "          'end': 412,\n",
       "          'text': 'a d ∈ℝ n d  ',\n",
       "          'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "          'ref_id': None},\n",
       "         {'start': 472,\n",
       "          'end': 483,\n",
       "          'text': 'h c j ',\n",
       "          'latex': 'h^j_{c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 507,\n",
       "          'end': 518,\n",
       "          'text': 'a c ∈ℝ n c  ',\n",
       "          'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 616,\n",
       "          'end': 627,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 672,\n",
       "          'end': 684,\n",
       "          'text': 'a ˜ c =a c ×ρ',\n",
       "          'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "          'ref_id': 'EQREF13'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 30,\n",
       "          'end': 41,\n",
       "          'text': 'c d i ',\n",
       "          'latex': 'c_d^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 79,\n",
       "          'end': 90,\n",
       "          'text': 'c c i ',\n",
       "          'latex': 'c_c^i',\n",
       "          'ref_id': None},\n",
       "         {'start': 240,\n",
       "          'end': 252,\n",
       "          'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "          'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "          'ref_id': 'EQREF14'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 64,\n",
       "          'end': 75,\n",
       "          'text': 's ˜ h i ',\n",
       "          'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 1,\n",
       "          'end': 12,\n",
       "          'text': '𝐒 z ',\n",
       "          'latex': '\\\\mathbf {S}_z',\n",
       "          'ref_id': None},\n",
       "         {'start': 15,\n",
       "          'end': 26,\n",
       "          'text': '𝐒 h ',\n",
       "          'latex': '\\\\mathbf {S}_h',\n",
       "          'ref_id': None},\n",
       "         {'start': 33,\n",
       "          'end': 44,\n",
       "          'text': '𝐒 x ',\n",
       "          'latex': '\\\\mathbf {S}_x',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "          'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 297,\n",
       "          'end': 308,\n",
       "          'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "          'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "          'ref_id': None},\n",
       "         {'start': 569,\n",
       "          'end': 581,\n",
       "          'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "          'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "          'ref_id': 'EQREF15'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "          'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "          'ref_id': None},\n",
       "         {'start': 170,\n",
       "          'end': 182,\n",
       "          'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "          'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "          'ref_id': 'EQREF16'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'Θ',\n",
       "          'latex': '\\\\Theta ',\n",
       "          'ref_id': None},\n",
       "         {'start': 110,\n",
       "          'end': 121,\n",
       "          'text': '𝐀 d ',\n",
       "          'latex': '\\\\mathbf {A}_d',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 199,\n",
       "          'end': 210,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None},\n",
       "         {'start': 348,\n",
       "          'end': 359,\n",
       "          'text': 'X d ',\n",
       "          'latex': 'X_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 390,\n",
       "          'end': 401,\n",
       "          'text': 'X c ',\n",
       "          'latex': 'X_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 434,\n",
       "          'end': 445,\n",
       "          'text': 'R∈ℝ n d ×n c  ',\n",
       "          'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "          'ref_id': None},\n",
       "         {'start': 450,\n",
       "          'end': 462,\n",
       "          'text': 'R=X d ×X c T ',\n",
       "          'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "          'ref_id': 'EQREF17'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 93,\n",
       "          'end': 105,\n",
       "          'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "          'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "          'ref_id': 'EQREF18'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 70,\n",
       "          'end': 81,\n",
       "          'text': '(0,1)',\n",
       "          'latex': '(0,1)',\n",
       "          'ref_id': None},\n",
       "         {'start': 84,\n",
       "          'end': 96,\n",
       "          'text': 'ρ=sigmoid(𝐫)',\n",
       "          'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "          'ref_id': 'EQREF19'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 332,\n",
       "          'end': 343,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 346,\n",
       "          'end': 358,\n",
       "          'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "          'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "          'ref_id': 'EQREF20'}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'ρ z ',\n",
       "          'latex': '\\\\rho _z',\n",
       "          'ref_id': None},\n",
       "         {'start': 22,\n",
       "          'end': 33,\n",
       "          'text': 'ρ x ',\n",
       "          'latex': '\\\\rho _x',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'ρ',\n",
       "          'latex': '\\\\rho ',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Reader-Aware Salience Estimation'},\n",
       "       {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "        'cite_spans': [{'start': 82,\n",
       "          'end': 89,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 94,\n",
       "          'end': 101,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 485,\n",
       "          'end': 497,\n",
       "          'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "          'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "          'ref_id': 'EQREF22'}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "        'cite_spans': [{'start': 466,\n",
       "          'end': 474,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF14'},\n",
       "         {'start': 477,\n",
       "          'end': 484,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'},\n",
       "         {'start': 491,\n",
       "          'end': 498,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'},\n",
       "         {'start': 644,\n",
       "          'end': 652,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF15'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 6,\n",
       "          'end': 17,\n",
       "          'text': 'α i ',\n",
       "          'latex': '\\\\alpha _i',\n",
       "          'ref_id': None},\n",
       "         {'start': 60,\n",
       "          'end': 71,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 74,\n",
       "          'end': 85,\n",
       "          'text': 'S i ',\n",
       "          'latex': 'S_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 112,\n",
       "          'end': 123,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 126,\n",
       "          'end': 137,\n",
       "          'text': 'α ij ',\n",
       "          'latex': '\\\\alpha _{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 142,\n",
       "          'end': 153,\n",
       "          'text': 'R ij ',\n",
       "          'latex': 'R_{ij}',\n",
       "          'ref_id': None},\n",
       "         {'start': 220,\n",
       "          'end': 231,\n",
       "          'text': 'P i ',\n",
       "          'latex': 'P_i',\n",
       "          'ref_id': None},\n",
       "         {'start': 234,\n",
       "          'end': 245,\n",
       "          'text': 'P j ',\n",
       "          'latex': 'P_j',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Summary Construction'},\n",
       "       {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Description'},\n",
       "       {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Background'},\n",
       "       {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Collection'},\n",
       "       {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 222,\n",
       "          'end': 229,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF7'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Data Properties'},\n",
       "       {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "        'cite_spans': [{'start': 113,\n",
       "          'end': 121,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF16'}],\n",
       "        'ref_spans': [{'start': 58,\n",
       "          'end': 66,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'SECREF28'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Dataset and Metrics'},\n",
       "       {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "        'cite_spans': [{'start': 10,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF9'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "        'cite_spans': [{'start': 5,\n",
       "          'end': 13,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF17'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "        'cite_spans': [{'start': 9,\n",
       "          'end': 17,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF18'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF1'},\n",
       "         {'start': 29,\n",
       "          'end': 37,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF19'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "        'cite_spans': [{'start': 8,\n",
       "          'end': 15,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF5'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Comparative Methods'},\n",
       "       {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "        'cite_spans': [{'start': 557,\n",
       "          'end': 565,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF20'},\n",
       "         {'start': 697,\n",
       "          'end': 705,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF21'}],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [{'start': 94,\n",
       "          'end': 105,\n",
       "          'text': '|V|',\n",
       "          'latex': '|V|',\n",
       "          'ref_id': None},\n",
       "         {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "         {'start': 194,\n",
       "          'end': 205,\n",
       "          'text': 'n d ',\n",
       "          'latex': 'n_d',\n",
       "          'ref_id': None},\n",
       "         {'start': 210,\n",
       "          'end': 221,\n",
       "          'text': 'n c ',\n",
       "          'latex': 'n_c',\n",
       "          'ref_id': None},\n",
       "         {'start': 360,\n",
       "          'end': 371,\n",
       "          'text': 'm=5',\n",
       "          'latex': 'm = 5',\n",
       "          'ref_id': None},\n",
       "         {'start': 431,\n",
       "          'end': 442,\n",
       "          'text': 'd h =500',\n",
       "          'latex': 'd_h = 500',\n",
       "          'ref_id': None},\n",
       "         {'start': 463,\n",
       "          'end': 474,\n",
       "          'text': 'K=100',\n",
       "          'latex': 'K = 100',\n",
       "          'ref_id': None},\n",
       "         {'start': 495,\n",
       "          'end': 506,\n",
       "          'text': 'λ p ',\n",
       "          'latex': '\\\\lambda _p',\n",
       "          'ref_id': None},\n",
       "         {'start': 538,\n",
       "          'end': 549,\n",
       "          'text': 'λ p =0.2',\n",
       "          'latex': '\\\\lambda _p=0.2',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Experimental Settings'},\n",
       "       {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 83,\n",
       "          'end': 91,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF40'}],\n",
       "        'eq_spans': [{'start': 240,\n",
       "          'end': 251,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Results on Our Dataset'},\n",
       "       {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "        'cite_spans': [{'start': 208,\n",
       "          'end': 215,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 260,\n",
       "          'end': 268,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF42'}],\n",
       "        'eq_spans': [{'start': 380,\n",
       "          'end': 391,\n",
       "          'text': 'p<0.05',\n",
       "          'latex': 'p<0.05',\n",
       "          'ref_id': None}],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "        'cite_spans': [{'start': 33,\n",
       "          'end': 40,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'BIBREF6'}],\n",
       "        'ref_spans': [{'start': 305,\n",
       "          'end': 313,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF43'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Further Investigation of Our Framework '},\n",
       "       {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [{'start': 250,\n",
       "          'end': 258,\n",
       "          'text': None,\n",
       "          'latex': None,\n",
       "          'ref_id': 'TABREF45'}],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Case Study'},\n",
       "       {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "        'cite_spans': [],\n",
       "        'ref_spans': [],\n",
       "        'eq_spans': [],\n",
       "        'section': 'Conclusions'}],\n",
       "      'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "        'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF9',\n",
       "        'type': 'equation'},\n",
       "       'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "        'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "        'ref_id': 'EQREF10',\n",
       "        'type': 'equation'},\n",
       "       'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "        'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "        'ref_id': 'EQREF11',\n",
       "        'type': 'equation'},\n",
       "       'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "        'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "        'ref_id': 'EQREF12',\n",
       "        'type': 'equation'},\n",
       "       'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "        'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "        'ref_id': 'EQREF13',\n",
       "        'type': 'equation'},\n",
       "       'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "        'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "        'ref_id': 'EQREF14',\n",
       "        'type': 'equation'},\n",
       "       'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "        'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "        'ref_id': 'EQREF15',\n",
       "        'type': 'equation'},\n",
       "       'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "        'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "        'ref_id': 'EQREF16',\n",
       "        'type': 'equation'},\n",
       "       'EQREF17': {'text': 'R=X d ×X c T',\n",
       "        'latex': 'R = X_d\\\\times X_c^T',\n",
       "        'ref_id': 'EQREF17',\n",
       "        'type': 'equation'},\n",
       "       'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "        'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "        'ref_id': 'EQREF18',\n",
       "        'type': 'equation'},\n",
       "       'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "        'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "        'ref_id': 'EQREF19',\n",
       "        'type': 'equation'},\n",
       "       'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "        'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "        'ref_id': 'EQREF20',\n",
       "        'type': 'equation'},\n",
       "       'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "        'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "        'ref_id': 'EQREF22',\n",
       "        'type': 'equation'},\n",
       "       'FIGREF2': {'text': '1',\n",
       "        'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF2',\n",
       "        'type': 'figure'},\n",
       "       'FIGREF7': {'text': '2',\n",
       "        'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "        'latex': None,\n",
       "        'ref_id': 'FIGREF7',\n",
       "        'type': 'figure'},\n",
       "       'TABREF40': {'text': '1',\n",
       "        'caption': 'Summarization performance.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF40',\n",
       "        'type': 'table'},\n",
       "       'TABREF42': {'text': '2',\n",
       "        'caption': 'Further investigation of RAVAESum.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF42',\n",
       "        'type': 'table'},\n",
       "       'TABREF43': {'text': '3',\n",
       "        'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF43',\n",
       "        'type': 'table'},\n",
       "       'TABREF45': {'text': '4',\n",
       "        'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF45',\n",
       "        'type': 'table'},\n",
       "       'TABREF46': {'text': '5',\n",
       "        'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "        'latex': [],\n",
       "        'ref_id': 'TABREF46',\n",
       "        'type': 'table'},\n",
       "       'SECREF1': {'text': 'Introduction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF1',\n",
       "        'type': 'section'},\n",
       "       'SECREF2': {'text': 'Framework',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF2',\n",
       "        'type': 'section'},\n",
       "       'SECREF6': {'text': 'Conclusions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF6',\n",
       "        'type': 'section'},\n",
       "       'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF8',\n",
       "        'type': 'section'},\n",
       "       'SECREF21': {'text': 'Summary Construction',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF21',\n",
       "        'type': 'section'},\n",
       "       'SECREF3': {'text': 'Data Description',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF3',\n",
       "        'type': 'section'},\n",
       "       'SECREF24': {'text': 'Background',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF24',\n",
       "        'type': 'section'},\n",
       "       'SECREF26': {'text': 'Data Collection',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF26',\n",
       "        'type': 'section'},\n",
       "       'SECREF28': {'text': 'Data Properties',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF28',\n",
       "        'type': 'section'},\n",
       "       'SECREF4': {'text': 'Experimental Setup',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF4',\n",
       "        'type': 'section'},\n",
       "       'SECREF29': {'text': 'Dataset and Metrics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF29',\n",
       "        'type': 'section'},\n",
       "       'SECREF31': {'text': 'Comparative Methods',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF31',\n",
       "        'type': 'section'},\n",
       "       'SECREF37': {'text': 'Experimental Settings',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF37',\n",
       "        'type': 'section'},\n",
       "       'SECREF5': {'text': 'Results and Discussions',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF5',\n",
       "        'type': 'section'},\n",
       "       'SECREF39': {'text': 'Results on Our Dataset',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF39',\n",
       "        'type': 'section'},\n",
       "       'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF41',\n",
       "        'type': 'section'},\n",
       "       'SECREF44': {'text': 'Case Study',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF44',\n",
       "        'type': 'section'},\n",
       "       'SECREF7': {'text': 'Topics',\n",
       "        'latex': None,\n",
       "        'ref_id': 'SECREF7',\n",
       "        'type': 'section'}},\n",
       "      'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "        'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "        'authors': [{'first': 'Lidong',\n",
       "          'middle': [],\n",
       "          'last': 'Bing',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "         {'first': 'Rebecca',\n",
       "          'middle': [],\n",
       "          'last': 'Passonneau',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1587--1597',\n",
       "        'other_ids': {},\n",
       "        'links': '8377315'},\n",
       "       'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "        'title': 'Linear programming 1: introduction',\n",
       "        'authors': [{'first': 'B',\n",
       "          'middle': [],\n",
       "          'last': 'George',\n",
       "          'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "         {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "        'year': 2006,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '53739754'},\n",
       "       'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "        'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "        'authors': [{'first': 'Günes',\n",
       "          'middle': [],\n",
       "          'last': 'Erkan',\n",
       "          'suffix': ''},\n",
       "         {'first': '',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '4',\n",
       "        'issn': '',\n",
       "        'pages': '365--371',\n",
       "        'other_ids': {},\n",
       "        'links': '10418456'},\n",
       "       'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "        'title': 'Multi-document summarization by sentence extraction',\n",
       "        'authors': [{'first': 'Jade',\n",
       "          'middle': [],\n",
       "          'last': 'Goldstein',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "         {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "         {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'NAACL-ANLPWorkshop',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '40--48',\n",
       "        'other_ids': {},\n",
       "        'links': '8294822'},\n",
       "       'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "        'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "        'authors': [{'first': 'Meishan',\n",
       "          'middle': [],\n",
       "          'last': 'Hu',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "         {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "        'year': 2008,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '291--298',\n",
       "        'other_ids': {},\n",
       "        'links': '13723748'},\n",
       "       'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "        'title': 'Adam: A method for stochastic optimization',\n",
       "        'authors': [{'first': 'Diederik',\n",
       "          'middle': [],\n",
       "          'last': 'Kingma',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '6628106'},\n",
       "       'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "        'title': 'Auto-encoding variational bayes',\n",
       "        'authors': [{'first': 'P',\n",
       "          'middle': [],\n",
       "          'last': 'Diederik',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICLR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '15789289'},\n",
       "       'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "        'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "         {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'IJCAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1270--1276',\n",
       "        'other_ids': {},\n",
       "        'links': '14777460'},\n",
       "       'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "        'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "        'authors': [{'first': 'Piji',\n",
       "          'middle': [],\n",
       "          'last': 'Li',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "         {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "         {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "         {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "        'year': 2017,\n",
       "        'venue': 'AAAI',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '3497--3503',\n",
       "        'other_ids': {},\n",
       "        'links': '29562039'},\n",
       "       'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "        'title': 'Effective approaches to attention-based neural machine translation',\n",
       "        'authors': [{'first': 'Minh-Thang',\n",
       "          'middle': [],\n",
       "          'last': 'Luong',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "         {'first': 'Christopher D',\n",
       "          'middle': [],\n",
       "          'last': 'Manning',\n",
       "          'suffix': ''}],\n",
       "        'year': 2015,\n",
       "        'venue': 'EMNLP',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1412--1421',\n",
       "        'other_ids': {},\n",
       "        'links': '1998416'},\n",
       "       'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "        'title': 'Textrank: Bringing order into texts',\n",
       "        'authors': [{'first': 'Rada',\n",
       "          'middle': [],\n",
       "          'last': 'Mihalcea',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "        'year': 2004,\n",
       "        'venue': '',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '',\n",
       "        'other_ids': {},\n",
       "        'links': '577937'},\n",
       "       'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "        'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "        'authors': [{'first': 'Yen',\n",
       "          'middle': ['Kan'],\n",
       "          'last': 'Ziheng Lin Min',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'COLING',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '2093--2108',\n",
       "        'other_ids': {},\n",
       "        'links': '6317274'},\n",
       "       'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "        'title': 'A survey of text summarization techniques',\n",
       "        'authors': [{'first': 'Ani',\n",
       "          'middle': [],\n",
       "          'last': 'Nenkova',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'Mining Text Data',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '43--76',\n",
       "        'other_ids': {},\n",
       "        'links': '556431'},\n",
       "       'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "        'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "        'authors': [{'first': 'Hongyan',\n",
       "          'middle': [],\n",
       "          'last': 'Dragomir R Radev',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "         {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "        'year': 2000,\n",
       "        'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '21--30',\n",
       "        'other_ids': {},\n",
       "        'links': '1320'},\n",
       "       'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "        'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "        'authors': [{'first': 'Danilo',\n",
       "          'middle': [],\n",
       "          'last': 'Jimenez Rezende',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "         {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "        'year': 2014,\n",
       "        'venue': 'ICML',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1278--1286',\n",
       "        'other_ids': {},\n",
       "        'links': '16895865'},\n",
       "       'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "        'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "        'authors': [{'first': 'Mark',\n",
       "          'middle': [],\n",
       "          'last': 'Wasson',\n",
       "          'suffix': ''}],\n",
       "        'year': 1998,\n",
       "        'venue': 'ACL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '1364--1368',\n",
       "        'other_ids': {},\n",
       "        'links': '12681629'},\n",
       "       'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "        'title': 'Multiple aspect summarization using integer linear programming',\n",
       "        'authors': [{'first': 'Kristian',\n",
       "          'middle': [],\n",
       "          'last': 'Woodsend',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "        'year': 2012,\n",
       "        'venue': 'EMNLP-CNLL',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '233--243',\n",
       "        'other_ids': {},\n",
       "        'links': '17497992'},\n",
       "       'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "        'title': 'Social context summarization',\n",
       "        'authors': [{'first': 'Zi',\n",
       "          'middle': [],\n",
       "          'last': 'Yang',\n",
       "          'suffix': ''},\n",
       "         {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "         {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "         {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "         {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "         {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "        'year': 2011,\n",
       "        'venue': 'SIGIR',\n",
       "        'volume': '',\n",
       "        'issn': '',\n",
       "        'pages': '255--264',\n",
       "        'other_ids': {},\n",
       "        'links': '704517'}}}}},\n",
       "   'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "     'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "     'authors': [{'first': 'Lidong',\n",
       "       'middle': [],\n",
       "       'last': 'Bing',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "      {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1587--1597',\n",
       "     'other_ids': {},\n",
       "     'links': '8377315'},\n",
       "    'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "     'title': 'Linear programming 1: introduction',\n",
       "     'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "      {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "     'year': 2006,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '53739754'},\n",
       "    'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "     'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "     'authors': [{'first': 'Günes',\n",
       "       'middle': [],\n",
       "       'last': 'Erkan',\n",
       "       'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '4',\n",
       "     'issn': '',\n",
       "     'pages': '365--371',\n",
       "     'other_ids': {},\n",
       "     'links': '10418456'},\n",
       "    'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "     'title': 'Multi-document summarization by sentence extraction',\n",
       "     'authors': [{'first': 'Jade',\n",
       "       'middle': [],\n",
       "       'last': 'Goldstein',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "      {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "      {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'NAACL-ANLPWorkshop',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '40--48',\n",
       "     'other_ids': {},\n",
       "     'links': '8294822'},\n",
       "    'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "     'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "     'authors': [{'first': 'Meishan',\n",
       "       'middle': [],\n",
       "       'last': 'Hu',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "      {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "     'year': 2008,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '291--298',\n",
       "     'other_ids': {},\n",
       "     'links': '13723748'},\n",
       "    'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "     'title': 'Adam: A method for stochastic optimization',\n",
       "     'authors': [{'first': 'Diederik',\n",
       "       'middle': [],\n",
       "       'last': 'Kingma',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '6628106'},\n",
       "    'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "     'title': 'Auto-encoding variational bayes',\n",
       "     'authors': [{'first': 'P',\n",
       "       'middle': [],\n",
       "       'last': 'Diederik',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '15789289'},\n",
       "    'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "     'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1270--1276',\n",
       "     'other_ids': {},\n",
       "     'links': '14777460'},\n",
       "    'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "     'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "     'year': 2017,\n",
       "     'venue': 'AAAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '3497--3503',\n",
       "     'other_ids': {},\n",
       "     'links': '29562039'},\n",
       "    'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "     'title': 'Effective approaches to attention-based neural machine translation',\n",
       "     'authors': [{'first': 'Minh-Thang',\n",
       "       'middle': [],\n",
       "       'last': 'Luong',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "      {'first': 'Christopher D',\n",
       "       'middle': [],\n",
       "       'last': 'Manning',\n",
       "       'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1412--1421',\n",
       "     'other_ids': {},\n",
       "     'links': '1998416'},\n",
       "    'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "     'title': 'Textrank: Bringing order into texts',\n",
       "     'authors': [{'first': 'Rada',\n",
       "       'middle': [],\n",
       "       'last': 'Mihalcea',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '577937'},\n",
       "    'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "     'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "     'authors': [{'first': 'Yen',\n",
       "       'middle': ['Kan'],\n",
       "       'last': 'Ziheng Lin Min',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'COLING',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '2093--2108',\n",
       "     'other_ids': {},\n",
       "     'links': '6317274'},\n",
       "    'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "     'title': 'A survey of text summarization techniques',\n",
       "     'authors': [{'first': 'Ani',\n",
       "       'middle': [],\n",
       "       'last': 'Nenkova',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'Mining Text Data',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '43--76',\n",
       "     'other_ids': {},\n",
       "     'links': '556431'},\n",
       "    'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "     'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "     'authors': [{'first': 'Hongyan',\n",
       "       'middle': [],\n",
       "       'last': 'Dragomir R Radev',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '21--30',\n",
       "     'other_ids': {},\n",
       "     'links': '1320'},\n",
       "    'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "     'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "     'authors': [{'first': 'Danilo',\n",
       "       'middle': [],\n",
       "       'last': 'Jimenez Rezende',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "      {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICML',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1278--1286',\n",
       "     'other_ids': {},\n",
       "     'links': '16895865'},\n",
       "    'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "     'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "     'authors': [{'first': 'Mark',\n",
       "       'middle': [],\n",
       "       'last': 'Wasson',\n",
       "       'suffix': ''}],\n",
       "     'year': 1998,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1364--1368',\n",
       "     'other_ids': {},\n",
       "     'links': '12681629'},\n",
       "    'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "     'title': 'Multiple aspect summarization using integer linear programming',\n",
       "     'authors': [{'first': 'Kristian',\n",
       "       'middle': [],\n",
       "       'last': 'Woodsend',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'EMNLP-CNLL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '233--243',\n",
       "     'other_ids': {},\n",
       "     'links': '17497992'},\n",
       "    'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "     'title': 'Social context summarization',\n",
       "     'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "      {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "      {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "      {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "      {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "     'year': 2011,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '255--264',\n",
       "     'other_ids': {},\n",
       "     'links': '704517'}}}}}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Introduction 13\n",
      "1 Overview 1\n",
      "2 Reader-Aware Salience Estimation 6\n",
      "3 Summary Construction 6\n",
      "4 Data Description 0\n",
      "5 Background 0\n",
      "6 Data Collection 0\n",
      "7 Data Properties 0\n",
      "8 Dataset and Metrics 1\n",
      "9 Comparative Methods 6\n",
      "10 Experimental Settings 2\n",
      "11 Results on Our Dataset 0\n",
      "12 Further Investigation of Our Framework  2\n",
      "13 Case Study 0\n",
      "14 Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for num_tex,(k,v) in enumerate(latex_parse_overview.items()):\n",
    "    print(num_tex,k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 13\n",
      "1 1\n",
      "2 6\n",
      "3 3\n",
      "4 0\n",
      "5 0\n",
      "6 0\n",
      "7 0\n",
      "8 1\n",
      "9 4\n",
      "10 2\n",
      "11 2\n",
      "12 0\n",
      "13 1\n"
     ]
    }
   ],
   "source": [
    "for k,v in grobid_parse_overview.items():\n",
    "    print(k,len(v['cite_spans']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упорядочим по количеству ссылок и возьмём абзац с максимальным значением, а также со 2 максимальным значением если количество ссылок в нём больше половины от максимального кол-ва "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), \n",
    "                                                 key=lambda item: len(item[1]['cite_spans']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cite_span_sum = 0\n",
    "max_grobid_parse_overview = dict()\n",
    "for k,v in grobid_parse_overview.items():\n",
    "    if max_cite_span_sum < len(v['cite_spans']):\n",
    "        max_cite_span_sum = len(v['cite_spans'])\n",
    "        max_grobid_parse_overview[k] = v\n",
    "    elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "        max_grobid_parse_overview[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, wellorganized summary for a topic which describes an event with a set of documents from different sources. (Goldstein et al., 2000; Erkan and Radev, 2004; Wan et al., 2007; Nenkova and McKeown, 2012; Min et al., 2012; Li et al., 2017) . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.With the development of social media and mobile equipments, more and more user generated * The work described in this paper is supported by a grant from the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414) .1 http://www.se.cuhk.edu.hk/˜textmine/ dataset/ra-mds/ NEWS: The most important announcements from Google\\'s big developers\\' conference content is available. Figure 1 is a snapshot of reader comments under the news report \"The most important announcements from Google\\'s big developers\\' conference\" 2 . The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in infor-mal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization (Hu et al., 2008; Yang et al., 2011) . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, Li et al. (2015) employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.Recently, Li et al. (2017) proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) (Kingma and Welling, 2014; Rezende et al., 2014) . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC 3 and TAC 4 are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "  'cite_spans': [{'start': 192,\n",
       "    'end': 216,\n",
       "    'text': '(Goldstein et al., 2000;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF6'},\n",
       "   {'start': 217,\n",
       "    'end': 239,\n",
       "    'text': 'Erkan and Radev, 2004;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF4'},\n",
       "   {'start': 240,\n",
       "    'end': 257,\n",
       "    'text': 'Wan et al., 2007;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF19'},\n",
       "   {'start': 258,\n",
       "    'end': 284,\n",
       "    'text': 'Nenkova and McKeown, 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF16'},\n",
       "   {'start': 285,\n",
       "    'end': 302,\n",
       "    'text': 'Min et al., 2012;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF15'},\n",
       "   {'start': 303,\n",
       "    'end': 319,\n",
       "    'text': 'Li et al., 2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 773,\n",
       "    'end': 797,\n",
       "    'text': '(Project Code: 14203414)',\n",
       "    'latex': None,\n",
       "    'ref_id': None},\n",
       "   {'start': 2288,\n",
       "    'end': 2305,\n",
       "    'text': '(Hu et al., 2008;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF7'},\n",
       "   {'start': 2306,\n",
       "    'end': 2324,\n",
       "    'text': 'Yang et al., 2011)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF22'},\n",
       "   {'start': 2582,\n",
       "    'end': 2598,\n",
       "    'text': 'Li et al. (2015)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF2'},\n",
       "   {'start': 2911,\n",
       "    'end': 2927,\n",
       "    'text': 'Li et al. (2017)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF11'},\n",
       "   {'start': 3069,\n",
       "    'end': 3095,\n",
       "    'text': '(Kingma and Welling, 2014;',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF9'},\n",
       "   {'start': 3096,\n",
       "    'end': 3117,\n",
       "    'text': 'Rezende et al., 2014)',\n",
       "    'latex': None,\n",
       "    'ref_id': 'BIBREF18'}],\n",
       "  'ref_spans': [{'start': 956,\n",
       "    'end': 964,\n",
       "    'text': 'Figure 1',\n",
       "    'latex': None,\n",
       "    'ref_id': 'FIGREF0'}],\n",
       "  'eq_spans': [],\n",
       "  'section': None}}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_grobid_parse_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), \n",
    "                                                 key=lambda item: sum(item[1]['cite_span_lens']), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cite_span_sum = 0\n",
    "max_latex_parse_overview = dict()\n",
    "for k,v in latex_parse_overview.items():\n",
    "    if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "        max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "        max_latex_parse_overview[k] = v\n",
    "    elif (max_cite_span_sum>7) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "        max_latex_parse_overview[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Introduction': {'text': ['The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "   \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "   'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "   'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "   'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "   'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.'],\n",
       "  'cite_spans': [[{'start': 193,\n",
       "     'end': 200,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF0'},\n",
       "    {'start': 203,\n",
       "     'end': 210,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF1'},\n",
       "    {'start': 213,\n",
       "     'end': 220,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF2'},\n",
       "    {'start': 223,\n",
       "     'end': 230,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF3'},\n",
       "    {'start': 233,\n",
       "     'end': 240,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF4'},\n",
       "    {'start': 243,\n",
       "     'end': 250,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF5'},\n",
       "    {'start': 253,\n",
       "     'end': 260,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF6'}],\n",
       "   [],\n",
       "   [{'start': 527,\n",
       "     'end': 534,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF7'},\n",
       "    {'start': 537,\n",
       "     'end': 544,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF8'},\n",
       "    {'start': 802,\n",
       "     'end': 809,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF9'}],\n",
       "   [{'start': 10, 'end': 17, 'text': None, 'latex': None, 'ref_id': 'BIBREF6'},\n",
       "    {'start': 159,\n",
       "     'end': 167,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF10'},\n",
       "    {'start': 170,\n",
       "     'end': 178,\n",
       "     'text': None,\n",
       "     'latex': None,\n",
       "     'ref_id': 'BIBREF11'}],\n",
       "   [],\n",
       "   []],\n",
       "  'cite_span_lens': [7, 0, 3, 3, 0, 0],\n",
       "  'section': ['Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction',\n",
       "   'Introduction'],\n",
       "  'bib_entries': {'abstract': [],\n",
       "   'body_text': [{'text': 'The goal of multi-document summarization (MDS) is to automatically generate a brief, well-organized summary for a topic which describes an event with a set of documents from different sources. BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . In the typical setting of MDS, the input is a set of news documents about the same topic. The output summary is a piece of short text document containing several sentences, generated only based on the input original documents.',\n",
       "     'cite_spans': [{'start': 193,\n",
       "       'end': 200,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF0'},\n",
       "      {'start': 203,\n",
       "       'end': 210,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'},\n",
       "      {'start': 213,\n",
       "       'end': 220,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF2'},\n",
       "      {'start': 223,\n",
       "       'end': 230,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF3'},\n",
       "      {'start': 233,\n",
       "       'end': 240,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF4'},\n",
       "      {'start': 243,\n",
       "       'end': 250,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'},\n",
       "      {'start': 253,\n",
       "       'end': 260,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Introduction'},\n",
       "    {'text': \"With the development of social media and mobile equipments, more and more user generated content is available. Figure FIGREF2 is a snapshot of reader comments under the news report “The most important announcements from Google's big developers' conference”. The content of the original news report talks about some new products based on AI techniques. The news report generally conveys an enthusiastic tone. However, while some readers share similar enthusiasms, some others express their worries about new products and technologies and these comments can also reflect their interests which may not be very salient in the original news reports. Unfortunately, existing MDS approaches cannot handle this issue. We investigate this problem known as reader-aware multi-document summarization (RA-MDS). Under the RA-MDS setting, one should jointly consider news documents and reader comments when generating the summaries.\",\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 118,\n",
       "       'end': 125,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF2'}],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Introduction'},\n",
       "    {'text': 'One challenge of the RA-MDS problem is how to conduct salience estimation by jointly considering the focus of news reports and the reader interests revealed by comments. Meanwhile, the model should be insensitive to the availability of diverse aspects of reader comments. Another challenge is that reader comments are very noisy, not fully grammatical and often expressed in informal expressions. Some previous works explore the effect of comments or social contexts in single document summarization such as blog summarization BIBREF7 , BIBREF8 . However, the problem setting of RA-MDS is more challenging because the considered comments are about an event which is described by multiple documents spanning a time period. Another challenge is that reader comments are very diverse and noisy. Recently, BIBREF9 employed a sparse coding based framework for RA-MDS jointly considering news documents and reader comments via an unsupervised data reconstruction strategy. However, they only used the bag-of-words method to represent texts, which cannot capture the complex relationship between documents and comments.',\n",
       "     'cite_spans': [{'start': 527,\n",
       "       'end': 534,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF7'},\n",
       "      {'start': 537,\n",
       "       'end': 544,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF8'},\n",
       "      {'start': 802,\n",
       "       'end': 809,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Introduction'},\n",
       "    {'text': 'Recently, BIBREF6 proposed a sentence salience estimation framework known as VAESum based on a neural generative model called Variational Auto-Encoders (VAEs) BIBREF10 , BIBREF11 . During our investigation, we find that the Gaussian based VAEs have a strong ability to capture the salience information and filter the noise from texts. Intuitively, if we feed both the news sentences and the comment sentences into the VAEs, commonly existed latent aspect information from both of them will be enhanced and become salient. Inspired by this consideration, to address the sentence salience estimation problem for RA-MDS by jointly considering news documents and reader comments, we extend the VAESum framework by training the news sentence latent model and the comment sentence latent model simultaneously by sharing the neural parameters. After estimating the sentence salience, we employ a phrase based compressive unified optimization framework to generate a final summary.',\n",
       "     'cite_spans': [{'start': 10,\n",
       "       'end': 17,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 159,\n",
       "       'end': 167,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF10'},\n",
       "      {'start': 170,\n",
       "       'end': 178,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Introduction'},\n",
       "    {'text': 'There is a lack of high-quality dataset suitable for RA-MDS. Existing datasets from DUC and TAC are not appropriate. Therefore, we introduce a new dataset for RA-MDS. We employed some experts to conduct the tasks of data collection, aspect annotation, and summary writing as well as scrutinizing. To our best knowledge, this is the first dataset for RA-MDS.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Introduction'},\n",
       "    {'text': 'Our contributions are as follows: (1) We investigate the RA-MDS problem and introduce a new dataset for the problem of RA-MDS. To our best knowledge, it is the first dataset for RA-MDS. (2) To tackle the RA-MDS, we extend a VAEs-based MDS framework by jointly considering news documents and reader comments. (3) Experimental results show that reader comments can improve the summarization performance, which also demonstrates the usefulness of the dataset.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Introduction'},\n",
       "    {'text': 'As shown in Figure FIGREF7 , our reader-aware news sentence salience framework has three main components: (1) latent semantic modeling; (2) comment weight estimation; (3) joint reconstruction. Consider a dataset INLINEFORM0 and INLINEFORM1 consisting of INLINEFORM2 news sentences and INLINEFORM3 comment sentences respectively from all the documents in a topic (event), represented by bag-of-words vectors. Our proposed news sentence salience estimation framework is extended from VAESum BIBREF6 , which can jointly consider news documents and reader comments. One extension is that, in order to absorb more useful information and filter the noisy data from comments, we design a weight estimation mechanism which can assign a real value INLINEFORM4 for a comment sentence INLINEFORM5 . The comment weight INLINEFORM6 is integrated into the VAEs based sentence modeling and data reconstruction component to handle comments.',\n",
       "     'cite_spans': [{'start': 489,\n",
       "       'end': 496,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     'ref_spans': [{'start': 19,\n",
       "       'end': 26,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'FIGREF7'}],\n",
       "     'eq_spans': [{'start': 212,\n",
       "       'end': 223,\n",
       "       'text': 'X d ',\n",
       "       'latex': 'X_d',\n",
       "       'ref_id': None},\n",
       "      {'start': 228,\n",
       "       'end': 239,\n",
       "       'text': 'X c ',\n",
       "       'latex': 'X_c',\n",
       "       'ref_id': None},\n",
       "      {'start': 254,\n",
       "       'end': 265,\n",
       "       'text': 'n d ',\n",
       "       'latex': 'n_d',\n",
       "       'ref_id': None},\n",
       "      {'start': 285,\n",
       "       'end': 296,\n",
       "       'text': 'n c ',\n",
       "       'latex': 'n_c',\n",
       "       'ref_id': None},\n",
       "      {'start': 739,\n",
       "       'end': 750,\n",
       "       'text': 'ρ i ',\n",
       "       'latex': '\\\\rho _i',\n",
       "       'ref_id': None},\n",
       "      {'start': 774,\n",
       "       'end': 785,\n",
       "       'text': '𝐱 c i ',\n",
       "       'latex': '\\\\mathbf {x}_c^i',\n",
       "       'ref_id': None},\n",
       "      {'start': 807,\n",
       "       'end': 818,\n",
       "       'text': 'ρ∈ℝ n c  ',\n",
       "       'latex': '\\\\rho  \\\\in \\\\mathbb {R}^{n_c}',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Overview'},\n",
       "    {'text': 'Variational Autoencoders (VAEs) BIBREF10 , BIBREF11 is a generative model based on neural networks which can be used to conduct latent semantic modeling. BIBREF6 employ VAEs to map the news sentences into a latent semantic space, which is helpful in improving the MDS performance. Similarly, we also employ VAEs to conduct the semantic modeling for news sentences and comment sentences. Assume that both the prior and posterior of the latent variables are Gaussian, i.e., INLINEFORM0 and INLINEFORM1 , where INLINEFORM2 and INLINEFORM3 denote the variational mean and standard deviation respectively, which can be calculated with a multilayer perceptron (MLP). VAEs can be divided into two phases, namely, encoding (inference), and decoding (generation). All the operations are depicted as follows: DISPLAYFORM0 ',\n",
       "     'cite_spans': [{'start': 32,\n",
       "       'end': 40,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF10'},\n",
       "      {'start': 43,\n",
       "       'end': 51,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF11'},\n",
       "      {'start': 154,\n",
       "       'end': 161,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 472,\n",
       "       'end': 483,\n",
       "       'text': 'p θ (𝐳)=𝒩(0,𝐈)',\n",
       "       'latex': 'p_\\\\theta (\\\\mathbf {z}) = \\\\mathcal {N}(0, \\\\mathbf {I})',\n",
       "       'ref_id': None},\n",
       "      {'start': 488,\n",
       "       'end': 499,\n",
       "       'text': 'q φ (𝐳|𝐱)=𝒩(𝐳;μ,σ 2 𝐈)',\n",
       "       'latex': 'q_{\\\\phi }(\\\\mathbf {z}|\\\\mathbf {x}) = \\\\mathcal {N}(\\\\mathbf {z}; \\\\mu , \\\\sigma ^2\\\\mathbf {I})',\n",
       "       'ref_id': None},\n",
       "      {'start': 508,\n",
       "       'end': 519,\n",
       "       'text': 'μ',\n",
       "       'latex': '\\\\mu ',\n",
       "       'ref_id': None},\n",
       "      {'start': 524,\n",
       "       'end': 535,\n",
       "       'text': 'σ',\n",
       "       'latex': '\\\\sigma ',\n",
       "       'ref_id': None},\n",
       "      {'start': 799,\n",
       "       'end': 811,\n",
       "       'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "       'latex': '\\n\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}\\n\\n',\n",
       "       'ref_id': 'EQREF9'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'Based on the reparameterization trick in Equation EQREF9 , we can get the analytical representation of the variational lower bound INLINEFORM0 : DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 50,\n",
       "       'end': 56,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'EQREF9'}],\n",
       "     'eq_spans': [{'start': 131,\n",
       "       'end': 142,\n",
       "       'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "       'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "       'ref_id': None},\n",
       "      {'start': 145,\n",
       "       'end': 157,\n",
       "       'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "       'latex': '\\n\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber ',\n",
       "       'ref_id': 'EQREF10'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'where INLINEFORM0 denotes a general sentence, and it can be a news sentence INLINEFORM1 or a comment sentnece INLINEFORM2 .',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 6,\n",
       "       'end': 17,\n",
       "       'text': '𝐱',\n",
       "       'latex': '\\\\mathbf {x}',\n",
       "       'ref_id': None},\n",
       "      {'start': 76,\n",
       "       'end': 87,\n",
       "       'text': '𝐱 d ',\n",
       "       'latex': '\\\\mathbf {x}_d',\n",
       "       'ref_id': None},\n",
       "      {'start': 110,\n",
       "       'end': 121,\n",
       "       'text': '𝐱 c ',\n",
       "       'latex': '\\\\mathbf {x}_c',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'By feeding both the news documents and the reader comments into VAEs, we equip the model a ability of capturing the information from them jointly. However, there is a large amount of noisy information hidden in the comments. Hence we design a weighted combination mechanism for fusing news and comments in the VAEs. Precisely, we split the variational lower bound INLINEFORM0 into two parts and fuse them using the comment weight INLINEFORM1 : DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 364,\n",
       "       'end': 375,\n",
       "       'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "       'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "       'ref_id': None},\n",
       "      {'start': 430,\n",
       "       'end': 441,\n",
       "       'text': 'ρ',\n",
       "       'latex': '\\\\rho ',\n",
       "       'ref_id': None},\n",
       "      {'start': 444,\n",
       "       'end': 456,\n",
       "       'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "       'latex': '\\n\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)\\n',\n",
       "       'ref_id': 'EQREF11'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'The calculation of INLINEFORM0 will be discussed later.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 19,\n",
       "       'end': 30,\n",
       "       'text': 'ρ',\n",
       "       'latex': '\\\\rho ',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'The news sentence salience estimation is conducted by an unsupervised data reconstruction framework. Assume that INLINEFORM0 are INLINEFORM1 latent aspect vectors used for reconstructing all the latent semantic vectors INLINEFORM2 . Thereafter, the variational-decoding progress of VAEs can map the latent aspect vector INLINEFORM3 to INLINEFORM4 , and then produce INLINEFORM5 new aspect term vectors INLINEFORM6 : DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 113,\n",
       "       'end': 124,\n",
       "       'text': '𝐒 z ={𝐬 z 1 ,𝐬 z 2 ,⋯,𝐬 z m }',\n",
       "       'latex': '\\\\mathbf {S}_z = \\\\lbrace \\\\mathbf {s}_z^1, \\\\mathbf {s}_z^2,\\\\cdots ,\\\\mathbf {s}_z^m\\\\rbrace ',\n",
       "       'ref_id': None},\n",
       "      {'start': 129, 'end': 140, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "      {'start': 219,\n",
       "       'end': 230,\n",
       "       'text': '𝐙={𝐳 1 ,𝐳 2 ,⋯,𝐳 n }',\n",
       "       'latex': '\\\\mathbf {Z} = \\\\lbrace \\\\mathbf {z}^1, \\\\mathbf {z}^2,\\\\cdots ,\\\\mathbf {z}^n\\\\rbrace ',\n",
       "       'ref_id': None},\n",
       "      {'start': 320,\n",
       "       'end': 331,\n",
       "       'text': '𝐒 z ',\n",
       "       'latex': '\\\\mathbf {S}_z',\n",
       "       'ref_id': None},\n",
       "      {'start': 335,\n",
       "       'end': 346,\n",
       "       'text': '𝐒 h ',\n",
       "       'latex': '\\\\mathbf {S}_h',\n",
       "       'ref_id': None},\n",
       "      {'start': 366, 'end': 377, 'text': 'm', 'latex': 'm', 'ref_id': None},\n",
       "      {'start': 402,\n",
       "       'end': 413,\n",
       "       'text': '𝐒 x ',\n",
       "       'latex': '\\\\mathbf {S}_x',\n",
       "       'ref_id': None},\n",
       "      {'start': 416,\n",
       "       'end': 428,\n",
       "       'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "       'latex': '\\n\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}\\n',\n",
       "       'ref_id': 'EQREF12'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'VAESum BIBREF6 employs an alignment mechanism BIBREF12 , BIBREF13 to recall the lost detailed information from the input sentence. Inspired this idea, we design a jointly weighted alignment mechanism by considering the news sentence and the comment sentence simultaneously. For each decoder hidden state INLINEFORM0 , we align it with each news encoder hidden state INLINEFORM1 by an alignment vector INLINEFORM2 . We also align it with each comments encoder hidden state INLINEFORM3 by an alignment vector INLINEFORM4 . In order to filter the noisy information from the comments, we again employ the comment weight INLINEFORM5 to adjust the alignment vector of comments: DISPLAYFORM0 ',\n",
       "     'cite_spans': [{'start': 7,\n",
       "       'end': 14,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'},\n",
       "      {'start': 46,\n",
       "       'end': 54,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF12'},\n",
       "      {'start': 57,\n",
       "       'end': 65,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF13'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 304,\n",
       "       'end': 315,\n",
       "       'text': 's h i ',\n",
       "       'latex': 's^i_{h}',\n",
       "       'ref_id': None},\n",
       "      {'start': 366,\n",
       "       'end': 377,\n",
       "       'text': 'h d j ',\n",
       "       'latex': 'h^j_{d}',\n",
       "       'ref_id': None},\n",
       "      {'start': 401,\n",
       "       'end': 412,\n",
       "       'text': 'a d ∈ℝ n d  ',\n",
       "       'latex': 'a^d \\\\in \\\\mathbb {R}^{n_d}',\n",
       "       'ref_id': None},\n",
       "      {'start': 472,\n",
       "       'end': 483,\n",
       "       'text': 'h c j ',\n",
       "       'latex': 'h^j_{c}',\n",
       "       'ref_id': None},\n",
       "      {'start': 507,\n",
       "       'end': 518,\n",
       "       'text': 'a c ∈ℝ n c  ',\n",
       "       'latex': 'a^c \\\\in \\\\mathbb {R}^{n_c}',\n",
       "       'ref_id': None},\n",
       "      {'start': 616,\n",
       "       'end': 627,\n",
       "       'text': 'ρ',\n",
       "       'latex': '\\\\rho ',\n",
       "       'ref_id': None},\n",
       "      {'start': 672,\n",
       "       'end': 684,\n",
       "       'text': 'a ˜ c =a c ×ρ',\n",
       "       'latex': '\\n{\\\\tilde{a}}^c = a^c \\\\times \\\\rho \\n',\n",
       "       'ref_id': 'EQREF13'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'The news-based context vector INLINEFORM0 and the comment-based context vector INLINEFORM1 can be obtained by linearly blending the input hidden states respectively. Then the output hidden state can be updated based on the context vectors: DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 30,\n",
       "       'end': 41,\n",
       "       'text': 'c d i ',\n",
       "       'latex': 'c_d^i',\n",
       "       'ref_id': None},\n",
       "      {'start': 79,\n",
       "       'end': 90,\n",
       "       'text': 'c c i ',\n",
       "       'latex': 'c_c^i',\n",
       "       'ref_id': None},\n",
       "      {'start': 240,\n",
       "       'end': 252,\n",
       "       'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "       'latex': '\\n{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})\\n',\n",
       "       'ref_id': 'EQREF14'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'Then we can generate the updated output aspect vectors based on INLINEFORM0 . We add a similar alignment mechanism into the output layer.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 64,\n",
       "       'end': 75,\n",
       "       'text': 's ˜ h i ',\n",
       "       'latex': '{{\\\\tilde{s}}_h^i}',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': ' INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 can be used to reconstruct the space to which they belong respectively. In order to capture the information from comments, we design a joint reconstruction approach here. Let INLINEFORM3 be the reconstruction coefficient matrix for news sentences, and INLINEFORM4 be the reconstruction coefficient matrix for comment sentences. The optimization objective contains three reconstruction terms, jointly considering the latent semantic reconstruction and the term vector space reconstruction for news and comments respectively: DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 1,\n",
       "       'end': 12,\n",
       "       'text': '𝐒 z ',\n",
       "       'latex': '\\\\mathbf {S}_z',\n",
       "       'ref_id': None},\n",
       "      {'start': 15,\n",
       "       'end': 26,\n",
       "       'text': '𝐒 h ',\n",
       "       'latex': '\\\\mathbf {S}_h',\n",
       "       'ref_id': None},\n",
       "      {'start': 33,\n",
       "       'end': 44,\n",
       "       'text': '𝐒 x ',\n",
       "       'latex': '\\\\mathbf {S}_x',\n",
       "       'ref_id': None},\n",
       "      {'start': 220,\n",
       "       'end': 231,\n",
       "       'text': '𝐀 d ∈ℝ n d ×m ',\n",
       "       'latex': '\\\\mathbf {A}_d \\\\in \\\\mathbb {R}^{n_d \\\\times m}',\n",
       "       'ref_id': None},\n",
       "      {'start': 297,\n",
       "       'end': 308,\n",
       "       'text': '𝐀 c ∈ℝ n c ×m ',\n",
       "       'latex': '\\\\mathbf {A}_c \\\\in \\\\mathbb {R}^{n_c \\\\times m}',\n",
       "       'ref_id': None},\n",
       "      {'start': 569,\n",
       "       'end': 581,\n",
       "       'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "       'latex': '\\n\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}\\n',\n",
       "       'ref_id': 'EQREF15'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'This objective is integrated with the variational lower bound of VAEs INLINEFORM0 and optimized in a multi-task learning fashion. Then the new optimization objective is: DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 70,\n",
       "       'end': 81,\n",
       "       'text': 'ℒ(θ,ϕ;𝐱)',\n",
       "       'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x})',\n",
       "       'ref_id': None},\n",
       "      {'start': 170,\n",
       "       'end': 182,\n",
       "       'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "       'latex': '\\n\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})\\n',\n",
       "       'ref_id': 'EQREF16'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'where INLINEFORM0 is a set of all the parameters related to this task. We define the magnitude of each row of INLINEFORM1 as the salience scores for the corresponding news sentences.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 6,\n",
       "       'end': 17,\n",
       "       'text': 'Θ',\n",
       "       'latex': '\\\\Theta ',\n",
       "       'ref_id': None},\n",
       "      {'start': 110,\n",
       "       'end': 121,\n",
       "       'text': '𝐀 d ',\n",
       "       'latex': '\\\\mathbf {A}_d',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'We should note that the most important variable in our framework is the comment weight vector INLINEFORM0 , which appears in all the three components of our framework. The basic idea for calculating INLINEFORM1 is that if the comment sentence is more similar to the news content, then it contains less noisy information. For all the news sentences INLINEFORM2 and all the comment sentences INLINEFORM3 , calculate the relation matrix INLINEFORM4 by: DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 94,\n",
       "       'end': 105,\n",
       "       'text': 'ρ',\n",
       "       'latex': '\\\\rho ',\n",
       "       'ref_id': None},\n",
       "      {'start': 199,\n",
       "       'end': 210,\n",
       "       'text': 'ρ',\n",
       "       'latex': '\\\\rho ',\n",
       "       'ref_id': None},\n",
       "      {'start': 348,\n",
       "       'end': 359,\n",
       "       'text': 'X d ',\n",
       "       'latex': 'X_d',\n",
       "       'ref_id': None},\n",
       "      {'start': 390,\n",
       "       'end': 401,\n",
       "       'text': 'X c ',\n",
       "       'latex': 'X_c',\n",
       "       'ref_id': None},\n",
       "      {'start': 434,\n",
       "       'end': 445,\n",
       "       'text': 'R∈ℝ n d ×n c  ',\n",
       "       'latex': 'R \\\\in \\\\mathbb {R}^{n_d \\\\times n_c}',\n",
       "       'ref_id': None},\n",
       "      {'start': 450,\n",
       "       'end': 462,\n",
       "       'text': 'R=X d ×X c T ',\n",
       "       'latex': '\\nR = X_d\\\\times X_c^T\\n',\n",
       "       'ref_id': 'EQREF17'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'Then we add an average pooling layer to get the coefficient value for each comment sentence: DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 93,\n",
       "       'end': 105,\n",
       "       'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "       'latex': '\\n\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}\\n',\n",
       "       'ref_id': 'EQREF18'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'Finally, we add a sigmoid function to adjust the coefficient value to INLINEFORM0 : DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 70,\n",
       "       'end': 81,\n",
       "       'text': '(0,1)',\n",
       "       'latex': '(0,1)',\n",
       "       'ref_id': None},\n",
       "      {'start': 84,\n",
       "       'end': 96,\n",
       "       'text': 'ρ=sigmoid(𝐫)',\n",
       "       'latex': '\\n\\\\rho  = sigmoid(\\\\mathbf {r})\\n',\n",
       "       'ref_id': 'EQREF19'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'Because we have different representations from different vector space for the sentences, therefore we can calculate the comment weight in different semantic vector space. Here we use two spaces, namely, latent semantic space obtained by VAEs, and the original bag-of-words vector space. Then we can merge the weights by a parameter INLINEFORM0 : DISPLAYFORM0 ',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 332,\n",
       "       'end': 343,\n",
       "       'text': 'λ p ',\n",
       "       'latex': '\\\\lambda _p',\n",
       "       'ref_id': None},\n",
       "      {'start': 346,\n",
       "       'end': 358,\n",
       "       'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x ',\n",
       "       'latex': '\\n\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x\\n',\n",
       "       'ref_id': 'EQREF20'}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'where INLINEFORM0 and INLINEFORM1 are the comment weight calculated from latent semantic space and term vector space. Actually, we can regard INLINEFORM2 as some gates to control the proportion of each comment sentence absorbed by the framework.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 6,\n",
       "       'end': 17,\n",
       "       'text': 'ρ z ',\n",
       "       'latex': '\\\\rho _z',\n",
       "       'ref_id': None},\n",
       "      {'start': 22,\n",
       "       'end': 33,\n",
       "       'text': 'ρ x ',\n",
       "       'latex': '\\\\rho _x',\n",
       "       'ref_id': None},\n",
       "      {'start': 142,\n",
       "       'end': 153,\n",
       "       'text': 'ρ',\n",
       "       'latex': '\\\\rho ',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Reader-Aware Salience Estimation'},\n",
       "    {'text': 'In order to produce reader-aware summaries, inspired by the phrase-based model in BIBREF5 and BIBREF9 , we refine this model to consider the news sentences salience information obtained by our framework. Based on the parsed constituency tree for each input sentence, we extract the noun-phrases (NPs) and verb-phrases (VPs). The overall objective function of this optimization formulation for selecting salient NPs and VPs is formulated as an integer linear programming (ILP) problem: DISPLAYFORM0 ',\n",
       "     'cite_spans': [{'start': 82,\n",
       "       'end': 89,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'},\n",
       "      {'start': 94,\n",
       "       'end': 101,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 485,\n",
       "       'end': 497,\n",
       "       'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "       'latex': '\\n\\n\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,\\n',\n",
       "       'ref_id': 'EQREF22'}],\n",
       "     'section': 'Summary Construction'},\n",
       "    {'text': 'where INLINEFORM0 is the selection indicator for the phrase INLINEFORM1 , INLINEFORM2 is the salience scores of INLINEFORM3 , INLINEFORM4 and INLINEFORM5 is co-occurrence indicator and the similarity a pair of phrases ( INLINEFORM6 , INLINEFORM7 ) respectively. The similarity is calculated with the Jaccard Index based method. In order to obtain coherent summaries with good readability, we add some constraints into the ILP framework. For details, please refer to BIBREF14 , BIBREF5 , and BIBREF9 . The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as simplex algorithms BIBREF15 . In the implementation, we use a package called lp_solve.',\n",
       "     'cite_spans': [{'start': 466,\n",
       "       'end': 474,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF14'},\n",
       "      {'start': 477,\n",
       "       'end': 484,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'},\n",
       "      {'start': 491,\n",
       "       'end': 498,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'},\n",
       "      {'start': 644,\n",
       "       'end': 652,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF15'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 6,\n",
       "       'end': 17,\n",
       "       'text': 'α i ',\n",
       "       'latex': '\\\\alpha _i',\n",
       "       'ref_id': None},\n",
       "      {'start': 60, 'end': 71, 'text': 'P i ', 'latex': 'P_i', 'ref_id': None},\n",
       "      {'start': 74, 'end': 85, 'text': 'S i ', 'latex': 'S_i', 'ref_id': None},\n",
       "      {'start': 112,\n",
       "       'end': 123,\n",
       "       'text': 'P i ',\n",
       "       'latex': 'P_i',\n",
       "       'ref_id': None},\n",
       "      {'start': 126,\n",
       "       'end': 137,\n",
       "       'text': 'α ij ',\n",
       "       'latex': '\\\\alpha _{ij}',\n",
       "       'ref_id': None},\n",
       "      {'start': 142,\n",
       "       'end': 153,\n",
       "       'text': 'R ij ',\n",
       "       'latex': 'R_{ij}',\n",
       "       'ref_id': None},\n",
       "      {'start': 220,\n",
       "       'end': 231,\n",
       "       'text': 'P i ',\n",
       "       'latex': 'P_i',\n",
       "       'ref_id': None},\n",
       "      {'start': 234,\n",
       "       'end': 245,\n",
       "       'text': 'P j ',\n",
       "       'latex': 'P_j',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Summary Construction'},\n",
       "    {'text': 'In this section, we describe the preparation process of the dataset. Then we provide some properties and statistics.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Data Description'},\n",
       "    {'text': 'The definition of the terminology related to the dataset is given as follows.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Background'},\n",
       "    {'text': 'Topic: A topic refers to an event and it is composed of a set of news documents from different sources.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Background'},\n",
       "    {'text': 'Document: A news article describing some aspects of the topic. The set of documents in the same topic typically span a period, say a few days.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Background'},\n",
       "    {'text': 'Category: Each topic belongs to a category. There are 6 predefined categories: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Background'},\n",
       "    {'text': 'Aspect: Each category has a set of predefined aspects. Each aspect describes one important element of an event. For example, for the category “Accidents and Natural Disasters”, the aspects are “WHAT”, “WHEN”, “WHERE”, “WHY”, “WHO_AFFECTED”, “DAMAGES”, and “COUNTERMEASURES”.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Background'},\n",
       "    {'text': 'Aspect facet: An aspect facet refers to the actual content of a particular aspect for a particular topic. Take the topic “Malaysia Airlines Disappearance” as an example, facets for the aspect “WHAT” include “missing Malaysia Airlines Flight 370”, “two passengers used passports stolen in Thailand from an Austrian and an Italian.” etc. Facets for the aspect “WHEN” are “ Saturday morning”, “about an hour into its flight from Kuala Lumpur”, etc.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Background'},\n",
       "    {'text': 'Comment: A piece of text written by a reader conveying his or her altitude, emotion, or any thought on a particular news document.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Background'},\n",
       "    {'text': 'The first step is to select topics. The selected topics should be in one of the above categories. We make use of several ways to find topics. The first way is to search the category name using Google News. The second way is to follow the related tags on Twitter. One more useful method is to scan the list of event archives on the Web, such as earthquakes happened in 2017 .',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Data Collection'},\n",
       "    {'text': 'For some news websites, in addition to provide news articles, they offer a platform to allow readers to enter comments. Regarding the collection of news documents, for a particular topic, one consideration is that reader comments can be easily found. Another consideration is that all the news documents under a topic must be collected from different websites as far as possible. Similar to the methods used in DUC and TAC, we also capture and store the content using XML format.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Data Collection'},\n",
       "    {'text': 'Each topic is assigned to 4 experts, who are major in journalism, to conduct the summary writing. The task of summary writing is divided into two phases, namely, aspect facet identification, and summary generation. For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic. Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as many aspects as possible, and should be well-organized using complete sentences with a length restriction of 100 words.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Data Collection'},\n",
       "    {'text': 'After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence. Finally, all the model summaries are stored in XML files.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Data Collection'},\n",
       "    {'text': 'The dataset contains 45 topics from those 6 predefined categories. Some examples of topics are “Malaysia Airlines Disappearance”, “Flappy Bird”, “Bitcoin Mt. Gox”, etc. All the topics and categories are listed in Appendix SECREF7 . Each topic contains 10 news documents and 4 model summaries. The length limit of the model summary is 100 words (slitted by space). On average, each topic contains 215 pieces of comments and 940 comment sentences. Each news document contains an average of 27 sentences, and each sentence contains an average of 25 words. 85% of non-stop model summary terms (entities, unigrams, bigrams) appeared in the news documents, and 51% of that appeared in the reader comments. The dataset contains 19k annotated aspect facets.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 222,\n",
       "       'end': 229,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'SECREF7'}],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Data Properties'},\n",
       "    {'text': 'The properties of our own dataset are depicted in Section SECREF28 . We use ROUGE score as our evaluation metric BIBREF16 with standard options. F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported.',\n",
       "     'cite_spans': [{'start': 113,\n",
       "       'end': 121,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF16'}],\n",
       "     'ref_spans': [{'start': 58,\n",
       "       'end': 66,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'SECREF28'}],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Dataset and Metrics'},\n",
       "    {'text': 'To evaluate the performance of our dataset and the proposed framework RAVAESum for RA-MDS, we compare our model with the following methods:',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Comparative Methods'},\n",
       "    {'text': 'RA-Sparse BIBREF9 : It is a framework to tackle the RA-MDS problem. A sparse-coding-based method is used to calculate the salience of the news sentences by jointly considering news documents and reader comments.',\n",
       "     'cite_spans': [{'start': 10,\n",
       "       'end': 17,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF9'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Comparative Methods'},\n",
       "    {'text': 'Lead BIBREF17 : It ranks the news sentences chronologically and extracts the leading sentences one by one until the length limit.',\n",
       "     'cite_spans': [{'start': 5,\n",
       "       'end': 13,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF17'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Comparative Methods'},\n",
       "    {'text': 'Centroid BIBREF18 : It summarizes clusters of news articles automatically grouped by a topic detection system, and then it uses information from the centroids of the clusters to select sentences.',\n",
       "     'cite_spans': [{'start': 9,\n",
       "       'end': 17,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF18'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Comparative Methods'},\n",
       "    {'text': 'LexRank BIBREF1 and TextRank BIBREF19 : Both methods are graph-based unsupervised framework for sentence salience estimation based on PageRank algorithm.',\n",
       "     'cite_spans': [{'start': 8,\n",
       "       'end': 15,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF1'},\n",
       "      {'start': 29,\n",
       "       'end': 37,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF19'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Comparative Methods'},\n",
       "    {'text': 'Concept BIBREF5 : It generates abstractive summaries using phrase-based optimization framework with concept weight as salience estimation. The concept set contains unigrams, bigrams, and entities. The weighted term-frequency is used as the concept weight.',\n",
       "     'cite_spans': [{'start': 8,\n",
       "       'end': 15,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF5'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Comparative Methods'},\n",
       "    {'text': 'We can see that only the method RA-Sparse can handle RA-MDS. All the other methods are only for traditional MDS without comments.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Comparative Methods'},\n",
       "    {'text': 'The input news sentences and comment sentences are represented as BoWs vectors with dimension INLINEFORM0 . The dictionary INLINEFORM1 is created using unigrams, bigrams and named entity terms. INLINEFORM2 and INLINEFORM3 are the number of news sentences and comment sentences respectively. For the number of latent aspects used in data reconstruction, we let INLINEFORM4 . For the neural network framework, we set the hidden size INLINEFORM5 and the latent size INLINEFORM6 . For the parameter INLINEFORM7 used in comment weight, we let INLINEFORM8 . Adam BIBREF20 is used for gradient based optimization with a learning rate 0.001. Our neural network based framework is implemented using Theano BIBREF21 on a single GPU.',\n",
       "     'cite_spans': [{'start': 557,\n",
       "       'end': 565,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF20'},\n",
       "      {'start': 697,\n",
       "       'end': 705,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF21'}],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [{'start': 94,\n",
       "       'end': 105,\n",
       "       'text': '|V|',\n",
       "       'latex': '|V|',\n",
       "       'ref_id': None},\n",
       "      {'start': 123, 'end': 134, 'text': 'V', 'latex': 'V', 'ref_id': None},\n",
       "      {'start': 194,\n",
       "       'end': 205,\n",
       "       'text': 'n d ',\n",
       "       'latex': 'n_d',\n",
       "       'ref_id': None},\n",
       "      {'start': 210,\n",
       "       'end': 221,\n",
       "       'text': 'n c ',\n",
       "       'latex': 'n_c',\n",
       "       'ref_id': None},\n",
       "      {'start': 360,\n",
       "       'end': 371,\n",
       "       'text': 'm=5',\n",
       "       'latex': 'm = 5',\n",
       "       'ref_id': None},\n",
       "      {'start': 431,\n",
       "       'end': 442,\n",
       "       'text': 'd h =500',\n",
       "       'latex': 'd_h = 500',\n",
       "       'ref_id': None},\n",
       "      {'start': 463,\n",
       "       'end': 474,\n",
       "       'text': 'K=100',\n",
       "       'latex': 'K = 100',\n",
       "       'ref_id': None},\n",
       "      {'start': 495,\n",
       "       'end': 506,\n",
       "       'text': 'λ p ',\n",
       "       'latex': '\\\\lambda _p',\n",
       "       'ref_id': None},\n",
       "      {'start': 538,\n",
       "       'end': 549,\n",
       "       'text': 'λ p =0.2',\n",
       "       'latex': '\\\\lambda _p=0.2',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Experimental Settings'},\n",
       "    {'text': 'The results of our framework as well as the baseline methods are depicted in Table TABREF40 . It is obvious that our framework RAVAESum is the best among all the comparison methods. Specifically, it is better than RA-Sparse significantly ( INLINEFORM0 ), which demonstrates that VAEs based latent semantic modeling and joint semantic space reconstruction can improve the MDS performance considerably. Both RAVAESum and RA-Sparse are better than the methods without considering reader comments.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 83,\n",
       "       'end': 91,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF40'}],\n",
       "     'eq_spans': [{'start': 240,\n",
       "       'end': 251,\n",
       "       'text': 'p<0.05',\n",
       "       'latex': 'p<0.05',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Results on Our Dataset'},\n",
       "    {'text': 'To further investigate the effectiveness of our proposed RAVAESum framework, we adjust our framework by removing the comments related components. Then the model settings of RAVAESum-noC are similar to VAESum BIBREF6 . The evaluation results are shown in Table TABREF42 , which illustrate that our framework with reader comments RAVAESum is better than RAVAESum-noC significantly( INLINEFORM0 ).',\n",
       "     'cite_spans': [{'start': 208,\n",
       "       'end': 215,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     'ref_spans': [{'start': 260,\n",
       "       'end': 268,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF42'}],\n",
       "     'eq_spans': [{'start': 380,\n",
       "       'end': 391,\n",
       "       'text': 'p<0.05',\n",
       "       'latex': 'p<0.05',\n",
       "       'ref_id': None}],\n",
       "     'section': 'Further Investigation of Our Framework '},\n",
       "    {'text': \"Moreover, as mentioned in VAESum BIBREF6 , the output aspect vectors contain the word salience information. Then we select the top-10 terms for event “Sony Virtual Reality PS4”, and “`Bitcoin Mt. Gox Offlile”' for model RAVAESum (+C) and RAVAESum-noC (-C) respectively, and the results are shown in Table TABREF43 . It is obvious that the rank of the top salience terms are different. We check from the news documents and reader comments and find that some terms are enhanced by the reader comments successfully. For example, for the topic “Sony Virtual Reality PS4”, many readers talked about the product of “Oculus”, hence the word “oculus” is assigned a high salience by our model.\",\n",
       "     'cite_spans': [{'start': 33,\n",
       "       'end': 40,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'BIBREF6'}],\n",
       "     'ref_spans': [{'start': 305,\n",
       "       'end': 313,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF43'}],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Further Investigation of Our Framework '},\n",
       "    {'text': 'Based on the news and comments of the topic “Sony Virtual Reality PS4”, we generate two summaries with our model considering comments (RAVAESum) and ignoring comments (RAVAESum-noC) respectively. The summaries and ROUGE evaluation are given in Table TABREF45 . All the ROUGE values of our model considering comments are better than those ignoring comments with large gaps. The sentences in italic bold of the two summaries are different. By reviewing the comments of this topic, we find that many readers talked about “Oculus”, the other product with virtual reality techniques. This issue is well identified by our model and select the sentence “Mr. Yoshida said that Sony was inspired and encouraged to do its own virtual reality project after the enthusiastic response to the efforts of Oculus VR and Valve, another game company working on the technology.”.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [{'start': 250,\n",
       "       'end': 258,\n",
       "       'text': None,\n",
       "       'latex': None,\n",
       "       'ref_id': 'TABREF45'}],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Case Study'},\n",
       "    {'text': 'We investigate the problem of reader-aware multi-document summarization (RA-MDS) and introduce a new dataset. To tackle the RA-MDS, we extend a variational auto-encodes (VAEs) based MDS framework by jointly considering news documents and reader comments. The methods for data collection, aspect annotation, and summary writing and scrutinizing by experts are described. Experimental results show that reader comments can improve the summarization performance, which demonstrate the usefulness of the proposed dataset.',\n",
       "     'cite_spans': [],\n",
       "     'ref_spans': [],\n",
       "     'eq_spans': [],\n",
       "     'section': 'Conclusions'}],\n",
       "   'ref_entries': {'EQREF9': {'text': \"h enc =relu(W xh x+b xh )μ=W hμ h enc +b hμ log(σ 2 )=W hσ h enc +b hσ ε∼𝒩(0,𝐈),z=μ+σ⊗εh dec =relu(W zh z+b zh )x ' =sigmoid(W hx h dec +b hx )\",\n",
       "     'latex': '\\\\begin{array}{l}\\n{h_{enc}} = relu({W_{xh}}x + {b_{xh}})\\\\\\\\\\n\\\\mu = {W_{h\\\\mu }}{h_{enc}} + {b_{h\\\\mu }}\\\\\\\\\\n\\\\log ({\\\\sigma ^2}) = {W_{h\\\\sigma }}{h_{enc}} + {b_{h\\\\sigma }}\\\\\\\\\\n\\\\varepsilon \\\\sim \\\\mathcal {N}(0, \\\\mathbf {I}), \\\\ \\\\ z = \\\\mu + \\\\sigma \\\\otimes \\\\varepsilon \\\\\\\\\\n{h_{dec}} = relu({W_{zh}}z + {b_{zh}})\\\\\\\\\\n{x^\\\\prime } = sigmoid({W_{hx}}{h_{dec}} + {b_{hx}})\\\\\\\\\\n\\\\end{array}',\n",
       "     'ref_id': 'EQREF9',\n",
       "     'type': 'equation'},\n",
       "    'EQREF10': {'text': \"logp(x|z)=∑ i=1 |V| x i logx i ' +(1-x i )·log(1-x i ' )-D KL [q ϕ (z|x)∥p θ (z)]=1 2∑ i=1 K (1+log(σ i 2 )-μ i 2 -σ i 2 )\",\n",
       "     'latex': '\\\\small \\\\begin{array}{l}\\n\\\\log p(x|z) = \\\\sum \\\\limits _{i = 1}^{|V|} {{x_i}\\\\log x_i^\\\\prime + (1 - {x_i}) \\\\cdot \\\\log (1 - x_i^\\\\prime )} \\\\\\\\\\n- {D_{KL}}[{q_\\\\varphi }(z|x)\\\\Vert {p_\\\\theta }(z)]{\\\\rm { = }}\\\\frac{1}{2}\\\\sum \\\\limits _{i = 1}^K {(1 + \\\\log (\\\\sigma _i^2) - \\\\mu _i^2 - \\\\sigma _i^2)}\\n\\\\end{array}\\n\\\\nonumber',\n",
       "     'ref_id': 'EQREF10',\n",
       "     'type': 'equation'},\n",
       "    'EQREF11': {'text': 'ℒ(θ,ϕ;𝐱)=ℒ(θ,ϕ;𝐱 d )+ρ×ℒ(θ,ϕ;𝐱 c )',\n",
       "     'latex': '\\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}) = \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_d) + \\\\rho  \\\\times \\\\mathcal {L}(\\\\theta ,\\\\varphi ;\\\\mathbf {x}_c)',\n",
       "     'ref_id': 'EQREF11',\n",
       "     'type': 'equation'},\n",
       "    'EQREF12': {'text': 's h =relu(W zh s z +b zh )s x =sigmoid(W hx s h +b hx )',\n",
       "     'latex': '\\\\begin{array}{l}\\n{s_{h}} = relu({W_{zh}}{s_z} + {b_{zh}})\\\\\\\\\\n{s_x} = sigmoid({W_{hx}}{s_{h}} + {b_{hx}})\\n\\\\end{array}',\n",
       "     'ref_id': 'EQREF12',\n",
       "     'type': 'equation'},\n",
       "    'EQREF13': {'text': 'a ˜ c =a c ×ρ',\n",
       "     'latex': '{\\\\tilde{a}}^c = a^c \\\\times \\\\rho',\n",
       "     'ref_id': 'EQREF13',\n",
       "     'type': 'equation'},\n",
       "    'EQREF14': {'text': 's ˜ h i =tanh(W dh h c d i +W ch h c c i +W hh a s h i )',\n",
       "     'latex': '{{\\\\tilde{s}}_h^i} = \\\\tanh (W_{dh}^hc_d^i + W_{ch}^hc_c^i + W_{hh}^a{s_h^i})',\n",
       "     'ref_id': 'EQREF14',\n",
       "     'type': 'equation'},\n",
       "    'EQREF15': {'text': 'ℒ A =(Z d -A d S z  2 2 +H d -A d S h  2 2 +X d -A d S x  2 2 )+ρ×(Z c -A c S z  2 2 +H c -A c S h  2 2 +X c -A c S x  2 2 )',\n",
       "     'latex': '\\\\begin{aligned}\\n\\\\mathcal {L}_A &= (\\\\left\\\\Vert  {{Z_d} - {A_d}{S_z}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{H_d} - {A_d}{S_h}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{X_d} - {A_d}{S_x}} \\\\right\\\\Vert _2^2) + \\\\rho  \\\\times (\\\\left\\\\Vert  {{Z_c} - {A_c}{S_z}} \\\\right\\\\Vert _2^2 \\\\\\\\\\n&+ \\\\left\\\\Vert  {{H_c} - {A_c}{S_h}} \\\\right\\\\Vert _2^2 + \\\\left\\\\Vert  {{X_c} - {A_c}{S_x}} \\\\right\\\\Vert _2^2)\\n\\\\end{aligned}',\n",
       "     'ref_id': 'EQREF15',\n",
       "     'type': 'equation'},\n",
       "    'EQREF16': {'text': '𝒥=min Θ (-ℒ(θ,ϕ;x)+ℒ A )',\n",
       "     'latex': '\\\\mathcal {J} = \\\\mathop {\\\\min }\\\\limits _\\\\Theta ( {\\\\rm { - }}\\\\mathcal {L}(\\\\theta ,\\\\varphi ;x){\\\\rm { + }} {\\\\rm {\\\\mathcal {L}_{A}}})',\n",
       "     'ref_id': 'EQREF16',\n",
       "     'type': 'equation'},\n",
       "    'EQREF17': {'text': 'R=X d ×X c T',\n",
       "     'latex': 'R = X_d\\\\times X_c^T',\n",
       "     'ref_id': 'EQREF17',\n",
       "     'type': 'equation'},\n",
       "    'EQREF18': {'text': '𝐫=1 n c ∑ i=1 n c  R[i,:]',\n",
       "     'latex': '\\\\mathbf {r} = \\\\frac{1}{{{n_c}}}\\\\sum \\\\limits _{i = 1}^{{n_c}} {R[i,:]}',\n",
       "     'ref_id': 'EQREF18',\n",
       "     'type': 'equation'},\n",
       "    'EQREF19': {'text': 'ρ=sigmoid(𝐫)',\n",
       "     'latex': '\\\\rho  = sigmoid(\\\\mathbf {r})',\n",
       "     'ref_id': 'EQREF19',\n",
       "     'type': 'equation'},\n",
       "    'EQREF20': {'text': 'ρ=λ p ×ρ z +(1-λ p )×ρ x',\n",
       "     'latex': '\\\\rho  = \\\\lambda _p \\\\times \\\\rho _z + (1-\\\\lambda _p) \\\\times \\\\rho _x',\n",
       "     'ref_id': 'EQREF20',\n",
       "     'type': 'equation'},\n",
       "    'EQREF22': {'text': 'max{∑ i α i S i -∑ i<j α ij (S i +S j )R ij },',\n",
       "     'latex': '\\\\max \\\\lbrace \\\\sum _i{\\\\alpha _i S_i} - \\\\sum _{i<j}{\\\\alpha _{ij}(S_i+S_j)R_{ij}}\\\\rbrace ,',\n",
       "     'ref_id': 'EQREF22',\n",
       "     'type': 'equation'},\n",
       "    'FIGREF2': {'text': '1',\n",
       "     'caption': \"Reader comments of the news “The most important announcements from Google's big developers' conference (May, 2017)”.\",\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF2',\n",
       "     'type': 'figure'},\n",
       "    'FIGREF7': {'text': '2',\n",
       "     'caption': 'Our proposed framework. Left: Latent semantic modeling via variation auto-encoders for news sentence 𝐱 d and comment sentence 𝐱 c . Middle: Comment sentence weight estimation. Right: Salience estimation by a joint data reconstruction method. 𝐀 d is a news reconstruction coefficient matrix which contains the news sentence salience information.',\n",
       "     'latex': None,\n",
       "     'ref_id': 'FIGREF7',\n",
       "     'type': 'figure'},\n",
       "    'TABREF40': {'text': '1',\n",
       "     'caption': 'Summarization performance.',\n",
       "     'latex': [],\n",
       "     'ref_id': 'TABREF40',\n",
       "     'type': 'table'},\n",
       "    'TABREF42': {'text': '2',\n",
       "     'caption': 'Further investigation of RAVAESum.',\n",
       "     'latex': [],\n",
       "     'ref_id': 'TABREF42',\n",
       "     'type': 'table'},\n",
       "    'TABREF43': {'text': '3',\n",
       "     'caption': 'Top-10 terms extracted from each topic according to the word salience values',\n",
       "     'latex': [],\n",
       "     'ref_id': 'TABREF43',\n",
       "     'type': 'table'},\n",
       "    'TABREF45': {'text': '4',\n",
       "     'caption': 'Generated summaries for the topic “Sony Virtual Reality PS4”.',\n",
       "     'latex': [],\n",
       "     'ref_id': 'TABREF45',\n",
       "     'type': 'table'},\n",
       "    'TABREF46': {'text': '5',\n",
       "     'caption': 'All the topics and the corresponding categories. The 6 predefined categories are: (1) Accidents and Natural Disasters, (2) Attacks (Criminal/Terrorist), (3) New Technology, (4) Health and Safety, (5) Endangered Resources, and (6) Investigations and Trials (Criminal/Legal/Other).',\n",
       "     'latex': [],\n",
       "     'ref_id': 'TABREF46',\n",
       "     'type': 'table'},\n",
       "    'SECREF1': {'text': 'Introduction',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF1',\n",
       "     'type': 'section'},\n",
       "    'SECREF2': {'text': 'Framework',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF2',\n",
       "     'type': 'section'},\n",
       "    'SECREF6': {'text': 'Conclusions',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF6',\n",
       "     'type': 'section'},\n",
       "    'SECREF8': {'text': 'Reader-Aware Salience Estimation',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF8',\n",
       "     'type': 'section'},\n",
       "    'SECREF21': {'text': 'Summary Construction',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF21',\n",
       "     'type': 'section'},\n",
       "    'SECREF3': {'text': 'Data Description',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF3',\n",
       "     'type': 'section'},\n",
       "    'SECREF24': {'text': 'Background',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF24',\n",
       "     'type': 'section'},\n",
       "    'SECREF26': {'text': 'Data Collection',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF26',\n",
       "     'type': 'section'},\n",
       "    'SECREF28': {'text': 'Data Properties',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF28',\n",
       "     'type': 'section'},\n",
       "    'SECREF4': {'text': 'Experimental Setup',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF4',\n",
       "     'type': 'section'},\n",
       "    'SECREF29': {'text': 'Dataset and Metrics',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF29',\n",
       "     'type': 'section'},\n",
       "    'SECREF31': {'text': 'Comparative Methods',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF31',\n",
       "     'type': 'section'},\n",
       "    'SECREF37': {'text': 'Experimental Settings',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF37',\n",
       "     'type': 'section'},\n",
       "    'SECREF5': {'text': 'Results and Discussions',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF5',\n",
       "     'type': 'section'},\n",
       "    'SECREF39': {'text': 'Results on Our Dataset',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF39',\n",
       "     'type': 'section'},\n",
       "    'SECREF41': {'text': 'Further Investigation of Our Framework ',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF41',\n",
       "     'type': 'section'},\n",
       "    'SECREF44': {'text': 'Case Study',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF44',\n",
       "     'type': 'section'},\n",
       "    'SECREF7': {'text': 'Topics',\n",
       "     'latex': None,\n",
       "     'ref_id': 'SECREF7',\n",
       "     'type': 'section'}},\n",
       "   'bib_entries': {'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "     'title': 'Abstractive multi-document summarization via phrase selection and merging',\n",
       "     'authors': [{'first': 'Lidong',\n",
       "       'middle': [],\n",
       "       'last': 'Bing',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Weiwei', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "      {'first': 'Rebecca', 'middle': [], 'last': 'Passonneau', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1587--1597',\n",
       "     'other_ids': {},\n",
       "     'links': '8377315'},\n",
       "    'BIBREF15': {'ref_id': 'BIBREF15',\n",
       "     'title': 'Linear programming 1: introduction',\n",
       "     'authors': [{'first': 'B', 'middle': [], 'last': 'George', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dantzig', 'suffix': ''},\n",
       "      {'first': 'N', 'middle': [], 'last': 'Mukund', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Thapa', 'suffix': ''}],\n",
       "     'year': 2006,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '53739754'},\n",
       "    'BIBREF1': {'ref_id': 'BIBREF1',\n",
       "     'title': 'Lexpagerank: Prestige in multi-document text summarization',\n",
       "     'authors': [{'first': 'Günes',\n",
       "       'middle': [],\n",
       "       'last': 'Erkan',\n",
       "       'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Dragomir R Radev', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '4',\n",
       "     'issn': '',\n",
       "     'pages': '365--371',\n",
       "     'other_ids': {},\n",
       "     'links': '10418456'},\n",
       "    'BIBREF0': {'ref_id': 'BIBREF0',\n",
       "     'title': 'Multi-document summarization by sentence extraction',\n",
       "     'authors': [{'first': 'Jade',\n",
       "       'middle': [],\n",
       "       'last': 'Goldstein',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Vibhu', 'middle': [], 'last': 'Mittal', 'suffix': ''},\n",
       "      {'first': 'Jaime', 'middle': [], 'last': 'Carbonell', 'suffix': ''},\n",
       "      {'first': 'Mark', 'middle': [], 'last': 'Kantrowitz', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'NAACL-ANLPWorkshop',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '40--48',\n",
       "     'other_ids': {},\n",
       "     'links': '8294822'},\n",
       "    'BIBREF7': {'ref_id': 'BIBREF7',\n",
       "     'title': \"Comments-oriented document summarization: Understanding documents with readers' feedback\",\n",
       "     'authors': [{'first': 'Meishan',\n",
       "       'middle': [],\n",
       "       'last': 'Hu',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Aixin', 'middle': [], 'last': 'Sun', 'suffix': ''},\n",
       "      {'first': 'Ee-Peng', 'middle': [], 'last': 'Lim', 'suffix': ''}],\n",
       "     'year': 2008,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '291--298',\n",
       "     'other_ids': {},\n",
       "     'links': '13723748'},\n",
       "    'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "     'title': 'Adam: A method for stochastic optimization',\n",
       "     'authors': [{'first': 'Diederik',\n",
       "       'middle': [],\n",
       "       'last': 'Kingma',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Jimmy', 'middle': [], 'last': 'Ba', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '6628106'},\n",
       "    'BIBREF10': {'ref_id': 'BIBREF10',\n",
       "     'title': 'Auto-encoding variational bayes',\n",
       "     'authors': [{'first': 'P',\n",
       "       'middle': [],\n",
       "       'last': 'Diederik',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Max', 'middle': [], 'last': 'Kingma', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Welling', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICLR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '15789289'},\n",
       "    'BIBREF9': {'ref_id': 'BIBREF9',\n",
       "     'title': 'Reader-aware multi-document summarization via sparse coding',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Hang', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Yi', 'middle': [], 'last': 'Liao', 'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'IJCAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1270--1276',\n",
       "     'other_ids': {},\n",
       "     'links': '14777460'},\n",
       "    'BIBREF6': {'ref_id': 'BIBREF6',\n",
       "     'title': 'Salience estimation via variational auto-encoders for multi-document summarization',\n",
       "     'authors': [{'first': 'Piji', 'middle': [], 'last': 'Li', 'suffix': ''},\n",
       "      {'first': 'Zihao', 'middle': [], 'last': 'Wang', 'suffix': ''},\n",
       "      {'first': 'Wai', 'middle': [], 'last': 'Lam', 'suffix': ''},\n",
       "      {'first': 'Zhaochun', 'middle': [], 'last': 'Ren', 'suffix': ''},\n",
       "      {'first': 'Lidong', 'middle': [], 'last': 'Bing', 'suffix': ''}],\n",
       "     'year': 2017,\n",
       "     'venue': 'AAAI',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '3497--3503',\n",
       "     'other_ids': {},\n",
       "     'links': '29562039'},\n",
       "    'BIBREF13': {'ref_id': 'BIBREF13',\n",
       "     'title': 'Effective approaches to attention-based neural machine translation',\n",
       "     'authors': [{'first': 'Minh-Thang',\n",
       "       'middle': [],\n",
       "       'last': 'Luong',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Hieu', 'middle': [], 'last': 'Pham', 'suffix': ''},\n",
       "      {'first': 'Christopher D',\n",
       "       'middle': [],\n",
       "       'last': 'Manning',\n",
       "       'suffix': ''}],\n",
       "     'year': 2015,\n",
       "     'venue': 'EMNLP',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1412--1421',\n",
       "     'other_ids': {},\n",
       "     'links': '1998416'},\n",
       "    'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "     'title': 'Textrank: Bringing order into texts',\n",
       "     'authors': [{'first': 'Rada',\n",
       "       'middle': [],\n",
       "       'last': 'Mihalcea',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Paul', 'middle': [], 'last': 'Tarau', 'suffix': ''}],\n",
       "     'year': 2004,\n",
       "     'venue': '',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '',\n",
       "     'other_ids': {},\n",
       "     'links': '577937'},\n",
       "    'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "     'title': 'Exploiting category-specific information for multi-document summarization',\n",
       "     'authors': [{'first': 'Yen',\n",
       "       'middle': ['Kan'],\n",
       "       'last': 'Ziheng Lin Min',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Lim', 'middle': [], 'last': 'Chew', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Tan', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'COLING',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '2093--2108',\n",
       "     'other_ids': {},\n",
       "     'links': '6317274'},\n",
       "    'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "     'title': 'A survey of text summarization techniques',\n",
       "     'authors': [{'first': 'Ani',\n",
       "       'middle': [],\n",
       "       'last': 'Nenkova',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Kathleen', 'middle': [], 'last': 'Mckeown', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'Mining Text Data',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '43--76',\n",
       "     'other_ids': {},\n",
       "     'links': '556431'},\n",
       "    'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "     'title': 'Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies',\n",
       "     'authors': [{'first': 'Hongyan',\n",
       "       'middle': [],\n",
       "       'last': 'Dragomir R Radev',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Malgorzata', 'middle': [], 'last': 'Jing', 'suffix': ''},\n",
       "      {'first': '', 'middle': [], 'last': 'Budzikowska', 'suffix': ''}],\n",
       "     'year': 2000,\n",
       "     'venue': 'Proceedings of the 2000 NAACL-ANLP Workshop on Automatic summarization',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '21--30',\n",
       "     'other_ids': {},\n",
       "     'links': '1320'},\n",
       "    'BIBREF11': {'ref_id': 'BIBREF11',\n",
       "     'title': 'Stochastic backpropagation and approximate inference in deep generative models',\n",
       "     'authors': [{'first': 'Danilo',\n",
       "       'middle': [],\n",
       "       'last': 'Jimenez Rezende',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Shakir', 'middle': [], 'last': 'Mohamed', 'suffix': ''},\n",
       "      {'first': 'Daan', 'middle': [], 'last': 'Wierstra', 'suffix': ''}],\n",
       "     'year': 2014,\n",
       "     'venue': 'ICML',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1278--1286',\n",
       "     'other_ids': {},\n",
       "     'links': '16895865'},\n",
       "    'BIBREF17': {'ref_id': 'BIBREF17',\n",
       "     'title': 'Using leading text for news summaries: Evaluation results and implications for commercial summarization applications',\n",
       "     'authors': [{'first': 'Mark',\n",
       "       'middle': [],\n",
       "       'last': 'Wasson',\n",
       "       'suffix': ''}],\n",
       "     'year': 1998,\n",
       "     'venue': 'ACL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '1364--1368',\n",
       "     'other_ids': {},\n",
       "     'links': '12681629'},\n",
       "    'BIBREF14': {'ref_id': 'BIBREF14',\n",
       "     'title': 'Multiple aspect summarization using integer linear programming',\n",
       "     'authors': [{'first': 'Kristian',\n",
       "       'middle': [],\n",
       "       'last': 'Woodsend',\n",
       "       'suffix': ''},\n",
       "      {'first': 'Mirella', 'middle': [], 'last': 'Lapata', 'suffix': ''}],\n",
       "     'year': 2012,\n",
       "     'venue': 'EMNLP-CNLL',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '233--243',\n",
       "     'other_ids': {},\n",
       "     'links': '17497992'},\n",
       "    'BIBREF8': {'ref_id': 'BIBREF8',\n",
       "     'title': 'Social context summarization',\n",
       "     'authors': [{'first': 'Zi', 'middle': [], 'last': 'Yang', 'suffix': ''},\n",
       "      {'first': 'Keke', 'middle': [], 'last': 'Cai', 'suffix': ''},\n",
       "      {'first': 'Jie', 'middle': [], 'last': 'Tang', 'suffix': ''},\n",
       "      {'first': 'Li', 'middle': [], 'last': 'Zhang', 'suffix': ''},\n",
       "      {'first': 'Zhong', 'middle': [], 'last': 'Su', 'suffix': ''},\n",
       "      {'first': 'Juanzi', 'middle': [], 'last': 'Li', 'suffix': ''}],\n",
       "     'year': 2011,\n",
       "     'venue': 'SIGIR',\n",
       "     'volume': '',\n",
       "     'issn': '',\n",
       "     'pages': '255--264',\n",
       "     'other_ids': {},\n",
       "     'links': '704517'}}}}}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_latex_parse_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction 13\n",
      "Reader-Aware Salience Estimation 6\n",
      "Summary Construction 6\n",
      "Comparative Methods 6\n",
      "Experimental Settings 2\n",
      "Further Investigation of Our Framework  2\n",
      "Overview 1\n",
      "Dataset and Metrics 1\n",
      "Data Description 0\n",
      "Background 0\n",
      "Data Collection 0\n",
      "Data Properties 0\n",
      "Results on Our Dataset 0\n",
      "Case Study 0\n",
      "Conclusions 0\n"
     ]
    }
   ],
   "source": [
    "for k,v in latex_parse_overview.items():\n",
    "    print(k,sum(v['cite_span_lens']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим для всех \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers = dict()\n",
    "for num_artic,article in enumerate(all_articles):\n",
    "    # проверяем что у статьи есть grobid_parse и latex_parse и естб текст\n",
    "    if (article['grobid_parse'] and article['grobid_parse']['body_text']) or (article['latex_parse'] and article['latex_parse']['body_text']):\n",
    "        # задаем шаблон отражения статьи в укороченном формате (чтобы занимать меньше памяти)\n",
    "        overview_papers[article['paper_id']] = { 'paper_id':article['paper_id'],   'metadata':article['metadata'],\n",
    "                                                 's2_pdf_hash':article['s2_pdf_hash'], 'grobid_parse':None,'latex_parse':None}\n",
    "        \n",
    "        grobid_parse_overview = None\n",
    "        # если у статьи есть article['grobid_parse']['body_text']\n",
    "        if article['grobid_parse'] and article['grobid_parse']['body_text']:\n",
    "            grobid_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            for num_sec,sections in enumerate(article['grobid_parse']['body_text']):\n",
    "                grobid_parse_overview[num_sec] = sections\n",
    "            \n",
    "            # отсортируем по количеству цитат абзацы\n",
    "            grobid_parse_overview = {k: v for k, v in sorted(grobid_parse_overview.items(), key=lambda item: len(item[1]['cite_spans']), reverse=True)}\n",
    "            \n",
    "#             # найдем 1 и 2 максимум по обзорной части\n",
    "#             max_cite_span_sum = 0\n",
    "#             max_grobid_parse_overview = dict()\n",
    "#             for k,v in grobid_parse_overview.items():\n",
    "#                 if max_cite_span_sum < len(v['cite_spans']):\n",
    "#                     max_cite_span_sum = len(v['cite_spans'])\n",
    "#                     max_grobid_parse_overview[k] = v\n",
    "#                 # записываем 2 максимум, если количество ссылок в егочасти больше половины от максимального \n",
    "#                 elif (max_cite_span_sum>7) and len(v['cite_spans'])>max_cite_span_sum//2:\n",
    "#                     max_grobid_parse_overview[k] = v\n",
    "            \n",
    "#             grobid_parse_overview = max_grobid_parse_overview\n",
    "            \n",
    "        latex_parse_overview = None\n",
    "        # если у статьи есть article['latex_parse']['body_text']\n",
    "        if article['latex_parse'] and article['latex_parse']['body_text']:\n",
    "            latex_parse_overview = dict()\n",
    "            # проходим по каждому абзацу текста\n",
    "            # в latex_parse \n",
    "            for sections in article['latex_parse']['body_text']:\n",
    "                if sections['section'] in latex_parse_overview:\n",
    "                    if latex_parse_overview[sections['section']] == sections:\n",
    "                        continue\n",
    "                    else:\n",
    "                        latex_parse_overview[sections['section']]['text'].append(sections['text'])\n",
    "                        latex_parse_overview[sections['section']]['cite_spans'].append(sections['cite_spans'])\n",
    "                        latex_parse_overview[sections['section']]['cite_span_lens'].append(len(sections['cite_spans']))\n",
    "                        latex_parse_overview[sections['section']]['section'].append(sections['section'])\n",
    "                else:\n",
    "                    latex_parse_overview[sections['section']] = {'text':[sections['text']],   'cite_spans':[sections['cite_spans']],\n",
    "                                                                  'cite_span_lens':[len(sections['cite_spans'])],\n",
    "                                                                  'section':[sections['section']]}\n",
    "            latex_parse_overview = {k: v for k, v in sorted(latex_parse_overview.items(), key=lambda item: item[1]['cite_span_lens'], reverse=True)}\n",
    "        \n",
    "        \n",
    "#             max_cite_span_sum = 0\n",
    "#             max_latex_parse_overview = dict()\n",
    "#             for k,v in latex_parse_overview.items():\n",
    "#                 if max_cite_span_sum < sum(v['cite_span_lens']):\n",
    "#                     max_cite_span_sum = sum(v['cite_span_lens'])\n",
    "#                     max_latex_parse_overview[k] = v\n",
    "#                 elif (max_cite_span_sum>0) and sum(v['cite_span_lens'])>max_cite_span_sum//2:\n",
    "#                     max_latex_parse_overview[k] = v\n",
    "            \n",
    "#             latex_parse_overview = max_latex_parse_overview\n",
    "        \n",
    "\n",
    "        if grobid_parse_overview:\n",
    "            overview_papers[article['paper_id']]['grobid_parse'] = {'abstract':None,\n",
    "                                                        'overview_text':grobid_parse_overview,  \n",
    "                                                        'bib_entries':None}\n",
    "            if article['grobid_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['abstract'] = article['grobid_parse']['abstract']\n",
    "            if article['grobid_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['grobid_parse']['bib_entries'] = article['grobid_parse']['bib_entries']\n",
    "            \n",
    "        if latex_parse_overview:            \n",
    "            overview_papers[article['paper_id']]['latex_parse'] = {'abstract':None,\n",
    "                                                                    'overview_text':latex_parse_overview,  \n",
    "                                                                    'bib_entries':None}\n",
    "            if article['latex_parse']['abstract']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['abstract'] = article['latex_parse']['abstract']\n",
    "            if article['latex_parse']['bib_entries']:\n",
    "                overview_papers[article['paper_id']]['latex_parse']['bib_entries'] = article['latex_parse']['bib_entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018',\n",
       " '14472576',\n",
       " '17302615',\n",
       " '3243536',\n",
       " '3248240',\n",
       " '2223737',\n",
       " '488',\n",
       " '14323173',\n",
       " '15251605',\n",
       " '8260435']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(overview_papers.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Применим критерий и посмотрим какие секции выделились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_papers_w_latex = {k:v for k,v in overview_papers.items() if v['latex_parse']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4039"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(overview_papers_w_latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10164018',\n",
       " '488',\n",
       " '189927790',\n",
       " '126168169',\n",
       " '184488238',\n",
       " '85517799',\n",
       " '16050464',\n",
       " '52155342',\n",
       " '52247458',\n",
       " '5267356']"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(overview_papers_w_latex.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5084110\n",
      "198922003\n",
      "1238927\n",
      "173990592\n",
      "52097879\n",
      "3698344\n",
      "6385589\n",
      "52118895\n",
      "53981714\n",
      "51862727\n",
      "868799\n",
      "102353905\n",
      "2753602\n",
      "131774178\n",
      "7193975\n",
      "165163629\n",
      "174798321\n",
      "5294994\n",
      "44140055\n",
      "52896891\n",
      "29151018\n",
      "174797858\n",
      "46938018\n",
      "67855637\n",
      "2478928\n",
      "29165442\n",
      "49358911\n",
      "371926\n",
      "102351546\n",
      "15213991\n",
      "53082498\n",
      "52114454\n",
      "3161327\n",
      "5740960\n",
      "52073201\n",
      "1762731\n",
      "2821908\n",
      "15359942\n",
      "170079259\n",
      "3502581\n",
      "102350959\n",
      "174798375\n",
      "53064621\n",
      "10643243\n",
      "67856324\n",
      "174801285\n",
      "29164993\n",
      "119105191\n",
      "102486945\n",
      "44170166\n",
      "52205000\n",
      "17953812\n",
      "56895551\n",
      "311594\n",
      "52290656\n",
      "14179380\n",
      "51878898\n",
      "52178091\n",
      "174797955\n",
      "8622019\n",
      "15295411\n",
      "247735\n",
      "9387600\n",
      "2955580\n",
      "2728774\n",
      "7887385\n",
      "52113877\n",
      "14922772\n",
      "196177814\n",
      "3204831\n",
      "52117484\n",
      "1452429\n",
      "12245103\n",
      "80628357\n",
      "4875809\n",
      "26397607\n",
      "131773929\n",
      "52143204\n",
      "47018994\n",
      "53082704\n",
      "118680003\n",
      "13747354\n",
      "85529973\n",
      "90262493\n",
      "14748840\n",
      "198229567\n",
      "5037669\n",
      "92994351\n",
      "18449288\n",
      "195345032\n",
      "5112203\n",
      "119184444\n",
      "4943905\n",
      "52074264\n",
      "52176506\n",
      "14206024\n",
      "2404341\n",
      "13052370\n",
      "102350797\n",
      "6376814\n",
      "49907944\n",
      "4889691\n",
      "1707814\n",
      "24129906\n",
      "739426\n",
      "4311819\n",
      "4812047\n",
      "52112703\n",
      "52099643\n",
      "23144639\n",
      "119186140\n",
      "19751968\n",
      "11839232\n",
      "51882837\n",
      "6519154\n",
      "1805495\n",
      "10361075\n",
      "174801821\n",
      "5169126\n",
      "174797793\n",
      "168169888\n",
      "13276568\n",
      "128344862\n",
      "20159073\n",
      "3425897\n",
      "6869582\n",
      "4718230\n",
      "49560309\n",
      "4328410\n",
      "5101528\n",
      "49687\n",
      "44099358\n",
      "6728280\n",
      "12051021\n",
      "102353391\n",
      "21678954\n",
      "189927878\n",
      "155091369\n",
      "83458807\n",
      "503475\n",
      "9784700\n",
      "155099789\n",
      "8462113\n",
      "427492\n",
      "2162648\n",
      "145048418\n",
      "12262773\n",
      "14900221\n",
      "278288\n",
      "52948134\n",
      "13533556\n",
      "52119091\n",
      "153311772\n",
      "75134948\n",
      "53046959\n",
      "91184245\n",
      "20272964\n",
      "195767175\n",
      "3036949\n",
      "1727568\n",
      "11406047\n",
      "7857808\n",
      "173991008\n",
      "1430604\n",
      "52057510\n",
      "1479529\n",
      "5083989\n",
      "44131945\n",
      "52167992\n",
      "57759363\n",
      "1770102\n",
      "174798001\n",
      "2140766\n",
      "67855269\n",
      "7178598\n",
      "65081\n",
      "1618800\n",
      "4829361\n",
      "52194540\n",
      "1572802\n",
      "13744121\n",
      "2783746\n",
      "146121245\n",
      "102351034\n",
      "14878319\n",
      "59553499\n",
      "159041465\n",
      "184488333\n",
      "4382470\n",
      "102354684\n",
      "7371343\n",
      "49875847\n",
      "6163451\n",
      "6551751\n",
      "52099904\n",
      "173990567\n",
      "9612525\n",
      "11379303\n",
      "4942082\n",
      "3851049\n",
      "67856712\n",
      "52198002\n",
      "167217355\n",
      "166228140\n",
      "53223643\n",
      "4656792\n",
      "52115814\n",
      "174797905\n",
      "52158178\n",
      "52190560\n",
      "174798173\n",
      "52183735\n",
      "574574\n",
      "5394019\n",
      "49742817\n",
      "106403072\n",
      "174801260\n",
      "196176630\n",
      "4957206\n",
      "49194412\n",
      "2952144\n",
      "44094607\n",
      "2640922\n",
      "49213885\n",
      "53080999\n",
      "3278107\n",
      "13696127\n",
      "10668422\n",
      "4406182\n",
      "3662564\n",
      "199022662\n",
      "7586401\n",
      "118679513\n",
      "52933530\n",
      "1862889\n",
      "52115097\n",
      "15476643\n",
      "10458880\n",
      "189898023\n",
      "135465247\n",
      "53235683\n",
      "12245213\n",
      "398440\n",
      "189898046\n",
      "386957\n",
      "195820280\n",
      "6206777\n",
      "155100252\n",
      "51972729\n",
      "182953047\n",
      "52196193\n",
      "201070267\n",
      "26501419\n",
      "7130747\n",
      "174799386\n",
      "6947186\n",
      "5602221\n",
      "3258671\n",
      "2786154\n",
      "6665511\n",
      "44130927\n",
      "15690223\n",
      "7287895\n",
      "52070059\n",
      "195873924\n",
      "13690180\n",
      "19968363\n",
      "13741849\n",
      "52144534\n",
      "52822214\n",
      "52171684\n",
      "29163973\n",
      "14151217\n",
      "4937880\n",
      "52074232\n",
      "51876439\n",
      "15065468\n",
      "5761781\n",
      "151184\n",
      "2948298\n",
      "52112576\n",
      "173188813\n",
      "5564363\n",
      "52095531\n",
      "11074199\n",
      "52171279\n",
      "6397654\n",
      "19245833\n",
      "3092365\n",
      "1515524\n",
      "52008860\n",
      "6670611\n",
      "6280810\n",
      "18415191\n",
      "196208241\n",
      "49556256\n",
      "21706141\n",
      "186206974\n",
      "102352301\n",
      "52270102\n",
      "52112235\n",
      "13725089\n",
      "2626026\n",
      "13375869\n",
      "16227864\n",
      "52010243\n",
      "1524421\n",
      "3205400\n",
      "52094\n",
      "39487\n",
      "53298878\n",
      "44108595\n",
      "52154304\n",
      "102352997\n",
      "155092869\n",
      "5066019\n",
      "80628347\n",
      "195316274\n",
      "15407650\n",
      "182952524\n",
      "59523594\n",
      "173990539\n",
      "52188604\n",
      "29170917\n",
      "2236811\n",
      "6993650\n",
      "28950708\n",
      "5003931\n",
      "4732544\n",
      "12373647\n",
      "6035643\n",
      "195584301\n",
      "21306064\n",
      "47017117\n",
      "170078951\n",
      "52289318\n",
      "6922426\n",
      "52160496\n",
      "14013684\n",
      "617993\n",
      "5317930\n",
      "1297432\n",
      "49470550\n",
      "25940328\n",
      "15164488\n",
      "52144417\n",
      "4865053\n",
      "4061916\n",
      "4438009\n",
      "8214692\n",
      "49669303\n",
      "53099899\n",
      "4559639\n",
      "53025882\n",
      "13053999\n",
      "1152008\n",
      "31298398\n",
      "52145734\n",
      "6749012\n",
      "173990506\n",
      "67855928\n",
      "2707276\n",
      "11336213\n",
      "52158121\n",
      "76666335\n",
      "174802873\n",
      "12744871\n",
      "1032\n",
      "167217399\n",
      "1964946\n",
      "2090262\n",
      "52157478\n",
      "44143441\n",
      "52127792\n",
      "49209628\n",
      "52115700\n",
      "21723747\n",
      "340852\n",
      "6246996\n",
      "174799008\n",
      "1766244\n",
      "145048628\n",
      "19176069\n",
      "5480561\n",
      "52215843\n",
      "3201076\n",
      "195750836\n",
      "994500\n",
      "7205805\n",
      "174800300\n",
      "10436313\n",
      "25320232\n",
      "174801519\n",
      "18750779\n",
      "1354459\n",
      "52290717\n",
      "102354588\n",
      "5953675\n",
      "46985924\n",
      "52898286\n",
      "52824771\n",
      "9150889\n",
      "52153770\n",
      "15939234\n",
      "8395799\n",
      "51865629\n",
      "9206785\n",
      "5590763\n",
      "129945615\n",
      "52273813\n",
      "6078795\n",
      "21700944\n",
      "53109787\n",
      "189998980\n",
      "2325035\n",
      "870\n",
      "166228482\n",
      "159041276\n",
      "53246468\n",
      "86716257\n",
      "102353198\n",
      "16763786\n",
      "52019251\n",
      "186762\n",
      "7562142\n",
      "174798022\n",
      "3282953\n",
      "53081574\n",
      "4327459\n",
      "14651385\n",
      "102354583\n",
      "8928715\n",
      "49208337\n",
      "12111397\n",
      "52191391\n",
      "1141127\n",
      "153313061\n",
      "195584234\n",
      "18193214\n",
      "174797779\n",
      "21731209\n",
      "6660863\n",
      "10479248\n",
      "52143467\n",
      "198229871\n",
      "52156433\n",
      "174799553\n",
      "2454882\n",
      "21724135\n",
      "17355453\n",
      "52136770\n",
      "186206883\n",
      "49271504\n",
      "14275144\n",
      "44097540\n",
      "196623838\n",
      "11004224\n",
      "52111803\n",
      "189928048\n",
      "6946103\n",
      "6618571\n",
      "182952931\n",
      "49656687\n",
      "998001\n",
      "182952555\n",
      "44091147\n",
      "22716243\n",
      "5591459\n",
      "7181359\n",
      "44220219\n",
      "10796110\n",
      "1954216\n",
      "196170479\n",
      "34353891\n",
      "19186315\n",
      "53223504\n",
      "195700014\n",
      "9508540\n",
      "2185441\n",
      "195657996\n",
      "24461982\n",
      "5858382\n",
      "174801080\n",
      "49212016\n",
      "54084368\n",
      "2381275\n",
      "7459836\n",
      "21710637\n",
      "611341\n",
      "14444828\n",
      "7833469\n",
      "155092004\n",
      "8577994\n",
      "353451\n",
      "5057185\n",
      "159041867\n",
      "4956308\n",
      "174802858\n",
      "759991\n",
      "2793335\n",
      "91184307\n",
      "189999659\n",
      "5163433\n",
      "1635749\n",
      "5973442\n",
      "52100117\n",
      "17589422\n",
      "155092736\n",
      "1104123\n",
      "4956705\n",
      "15332825\n",
      "182952940\n",
      "1918428\n",
      "5332396\n",
      "85499373\n",
      "52346770\n",
      "13682858\n",
      "52113483\n",
      "1726501\n",
      "1245593\n",
      "49314365\n",
      "1527867\n",
      "195322963\n",
      "52056726\n",
      "6584348\n",
      "4943987\n",
      "61153666\n",
      "159041048\n",
      "6053988\n",
      "189927972\n",
      "12813872\n",
      "13746563\n",
      "44062452\n",
      "153312413\n",
      "1665037\n",
      "7597872\n",
      "196198179\n",
      "5053850\n",
      "53019653\n",
      "53079301\n",
      "173188048\n",
      "159040938\n",
      "49409893\n",
      "76663609\n",
      "21697330\n",
      "12250122\n",
      "196204042\n",
      "46955525\n",
      "102485964\n",
      "560565\n",
      "52100723\n",
      "1854889\n",
      "3305987\n",
      "90262473\n",
      "59413918\n",
      "52182354\n",
      "11349626\n",
      "173990261\n",
      "91184042\n",
      "155100063\n",
      "11438889\n",
      "52136564\n",
      "84843987\n",
      "12203802\n",
      "643707\n",
      "52183585\n",
      "1576593\n",
      "12998432\n",
      "104291898\n",
      "43940286\n",
      "3051772\n",
      "4947959\n",
      "6525455\n",
      "174801632\n",
      "52124023\n",
      "118589015\n",
      "196181734\n",
      "622026\n",
      "4790538\n",
      "51715039\n",
      "51875796\n",
      "5638176\n",
      "25113027\n",
      "10695055\n",
      "51878335\n",
      "119301360\n",
      "21299649\n",
      "21726677\n",
      "52155604\n",
      "1104940\n",
      "195218673\n",
      "52310841\n",
      "49346449\n",
      "53081188\n",
      "6826032\n",
      "4933708\n",
      "5252952\n",
      "174803618\n",
      "53083244\n",
      "6683636\n",
      "3205175\n",
      "52096531\n",
      "80628248\n",
      "198898875\n",
      "9914140\n",
      "2646329\n",
      "2001317\n",
      "2816661\n",
      "52191889\n",
      "2255078\n",
      "5023089\n",
      "52155263\n",
      "5512173\n",
      "1260503\n",
      "4537113\n",
      "4460159\n",
      "1541076\n",
      "29151507\n",
      "52054725\n",
      "52112317\n",
      "4808444\n",
      "91183938\n",
      "155100205\n",
      "7699280\n",
      "13679932\n",
      "49300957\n",
      "195886068\n",
      "15620570\n",
      "174797942\n",
      "52044834\n",
      "12087925\n",
      "7116029\n",
      "53046408\n",
      "52090220\n",
      "19973349\n",
      "104292554\n",
      "59599828\n",
      "53081356\n",
      "167217689\n",
      "9142609\n",
      "52169385\n",
      "6468765\n",
      "19161058\n",
      "11584686\n",
      "174798275\n",
      "52986570\n",
      "49656757\n",
      "4778308\n",
      "52916603\n",
      "102350523\n",
      "53082542\n",
      "189762150\n",
      "102351981\n",
      "14604520\n",
      "16088818\n",
      "81979508\n",
      "16960682\n",
      "4937809\n",
      "2976840\n",
      "13700629\n",
      "919901\n",
      "5191821\n",
      "13741562\n",
      "7739920\n",
      "3087701\n",
      "167217880\n",
      "30046385\n",
      "52078335\n",
      "13745308\n",
      "21725048\n",
      "13747066\n",
      "8363899\n",
      "49672007\n",
      "21689288\n",
      "10619801\n",
      "3264224\n",
      "174799117\n",
      "19488885\n",
      "59553563\n",
      "4889385\n",
      "174798146\n",
      "85498775\n",
      "2061169\n",
      "81978369\n",
      "3281822\n",
      "4888341\n",
      "182953211\n",
      "1371374\n",
      "186206852\n",
      "102353371\n",
      "5809776\n",
      "52967399\n",
      "1687938\n",
      "52119483\n",
      "4539080\n",
      "12223284\n",
      "2882092\n",
      "14351566\n",
      "102350965\n",
      "14624362\n",
      "106054\n",
      "159040684\n",
      "7363686\n",
      "52154496\n",
      "4573092\n",
      "19651289\n",
      "398\n",
      "29161506\n",
      "647006\n",
      "4711425\n",
      "9931676\n",
      "52162085\n",
      "102351730\n",
      "52895001\n",
      "7100502\n",
      "49653963\n",
      "173188058\n",
      "481343\n",
      "195699881\n",
      "182953261\n",
      "21730065\n",
      "28909912\n",
      "29150573\n",
      "37390552\n",
      "11690925\n",
      "104292348\n",
      "7384097\n",
      "41480412\n",
      "44152851\n",
      "6137675\n",
      "4955031\n",
      "14724419\n",
      "1274623\n",
      "28982109\n",
      "3587087\n",
      "13706308\n",
      "9707387\n",
      "7303682\n",
      "3541996\n",
      "49544037\n",
      "52879916\n",
      "9048146\n",
      "189898035\n",
      "196211654\n",
      "80895\n",
      "104292148\n",
      "4940747\n",
      "189762189\n",
      "52055964\n",
      "52119752\n",
      "6820419\n",
      "705\n",
      "4180426\n",
      "196198494\n",
      "4731156\n",
      "189762527\n",
      "52100019\n",
      "4859466\n",
      "17476563\n",
      "52099983\n",
      "15659560\n",
      "5836739\n",
      "3526501\n",
      "44073277\n",
      "14593464\n",
      "16036784\n",
      "76664821\n",
      "21686013\n",
      "174802358\n",
      "53082673\n",
      "6360322\n",
      "53081280\n",
      "940724\n",
      "173990725\n",
      "4956100\n"
     ]
    }
   ],
   "source": [
    "for k,v in overview_papers_w_latex.items():\n",
    "    if len(list(overview_papers_w_latex[k]['latex_parse']['overview_text']))<=1:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['paper_id', 'metadata', 's2_pdf_hash', 'grobid_parse', 'latex_parse'])\n",
      "dict_keys(['abstract', 'overview_text', 'bib_entries'])\n"
     ]
    }
   ],
   "source": [
    "print(overview_papers_w_latex['5084110'].keys())\n",
    "print(overview_papers_w_latex['5084110']['latex_parse'].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_in(list_toks,line):\n",
    "    flag = False\n",
    "    line = line.lower()\n",
    "    for tok in list_toks:\n",
    "        if tok.lower() in line:\n",
    "            flag = tok\n",
    "            break\n",
    "    return flag\n",
    "RW_names = [\n",
    "    'related wor','background','previous w'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_covering_max = list(map(lambda x: list(overview_papers_w_latex[x]['latex_parse']['overview_text'])[0].lower() if list(overview_papers_w_latex[x]['latex_parse']['overview_text'])[0] else 'None',overview_papers_w_latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             introduction|30.30 % |   1224\n",
      "             related work|17.20 % |    693\n",
      "                     None|9.50 % |    382\n",
      "          acknowledgments|4.60 % |    185\n",
      "         acknowledgements|2.10 % |     84\n",
      "               conclusion|1.70 % |     70\n",
      "               background|0.70 % |     29\n",
      "              conclusions|0.60 % |     23\n",
      "            related works|0.50 % |     20\n",
      "              experiments|0.50 % |     19\n",
      "                 sections|0.40 % |     18\n",
      "                  results|0.40 % |     16\n",
      "               discussion|0.40 % |     16\n",
      "       experimental setup|0.30 % |     13\n",
      "                 datasets|0.30 % |     12\n",
      "                     data|0.30 % |     12\n",
      "                  dataset|0.30 % |     11\n",
      "    experimental settings|0.20 % |     10\n",
      "                baselines|0.20 % |     10\n",
      "conclusion and future work|0.20 % |     10\n",
      "               motivation|0.20 % |      9\n",
      "            previous work|0.20 % |      9\n",
      "               references|0.20 % |      9\n",
      "          acknowledgement|0.20 % |      9\n",
      "               evaluation|0.20 % |      8\n",
      "   implementation details|0.20 % |      8\n",
      "              methodology|0.20 % |      8\n",
      "                    model|0.20 % |      7\n",
      "                   models|0.10 % |      6\n",
      "         training details|0.10 % |      5\n",
      "           acknowledgment|0.10 % |      5\n",
      "                    setup|0.10 % |      5\n",
      "                 overview|0.10 % |      5\n",
      "     experimental setting|0.10 % |      5\n",
      "related work and discussion|0.10 % |      5\n",
      "          baseline models|0.10 % |      4\n",
      "comparison with previous work|0.10 % |      4\n",
      "                 training|0.10 % |      4\n",
      "conclusions and future work|0.10 % |      4\n",
      "                  methods|0.10 % |      4\n",
      "       model architecture|0.10 % |      4\n",
      "          hyperparameters|0.10 % |      4\n",
      "            introduction |0.10 % |      4\n",
      "                  systems|0.10 % |      3\n",
      "     experimental results|0.10 % |      3\n",
      "          reproducibility|0.10 % |      3\n",
      "introduction and background|0.10 % |      3\n",
      "       evaluation metrics|0.10 % |      3\n",
      "     further related work|0.10 % |      3\n",
      "          word embeddings|0.10 % |      3\n",
      "      experiment settings|0.00 % |      2\n",
      "  experiments and results|0.00 % |      2\n",
      "    neural network models|0.00 % |      2\n",
      "        appendix overview|0.00 % |      2\n",
      "assigning grammatical function labels|0.00 % |      2\n",
      "       speech recognition|0.00 % |      2\n",
      "introduction and motivation|0.00 % |      2\n",
      "   results and discussion|0.00 % |      2\n",
      "         other approaches|0.00 % |      2\n",
      "      previous approaches|0.00 % |      2\n",
      "background and preliminaries|0.00 % |      2\n",
      "   supplementary material|0.00 % |      2\n",
      "  quantitative evaluation|0.00 % |      2\n",
      "                 analysis|0.00 % |      2\n",
      "  zero pronoun resolution|0.00 % |      2\n",
      "       quality estimation|0.00 % |      2\n",
      "               the parser|0.00 % |      2\n",
      "                  corpora|0.00 % |      2\n",
      "  unsupervised clustering|0.00 % |      2\n",
      "visual question answering|0.00 % |      2\n",
      "       sentiment analysis|0.00 % |      2\n",
      "  illustrative experiment|0.00 % |      2\n",
      "             architecture|0.00 % |      2\n",
      "  related and future work|0.00 % |      2\n",
      "                 settings|0.00 % |      2\n",
      "  data-to-text generation|0.00 % |      2\n",
      "           baseline model|0.00 % |      2\n",
      "      multi-task learning|0.00 % |      2\n",
      "                attention|0.00 % |      2\n",
      "       question answering|0.00 % |      2\n",
      "     automatic evaluation|0.00 % |      2\n",
      "         task description|0.00 % |      2\n",
      "      data and evaluation|0.00 % |      2\n",
      "motivation and related work|0.00 % |      2\n",
      "         acknowledgments.|0.00 % |      2\n",
      "experimental results and discussion|0.00 % |      2\n",
      "           model training|0.00 % |      2\n",
      "      system architecture|0.00 % |      2\n",
      "       inferring entities|0.00 % |      2\n",
      "discussion and future work|0.00 % |      2\n",
      "        data and settings|0.00 % |      2\n",
      "discussion and conclusions|0.00 % |      2\n",
      "implicit discourse relation recognition|0.00 % |      2\n",
      "    reading comprehension|0.00 % |      2\n",
      "semantic or task-specific similarity?|0.00 % |      1\n",
      "dataset and experimental setup|0.00 % |      1\n",
      "               experiment|0.00 % |      1\n",
      "       multitask learning|0.00 % |      1\n",
      "comparison with state of the arts|0.00 % |      1\n",
      "   pltig and related work|0.00 % |      1\n",
      "  evaluating with sembleu|0.00 % |      1\n",
      "general properties of parlai|0.00 % |      1\n",
      "training text classifiers |0.00 % |      1\n",
      "baselines and implementation details|0.00 % |      1\n",
      "dialogue act classification|0.00 % |      1\n",
      "      attention mechanism|0.00 % |      1\n",
      "            on evaluation|0.00 % |      1\n",
      "related work and datasets|0.00 % |      1\n",
      "concept extraction and term similarity|0.00 % |      1\n",
      "implementation and training details|0.00 % |      1\n",
      "        de-identification|0.00 % |      1\n",
      "     collocations and lfs|0.00 % |      1\n",
      "          data collection|0.00 % |      1\n",
      "clinical temporal relation extraction|0.00 % |      1\n",
      "semeval10 shared task 8 dataset|0.00 % |      1\n",
      "        language typology|0.00 % |      1\n",
      "identifying narrative paragraphs from three text corpora|0.00 % |      1\n",
      "compositionality in distributional models|0.00 % |      1\n",
      "        existing datasets|0.00 % |      1\n",
      "tools for semantic construction|0.00 % |      1\n",
      "    related work and data|0.00 % |      1\n",
      "          metric learning|0.00 % |      1\n",
      "paragraph-level sequence labeling|0.00 % |      1\n",
      " analyzing trained models|0.00 % |      1\n",
      "morphology in language modeling|0.00 % |      1\n",
      "     word-sense induction|0.00 % |      1\n",
      "when intonation and centering collide|0.00 % |      1\n",
      "     multi-encoder models|0.00 % |      1\n",
      "model advice generation details|0.00 % |      1\n",
      "joint multimodal processing|0.00 % |      1\n",
      "      em-based clustering|0.00 % |      1\n",
      "language model pre-training|0.00 % |      1\n",
      "improving low-resource neural machine translation|0.00 % |      1\n",
      "     results & discussion|0.00 % |      1\n",
      "        syntactic islands|0.00 % |      1\n",
      "language model construction|0.00 % |      1\n",
      "natural language inference|0.00 % |      1\n",
      "word embedding based models|0.00 % |      1\n",
      "    cross-lingual mapping|0.00 % |      1\n",
      "abstractive summarization|0.00 % |      1\n",
      "    an incremental parser|0.00 % |      1\n",
      "deriving training data from umls|0.00 % |      1\n",
      "             applications|0.00 % |      1\n",
      "          maximum entropy|0.00 % |      1\n",
      "the type-token relationship (ttr)|0.00 % |      1\n",
      "use-case – adaptive text simplification using crowdsourcing|0.00 % |      1\n",
      "influence in interactions|0.00 % |      1\n",
      "         data and methods|0.00 % |      1\n",
      "long-short term memory network|0.00 % |      1\n",
      "          graph-based mds|0.00 % |      1\n",
      "approaches to identifying dogmatism|0.00 % |      1\n",
      "  application and outlook|0.00 % |      1\n",
      "results and error analysis|0.00 % |      1\n",
      "wsj treebank inconsistencies|0.00 % |      1\n",
      "          semantic parser|0.00 % |      1\n",
      "analysis of complex models|0.00 % |      1\n",
      "          the lhip system|0.00 % |      1\n",
      "       what's morphology?|0.00 % |      1\n",
      "syntactically rich language generation|0.00 % |      1\n",
      "background and motivation|0.00 % |      1\n",
      "bi-directional reconstruction|0.00 % |      1\n",
      "text classification with deep learning|0.00 % |      1\n",
      "  coordination in the ptb|0.00 % |      1\n",
      "experiment 1: optimizing text-based machine translation|0.00 % |      1\n",
      "comparison of model performance and error analysis|0.00 % |      1\n",
      "      task and evaluation|0.00 % |      1\n",
      "    speaker role modeling|0.00 % |      1\n",
      "test suite administration|0.00 % |      1\n",
      "neural relation extraction|0.00 % |      1\n",
      "support-vector machine method|0.00 % |      1\n",
      "              evaluations|0.00 % |      1\n",
      "  restricting the grammar|0.00 % |      1\n",
      "          the source data|0.00 % |      1\n",
      "               pet system|0.00 % |      1\n",
      "            models tested|0.00 % |      1\n",
      "      duluth38 background|0.00 % |      1\n",
      "document-level sentiment classification|0.00 % |      1\n",
      "composition in distributional models|0.00 % |      1\n",
      "            preliminaries|0.00 % |      1\n",
      "emoji sentence representations|0.00 % |      1\n",
      "cross-lingual sentiment analysis|0.00 % |      1\n",
      "     bi-directional lstms|0.00 % |      1\n",
      "                subtask a|0.00 % |      1\n",
      "subtask a - sdqc support/ rumour stance classification|0.00 % |      1\n",
      "           viterbi e-step|0.00 % |      1\n",
      "                babi task|0.00 % |      1\n",
      "english-czech translation|0.00 % |      1\n",
      "         proof of lemma 1|0.00 % |      1\n",
      "  implementations details|0.00 % |      1\n",
      "results on cross-sentence nn-ary relation extraction|0.00 % |      1\n",
      "related work and conclusion|0.00 % |      1\n",
      "             full parsing|0.00 % |      1\n",
      "    the igtree algorithms|0.00 % |      1\n",
      "             rnn variants|0.00 % |      1\n",
      "            interrogative|0.00 % |      1\n",
      "                alignment|0.00 % |      1\n",
      "     directed replacement|0.00 % |      1\n",
      "l k,p 2 \\mathord {l^2_{k,p}}—the monadic second-order language of trees|0.00 % |      1\n",
      "         ethical approval|0.00 % |      1\n",
      "bag generation algorithms|0.00 % |      1\n",
      "relevance detection model|0.00 % |      1\n",
      "   defining the objective|0.00 % |      1\n",
      "             cube pruning|0.00 % |      1\n",
      "       creating instances|0.00 % |      1\n",
      " stochastic top-k listnet|0.00 % |      1\n",
      "       lexical challenges|0.00 % |      1\n",
      "modeling ambiguous pronouns|0.00 % |      1\n",
      "         b-type sequences|0.00 % |      1\n",
      "         table extraction|0.00 % |      1\n",
      "a lemmatizer: p(ℓ i ∣m i ,w i )p(\\ell _i \\mid m_i, w_i)|0.00 % |      1\n",
      "feature-engineering models|0.00 % |      1\n",
      "       sentence selection|0.00 % |      1\n",
      "        the grammar model|0.00 % |      1\n",
      "word embedding feature models|0.00 % |      1\n",
      "cross-lingual word representations|0.00 % |      1\n",
      " maximum mean discrepancy|0.00 % |      1\n",
      "                 modeling|0.00 % |      1\n",
      "interaction with the dialogue manager|0.00 % |      1\n",
      " datasets & architectures|0.00 % |      1\n",
      "     learning experiments|0.00 % |      1\n",
      "background: disambiguation of prepositions and possessives|0.00 % |      1\n",
      "            video channel|0.00 % |      1\n",
      "word vector-based tree edit distance|0.00 % |      1\n",
      "towards a classification scheme: linguistic theories of definite descriptions|0.00 % |      1\n",
      "         reproduciblility|0.00 % |      1\n",
      "            skip-thoughts|0.00 % |      1\n",
      "model selection criterion|0.00 % |      1\n",
      "multilingual nmt versus bilingual nmt|0.00 % |      1\n",
      "the task of grading lexical entailment|0.00 % |      1\n",
      "potential shortcomings of shallow generation methods|0.00 % |      1\n",
      "   additional experiments|0.00 % |      1\n",
      "                embedding|0.00 % |      1\n",
      "rethinking what constitutes abuse|0.00 % |      1\n",
      "evaluating the informative translator|0.00 % |      1\n",
      "knowledge graph embeddings|0.00 % |      1\n",
      "     argumentation mining|0.00 % |      1\n",
      "      data representation|0.00 % |      1\n",
      "heuristic rules for demonstratives|0.00 % |      1\n",
      "definite clause characterization of filtering|0.00 % |      1\n",
      "        three srl systems|0.00 % |      1\n",
      "distributed word representations|0.00 % |      1\n",
      "  a tool for phonologists|0.00 % |      1\n",
      "         proposed methods|0.00 % |      1\n",
      "          two-level rules|0.00 % |      1\n",
      "    proposed architecture|0.00 % |      1\n",
      "revisiting the feature augmentation method|0.00 % |      1\n",
      "abstractive text summarization|0.00 % |      1\n",
      "     extrinsic evaluation|0.00 % |      1\n",
      "hierarchy label graph construction|0.00 % |      1\n",
      "         mean+max pooling|0.00 % |      1\n",
      "discourse information for completing incomplete parses|0.00 % |      1\n",
      "shallow parsers with hand-written rules|0.00 % |      1\n",
      "singular value decomposition|0.00 % |      1\n",
      "                   mctest|0.00 % |      1\n",
      "             autoencoding|0.00 % |      1\n",
      " issues with tokenization|0.00 % |      1\n",
      "    exploiting rationales|0.00 % |      1\n",
      "mdp versus two stage interpretation|0.00 % |      1\n",
      "overlapping mention recognition|0.00 % |      1\n",
      "existing corpora annotated with argumentation structures|0.00 % |      1\n",
      "neural language model architectures|0.00 % |      1\n",
      "sentence compression using multi-task deep bi-lstms|0.00 % |      1\n",
      "the corpus and intercoder reliability study|0.00 % |      1\n",
      "       tree-based encoder|0.00 % |      1\n",
      "aggregation readers and explicit reference readers|0.00 % |      1\n",
      "      learning techniques|0.00 % |      1\n",
      "            co-occurrence|0.00 % |      1\n",
      "            unboundedness|0.00 % |      1\n",
      "               robustness|0.00 % |      1\n",
      "             2. algorithm|0.00 % |      1\n",
      "undirected mst with the boruvka algorithm|0.00 % |      1\n",
      "joint model for two tasks|0.00 % |      1\n",
      " applications and results|0.00 % |      1\n",
      "accounting for linguistic structure|0.00 % |      1\n",
      "      multiple choice cnn|0.00 % |      1\n",
      "      experiments on swda|0.00 % |      1\n",
      "   geocoding reddit users|0.00 % |      1\n",
      "phenomena treated by two-level rules in the czech lexicon|0.00 % |      1\n",
      "constructing modality training data|0.00 % |      1\n",
      "     annotated paragraphs|0.00 % |      1\n",
      "     extended combinators|0.00 % |      1\n",
      "              hocus pocus|0.00 % |      1\n",
      "span-attribute tagging (sa-t) model|0.00 % |      1\n",
      "                  parsing|0.00 % |      1\n",
      "discourse marker applications|0.00 % |      1\n",
      "          neural networks|0.00 % |      1\n",
      "architecture of the grapheme-to-phoneme converter|0.00 % |      1\n",
      "lexical resources for nlp|0.00 % |      1\n",
      "            low-rank grus|0.00 % |      1\n",
      "differences between the two systems|0.00 % |      1\n",
      "neural machine translation background|0.00 % |      1\n",
      "experimental settings and detail|0.00 % |      1\n",
      "a case study: model-building for predicting entailment|0.00 % |      1\n",
      "approaches using prosodic cues|0.00 % |      1\n",
      "graph representation of vector embeddings|0.00 % |      1\n",
      "future works and conclusions|0.00 % |      1\n",
      "            tree lowering|0.00 % |      1\n",
      "contrast to phrase-based smt|0.00 % |      1\n",
      "               ir engines|0.00 % |      1\n",
      "       keystroke dynamics|0.00 % |      1\n",
      "model descriptions and training details|0.00 % |      1\n",
      "  breaking news detection|0.00 % |      1\n",
      "neural network for sentiment classification|0.00 % |      1\n",
      "grammar extraction algorithm|0.00 % |      1\n",
      "              performance|0.00 % |      1\n",
      "nce as a matrix factorization|0.00 % |      1\n",
      "   argument structure svm|0.00 % |      1\n",
      "     speech-to-text model|0.00 % |      1\n",
      "  experimental evaluation|0.00 % |      1\n",
      "     nist chinese-english|0.00 % |      1\n",
      "related cognitive work on metaphor aptness|0.00 % |      1\n",
      "        ranking framework|0.00 % |      1\n",
      "          overall results|0.00 % |      1\n",
      "the right frontier constraint in sdrt|0.00 % |      1\n",
      "naive bayesian classifiers|0.00 % |      1\n",
      "                     code|0.00 % |      1\n",
      "semantic relation classification|0.00 % |      1\n",
      "evaluating existing approaches|0.00 % |      1\n",
      "intra attention mechanism|0.00 % |      1\n",
      "    linguistic hypotheses|0.00 % |      1\n",
      "comparison with direct assessment|0.00 % |      1\n",
      "         systems and data|0.00 % |      1\n",
      "      word representation|0.00 % |      1\n",
      "mechanisms for switching control|0.00 % |      1\n",
      "parsing strategies with spp|0.00 % |      1\n",
      "application to other formalisms|0.00 % |      1\n",
      "          structural cues|0.00 % |      1\n",
      "  linear sequential model|0.00 % |      1\n",
      "        word-level models|0.00 % |      1\n",
      "      linguistic features|0.00 % |      1\n",
      "measuring pragmatic variation|0.00 % |      1\n",
      "graph neural networks (gnns)|0.00 % |      1\n",
      "    dialog state tracking|0.00 % |      1\n",
      "supervised learning of linguistic generalizations|0.00 % |      1\n",
      "         network training|0.00 % |      1\n",
      "    part-of-speech tagger|0.00 % |      1\n",
      " conclusion & future work|0.00 % |      1\n",
      "     typed co-occurrences|0.00 % |      1\n",
      "introductionthis paper also appears in the proceedings of the sixth international conference on applied natural language processing, seattle, wa, april 2000.|0.00 % |      1\n",
      "     transformation stage|0.00 % |      1\n",
      "      similarity features|0.00 % |      1\n",
      "existing methods and datasets for word similarity evaluation|0.00 % |      1\n",
      "lexical rules in categorial grammar|0.00 % |      1\n",
      "cohesiveness of a word list|0.00 % |      1\n",
      "convolution and max-over-time pooling|0.00 % |      1\n",
      "evaluation: noun-compound interpretation tasks|0.00 % |      1\n",
      "  evidence identification|0.00 % |      1\n",
      " recurrent model and data|0.00 % |      1\n",
      "dimensions of cca and kcca projections.|0.00 % |      1\n",
      "methodology of extracting semantic shifts from data|0.00 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    non-task-oriented sds|0.00 % |      1\n",
      "keyphrase extraction and generation|0.00 % |      1\n",
      "referential property constraint|0.00 % |      1\n",
      "nlp event representations|0.00 % |      1\n",
      "     adversarial heatmaps|0.00 % |      1\n",
      "   cross-lingual analysis|0.00 % |      1\n",
      "spanish, dutch and german|0.00 % |      1\n",
      "           word embedding|0.00 % |      1\n",
      "regular tree grammars of derivations|0.00 % |      1\n",
      "     top-down propagation|0.00 % |      1\n",
      "                 examples|0.00 % |      1\n",
      "          unbounded model|0.00 % |      1\n",
      "arabic dialect identification|0.00 % |      1\n",
      "    datasets and settings|0.00 % |      1\n",
      "             online abuse|0.00 % |      1\n",
      "                  summary|0.00 % |      1\n",
      "       evaluation details|0.00 % |      1\n",
      "               base model|0.00 % |      1\n",
      "       dual-coding theory|0.00 % |      1\n",
      "step 3: measure collocational distributions|0.00 % |      1\n",
      "subset approximation by transforming the grammar|0.00 % |      1\n",
      "phrase-based machine translation|0.00 % |      1\n",
      "            fact-checking|0.00 % |      1\n",
      "          morphophonology|0.00 % |      1\n",
      " fixed confidence by ttts|0.00 % |      1\n",
      "    left-to-right parsing|0.00 % |      1\n",
      "   traditional approaches|0.00 % |      1\n",
      "     experimental details|0.00 % |      1\n",
      "           model settings|0.00 % |      1\n",
      "         restricted track|0.00 % |      1\n",
      "     rl phase performance|0.00 % |      1\n",
      "            text matching|0.00 % |      1\n",
      "headword percolation and binarization|0.00 % |      1\n",
      "natural language adversarial examples|0.00 % |      1\n",
      "  the notion of placeness|0.00 % |      1\n",
      "        literature review|0.00 % |      1\n",
      "       data and baselines|0.00 % |      1\n",
      "                data sets|0.00 % |      1\n",
      "related work & background|0.00 % |      1\n",
      "  verb-noun constructions|0.00 % |      1\n",
      "       on genre detection|0.00 % |      1\n",
      "        review of methods|0.00 % |      1\n",
      "automatic discourse tagging|0.00 % |      1\n",
      "multilingual nlp and the role of typologies|0.00 % |      1\n",
      "           gold standards|0.00 % |      1\n",
      " supervised and `pseudo-supervised' methods|0.00 % |      1\n",
      "word similarity evaluation|0.00 % |      1\n",
      "       dataset properties|0.00 % |      1\n",
      "interactive attention vs. coverage model|0.00 % |      1\n",
      "       additional results|0.00 % |      1\n",
      "multilingual sentence embeddings|0.00 % |      1\n",
      "multimodal machine translation|0.00 % |      1\n",
      "  the conversational game|0.00 % |      1\n",
      "mmd vs gan in distribution matching|0.00 % |      1\n",
      "      convolutional layer|0.00 % |      1\n",
      "political ideology detection|0.00 % |      1\n",
      "            pp attachment|0.00 % |      1\n",
      " rnn-based language model|0.00 % |      1\n",
      "learning across representations|0.00 % |      1\n",
      "evidence for limited attention from anaphoric processing|0.00 % |      1\n",
      "test data maintenance and retrieval|0.00 % |      1\n",
      "             introduction|0.00 % |      1\n",
      "previous non-nlp benchmarks|0.00 % |      1\n",
      "automatic keyphrase extraction|0.00 % |      1\n",
      "background and task definition|0.00 % |      1\n",
      "techniques for gans for text|0.00 % |      1\n",
      "temporal information processing|0.00 % |      1\n",
      "    binary classification|0.00 % |      1\n",
      "measuring semantic textual similarity|0.00 % |      1\n",
      "      learning algorithms|0.00 % |      1\n",
      "neural language generation|0.00 % |      1\n",
      "      embeddings training|0.00 % |      1\n",
      "       model improvements|0.00 % |      1\n",
      "                 approach|0.00 % |      1\n",
      "representation and length formulæ|0.00 % |      1\n",
      "         data exploration|0.00 % |      1\n",
      "                 the data|0.00 % |      1\n",
      "       graph-based parser|0.00 % |      1\n",
      "graph convolutional networks|0.00 % |      1\n",
      "implementation details for lace|0.00 % |      1\n",
      "applying probability measures to tree adjoining languages|0.00 % |      1\n",
      "           neural readers|0.00 % |      1\n",
      "appendix i: hyper-parameter details|0.00 % |      1\n",
      "classifying speech segments in isolation|0.00 % |      1\n",
      "centering in japanese discourse|0.00 % |      1\n",
      "          timex templates|0.00 % |      1\n",
      "              future work|0.00 % |      1\n",
      "     a novel architecture|0.00 % |      1\n",
      "sequence tagging with bidirectional rnn|0.00 % |      1\n",
      "             multi-hop rc|0.00 % |      1\n",
      "       sg-mcmc algorithms|0.00 % |      1\n",
      "                  resnets|0.00 % |      1\n",
      "sandhi splitting challenges|0.00 % |      1\n",
      "  discourse embellishment|0.00 % |      1\n",
      "document retrieval and reranking|0.00 % |      1\n",
      "   text generation models|0.00 % |      1\n",
      "                datasets |0.00 % |      1\n",
      "new applications and algorithmic extensions in active learning|0.00 % |      1\n",
      "      evaluation measures|0.00 % |      1\n",
      "   the learning algorithm|0.00 % |      1\n",
      "augmenting data using word2vec|0.00 % |      1\n",
      "      similarity measures|0.00 % |      1\n",
      "        proposed approach|0.00 % |      1\n",
      "        annotation scheme|0.00 % |      1\n",
      "     preparing your paper|0.00 % |      1\n",
      "related work & conclusion|0.00 % |      1\n",
      "          basic algorithm|0.00 % |      1\n",
      "       dual decomposition|0.00 % |      1\n",
      "       improving coverage|0.00 % |      1\n",
      "   japanese `classifiers'|0.00 % |      1\n",
      "       response generator|0.00 % |      1\n",
      "           related works |0.00 % |      1\n",
      "recurrent neural network models|0.00 % |      1\n",
      "         vhmsg components|0.00 % |      1\n",
      "machine translation of varieties|0.00 % |      1\n",
      "         related research|0.00 % |      1\n",
      "conditioned language model|0.00 % |      1\n",
      "          product reviews|0.00 % |      1\n",
      "        model description|0.00 % |      1\n",
      "generating training data from raw text|0.00 % |      1\n",
      "       concluding remarks|0.00 % |      1\n",
      "multimodal machine translation with embedding prediction|0.00 % |      1\n",
      "results on public datasets|0.00 % |      1\n",
      "         story generation|0.00 % |      1\n",
      "             baseline nmt|0.00 % |      1\n",
      "network training and hyper-parameters|0.00 % |      1\n",
      "          problem setting|0.00 % |      1\n",
      " existing related methods|0.00 % |      1\n",
      "     clustering algorithm|0.00 % |      1\n",
      "    hyperparameter tuning|0.00 % |      1\n",
      "           common setting|0.00 % |      1\n",
      "evidence-based models of context|0.00 % |      1\n",
      "                 framenet|0.00 % |      1\n",
      "other vision and language tasks|0.00 % |      1\n",
      "grammatical error correction|0.00 % |      1\n",
      "     coreference in lasie|0.00 % |      1\n",
      "       sentence embedding|0.00 % |      1\n",
      "            the algorithm|0.00 % |      1\n",
      "   methods not considered|0.00 % |      1\n",
      "       the annotated data|0.00 % |      1\n",
      "            memory module|0.00 % |      1\n",
      "         the sense tagger|0.00 % |      1\n",
      "                 seq-rnns|0.00 % |      1\n",
      "           classification|0.00 % |      1\n",
      "large dataset for training and validation|0.00 % |      1\n",
      "        analysis of depnn|0.00 % |      1\n",
      "       nn-gram experiment|0.00 % |      1\n",
      "            final results|0.00 % |      1\n",
      "porting grammars and lexica between closely related languages|0.00 % |      1\n",
      "  the bible: 62 languages|0.00 % |      1\n",
      "implications for annotation|0.00 % |      1\n",
      "       sentiment lexicons|0.00 % |      1\n",
      "multi-channel convolutional layer|0.00 % |      1\n",
      "description of heuristics baseline|0.00 % |      1\n",
      "reuters cross-lingual document classification|0.00 % |      1\n",
      "     empirical comparison|0.00 % |      1\n",
      "neural sequence-to-sequence model|0.00 % |      1\n",
      "   measures of similarity|0.00 % |      1\n",
      "parameter estimation and inference via posterior regularization|0.00 % |      1\n",
      " attention models for nlp|0.00 % |      1\n",
      "     candidate generation|0.00 % |      1\n",
      "                  derinet|0.00 % |      1\n",
      "                  tagging|0.00 % |      1\n",
      "a basic sequence-to-sequence approach|0.00 % |      1\n",
      "      clustering with mdl|0.00 % |      1\n",
      "    next token prediction|0.00 % |      1\n",
      "range concatenation grammar as a pivot formalism|0.00 % |      1\n",
      "    the learning approach|0.00 % |      1\n",
      "      the test collection|0.00 % |      1\n",
      "word vectors and wordless word vectors|0.00 % |      1\n",
      "               simulation|0.00 % |      1\n",
      "          state generator|0.00 % |      1\n",
      "          data statistics|0.00 % |      1\n",
      "using randomization for precision and f-score|0.00 % |      1\n",
      "dropped pronoun recovery.|0.00 % |      1\n",
      "    applications of focus|0.00 % |      1\n",
      "application of relaxation labeling to nlp|0.00 % |      1\n",
      "       datasets and tasks|0.00 % |      1\n",
      "       domain differences|0.00 % |      1\n",
      "statistical inference in bayesum|0.00 % |      1\n",
      "datasets and graph construction|0.00 % |      1\n",
      "    comparing the parsers|0.00 % |      1\n",
      "   geometry of embeddings|0.00 % |      1\n",
      " measuring dialogue costs|0.00 % |      1\n",
      "measuring attributional similarity|0.00 % |      1\n",
      "     bio-based srl models|0.00 % |      1\n",
      "comparison against baselines|0.00 % |      1\n",
      "categorization of hate speech|0.00 % |      1\n",
      "                  duluth2|0.00 % |      1\n",
      "          temporal search|0.00 % |      1\n",
      "    composition functions|0.00 % |      1\n",
      "             dataset used|0.00 % |      1\n",
      "recurrent layer adaptation|0.00 % |      1\n",
      "     reframing entailment|0.00 % |      1\n",
      "textual clues: object oriented description|0.00 % |      1\n",
      "rte classification experiments for contradiction and disagreeing reply detection|0.00 % |      1\n",
      "further discussion: specialising semantic spaces|0.00 % |      1\n",
      "many tasks one sequence to sequence|0.00 % |      1\n",
      "       sentence rewriting|0.00 % |      1\n",
      "incremental grammar development|0.00 % |      1\n",
      "syntax marginal inference for dependency paths|0.00 % |      1\n",
      "     persistence diagrams|0.00 % |      1\n",
      "              pos tagging|0.00 % |      1\n",
      "       training the model|0.00 % |      1\n",
      "         design solutions|0.00 % |      1\n",
      "                     hnmt|0.00 % |      1\n",
      " table-to-text generation|0.00 % |      1\n",
      "      training algorithms|0.00 % |      1\n",
      "formalizing inflectional morphology|0.00 % |      1\n",
      "                 corpuses|0.00 % |      1\n",
      "fine-grained entity typing|0.00 % |      1\n",
      "combinatory category grammar supertagging|0.00 % |      1\n",
      "            related tasks|0.00 % |      1\n",
      "experimental setup and results|0.00 % |      1\n",
      "semi-supervised wake-sleep|0.00 % |      1\n",
      "    supervised evaluation|0.00 % |      1\n",
      "bridging nlp models and neurolinguistics|0.00 % |      1\n",
      "phrase-based and neural machine translation|0.00 % |      1\n",
      "gated graph neural networks|0.00 % |      1\n",
      "semi-supervised learning using multilingual data|0.00 % |      1\n",
      "  response selection task|0.00 % |      1\n",
      "          document reader|0.00 % |      1\n",
      " online dialogue features|0.00 % |      1\n",
      "         experiment setup|0.00 % |      1\n",
      "           proposed model|0.00 % |      1\n",
      "             segmentation|0.00 % |      1\n",
      "a probabilistically-derived measure for unithood determination|0.00 % |      1\n",
      "      overall performance|0.00 % |      1\n",
      "introduction and related work|0.00 % |      1\n",
      "          lambek calculus|0.00 % |      1\n",
      "subtask 1: mining semantic fields|0.00 % |      1\n",
      "    neural-based approach|0.00 % |      1\n",
      "                 features|0.00 % |      1\n",
      "residual vq-vae for unsupervised monolingual paraphrasing|0.00 % |      1\n",
      "        domain adaptation|0.00 % |      1\n",
      "encoding structured data of kbs|0.00 % |      1\n",
      "deep learning on extra-linguistic features|0.00 % |      1\n",
      "        task descriptions|0.00 % |      1\n",
      " semi-supervised learning|0.00 % |      1\n",
      "             paraphrasing|0.00 % |      1\n",
      "results and interpretation|0.00 % |      1\n",
      "       rnn-based students|0.00 % |      1\n",
      "distributional semantic word representation|0.00 % |      1\n",
      "nmt with relation networks|0.00 % |      1\n",
      "       spanish taxonomies|0.00 % |      1\n",
      "  shared task competition|0.00 % |      1\n",
      "        the grammar (set)|0.00 % |      1\n",
      "        naive translation|0.00 % |      1\n",
      "statistical machine translation|0.00 % |      1\n",
      "    finite-state calculus|0.00 % |      1\n",
      "      system descriptions|0.00 % |      1\n",
      "sentiment analysis and text categorization|0.00 % |      1\n",
      "              integration|0.00 % |      1\n",
      "multilingual attentional nmt|0.00 % |      1\n",
      "               estimation|0.00 % |      1\n",
      " technical implementation|0.00 % |      1\n",
      " a feature-based approach|0.00 % |      1\n",
      "         preliminary test|0.00 % |      1\n",
      "a nonstationary language model|0.00 % |      1\n",
      "deep learning for sentiment analysis|0.00 % |      1\n",
      "      sentence similarity|0.00 % |      1\n",
      "              environment|0.00 % |      1\n",
      "introduction and related works|0.00 % |      1\n",
      "extending annotation coverage|0.00 % |      1\n",
      "        neural benchmarks|0.00 % |      1\n",
      "dependency-based algorithm (dba)|0.00 % |      1\n",
      "unicon: an implementation|0.00 % |      1\n",
      "sequence labeling neural models|0.00 % |      1\n",
      "                  setting|0.00 % |      1\n",
      "        annotation graphs|0.00 % |      1\n",
      "                 decoding|0.00 % |      1\n",
      "conditional variational autoencoder |0.00 % |      1\n",
      "     baseline system (bl)|0.00 % |      1\n",
      "near threshold structure in j/ψ→γpp ¯j/\\psi \\rightarrow \\gamma p \\bar{p}|0.00 % |      1\n",
      "stack long short-term memory (lstm)|0.00 % |      1\n",
      "      a dynamic framework|0.00 % |      1\n",
      "hashing under the nvi framework|0.00 % |      1\n",
      "   treebank concatenation|0.00 % |      1\n",
      "       transfer algorithm|0.00 % |      1\n",
      "human evaluation of machine translation|0.00 % |      1\n",
      "sparse representations for expansion|0.00 % |      1\n",
      "sequence to sequence learning|0.00 % |      1\n",
      "adding adjoining constraints|0.00 % |      1\n",
      "contextualized word embeddings|0.00 % |      1\n",
      "neural network approaches|0.00 % |      1\n",
      "        nmt architectures|0.00 % |      1\n",
      "               appendices|0.00 % |      1\n",
      "               appendix a|0.00 % |      1\n",
      "learning base noun phrases by machine|0.00 % |      1\n",
      "pairwise system agreement|0.00 % |      1\n",
      "                   inputs|0.00 % |      1\n",
      "     monolingual baseline|0.00 % |      1\n",
      "enhanced sequential inference model|0.00 % |      1\n",
      "transfer learning for nmt|0.00 % |      1\n",
      "background and related work|0.00 % |      1\n",
      "                 resource|0.00 % |      1\n",
      "   subword-unit-based nmt|0.00 % |      1\n",
      "learning as search optimization|0.00 % |      1\n",
      "cues for tracking initiative|0.00 % |      1\n",
      "non-dl learning algorithms|0.00 % |      1\n",
      "  features and agreements|0.00 % |      1\n",
      "       system description|0.00 % |      1\n",
      "       emotion annotation|0.00 % |      1\n",
      "features of individual documents|0.00 % |      1\n",
      "manifestation of highly contrasting word pairs in text|0.00 % |      1\n",
      "impacts of integrating the simple ppdb|0.00 % |      1\n",
      "       out-of-domain data|0.00 % |      1\n",
      "lexical semantic collocations|0.00 % |      1\n",
      "   distance-based methods|0.00 % |      1\n",
      "multiple timescale gated recurrent unit|0.00 % |      1\n",
      "universal language model fine-tuning|0.00 % |      1\n",
      "     external comparisons|0.00 % |      1\n",
      "conditional training (ct)|0.00 % |      1\n",
      "software engineering vs. grammar engineering|0.00 % |      1\n",
      "neural machine translation: conditional language modelling|0.00 % |      1\n",
      "background: cross-lingual embeddings|0.00 % |      1\n",
      "evaluating sentence representations|0.00 % |      1\n",
      "     word sense induction|0.00 % |      1\n",
      "distributional word representations|0.00 % |      1\n",
      "limitations of the grammar|0.00 % |      1\n",
      "compilation of sorted feature terms|0.00 % |      1\n",
      "    modality and negation|0.00 % |      1\n",
      "summary, comparisons, and ongoing work|0.00 % |      1\n",
      "mildly context-sensitive language recognition|0.00 % |      1\n",
      "                   set-up|0.00 % |      1\n",
      "               classifier|0.00 % |      1\n",
      "distributional language embeddings|0.00 % |      1\n",
      "            collaboration|0.00 % |      1\n",
      "      contextual encoding|0.00 % |      1\n",
      "              gap dataset|0.00 % |      1\n",
      "             encoder: gru|0.00 % |      1\n",
      "grounding kernels in natural language dictionaries|0.00 % |      1\n",
      "transduction with soft attention|0.00 % |      1\n",
      "         image captioning|0.00 % |      1\n",
      "                    tasks|0.00 % |      1\n",
      "related works and conclusions|0.00 % |      1\n",
      "approximation by dendroid distribution|0.00 % |      1\n",
      "            task overview|0.00 % |      1\n",
      "    word similarity tasks|0.00 % |      1\n",
      "communicative function: a common thread in generation resources|0.00 % |      1\n",
      "encoder-decoder models for structured output prediction|0.00 % |      1\n",
      "datasets and experimental setup|0.00 % |      1\n",
      "      anatomy of a parser|0.00 % |      1\n",
      "   gated-attention reader|0.00 % |      1\n",
      "             exp-ii: absa|0.00 % |      1\n",
      "modeling constraint quality|0.00 % |      1\n",
      "   multi-source neural mt|0.00 % |      1\n",
      "     ibm computer manuals|0.00 % |      1\n",
      "   reinforcement learning|0.00 % |      1\n",
      "  additional related work|0.00 % |      1\n",
      "              medical ner|0.00 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           nmt background|0.00 % |      1\n",
      "            penn treebank|0.00 % |      1\n",
      "               procedure:|0.00 % |      1\n",
      "      the pointer softmax|0.00 % |      1\n",
      "      classifier settings|0.00 % |      1\n",
      "construction of an s-type transducer |0.00 % |      1\n",
      " predicting dialogue acts|0.00 % |      1\n",
      " augmented word embedding|0.00 % |      1\n",
      "   typed feature grammars|0.00 % |      1\n",
      "hedging as a sign of scientific discourse|0.00 % |      1\n",
      "            the formalism|0.00 % |      1\n",
      "further details on evaluation dataset|0.00 % |      1\n",
      "                algorithm|0.00 % |      1\n",
      "          text classifier|0.00 % |      1\n",
      "lstm neural reordering model|0.00 % |      1\n",
      "   training and test data|0.00 % |      1\n",
      "existing models of attachment|0.00 % |      1\n",
      "      extended levi graph|0.00 % |      1\n",
      "    conversational models|0.00 % |      1\n",
      "the task: base np chunking|0.00 % |      1\n",
      "       maximizing metrics|0.00 % |      1\n",
      "deriving chunks from treebank parses|0.00 % |      1\n",
      "                     glue|0.00 % |      1\n",
      "  generation architecture|0.00 % |      1\n",
      "neural embedding methods for relational learning|0.00 % |      1\n",
      "asymptotic normality and statistical efficiency|0.00 % |      1\n",
      "experimental evaluation: bucc shared task on mining bitexts|0.00 % |      1\n",
      "              translation|0.00 % |      1\n",
      "     encoder pre-training|0.00 % |      1\n",
      "             nli datasets|0.00 % |      1\n",
      "related work on grammar induction|0.00 % |      1\n",
      "improving over wmt2017 systems|0.00 % |      1\n",
      "         captioning model|0.00 % |      1\n",
      "     quantitative results|0.00 % |      1\n",
      "neural machine translation|0.00 % |      1\n",
      "   long-short term memory|0.00 % |      1\n",
      "“soft\" (em) joint training|0.00 % |      1\n",
      "the problem of vp ellipsis|0.00 % |      1\n",
      "       mlstm-based system|0.00 % |      1\n",
      "an overview of tutorialbank|0.00 % |      1\n",
      "            topic vectors|0.00 % |      1\n",
      "linguistic input features|0.00 % |      1\n",
      "   basic parsing paradigm|0.00 % |      1\n",
      "           ampere dataset|0.00 % |      1\n",
      " distributional semantics|0.00 % |      1\n",
      "experiment and discussion|0.00 % |      1\n",
      "multi-label classification|0.00 % |      1\n",
      "adapting esa to 2011 wikipedia|0.00 % |      1\n",
      "            system setup |0.00 % |      1\n",
      "interpretable paraphrase generation|0.00 % |      1\n",
      "retrieval of relevant documents and sentences|0.00 % |      1\n",
      "graph convolutional network encoders|0.00 % |      1\n",
      "phrase-based baseline systems|0.00 % |      1\n",
      "dialogue generation and visual dialogue|0.00 % |      1\n",
      "similar, associated, and both|0.00 % |      1\n",
      " other similarity methods|0.00 % |      1\n",
      "             model design|0.00 % |      1\n",
      "     representation model|0.00 % |      1\n",
      "       sememes and hownet|0.00 % |      1\n",
      "                 chunking|0.00 % |      1\n",
      "eye-tracking database for sarcasm analysis|0.00 % |      1\n",
      "          stanford reader|0.00 % |      1\n",
      "           string kernels|0.00 % |      1\n",
      "        distance matrices|0.00 % |      1\n",
      "  semantic representation|0.00 % |      1\n",
      "consonant co-occurrence network|0.00 % |      1\n",
      "applying error-correcting codes|0.00 % |      1\n",
      "      results on ace 2005|0.00 % |      1\n",
      "               the corpus|0.00 % |      1\n",
      "         neural mt system|0.00 % |      1\n",
      "            arbitrariness|0.00 % |      1\n",
      "pre-training the projection layer|0.00 % |      1\n",
      "deep convolutional networks|0.00 % |      1\n",
      "exposure bias and error propagation|0.00 % |      1\n",
      "deep learning in summarization|0.00 % |      1\n",
      "         baseline methods|0.00 % |      1\n",
      "using multilayer annotations|0.00 % |      1\n",
      "    theoretical framework|0.00 % |      1\n",
      "models for temporal relations extraction|0.00 % |      1\n",
      "  vae for text generation|0.00 % |      1\n",
      "stylistic similarity evaluation|0.00 % |      1\n",
      "generalization to n>2n > 2|0.00 % |      1\n",
      "         multi-task model|0.00 % |      1\n",
      "detailed analysis for english-french|0.00 % |      1\n",
      "comparison to state-of-the-art supervised methods|0.00 % |      1\n",
      "          text collection|0.00 % |      1\n",
      "                reranking|0.00 % |      1\n",
      "          attention layer|0.00 % |      1\n",
      "measuring domain and noise in data|0.00 % |      1\n",
      "      fine-tuning details|0.00 % |      1\n",
      "stir: strongly incremental repair detection|0.00 % |      1\n",
      "bilm derived substitutions|0.00 % |      1\n",
      "        utterance encoder|0.00 % |      1\n",
      "    gender identification|0.00 % |      1\n",
      "            related work |0.00 % |      1\n",
      "performance improvement from answer re-ranking|0.00 % |      1\n",
      "adversarial domain adaptation and domain generation|0.00 % |      1\n",
      "   information extraction|0.00 % |      1\n",
      "        transfer learning|0.00 % |      1\n",
      "patterns extraction and analysis|0.00 % |      1\n",
      "neural auto-regressive topic model|0.00 % |      1\n",
      "               parameters|0.00 % |      1\n",
      "inter-annotator agreement|0.00 % |      1\n",
      "topic-based cross-referencing|0.00 % |      1\n",
      "     word representations|0.00 % |      1\n",
      "building the benchmark corpus|0.00 % |      1\n",
      "previous systems built on acl anthology|0.00 % |      1\n",
      "an alternative: evaluation via gap-filling|0.00 % |      1\n",
      "   application-evaluation|0.00 % |      1\n",
      "    error detection model|0.00 % |      1\n",
      "applications of phonetic vectors|0.00 % |      1\n",
      " iterated response models|0.00 % |      1\n",
      "language modeling baselines|0.00 % |      1\n",
      "monolingual and bilingual embeddings|0.00 % |      1\n",
      "       sentiment features|0.00 % |      1\n",
      "  document classification|0.00 % |      1\n",
      "      methods and results|0.00 % |      1\n",
      "      modifying particles|0.00 % |      1\n",
      "semi-supervised spoken language understanding|0.00 % |      1\n",
      "comparison with other work|0.00 % |      1\n",
      "          system overview|0.00 % |      1\n",
      "        sentence matching|0.00 % |      1\n",
      " advcls: adversarial classifier|0.00 % |      1\n",
      "   common topic inference|0.00 % |      1\n",
      "          required skills|0.00 % |      1\n",
      "background: attention-based neural machine translation|0.00 % |      1\n",
      "arabic morphological analyzer/generator|0.00 % |      1\n",
      "machine reading comprehension|0.00 % |      1\n",
      "dataset and evaluation criteria|0.00 % |      1\n",
      "   noun compound analysis|0.00 % |      1\n",
      "       temporal reasoning|0.00 % |      1\n",
      "multi-class classification|0.00 % |      1\n",
      "        parallel datasets|0.00 % |      1\n",
      "features of collaborative negotiation|0.00 % |      1\n",
      "experiments with large margin loss|0.00 % |      1\n",
      "degree adverbs in linguistics|0.00 % |      1\n",
      "word-synchronous beam search|0.00 % |      1\n",
      "  short text conversation|0.00 % |      1\n",
      "neural machine translation evaluation|0.00 % |      1\n",
      "evaluation of pronoun translation|0.00 % |      1\n",
      "story generation with planning|0.00 % |      1\n",
      "    training and decoding|0.00 % |      1\n",
      "word embedding recommendations|0.00 % |      1\n",
      "                materials|0.00 % |      1\n",
      "       entity recognition|0.00 % |      1\n",
      "actions and attributes dataset|0.00 % |      1\n",
      "       semantic functions|0.00 % |      1\n",
      "    content determination|0.00 % |      1\n",
      "datasets and representation of ideas|0.00 % |      1\n",
      "              file format|0.00 % |      1\n",
      "         dialogue systems|0.00 % |      1\n",
      "analysis of frequency and polysemy|0.00 % |      1\n",
      "    hate speech detection|0.00 % |      1\n",
      "variational encoder-decoder (ved)|0.00 % |      1\n",
      "   abstraction approaches|0.00 % |      1\n",
      "selection of the closest variant|0.00 % |      1\n",
      "estimating p(k j |c i )p(k_j|c_i)|0.00 % |      1\n",
      "static and contextualized words embeddings|0.00 % |      1\n",
      "translation lexicon extraction|0.00 % |      1\n",
      "       lda baseline model|0.00 % |      1\n",
      "memory augmented neural networks|0.00 % |      1\n",
      "        ans model fitting|0.00 % |      1\n",
      " hyper-parameter settings|0.00 % |      1\n",
      "         final evaluation|0.00 % |      1\n",
      "comparing statistical parsers|0.00 % |      1\n",
      "               validation|0.00 % |      1\n",
      "dual conditional cross-entropy filtering|0.00 % |      1\n",
      "true label prediction in crowdsourcing|0.00 % |      1\n",
      "           representation|0.00 % |      1\n",
      "   sequence tagging model|0.00 % |      1\n",
      "   ed baselines & results|0.00 % |      1\n",
      "measuring grammar quality|0.00 % |      1\n",
      "   comparison of coverage|0.00 % |      1\n",
      "   the statistical tagger|0.00 % |      1\n",
      "word and sense interconnectivity|0.00 % |      1\n",
      "   baselines & evaluation|0.00 % |      1\n",
      "recurrent models for text classification|0.00 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         extracting edits|0.00 % |      1\n",
      "   affective text dataset|0.00 % |      1\n",
      "syntactic features in text classification|0.00 % |      1\n",
      "assigning thesaurus categories|0.00 % |      1\n",
      "     empirical evaluation|0.00 % |      1\n",
      " a specification language|0.00 % |      1\n",
      "    conclusion and future|0.00 % |      1\n",
      "  confidence and salience|0.00 % |      1\n",
      "     readability measures|0.00 % |      1\n",
      "                 ace 2005|0.00 % |      1\n",
      "analysis of translation errors|0.00 % |      1\n",
      "      result and thoughts|0.00 % |      1\n",
      "            existing work|0.00 % |      1\n",
      "        nmt configuration|0.00 % |      1\n",
      "rules of sampling sql queries|0.00 % |      1\n",
      "                     arae|0.00 % |      1\n",
      "tree structure enhanced neural machine translation|0.00 % |      1\n",
      "implementing hpsg in a clp framework|0.00 % |      1\n",
      "   end-to-end nlg systems|0.00 % |      1\n",
      "operationalizing irregularity|0.00 % |      1\n",
      "              equivalence|0.00 % |      1\n",
      "gender stereotypes in text|0.00 % |      1\n",
      "search-based structured prediction|0.00 % |      1\n",
      "           related models|0.00 % |      1\n",
      "historical spelling normalization|0.00 % |      1\n",
      "introductionthe author is currently at texas instruments and all inquiries should be addressed to rajeev@csc.ti.com.|0.00 % |      1\n",
      "    inflection generation|0.00 % |      1\n",
      "the elements of the olac metadata set|0.00 % |      1\n",
      "      evaluation datasets|0.00 % |      1\n",
      "   semantic role labeling|0.00 % |      1\n",
      "   relevant previous work|0.00 % |      1\n",
      "fake news detection using textual information|0.00 % |      1\n",
      "   maximum-entropy method|0.00 % |      1\n",
      "     model initialization|0.00 % |      1\n",
      " link function of bigrams|0.00 % |      1\n",
      "     major and minor keys|0.00 % |      1\n",
      " semantic representations|0.00 % |      1\n",
      "              the problem|0.00 % |      1\n",
      "  summary and future work|0.00 % |      1\n",
      "      experimental set-up|0.00 % |      1\n",
      "  deterministic inference|0.00 % |      1\n",
      "recognizing textual entailment|0.00 % |      1\n",
      " bidirectional lstm layer|0.00 % |      1\n",
      "   the tagger and corpora|0.00 % |      1\n",
      "                   system|0.00 % |      1\n",
      "           tasks and data|0.00 % |      1\n",
      "experimental data, setup and results|0.00 % |      1\n",
      "      when to communicate|0.00 % |      1\n",
      "          coding features|0.00 % |      1\n",
      "integrating language technology with machine learning|0.00 % |      1\n",
      "          data for task 2|0.00 % |      1\n",
      "    experimental datasets|0.00 % |      1\n",
      "action recognition models|0.00 % |      1\n",
      "       weka configuration|0.00 % |      1\n",
      "compared against other models|0.00 % |      1\n",
      "a long time ago, in a galaxy far, far away|0.00 % |      1\n",
      "related work and existing datasets|0.00 % |      1\n",
      "visualization of perceptron weights.|0.00 % |      1\n",
      "frame tracking: an extension of state tracking|0.00 % |      1\n",
      "current sign language research|0.00 % |      1\n",
      "supervised review summary generation|0.00 % |      1\n",
      " data augmentation in nlp|0.00 % |      1\n",
      "handling of unknown words|0.00 % |      1\n",
      "         importance score|0.00 % |      1\n",
      "pens classification scheme|0.00 % |      1\n",
      "   combination techniques|0.00 % |      1\n",
      "ensembles and model diversity|0.00 % |      1\n",
      "               text model|0.00 % |      1\n",
      "       sentiment baseline|0.00 % |      1\n",
      "relation to knowledge graph embedding|0.00 % |      1\n",
      "                  tag set|0.00 % |      1\n",
      "      problem formulation|0.00 % |      1\n",
      "symmetry between conjuncts|0.00 % |      1\n",
      "english–finnish and finnish–english|0.00 % |      1\n",
      "  a grammar for discourse|0.00 % |      1\n",
      "coupled-lstms for strong sentence interaction|0.00 % |      1\n",
      "lexicalization and the probability model|0.00 % |      1\n",
      "            visualization|0.00 % |      1\n",
      "       keyword extraction|0.00 % |      1\n",
      "3 examples of defaults in vm–gen|0.00 % |      1\n",
      "outline of the generation process|0.00 % |      1\n",
      "selectional preference and sense ambiguity|0.00 % |      1\n",
      "       bidirectional lstm|0.00 % |      1\n",
      " machine learning results|0.00 % |      1\n",
      "       syntactic analysis|0.00 % |      1\n",
      "c-test difficulty prediction|0.00 % |      1\n",
      "       fluency evaluation|0.00 % |      1\n",
      "replicability analysis for nlp|0.00 % |      1\n",
      "ucca's semantic structures|0.00 % |      1\n",
      "sampling argument scrambling via syntactic transformations|0.00 % |      1\n",
      "              nmt systems|0.00 % |      1\n",
      "       feature importance|0.00 % |      1\n",
      "       model and training|0.00 % |      1\n",
      "              test corpus|0.00 % |      1\n",
      "general neural model for chinese word segmentation|0.00 % |      1\n",
      "statistics on collected data (mturk subset)|0.00 % |      1\n",
      "          proposed models|0.00 % |      1\n",
      "         paragraph reader|0.00 % |      1\n",
      "flexible sense distinctions|0.00 % |      1\n",
      "debiasing word embeddings|0.00 % |      1\n",
      "    alignment-based model|0.00 % |      1\n",
      "           simulated data|0.00 % |      1\n",
      "baseline rnn-based encdec model|0.00 % |      1\n",
      "  topic modeling analysis|0.00 % |      1\n",
      "conditional probability model|0.00 % |      1\n",
      "the dynamic programming table|0.00 % |      1\n",
      "  verb frame alternations|0.00 % |      1\n",
      "                inference|0.00 % |      1\n",
      "compositionality in vector space|0.00 % |      1\n",
      "          atis experiment|0.00 % |      1\n",
      "         centering theory|0.00 % |      1\n",
      "word-level polarity features|0.00 % |      1\n",
      "   learning and inference|0.00 % |      1\n",
      "       learning algorithm|0.00 % |      1\n",
      "the proposed doc architecture|0.00 % |      1\n",
      "naive bayes and logistic regression|0.00 % |      1\n",
      "   knowledge distillation|0.00 % |      1\n",
      "multi-space variational autoencoders|0.00 % |      1\n",
      "scientific paper datasets|0.00 % |      1\n",
      "       rule-based systems|0.00 % |      1\n",
      "       evaluation dataset|0.00 % |      1\n",
      " negation scope detection|0.00 % |      1\n",
      "parallelism and inference|0.00 % |      1\n",
      "           transfer tasks|0.00 % |      1\n",
      "independent normalisation|0.00 % |      1\n",
      "related work and conclusions|0.00 % |      1\n",
      "applications and future work|0.00 % |      1\n",
      "     data and experiments|0.00 % |      1\n",
      "      semi-supervised nmt|0.00 % |      1\n",
      "  alignment visualization|0.00 % |      1\n",
      "the glr parsing algorithm|0.00 % |      1\n",
      "              pos-tagging|0.00 % |      1\n",
      "      evaluation settings|0.00 % |      1\n",
      "related work on temporal relation modeling|0.00 % |      1\n",
      "                 treebank|0.00 % |      1\n",
      "shortcomings of current summarization models|0.00 % |      1\n",
      "     network architecture|0.00 % |      1\n",
      "sexual predator detection|0.00 % |      1\n",
      "dataset and experimental settings|0.00 % |      1\n",
      "            the treebanks|0.00 % |      1\n",
      "motivation: robust nlp systems|0.00 % |      1\n",
      "         the second model|0.00 % |      1\n",
      "lexical replacement in phylogenetics|0.00 % |      1\n",
      "bootstrapping a first-person sentiment corpus|0.00 % |      1\n",
      "            dag gru model|0.00 % |      1\n",
      "     character-level nlp.|0.00 % |      1\n",
      "data intelligence methods|0.00 % |      1\n",
      " targeted feature dropout|0.00 % |      1\n",
      "underspecified semantic tagging|0.00 % |      1\n",
      "training and hyperparameter tuning|0.00 % |      1\n",
      "                 appendix|0.00 % |      1\n",
      "          model debiasing|0.00 % |      1\n",
      "comparison with related work|0.00 % |      1\n",
      "predicting ordinal judgments|0.00 % |      1\n"
     ]
    }
   ],
   "source": [
    "for ind,x,y in zip(pd.Series(tex_covering_max).value_counts().index,pd.Series(tex_covering_max).value_counts()/len(tex_covering_max)*100,pd.Series(tex_covering_max).value_counts()):\n",
    "    print(\"%25s|%.2f %% |%7d\" % (ind,round(x,1),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "covering_by_tex = list(map(lambda x: 'related work' if ('related work' in x) or ('background' in x) or ('previous work' in x) or ('overview' in x) else x, covering_by_tex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             introduction|37.70 % |   1208\n",
      "             related work|25.00 % |    802\n",
      "               conclusion|0.60 % |     19\n",
      "                     None|0.60 % |     19\n",
      "              experiments|0.60 % |     19\n",
      "                 sections|0.60 % |     18\n",
      "                  results|0.50 % |     16\n",
      "               discussion|0.50 % |     15\n",
      "       experimental setup|0.40 % |     13\n",
      "                 datasets|0.40 % |     12\n",
      "                     data|0.40 % |     12\n",
      "                  dataset|0.30 % |     11\n",
      "    experimental settings|0.30 % |     10\n",
      "                baselines|0.30 % |     10\n",
      "          acknowledgments|0.30 % |      9\n",
      "               motivation|0.30 % |      9\n",
      "              methodology|0.20 % |      8\n",
      "   implementation details|0.20 % |      8\n",
      "               evaluation|0.20 % |      8\n",
      "                    model|0.20 % |      7\n",
      "                   models|0.20 % |      5\n",
      "                    setup|0.20 % |      5\n",
      "              conclusions|0.20 % |      5\n",
      "     experimental setting|0.20 % |      5\n",
      "                 training|0.10 % |      4\n",
      "          baseline models|0.10 % |      4\n",
      "conclusion and future work|0.10 % |      4\n",
      "       model architecture|0.10 % |      4\n",
      "          hyperparameters|0.10 % |      4\n",
      "            introduction |0.10 % |      4\n",
      "         training details|0.10 % |      4\n",
      "conclusions and future work|0.10 % |      3\n",
      "       evaluation metrics|0.10 % |      3\n",
      "                  methods|0.10 % |      3\n",
      "                  systems|0.10 % |      3\n",
      "          word embeddings|0.10 % |      3\n",
      "         task description|0.10 % |      2\n",
      "discussion and future work|0.10 % |      2\n",
      "       speech recognition|0.10 % |      2\n",
      "     experimental results|0.10 % |      2\n",
      "introduction and motivation|0.10 % |      2\n",
      "implicit discourse relation recognition|0.10 % |      2\n",
      "                  corpora|0.10 % |      2\n",
      "       quality estimation|0.10 % |      2\n",
      "visual question answering|0.10 % |      2\n",
      "      experiment settings|0.10 % |      2\n",
      "   results and discussion|0.10 % |      2\n",
      "      previous approaches|0.10 % |      2\n",
      "  unsupervised clustering|0.10 % |      2\n",
      "               the parser|0.10 % |      2\n",
      "  experiments and results|0.10 % |      2\n",
      "  zero pronoun resolution|0.10 % |      2\n",
      "assigning grammatical function labels|0.10 % |      2\n",
      "        data and settings|0.10 % |      2\n",
      "       sentiment analysis|0.10 % |      2\n",
      "    neural network models|0.10 % |      2\n",
      "       question answering|0.10 % |      2\n",
      "                 settings|0.10 % |      2\n",
      "  data-to-text generation|0.10 % |      2\n",
      "      multi-task learning|0.10 % |      2\n",
      "  related and future work|0.10 % |      2\n",
      "             architecture|0.10 % |      2\n",
      "     automatic evaluation|0.10 % |      2\n",
      "                attention|0.10 % |      2\n",
      "experimental results and discussion|0.10 % |      2\n",
      "      system architecture|0.10 % |      2\n",
      "      data and evaluation|0.10 % |      2\n",
      "           baseline model|0.10 % |      2\n",
      "  illustrative experiment|0.10 % |      2\n",
      "           model training|0.10 % |      2\n",
      "    reading comprehension|0.10 % |      2\n",
      "         other approaches|0.10 % |      2\n",
      "                 chunking|0.00 % |      1\n",
      "  evaluating with sembleu|0.00 % |      1\n",
      "emoji sentence representations|0.00 % |      1\n",
      "discourse information for completing incomplete parses|0.00 % |      1\n",
      "baselines and implementation details|0.00 % |      1\n",
      "neural machine translation|0.00 % |      1\n",
      "       multitask learning|0.00 % |      1\n",
      "sampling argument scrambling via syntactic transformations|0.00 % |      1\n",
      "compositionality in distributional models|0.00 % |      1\n",
      "               experiment|0.00 % |      1\n",
      "comparison with state of the arts|0.00 % |      1\n",
      "          data collection|0.00 % |      1\n",
      "dialogue act classification|0.00 % |      1\n",
      "dataset and experimental setup|0.00 % |      1\n",
      "concept extraction and term similarity|0.00 % |      1\n",
      "        existing datasets|0.00 % |      1\n",
      "     collocations and lfs|0.00 % |      1\n",
      "clinical temporal relation extraction|0.00 % |      1\n",
      "semeval10 shared task 8 dataset|0.00 % |      1\n",
      "        language typology|0.00 % |      1\n",
      "semantic or task-specific similarity?|0.00 % |      1\n",
      "        de-identification|0.00 % |      1\n",
      "identifying narrative paragraphs from three text corpora|0.00 % |      1\n",
      "          metric learning|0.00 % |      1\n",
      "            on evaluation|0.00 % |      1\n",
      "implementation and training details|0.00 % |      1\n",
      "tools for semantic construction|0.00 % |      1\n",
      "      task and evaluation|0.00 % |      1\n",
      "memory augmented neural networks|0.00 % |      1\n",
      "          semantic parser|0.00 % |      1\n",
      " analyzing trained models|0.00 % |      1\n",
      "morphology in language modeling|0.00 % |      1\n",
      "     word-sense induction|0.00 % |      1\n",
      "          required skills|0.00 % |      1\n",
      "     multi-encoder models|0.00 % |      1\n",
      "model advice generation details|0.00 % |      1\n",
      "joint multimodal processing|0.00 % |      1\n",
      "   end-to-end nlg systems|0.00 % |      1\n",
      "language model pre-training|0.00 % |      1\n",
      "improving low-resource neural machine translation|0.00 % |      1\n",
      "     results & discussion|0.00 % |      1\n",
      "natural language inference|0.00 % |      1\n",
      "word embedding based models|0.00 % |      1\n",
      "    cross-lingual mapping|0.00 % |      1\n",
      "abstractive summarization|0.00 % |      1\n",
      "                embedding|0.00 % |      1\n",
      "deriving training data from umls|0.00 % |      1\n",
      "             applications|0.00 % |      1\n",
      "          maximum entropy|0.00 % |      1\n",
      "      problem formulation|0.00 % |      1\n",
      "use-case – adaptive text simplification using crowdsourcing|0.00 % |      1\n",
      "influence in interactions|0.00 % |      1\n",
      "         data and methods|0.00 % |      1\n",
      "paragraph-level sequence labeling|0.00 % |      1\n",
      "          graph-based mds|0.00 % |      1\n",
      "approaches to identifying dogmatism|0.00 % |      1\n",
      "            models tested|0.00 % |      1\n",
      "analysis of complex models|0.00 % |      1\n",
      "             dataset used|0.00 % |      1\n",
      "       what's morphology?|0.00 % |      1\n",
      "syntactically rich language generation|0.00 % |      1\n",
      "            final results|0.00 % |      1\n",
      "text classification with deep learning|0.00 % |      1\n",
      "  coordination in the ptb|0.00 % |      1\n",
      "experiment 1: optimizing text-based machine translation|0.00 % |      1\n",
      "comparison of model performance and error analysis|0.00 % |      1\n",
      "results and error analysis|0.00 % |      1\n",
      "test suite administration|0.00 % |      1\n",
      "         neural mt system|0.00 % |      1\n",
      "neural relation extraction|0.00 % |      1\n",
      "support-vector machine method|0.00 % |      1\n",
      "              evaluations|0.00 % |      1\n",
      "  restricting the grammar|0.00 % |      1\n",
      "          the source data|0.00 % |      1\n",
      "phrase-based machine translation|0.00 % |      1\n",
      "composition in distributional models|0.00 % |      1\n",
      "            preliminaries|0.00 % |      1\n",
      "    speaker role modeling|0.00 % |      1\n",
      "  application and outlook|0.00 % |      1\n",
      "definite clause characterization of filtering|0.00 % |      1\n",
      "revisiting the feature augmentation method|0.00 % |      1\n",
      "       tree-based encoder|0.00 % |      1\n",
      "             full parsing|0.00 % |      1\n",
      "a lemmatizer: p(ℓ i ∣m i ,w i )p(\\ell _i \\mid m_i, w_i)|0.00 % |      1\n",
      "         table extraction|0.00 % |      1\n",
      "subtask a - sdqc support/ rumour stance classification|0.00 % |      1\n",
      "           viterbi e-step|0.00 % |      1\n",
      "                babi task|0.00 % |      1\n",
      "              environment|0.00 % |      1\n",
      "english-czech translation|0.00 % |      1\n",
      "         proof of lemma 1|0.00 % |      1\n",
      "  implementations details|0.00 % |      1\n",
      "        transfer learning|0.00 % |      1\n",
      "replicability analysis for nlp|0.00 % |      1\n",
      "   measures of similarity|0.00 % |      1\n",
      "                alignment|0.00 % |      1\n",
      "     directed replacement|0.00 % |      1\n",
      "l k,p 2 \\mathord {l^2_{k,p}}—the monadic second-order language of trees|0.00 % |      1\n",
      "bag generation algorithms|0.00 % |      1\n",
      "relevance detection model|0.00 % |      1\n",
      " distributional semantics|0.00 % |      1\n",
      "             cube pruning|0.00 % |      1\n",
      "       creating instances|0.00 % |      1\n",
      " stochastic top-k listnet|0.00 % |      1\n",
      "       lexical challenges|0.00 % |      1\n",
      "feature-engineering models|0.00 % |      1\n",
      "     argumentation mining|0.00 % |      1\n",
      "sentence compression using multi-task deep bi-lstms|0.00 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word vector-based tree edit distance|0.00 % |      1\n",
      "        the grammar model|0.00 % |      1\n",
      "word embedding feature models|0.00 % |      1\n",
      "cross-lingual word representations|0.00 % |      1\n",
      " maximum mean discrepancy|0.00 % |      1\n",
      "                 modeling|0.00 % |      1\n",
      "sparse representations for expansion|0.00 % |      1\n",
      "interaction with the dialogue manager|0.00 % |      1\n",
      " datasets & architectures|0.00 % |      1\n",
      "     learning experiments|0.00 % |      1\n",
      "            video channel|0.00 % |      1\n",
      "       sentence selection|0.00 % |      1\n",
      "knowledge graph embeddings|0.00 % |      1\n",
      "towards a classification scheme: linguistic theories of definite descriptions|0.00 % |      1\n",
      "            skip-thoughts|0.00 % |      1\n",
      "model selection criterion|0.00 % |      1\n",
      "multilingual nmt versus bilingual nmt|0.00 % |      1\n",
      "the task of grading lexical entailment|0.00 % |      1\n",
      "          overall results|0.00 % |      1\n",
      "   additional experiments|0.00 % |      1\n",
      "         multi-task model|0.00 % |      1\n",
      "rethinking what constitutes abuse|0.00 % |      1\n",
      "               robustness|0.00 % |      1\n",
      "  linear sequential model|0.00 % |      1\n",
      "modeling ambiguous pronouns|0.00 % |      1\n",
      "      data representation|0.00 % |      1\n",
      "hierarchy label graph construction|0.00 % |      1\n",
      "            co-occurrence|0.00 % |      1\n",
      "      learning techniques|0.00 % |      1\n",
      "distributed word representations|0.00 % |      1\n",
      "  a tool for phonologists|0.00 % |      1\n",
      "         proposed methods|0.00 % |      1\n",
      "cross-lingual sentiment analysis|0.00 % |      1\n",
      "             rnn variants|0.00 % |      1\n",
      "approaches using prosodic cues|0.00 % |      1\n",
      "abstractive text summarization|0.00 % |      1\n",
      "     extrinsic evaluation|0.00 % |      1\n",
      "         mean+max pooling|0.00 % |      1\n",
      "                subtask a|0.00 % |      1\n",
      "graph neural networks (gnns)|0.00 % |      1\n",
      "singular value decomposition|0.00 % |      1\n",
      "                  duluth2|0.00 % |      1\n",
      "             autoencoding|0.00 % |      1\n",
      " issues with tokenization|0.00 % |      1\n",
      "    exploiting rationales|0.00 % |      1\n",
      "mdp versus two stage interpretation|0.00 % |      1\n",
      "overlapping mention recognition|0.00 % |      1\n",
      "existing corpora annotated with argumentation structures|0.00 % |      1\n",
      "neural language model architectures|0.00 % |      1\n",
      "            unboundedness|0.00 % |      1\n",
      "a case study: model-building for predicting entailment|0.00 % |      1\n",
      "experimental settings and detail|0.00 % |      1\n",
      "differences between the two systems|0.00 % |      1\n",
      "heuristic rules for demonstratives|0.00 % |      1\n",
      "        three srl systems|0.00 % |      1\n",
      "  generation architecture|0.00 % |      1\n",
      "undirected mst with the boruvka algorithm|0.00 % |      1\n",
      "joint model for two tasks|0.00 % |      1\n",
      " applications and results|0.00 % |      1\n",
      "       syntactic analysis|0.00 % |      1\n",
      "      multiple choice cnn|0.00 % |      1\n",
      "      experiments on swda|0.00 % |      1\n",
      "   geocoding reddit users|0.00 % |      1\n",
      "phenomena treated by two-level rules in the czech lexicon|0.00 % |      1\n",
      "     extended combinators|0.00 % |      1\n",
      "software engineering vs. grammar engineering|0.00 % |      1\n",
      "multilingual sentence embeddings|0.00 % |      1\n",
      "span-attribute tagging (sa-t) model|0.00 % |      1\n",
      "                  parsing|0.00 % |      1\n",
      "discourse marker applications|0.00 % |      1\n",
      "          neural networks|0.00 % |      1\n",
      "architecture of the grapheme-to-phoneme converter|0.00 % |      1\n",
      "lexical resources for nlp|0.00 % |      1\n",
      "            low-rank grus|0.00 % |      1\n",
      "long-short term memory network|0.00 % |      1\n",
      "machine translation of varieties|0.00 % |      1\n",
      "          two-level rules|0.00 % |      1\n",
      "     speech-to-text model|0.00 % |      1\n",
      "contrast to phrase-based smt|0.00 % |      1\n",
      "               ir engines|0.00 % |      1\n",
      "       keystroke dynamics|0.00 % |      1\n",
      "model descriptions and training details|0.00 % |      1\n",
      "  breaking news detection|0.00 % |      1\n",
      "neural network for sentiment classification|0.00 % |      1\n",
      "       out-of-domain data|0.00 % |      1\n",
      "grammar extraction algorithm|0.00 % |      1\n",
      "              performance|0.00 % |      1\n",
      "nce as a matrix factorization|0.00 % |      1\n",
      "   argument structure svm|0.00 % |      1\n",
      "  experimental evaluation|0.00 % |      1\n",
      "   defining the objective|0.00 % |      1\n",
      "     nist chinese-english|0.00 % |      1\n",
      "related cognitive work on metaphor aptness|0.00 % |      1\n",
      "        ranking framework|0.00 % |      1\n",
      "the right frontier constraint in sdrt|0.00 % |      1\n",
      "naive bayesian classifiers|0.00 % |      1\n",
      "semantic relation classification|0.00 % |      1\n",
      "intra attention mechanism|0.00 % |      1\n",
      "    linguistic hypotheses|0.00 % |      1\n",
      "comparison with direct assessment|0.00 % |      1\n",
      "         systems and data|0.00 % |      1\n",
      "      word representation|0.00 % |      1\n",
      "parsing strategies with spp|0.00 % |      1\n",
      "      the pointer softmax|0.00 % |      1\n",
      "application to other formalisms|0.00 % |      1\n",
      "   treebank concatenation|0.00 % |      1\n",
      "      learning algorithms|0.00 % |      1\n",
      "measuring semantic textual similarity|0.00 % |      1\n",
      "    binary classification|0.00 % |      1\n",
      "      linguistic features|0.00 % |      1\n",
      "measuring pragmatic variation|0.00 % |      1\n",
      "    dialog state tracking|0.00 % |      1\n",
      "         network training|0.00 % |      1\n",
      "    part-of-speech tagger|0.00 % |      1\n",
      "     typed co-occurrences|0.00 % |      1\n",
      "introductionthis paper also appears in the proceedings of the sixth international conference on applied natural language processing, seattle, wa, april 2000.|0.00 % |      1\n",
      "      similarity features|0.00 % |      1\n",
      "existing methods and datasets for word similarity evaluation|0.00 % |      1\n",
      "distributional language embeddings|0.00 % |      1\n",
      "convolution and max-over-time pooling|0.00 % |      1\n",
      "  evidence identification|0.00 % |      1\n",
      " recurrent model and data|0.00 % |      1\n",
      "  verb frame alternations|0.00 % |      1\n",
      "methodology of extracting semantic shifts from data|0.00 % |      1\n",
      "    non-task-oriented sds|0.00 % |      1\n",
      "keyphrase extraction and generation|0.00 % |      1\n",
      "referential property constraint|0.00 % |      1\n",
      "nlp event representations|0.00 % |      1\n",
      "        word-level models|0.00 % |      1\n",
      "aggregation readers and explicit reference readers|0.00 % |      1\n",
      "   cross-lingual analysis|0.00 % |      1\n",
      "future works and conclusions|0.00 % |      1\n",
      "            fact-checking|0.00 % |      1\n",
      "          unbounded model|0.00 % |      1\n",
      "arabic dialect identification|0.00 % |      1\n",
      "    datasets and settings|0.00 % |      1\n",
      "             online abuse|0.00 % |      1\n",
      "                  summary|0.00 % |      1\n",
      "       evaluation details|0.00 % |      1\n",
      "               base model|0.00 % |      1\n",
      "       dual-coding theory|0.00 % |      1\n",
      "               references|0.00 % |      1\n",
      "subset approximation by transforming the grammar|0.00 % |      1\n",
      "applications and future work|0.00 % |      1\n",
      " fixed confidence by ttts|0.00 % |      1\n",
      "          structural cues|0.00 % |      1\n",
      "    left-to-right parsing|0.00 % |      1\n",
      "   traditional approaches|0.00 % |      1\n",
      "     experimental details|0.00 % |      1\n",
      "           model settings|0.00 % |      1\n",
      "         restricted track|0.00 % |      1\n",
      "     rl phase performance|0.00 % |      1\n",
      "            text matching|0.00 % |      1\n",
      "headword percolation and binarization|0.00 % |      1\n",
      "natural language adversarial examples|0.00 % |      1\n",
      "  the notion of placeness|0.00 % |      1\n",
      "        literature review|0.00 % |      1\n",
      "                 examples|0.00 % |      1\n",
      "       data and baselines|0.00 % |      1\n",
      "                data sets|0.00 % |      1\n",
      "  verb-noun constructions|0.00 % |      1\n",
      "spanish, dutch and german|0.00 % |      1\n",
      "        review of methods|0.00 % |      1\n",
      "syntax marginal inference for dependency paths|0.00 % |      1\n",
      "multilingual nlp and the role of typologies|0.00 % |      1\n",
      "           gold standards|0.00 % |      1\n",
      " supervised and `pseudo-supervised' methods|0.00 % |      1\n",
      "word similarity evaluation|0.00 % |      1\n",
      "       dataset properties|0.00 % |      1\n",
      "interactive attention vs. coverage model|0.00 % |      1\n",
      "       additional results|0.00 % |      1\n",
      "transfer learning for nmt|0.00 % |      1\n",
      "  the conversational game|0.00 % |      1\n",
      "mmd vs gan in distribution matching|0.00 % |      1\n",
      "      convolutional layer|0.00 % |      1\n",
      "political ideology detection|0.00 % |      1\n",
      "            pp attachment|0.00 % |      1\n",
      " rnn-based language model|0.00 % |      1\n",
      "learning across representations|0.00 % |      1\n",
      "evidence for limited attention from anaphoric processing|0.00 % |      1\n",
      "previous non-nlp benchmarks|0.00 % |      1\n",
      "automatic keyphrase extraction|0.00 % |      1\n",
      "techniques for gans for text|0.00 % |      1\n",
      "temporal information processing|0.00 % |      1\n",
      "augmenting data using word2vec|0.00 % |      1\n",
      "reuters cross-lingual document classification|0.00 % |      1\n",
      "parameter estimation and inference via posterior regularization|0.00 % |      1\n",
      "applying error-correcting codes|0.00 % |      1\n",
      "   the learning algorithm|0.00 % |      1\n",
      "neural language generation|0.00 % |      1\n",
      "      evaluation measures|0.00 % |      1\n",
      "                  resnets|0.00 % |      1\n",
      "       model improvements|0.00 % |      1\n",
      "                 approach|0.00 % |      1\n",
      "         data exploration|0.00 % |      1\n",
      "       graph-based parser|0.00 % |      1\n",
      "graph convolutional networks|0.00 % |      1\n",
      "implementation details for lace|0.00 % |      1\n",
      "applying probability measures to tree adjoining languages|0.00 % |      1\n",
      "classifying speech segments in isolation|0.00 % |      1\n",
      "the type-token relationship (ttr)|0.00 % |      1\n",
      "centering in japanese discourse|0.00 % |      1\n",
      "     bi-directional lstms|0.00 % |      1\n",
      "              future work|0.00 % |      1\n",
      "     a novel architecture|0.00 % |      1\n",
      "sequence tagging with bidirectional rnn|0.00 % |      1\n",
      "             multi-hop rc|0.00 % |      1\n",
      "       sg-mcmc algorithms|0.00 % |      1\n",
      "              hocus pocus|0.00 % |      1\n",
      "pairwise system agreement|0.00 % |      1\n",
      "  discourse embellishment|0.00 % |      1\n",
      "document retrieval and reranking|0.00 % |      1\n",
      "      similarity measures|0.00 % |      1\n",
      "     clustering algorithm|0.00 % |      1\n",
      "compositionality in vector space|0.00 % |      1\n",
      "          problem setting|0.00 % |      1\n",
      "           word embedding|0.00 % |      1\n",
      "        proposed approach|0.00 % |      1\n",
      "        annotation scheme|0.00 % |      1\n",
      "          basic algorithm|0.00 % |      1\n",
      "       dual decomposition|0.00 % |      1\n",
      "       improving coverage|0.00 % |      1\n",
      "   japanese `classifiers'|0.00 % |      1\n",
      "   knowledge distillation|0.00 % |      1\n",
      "          state generator|0.00 % |      1\n",
      "recurrent neural network models|0.00 % |      1\n",
      "  shared task competition|0.00 % |      1\n",
      "actions and attributes dataset|0.00 % |      1\n",
      "       concluding remarks|0.00 % |      1\n",
      "         related research|0.00 % |      1\n",
      "conditioned language model|0.00 % |      1\n",
      "          product reviews|0.00 % |      1\n",
      "        model description|0.00 % |      1\n",
      "general properties of parlai|0.00 % |      1\n",
      "graph representation of vector embeddings|0.00 % |      1\n",
      "multimodal machine translation with embedding prediction|0.00 % |      1\n",
      "         story generation|0.00 % |      1\n",
      "             baseline nmt|0.00 % |      1\n",
      "network training and hyper-parameters|0.00 % |      1\n",
      "   text generation models|0.00 % |      1\n",
      "                datasets |0.00 % |      1\n",
      "new applications and algorithmic extensions in active learning|0.00 % |      1\n",
      "          temporal search|0.00 % |      1\n",
      "                  tagging|0.00 % |      1\n",
      "                  derinet|0.00 % |      1\n",
      "     candidate generation|0.00 % |      1\n",
      "other vision and language tasks|0.00 % |      1\n",
      "grammatical error correction|0.00 % |      1\n",
      "       sentence embedding|0.00 % |      1\n",
      "   methods not considered|0.00 % |      1\n",
      "       the annotated data|0.00 % |      1\n",
      "            memory module|0.00 % |      1\n",
      "         the sense tagger|0.00 % |      1\n",
      "                 seq-rnns|0.00 % |      1\n",
      "           classification|0.00 % |      1\n",
      "        analysis of depnn|0.00 % |      1\n",
      "                inference|0.00 % |      1\n",
      "porting grammars and lexica between closely related languages|0.00 % |      1\n",
      "  the bible: 62 languages|0.00 % |      1\n",
      "implications for annotation|0.00 % |      1\n",
      "       sentiment lexicons|0.00 % |      1\n",
      "multi-channel convolutional layer|0.00 % |      1\n",
      "results on cross-sentence nn-ary relation extraction|0.00 % |      1\n",
      " attention models for nlp|0.00 % |      1\n",
      "     empirical comparison|0.00 % |      1\n",
      "neural sequence-to-sequence model|0.00 % |      1\n",
      "       datasets and tasks|0.00 % |      1\n",
      " hyper-parameter settings|0.00 % |      1\n",
      "           common setting|0.00 % |      1\n",
      "categorization of hate speech|0.00 % |      1\n",
      "constructing modality training data|0.00 % |      1\n",
      "evidence-based models of context|0.00 % |      1\n",
      "                 framenet|0.00 % |      1\n",
      "      clustering with mdl|0.00 % |      1\n",
      "    next token prediction|0.00 % |      1\n",
      "range concatenation grammar as a pivot formalism|0.00 % |      1\n",
      "      the test collection|0.00 % |      1\n",
      "word vectors and wordless word vectors|0.00 % |      1\n",
      "          data statistics|0.00 % |      1\n",
      "using randomization for precision and f-score|0.00 % |      1\n",
      "dropped pronoun recovery.|0.00 % |      1\n",
      "a basic sequence-to-sequence approach|0.00 % |      1\n",
      "application of relaxation labeling to nlp|0.00 % |      1\n",
      "       domain differences|0.00 % |      1\n",
      "statistical inference in bayesum|0.00 % |      1\n",
      "      attention mechanism|0.00 % |      1\n",
      "datasets and graph construction|0.00 % |      1\n",
      "rte classification experiments for contradiction and disagreeing reply detection|0.00 % |      1\n",
      "   geometry of embeddings|0.00 % |      1\n",
      " measuring dialogue costs|0.00 % |      1\n",
      "measuring attributional similarity|0.00 % |      1\n",
      "     bio-based srl models|0.00 % |      1\n",
      "comparison against baselines|0.00 % |      1\n",
      "    composition functions|0.00 % |      1\n",
      "translation lexicon extraction|0.00 % |      1\n",
      "         extracting edits|0.00 % |      1\n",
      "fine-grained entity typing|0.00 % |      1\n",
      "further discussion: specialising semantic spaces|0.00 % |      1\n",
      "many tasks one sequence to sequence|0.00 % |      1\n",
      "       sentence rewriting|0.00 % |      1\n",
      "incremental grammar development|0.00 % |      1\n",
      "             introduction|0.00 % |      1\n",
      "     persistence diagrams|0.00 % |      1\n",
      "              pos tagging|0.00 % |      1\n",
      "                     hnmt|0.00 % |      1\n",
      " table-to-text generation|0.00 % |      1\n",
      "      training algorithms|0.00 % |      1\n",
      "                 corpuses|0.00 % |      1\n",
      "combinatory category grammar supertagging|0.00 % |      1\n",
      "               estimation|0.00 % |      1\n",
      "        sentence matching|0.00 % |      1\n",
      "experimental setup and results|0.00 % |      1\n",
      "semi-supervised wake-sleep|0.00 % |      1\n",
      "    supervised evaluation|0.00 % |      1\n",
      "bridging nlp models and neurolinguistics|0.00 % |      1\n",
      "phrase-based and neural machine translation|0.00 % |      1\n",
      "gated graph neural networks|0.00 % |      1\n",
      "semi-supervised learning using multilingual data|0.00 % |      1\n",
      "  response selection task|0.00 % |      1\n",
      "     transformation stage|0.00 % |      1\n",
      "              integration|0.00 % |      1\n",
      "textual clues: object oriented description|0.00 % |      1\n",
      "          document reader|0.00 % |      1\n",
      "     network architecture|0.00 % |      1\n",
      "sentiment analysis and text categorization|0.00 % |      1\n",
      "a probabilistically-derived measure for unithood determination|0.00 % |      1\n",
      "      overall performance|0.00 % |      1\n",
      "          lambek calculus|0.00 % |      1\n",
      "subtask 1: mining semantic fields|0.00 % |      1\n",
      "    neural-based approach|0.00 % |      1\n",
      "                 features|0.00 % |      1\n",
      "residual vq-vae for unsupervised monolingual paraphrasing|0.00 % |      1\n",
      "        domain adaptation|0.00 % |      1\n",
      "encoding structured data of kbs|0.00 % |      1\n",
      "deep learning on extra-linguistic features|0.00 % |      1\n",
      "        task descriptions|0.00 % |      1\n",
      "             paraphrasing|0.00 % |      1\n",
      "results and interpretation|0.00 % |      1\n",
      "       rnn-based students|0.00 % |      1\n",
      "distributional semantic word representation|0.00 % |      1\n",
      "nmt with relation networks|0.00 % |      1\n",
      "       spanish taxonomies|0.00 % |      1\n",
      "         vhmsg components|0.00 % |      1\n",
      "variational encoder-decoder (ved)|0.00 % |      1\n",
      "        naive translation|0.00 % |      1\n",
      "statistical machine translation|0.00 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    finite-state calculus|0.00 % |      1\n",
      "      system descriptions|0.00 % |      1\n",
      "multilingual attentional nmt|0.00 % |      1\n",
      " technical implementation|0.00 % |      1\n",
      "recurrent models for text classification|0.00 % |      1\n",
      "conditional variational autoencoder |0.00 % |      1\n",
      "a nonstationary language model|0.00 % |      1\n",
      "deep learning for sentiment analysis|0.00 % |      1\n",
      "      sentence similarity|0.00 % |      1\n",
      "extending annotation coverage|0.00 % |      1\n",
      "        neural benchmarks|0.00 % |      1\n",
      "dependency-based algorithm (dba)|0.00 % |      1\n",
      "unicon: an implementation|0.00 % |      1\n",
      "sequence labeling neural models|0.00 % |      1\n",
      "                  setting|0.00 % |      1\n",
      "        annotation graphs|0.00 % |      1\n",
      "                 decoding|0.00 % |      1\n",
      "     baseline system (bl)|0.00 % |      1\n",
      "learning base noun phrases by machine|0.00 % |      1\n",
      "near threshold structure in j/ψ→γpp ¯j/\\psi \\rightarrow \\gamma p \\bar{p}|0.00 % |      1\n",
      "       semantic functions|0.00 % |      1\n",
      "      a dynamic framework|0.00 % |      1\n",
      "hashing under the nvi framework|0.00 % |      1\n",
      "evaluating the informative translator|0.00 % |      1\n",
      "human evaluation of machine translation|0.00 % |      1\n",
      "      em-based clustering|0.00 % |      1\n",
      "sequence to sequence learning|0.00 % |      1\n",
      "adding adjoining constraints|0.00 % |      1\n",
      "          text collection|0.00 % |      1\n",
      "                reranking|0.00 % |      1\n",
      "         preliminary test|0.00 % |      1\n",
      " a feature-based approach|0.00 % |      1\n",
      "contextualized word embeddings|0.00 % |      1\n",
      "neural network approaches|0.00 % |      1\n",
      "                   inputs|0.00 % |      1\n",
      "     monolingual baseline|0.00 % |      1\n",
      "enhanced sequential inference model|0.00 % |      1\n",
      "search-based structured prediction|0.00 % |      1\n",
      "                 resource|0.00 % |      1\n",
      "   subword-unit-based nmt|0.00 % |      1\n",
      "learning as search optimization|0.00 % |      1\n",
      "cues for tracking initiative|0.00 % |      1\n",
      "non-dl learning algorithms|0.00 % |      1\n",
      "       system description|0.00 % |      1\n",
      "phrase-based baseline systems|0.00 % |      1\n",
      "features of individual documents|0.00 % |      1\n",
      "manifestation of highly contrasting word pairs in text|0.00 % |      1\n",
      "impacts of integrating the simple ppdb|0.00 % |      1\n",
      "  summary and future work|0.00 % |      1\n",
      "automatic discourse tagging|0.00 % |      1\n",
      "   distance-based methods|0.00 % |      1\n",
      "multiple timescale gated recurrent unit|0.00 % |      1\n",
      "universal language model fine-tuning|0.00 % |      1\n",
      "     external comparisons|0.00 % |      1\n",
      "conditional training (ct)|0.00 % |      1\n",
      "  features and agreements|0.00 % |      1\n",
      "        nmt architectures|0.00 % |      1\n",
      "             segmentation|0.00 % |      1\n",
      "frame tracking: an extension of state tracking|0.00 % |      1\n",
      "         experiment setup|0.00 % |      1\n",
      "             encoder: gru|0.00 % |      1\n",
      "limitations of the grammar|0.00 % |      1\n",
      "compilation of sorted feature terms|0.00 % |      1\n",
      "    modality and negation|0.00 % |      1\n",
      "summary, comparisons, and ongoing work|0.00 % |      1\n",
      "mildly context-sensitive language recognition|0.00 % |      1\n",
      "                   set-up|0.00 % |      1\n",
      "      embeddings training|0.00 % |      1\n",
      "bi-directional reconstruction|0.00 % |      1\n",
      "            collaboration|0.00 % |      1\n",
      "experimental evaluation: bucc shared task on mining bitexts|0.00 % |      1\n",
      "              gap dataset|0.00 % |      1\n",
      "grounding kernels in natural language dictionaries|0.00 % |      1\n",
      "neural machine translation: conditional language modelling|0.00 % |      1\n",
      "evaluating existing approaches|0.00 % |      1\n",
      "         image captioning|0.00 % |      1\n",
      "                    tasks|0.00 % |      1\n",
      "approximation by dendroid distribution|0.00 % |      1\n",
      "    word similarity tasks|0.00 % |      1\n",
      "communicative function: a common thread in generation resources|0.00 % |      1\n",
      "encoder-decoder models for structured output prediction|0.00 % |      1\n",
      "datasets and experimental setup|0.00 % |      1\n",
      "      anatomy of a parser|0.00 % |      1\n",
      "   gated-attention reader|0.00 % |      1\n",
      "neural embedding methods for relational learning|0.00 % |      1\n",
      "distributional word representations|0.00 % |      1\n",
      "     word sense induction|0.00 % |      1\n",
      "             exp-ii: absa|0.00 % |      1\n",
      "evaluating sentence representations|0.00 % |      1\n",
      "   reinforcement learning|0.00 % |      1\n",
      "              medical ner|0.00 % |      1\n",
      "            penn treebank|0.00 % |      1\n",
      "               procedure:|0.00 % |      1\n",
      "      classifier settings|0.00 % |      1\n",
      "     reframing entailment|0.00 % |      1\n",
      "patterns extraction and analysis|0.00 % |      1\n",
      " predicting dialogue acts|0.00 % |      1\n",
      " augmented word embedding|0.00 % |      1\n",
      "   typed feature grammars|0.00 % |      1\n",
      "further details on evaluation dataset|0.00 % |      1\n",
      "                algorithm|0.00 % |      1\n",
      "          text classifier|0.00 % |      1\n",
      "lstm neural reordering model|0.00 % |      1\n",
      "   training and test data|0.00 % |      1\n",
      "existing models of attachment|0.00 % |      1\n",
      "      extended levi graph|0.00 % |      1\n",
      "    conversational models|0.00 % |      1\n",
      "the task: base np chunking|0.00 % |      1\n",
      "       maximizing metrics|0.00 % |      1\n",
      "deriving chunks from treebank parses|0.00 % |      1\n",
      "                     glue|0.00 % |      1\n",
      "               classifier|0.00 % |      1\n",
      "asymptotic normality and statistical efficiency|0.00 % |      1\n",
      "     coreference in lasie|0.00 % |      1\n",
      "eye-tracking database for sarcasm analysis|0.00 % |      1\n",
      "shallow parsers with hand-written rules|0.00 % |      1\n",
      "             nli datasets|0.00 % |      1\n",
      "improving over wmt2017 systems|0.00 % |      1\n",
      "         captioning model|0.00 % |      1\n",
      "     quantitative results|0.00 % |      1\n",
      "   long-short term memory|0.00 % |      1\n",
      "“soft\" (em) joint training|0.00 % |      1\n",
      "the problem of vp ellipsis|0.00 % |      1\n",
      "measuring domain and noise in data|0.00 % |      1\n",
      "            topic vectors|0.00 % |      1\n",
      "linguistic input features|0.00 % |      1\n",
      "   basic parsing paradigm|0.00 % |      1\n",
      "                   mctest|0.00 % |      1\n",
      "experiment and discussion|0.00 % |      1\n",
      "multi-label classification|0.00 % |      1\n",
      "adapting esa to 2011 wikipedia|0.00 % |      1\n",
      "            system setup |0.00 % |      1\n",
      "retrieval of relevant documents and sentences|0.00 % |      1\n",
      "graph convolutional network encoders|0.00 % |      1\n",
      "                 analysis|0.00 % |      1\n",
      "           neural readers|0.00 % |      1\n",
      "dialogue generation and visual dialogue|0.00 % |      1\n",
      "similar, associated, and both|0.00 % |      1\n",
      "   multi-source neural mt|0.00 % |      1\n",
      "     encoder pre-training|0.00 % |      1\n",
      "             model design|0.00 % |      1\n",
      "          stanford reader|0.00 % |      1\n",
      "training text classifiers |0.00 % |      1\n",
      "           string kernels|0.00 % |      1\n",
      "  semantic representation|0.00 % |      1\n",
      "consonant co-occurrence network|0.00 % |      1\n",
      "handling of unknown words|0.00 % |      1\n",
      "      results on ace 2005|0.00 % |      1\n",
      "               the corpus|0.00 % |      1\n",
      "              nmt systems|0.00 % |      1\n",
      "            arbitrariness|0.00 % |      1\n",
      "   supplementary material|0.00 % |      1\n",
      "       sememes and hownet|0.00 % |      1\n",
      "pre-training the projection layer|0.00 % |      1\n",
      "exposure bias and error propagation|0.00 % |      1\n",
      "deep learning in summarization|0.00 % |      1\n",
      "         baseline methods|0.00 % |      1\n",
      "    theoretical framework|0.00 % |      1\n",
      "models for temporal relations extraction|0.00 % |      1\n",
      "  vae for text generation|0.00 % |      1\n",
      "stylistic similarity evaluation|0.00 % |      1\n",
      "generalization to n>2n > 2|0.00 % |      1\n",
      "potential shortcomings of shallow generation methods|0.00 % |      1\n",
      "detailed analysis for english-french|0.00 % |      1\n",
      "comparison to state-of-the-art supervised methods|0.00 % |      1\n",
      "deep convolutional networks|0.00 % |      1\n",
      "          attention layer|0.00 % |      1\n",
      "semi-supervised spoken language understanding|0.00 % |      1\n",
      "ensembles and model diversity|0.00 % |      1\n",
      "stack long short-term memory (lstm)|0.00 % |      1\n",
      "bilm derived substitutions|0.00 % |      1\n",
      "        utterance encoder|0.00 % |      1\n",
      "    gender identification|0.00 % |      1\n",
      "performance improvement from answer re-ranking|0.00 % |      1\n",
      "adversarial domain adaptation and domain generation|0.00 % |      1\n",
      "   information extraction|0.00 % |      1\n",
      "discussion and conclusions|0.00 % |      1\n",
      " online dialogue features|0.00 % |      1\n",
      "neural auto-regressive topic model|0.00 % |      1\n",
      "               parameters|0.00 % |      1\n",
      "inter-annotator agreement|0.00 % |      1\n",
      "topic-based cross-referencing|0.00 % |      1\n",
      " advcls: adversarial classifier|0.00 % |      1\n",
      "     word representations|0.00 % |      1\n",
      "building the benchmark corpus|0.00 % |      1\n",
      "transduction with soft attention|0.00 % |      1\n",
      "previous systems built on acl anthology|0.00 % |      1\n",
      "an alternative: evaluation via gap-filling|0.00 % |      1\n",
      "    error detection model|0.00 % |      1\n",
      "applications of phonetic vectors|0.00 % |      1\n",
      " iterated response models|0.00 % |      1\n",
      "language modeling baselines|0.00 % |      1\n",
      "monolingual and bilingual embeddings|0.00 % |      1\n",
      "       sentiment features|0.00 % |      1\n",
      "  quantitative evaluation|0.00 % |      1\n",
      "stir: strongly incremental repair detection|0.00 % |      1\n",
      "      fine-tuning details|0.00 % |      1\n",
      "      methods and results|0.00 % |      1\n",
      "arabic morphological analyzer/generator|0.00 % |      1\n",
      "machine reading comprehension|0.00 % |      1\n",
      "dataset and evaluation criteria|0.00 % |      1\n",
      "       temporal reasoning|0.00 % |      1\n",
      "multi-class classification|0.00 % |      1\n",
      "        parallel datasets|0.00 % |      1\n",
      "features of collaborative negotiation|0.00 % |      1\n",
      "experiments with large margin loss|0.00 % |      1\n",
      "degree adverbs in linguistics|0.00 % |      1\n",
      " semi-supervised learning|0.00 % |      1\n",
      "  short text conversation|0.00 % |      1\n",
      "neural machine translation evaluation|0.00 % |      1\n",
      "evaluation of pronoun translation|0.00 % |      1\n",
      "story generation with planning|0.00 % |      1\n",
      "    training and decoding|0.00 % |      1\n",
      "word embedding recommendations|0.00 % |      1\n",
      "                materials|0.00 % |      1\n",
      "       entity recognition|0.00 % |      1\n",
      " existing related methods|0.00 % |      1\n",
      "recurrent layer adaptation|0.00 % |      1\n",
      "comparison with other work|0.00 % |      1\n",
      "      modifying particles|0.00 % |      1\n",
      "     ibm computer manuals|0.00 % |      1\n",
      "  document classification|0.00 % |      1\n",
      "    content determination|0.00 % |      1\n",
      "datasets and representation of ideas|0.00 % |      1\n",
      "syntactic features in text classification|0.00 % |      1\n",
      "analysis of frequency and polysemy|0.00 % |      1\n",
      "    hate speech detection|0.00 % |      1\n",
      "   abstraction approaches|0.00 % |      1\n",
      "estimating p(k j |c i )p(k_j|c_i)|0.00 % |      1\n",
      "static and contextualized words embeddings|0.00 % |      1\n",
      "selection of the closest variant|0.00 % |      1\n",
      "       lda baseline model|0.00 % |      1\n",
      " other similarity methods|0.00 % |      1\n",
      "     representation model|0.00 % |      1\n",
      "         final evaluation|0.00 % |      1\n",
      "comparing statistical parsers|0.00 % |      1\n",
      "document-level sentiment classification|0.00 % |      1\n",
      "    experimental datasets|0.00 % |      1\n",
      "dual conditional cross-entropy filtering|0.00 % |      1\n",
      "true label prediction in crowdsourcing|0.00 % |      1\n",
      "           representation|0.00 % |      1\n",
      "   sequence tagging model|0.00 % |      1\n",
      "   ed baselines & results|0.00 % |      1\n",
      "formalizing inflectional morphology|0.00 % |      1\n",
      "   comparison of coverage|0.00 % |      1\n",
      "   the statistical tagger|0.00 % |      1\n",
      "word and sense interconnectivity|0.00 % |      1\n",
      "   baselines & evaluation|0.00 % |      1\n",
      "   affective text dataset|0.00 % |      1\n",
      "assigning thesaurus categories|0.00 % |      1\n",
      "              file format|0.00 % |      1\n",
      "results on public datasets|0.00 % |      1\n",
      "    conclusion and future|0.00 % |      1\n",
      "  confidence and salience|0.00 % |      1\n",
      "     readability measures|0.00 % |      1\n",
      "                 ace 2005|0.00 % |      1\n",
      "analysis of translation errors|0.00 % |      1\n",
      "      result and thoughts|0.00 % |      1\n",
      "            existing work|0.00 % |      1\n",
      "        nmt configuration|0.00 % |      1\n",
      "                     arae|0.00 % |      1\n",
      "tree structure enhanced neural machine translation|0.00 % |      1\n",
      "     empirical evaluation|0.00 % |      1\n",
      "implementing hpsg in a clp framework|0.00 % |      1\n",
      "operationalizing irregularity|0.00 % |      1\n",
      "              equivalence|0.00 % |      1\n",
      "gender stereotypes in text|0.00 % |      1\n",
      "           related models|0.00 % |      1\n",
      "historical spelling normalization|0.00 % |      1\n",
      "introductionthe author is currently at texas instruments and all inquiries should be addressed to rajeev@csc.ti.com.|0.00 % |      1\n",
      "    inflection generation|0.00 % |      1\n",
      "the elements of the olac metadata set|0.00 % |      1\n",
      "      evaluation datasets|0.00 % |      1\n",
      "   semantic role labeling|0.00 % |      1\n",
      "        ans model fitting|0.00 % |      1\n",
      "   common topic inference|0.00 % |      1\n",
      "            related tasks|0.00 % |      1\n",
      "               text model|0.00 % |      1\n",
      "integrating language technology with machine learning|0.00 % |      1\n",
      " semantic representations|0.00 % |      1\n",
      "              the problem|0.00 % |      1\n",
      "      experimental set-up|0.00 % |      1\n",
      "  deterministic inference|0.00 % |      1\n",
      "recognizing textual entailment|0.00 % |      1\n",
      " bidirectional lstm layer|0.00 % |      1\n",
      "   the tagger and corpora|0.00 % |      1\n",
      "                   system|0.00 % |      1\n",
      "           tasks and data|0.00 % |      1\n",
      "experimental data, setup and results|0.00 % |      1\n",
      "          coding features|0.00 % |      1\n",
      "          data for task 2|0.00 % |      1\n",
      "word-synchronous beam search|0.00 % |      1\n",
      "       response generator|0.00 % |      1\n",
      "action recognition models|0.00 % |      1\n",
      "       weka configuration|0.00 % |      1\n",
      "compared against other models|0.00 % |      1\n",
      "a long time ago, in a galaxy far, far away|0.00 % |      1\n",
      "       mlstm-based system|0.00 % |      1\n",
      "current sign language research|0.00 % |      1\n",
      "supervised review summary generation|0.00 % |      1\n",
      " data augmentation in nlp|0.00 % |      1\n",
      "       model and training|0.00 % |      1\n",
      "       emotion annotation|0.00 % |      1\n",
      "     major and minor keys|0.00 % |      1\n",
      "         importance score|0.00 % |      1\n",
      "pens classification scheme|0.00 % |      1\n",
      "               validation|0.00 % |      1\n",
      "       sentiment baseline|0.00 % |      1\n",
      "relation to knowledge graph embedding|0.00 % |      1\n",
      "                  tag set|0.00 % |      1\n",
      "generating training data from raw text|0.00 % |      1\n",
      "         dialogue systems|0.00 % |      1\n",
      "symmetry between conjuncts|0.00 % |      1\n",
      "english–finnish and finnish–english|0.00 % |      1\n",
      "  a grammar for discourse|0.00 % |      1\n",
      "coupled-lstms for strong sentence interaction|0.00 % |      1\n",
      "    the learning approach|0.00 % |      1\n",
      "            visualization|0.00 % |      1\n",
      "   combination techniques|0.00 % |      1\n",
      "       keyword extraction|0.00 % |      1\n",
      "outline of the generation process|0.00 % |      1\n",
      "selectional preference and sense ambiguity|0.00 % |      1\n",
      "       bidirectional lstm|0.00 % |      1\n",
      " machine learning results|0.00 % |      1\n",
      "c-test difficulty prediction|0.00 % |      1\n",
      "       fluency evaluation|0.00 % |      1\n",
      "ucca's semantic structures|0.00 % |      1\n",
      "hedging as a sign of scientific discourse|0.00 % |      1\n",
      "        syntactic islands|0.00 % |      1\n",
      "       feature importance|0.00 % |      1\n",
      " link function of bigrams|0.00 % |      1\n",
      "              test corpus|0.00 % |      1\n",
      "     data and experiments|0.00 % |      1\n",
      "debiasing word embeddings|0.00 % |      1\n",
      "           simulated data|0.00 % |      1\n",
      "baseline rnn-based encdec model|0.00 % |      1\n",
      "  topic modeling analysis|0.00 % |      1\n",
      "conditional probability model|0.00 % |      1\n",
      "the dynamic programming table|0.00 % |      1\n",
      "    proposed architecture|0.00 % |      1\n",
      "          atis experiment|0.00 % |      1\n",
      "         centering theory|0.00 % |      1\n",
      "word-level polarity features|0.00 % |      1\n",
      "   learning and inference|0.00 % |      1\n",
      "       learning algorithm|0.00 % |      1\n",
      "the proposed doc architecture|0.00 % |      1\n",
      "naive bayes and logistic regression|0.00 % |      1\n",
      "multi-space variational autoencoders|0.00 % |      1\n",
      "scientific paper datasets|0.00 % |      1\n",
      "       rule-based systems|0.00 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       evaluation dataset|0.00 % |      1\n",
      " negation scope detection|0.00 % |      1\n",
      "parallelism and inference|0.00 % |      1\n",
      "general neural model for chinese word segmentation|0.00 % |      1\n",
      "     model initialization|0.00 % |      1\n",
      "   maximum-entropy method|0.00 % |      1\n",
      "fake news detection using textual information|0.00 % |      1\n",
      "    alignment-based model|0.00 % |      1\n",
      "flexible sense distinctions|0.00 % |      1\n",
      "      semi-supervised nmt|0.00 % |      1\n",
      "         paragraph reader|0.00 % |      1\n",
      "  alignment visualization|0.00 % |      1\n",
      "              pos-tagging|0.00 % |      1\n",
      "      evaluation settings|0.00 % |      1\n",
      "                 treebank|0.00 % |      1\n",
      "shortcomings of current summarization models|0.00 % |      1\n",
      "sexual predator detection|0.00 % |      1\n",
      "dataset and experimental settings|0.00 % |      1\n",
      "            the treebanks|0.00 % |      1\n",
      "         the second model|0.00 % |      1\n",
      "lexical replacement in phylogenetics|0.00 % |      1\n",
      "bootstrapping a first-person sentiment corpus|0.00 % |      1\n",
      "            dag gru model|0.00 % |      1\n",
      "     character-level nlp.|0.00 % |      1\n",
      "data intelligence methods|0.00 % |      1\n",
      " targeted feature dropout|0.00 % |      1\n",
      "underspecified semantic tagging|0.00 % |      1\n",
      "training and hyperparameter tuning|0.00 % |      1\n",
      "the glr parsing algorithm|0.00 % |      1\n",
      "          model debiasing|0.00 % |      1\n",
      "interpretable paraphrase generation|0.00 % |      1\n",
      "motivation: robust nlp systems|0.00 % |      1\n",
      "independent normalisation|0.00 % |      1\n",
      "           transfer tasks|0.00 % |      1\n",
      "predicting ordinal judgments|0.00 % |      1\n"
     ]
    }
   ],
   "source": [
    "for ind,x,y in zip(pd.Series(covering_by_tex).value_counts().index,pd.Series(covering_by_tex).value_counts()/len(covering_by_tex)*100,pd.Series(covering_by_tex).value_counts()):\n",
    "    print(\"%25s|%.2f %% |%7d\" % (ind,round(x,1),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_covering_max_2 = list(map(lambda x: list(overview_papers_w_latex[x]['latex_parse']['overview_text'])[1].lower() if len(list(overview_papers_w_latex[x]['latex_parse']['overview_text']))>=2 and list(overview_papers_w_latex[x]['latex_parse']['overview_text'])[1]!=None else 'None',overview_papers_w_latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_detecting(x):\n",
    "    if list(x['latex_parse']['overview_text'])[0]!=None:\n",
    "        line1 = list(x['latex_parse']['overview_text'])\n",
    "#         print(line1)\n",
    "        line1 = [x for x in line1 if x is not None]\n",
    "        line1 = ' '.join(line1).lower()\n",
    "        \n",
    "        if mult_in(RW_names,line1):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw_detecting(overview_papers_w_latex['5084110'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_covering_rw = list(map(lambda x: rw_detecting(overview_papers_w_latex[x]),overview_papers_w_latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex_ids = list(map(lambda x: overview_papers_w_latex[x]['paper_id'] ,overview_papers_w_latex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'id':tex_ids,'max_title':tex_covering_max,'max_title_2':tex_covering_max_2,'rw_exact':tex_covering_rw}\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>max_title</th>\n",
       "      <th>max_title_2</th>\n",
       "      <th>rw_exact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10164018</td>\n",
       "      <td>introduction</td>\n",
       "      <td>reader-aware salience estimation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>introduction</td>\n",
       "      <td>the grammar</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189927790</td>\n",
       "      <td>introduction</td>\n",
       "      <td>relevance-based auxiliary task (rat)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5084110</td>\n",
       "      <td>acknowledgments</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198922003</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        max_title                           max_title_2  rw_exact\n",
       "0   10164018     introduction      reader-aware salience estimation         1\n",
       "1        488     introduction                           the grammar         0\n",
       "2  189927790     introduction  relevance-based auxiliary task (rat)         0\n",
       "3    5084110  acknowledgments                                  None         0\n",
       "4  198922003             None                                  None         0"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4039, 4), (382, 4))"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape,df[(df.max_title == 'None') & (df.max_title_2 == 'None')].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3657, 4)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[~((df.max_title == 'None') & (df.max_title_2 == 'None'))]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>max_title</th>\n",
       "      <th>max_title_2</th>\n",
       "      <th>rw_exact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, max_title, max_title_2, rw_exact]\n",
       "Index: []"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.max_title == 'background'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(['related works','background','related works','related work and discussion','comparison with previous work',\n",
    "            'previous work','further related work','introduction and background'], 'related work',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 4)"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.rw_exact == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          название секции| % от всех |кол-во \n",
      "--------------------------------------------------\n",
      "             related work|42.94 % |    766\n",
      "             introduction|30.72 % |    548\n",
      "              experiments|0.56 % |     10\n",
      "                  results|0.45 % |      8\n",
      "                 datasets|0.45 % |      8\n",
      "                  dataset|0.39 % |      7\n",
      "                    model|0.34 % |      6\n",
      "       experimental setup|0.28 % |      5\n",
      "               evaluation|0.28 % |      5\n",
      "                baselines|0.28 % |      5\n",
      "                     data|0.22 % |      4\n",
      "    experimental settings|0.22 % |      4\n",
      "                    setup|0.22 % |      4\n",
      "     experimental setting|0.22 % |      4\n",
      "              methodology|0.22 % |      4\n",
      "               discussion|0.22 % |      4\n",
      "               conclusion|0.22 % |      4\n",
      "                   models|0.22 % |      4\n",
      "   implementation details|0.17 % |      3\n",
      "               references|0.17 % |      3\n",
      "                  methods|0.11 % |      2\n",
      "       evaluation metrics|0.11 % |      2\n",
      "          baseline models|0.11 % |      2\n",
      "motivation and related work|0.11 % |      2\n",
      "conclusions and future work|0.11 % |      2\n",
      "            introduction |0.11 % |      2\n",
      "                 overview|0.11 % |      2\n",
      "background and preliminaries|0.11 % |      2\n",
      "                  corpora|0.11 % |      2\n",
      "      experiment settings|0.11 % |      2\n",
      "          word embeddings|0.11 % |      2\n",
      "conclusion and future work|0.11 % |      2\n",
      "         task description|0.11 % |      2\n",
      "    neural network models|0.11 % |      2\n",
      "                 corpuses|0.06 % |      1\n",
      "  evidence identification|0.06 % |      1\n",
      "  coordination in the ptb|0.06 % |      1\n",
      "fine-grained entity typing|0.06 % |      1\n",
      "search-based structured prediction|0.06 % |      1\n",
      "conditional probability model|0.06 % |      1\n",
      "      semi-supervised nmt|0.06 % |      1\n",
      " supervised and `pseudo-supervised' methods|0.06 % |      1\n",
      "                 training|0.06 % |      1\n",
      "background and motivation|0.06 % |      1\n",
      "static and contextualized words embeddings|0.06 % |      1\n",
      "   subword-unit-based nmt|0.06 % |      1\n",
      "visual question answering|0.06 % |      1\n",
      "phrase-based machine translation|0.06 % |      1\n",
      "          structural cues|0.06 % |      1\n",
      "      duluth38 background|0.06 % |      1\n",
      "          stanford reader|0.06 % |      1\n",
      "language modeling baselines|0.06 % |      1\n",
      "  unsupervised clustering|0.06 % |      1\n",
      "related work & background|0.06 % |      1\n",
      "phrase-based and neural machine translation|0.06 % |      1\n",
      "            preliminaries|0.06 % |      1\n",
      "         data and methods|0.06 % |      1\n",
      "debiasing word embeddings|0.06 % |      1\n",
      "semi-supervised wake-sleep|0.06 % |      1\n",
      "    error detection model|0.06 % |      1\n",
      "              the problem|0.06 % |      1\n",
      "        sentence matching|0.06 % |      1\n",
      "       quality estimation|0.06 % |      1\n",
      "    finite-state calculus|0.06 % |      1\n",
      "    supervised evaluation|0.06 % |      1\n",
      " semi-supervised learning|0.06 % |      1\n",
      "neural machine translation: conditional language modelling|0.06 % |      1\n",
      "related work and datasets|0.06 % |      1\n",
      "                 framenet|0.06 % |      1\n",
      "contrast to phrase-based smt|0.06 % |      1\n",
      "a lemmatizer: p(ℓ i ∣m i ,w i )p(\\ell _i \\mid m_i, w_i)|0.06 % |      1\n",
      "         training details|0.06 % |      1\n",
      "application to other formalisms|0.06 % |      1\n",
      "       creating instances|0.06 % |      1\n",
      "statistical machine translation|0.06 % |      1\n",
      "related work and existing datasets|0.06 % |      1\n",
      "        the grammar (set)|0.06 % |      1\n",
      "hashing under the nvi framework|0.06 % |      1\n",
      "          the source data|0.06 % |      1\n",
      "network training and hyper-parameters|0.06 % |      1\n",
      "       domain differences|0.06 % |      1\n",
      "           proposed model|0.06 % |      1\n",
      "nmt with relation networks|0.06 % |      1\n",
      "  verb frame alternations|0.06 % |      1\n",
      "results and interpretation|0.06 % |      1\n",
      "          data collection|0.06 % |      1\n",
      "stir: strongly incremental repair detection|0.06 % |      1\n",
      "  quantitative evaluation|0.06 % |      1\n",
      "word embedding feature models|0.06 % |      1\n",
      "neural network for sentiment classification|0.06 % |      1\n",
      "mildly context-sensitive language recognition|0.06 % |      1\n",
      "abstractive summarization|0.06 % |      1\n",
      "     persistence diagrams|0.06 % |      1\n",
      "syntax marginal inference for dependency paths|0.06 % |      1\n",
      "neural auto-regressive topic model|0.06 % |      1\n",
      "syntactically rich language generation|0.06 % |      1\n",
      " online dialogue features|0.06 % |      1\n",
      "     speech-to-text model|0.06 % |      1\n",
      "       evaluation details|0.06 % |      1\n",
      "   argument structure svm|0.06 % |      1\n",
      "a basic sequence-to-sequence approach|0.06 % |      1\n",
      "              performance|0.06 % |      1\n",
      "           simulated data|0.06 % |      1\n",
      "        naive translation|0.06 % |      1\n",
      "          metric learning|0.06 % |      1\n",
      "            related work |0.06 % |      1\n",
      "    related work and data|0.06 % |      1\n",
      "datasets and representation of ideas|0.06 % |      1\n",
      "morphology in language modeling|0.06 % |      1\n",
      "     monolingual baseline|0.06 % |      1\n",
      "   sequence tagging model|0.06 % |      1\n",
      "human evaluation of machine translation|0.06 % |      1\n",
      "  implementations details|0.06 % |      1\n",
      "           related works |0.06 % |      1\n",
      " fixed confidence by ttts|0.06 % |      1\n",
      "            fact-checking|0.06 % |      1\n",
      "document-level sentiment classification|0.06 % |      1\n",
      "frame tracking: an extension of state tracking|0.06 % |      1\n",
      "multiple timescale gated recurrent unit|0.06 % |      1\n",
      "step 3: measure collocational distributions|0.06 % |      1\n",
      "word vector-based tree edit distance|0.06 % |      1\n",
      "sparse representations for expansion|0.06 % |      1\n",
      "                 decoding|0.06 % |      1\n",
      "                  summary|0.06 % |      1\n",
      "            dag gru model|0.06 % |      1\n",
      "enhanced sequential inference model|0.06 % |      1\n",
      "           word embedding|0.06 % |      1\n",
      "arabic dialect identification|0.06 % |      1\n",
      "analysis of frequency and polysemy|0.06 % |      1\n",
      "     top-down propagation|0.06 % |      1\n",
      "   end-to-end nlg systems|0.06 % |      1\n",
      "   defining the objective|0.06 % |      1\n",
      "                data sets|0.06 % |      1\n",
      "  related and future work|0.06 % |      1\n",
      "compositionality in vector space|0.06 % |      1\n",
      "    the learning approach|0.06 % |      1\n",
      "coupled-lstms for strong sentence interaction|0.06 % |      1\n",
      "    composition functions|0.06 % |      1\n",
      "categorization of hate speech|0.06 % |      1\n",
      "existing corpora annotated with argumentation structures|0.06 % |      1\n",
      "recurrent layer adaptation|0.06 % |      1\n",
      "background: cross-lingual embeddings|0.06 % |      1\n",
      "        model description|0.06 % |      1\n",
      "                  tag set|0.06 % |      1\n",
      "    cross-lingual mapping|0.06 % |      1\n",
      "            text matching|0.06 % |      1\n",
      "multimodal machine translation with embedding prediction|0.06 % |      1\n",
      "graph representation of vector embeddings|0.06 % |      1\n",
      "generating training data from raw text|0.06 % |      1\n",
      "word and sense interconnectivity|0.06 % |      1\n",
      "     experimental details|0.06 % |      1\n",
      "conditioned language model|0.06 % |      1\n",
      "abstractive text summarization|0.06 % |      1\n",
      "       data and baselines|0.06 % |      1\n",
      "results on public datasets|0.06 % |      1\n",
      "                     glue|0.06 % |      1\n",
      "       model architecture|0.06 % |      1\n",
      "       evaluation dataset|0.06 % |      1\n",
      "            pp attachment|0.06 % |      1\n",
      "      the pointer softmax|0.06 % |      1\n",
      "             cube pruning|0.06 % |      1\n",
      "      convolutional layer|0.06 % |      1\n",
      "automatic discourse tagging|0.06 % |      1\n",
      "               ir engines|0.06 % |      1\n",
      "       out-of-domain data|0.06 % |      1\n",
      "manifestation of highly contrasting word pairs in text|0.06 % |      1\n",
      "        nmt configuration|0.06 % |      1\n",
      "interactive attention vs. coverage model|0.06 % |      1\n",
      "a probabilistically-derived measure for unithood determination|0.06 % |      1\n",
      "word similarity evaluation|0.06 % |      1\n",
      "        task descriptions|0.06 % |      1\n",
      "ensembles and model diversity|0.06 % |      1\n",
      "comparison against baselines|0.06 % |      1\n",
      "      system architecture|0.06 % |      1\n",
      "    conclusion and future|0.06 % |      1\n",
      "transfer learning for nmt|0.06 % |      1\n",
      "baselines and implementation details|0.06 % |      1\n",
      "variational encoder-decoder (ved)|0.06 % |      1\n",
      "long-short term memory network|0.06 % |      1\n",
      "           baseline model|0.06 % |      1\n",
      "  verb-noun constructions|0.06 % |      1\n",
      "introduction and related works|0.06 % |      1\n",
      "temporal information processing|0.06 % |      1\n",
      "   semantic role labeling|0.06 % |      1\n",
      "background and task definition|0.06 % |      1\n",
      "influence in interactions|0.06 % |      1\n",
      "   relevant previous work|0.06 % |      1\n",
      "          text collection|0.06 % |      1\n",
      "      problem formulation|0.06 % |      1\n",
      "          maximum entropy|0.06 % |      1\n",
      "convolution and max-over-time pooling|0.06 % |      1\n",
      " a feature-based approach|0.06 % |      1\n",
      "test data maintenance and retrieval|0.06 % |      1\n",
      "contextualized word embeddings|0.06 % |      1\n",
      "gender stereotypes in text|0.06 % |      1\n",
      "  the conversational game|0.06 % |      1\n",
      "neural network approaches|0.06 % |      1\n",
      "                  setting|0.06 % |      1\n",
      "               the parser|0.06 % |      1\n",
      "identifying narrative paragraphs from three text corpora|0.06 % |      1\n",
      "        de-identification|0.06 % |      1\n",
      "    next token prediction|0.06 % |      1\n",
      "       lexical challenges|0.06 % |      1\n",
      "word vectors and wordless word vectors|0.06 % |      1\n",
      "           model training|0.06 % |      1\n",
      "measuring semantic textual similarity|0.06 % |      1\n",
      "             encoder: gru|0.06 % |      1\n",
      "              gap dataset|0.06 % |      1\n",
      "experimental evaluation: bucc shared task on mining bitexts|0.06 % |      1\n",
      "            collaboration|0.06 % |      1\n",
      "     experimental results|0.06 % |      1\n",
      "               classifier|0.06 % |      1\n",
      "      data and evaluation|0.06 % |      1\n",
      "stack long short-term memory (lstm)|0.06 % |      1\n",
      "   the tagger and corpora|0.06 % |      1\n",
      "                   set-up|0.06 % |      1\n",
      "        language typology|0.06 % |      1\n",
      " table-to-text generation|0.06 % |      1\n",
      "analysis of translation errors|0.06 % |      1\n",
      "    modality and negation|0.06 % |      1\n",
      "             rnn variants|0.06 % |      1\n",
      "             full parsing|0.06 % |      1\n",
      " stochastic top-k listnet|0.06 % |      1\n",
      "document retrieval and reranking|0.06 % |      1\n",
      "    experimental datasets|0.06 % |      1\n",
      "background and related work|0.06 % |      1\n",
      "joint model for two tasks|0.06 % |      1\n",
      "bootstrapping a first-person sentiment corpus|0.06 % |      1\n",
      "bi-directional reconstruction|0.06 % |      1\n",
      "  discourse embellishment|0.06 % |      1\n",
      "language model construction|0.06 % |      1\n",
      "   gated-attention reader|0.06 % |      1\n",
      "sexual predator detection|0.06 % |      1\n",
      "datasets and experimental setup|0.06 % |      1\n",
      "      attention mechanism|0.06 % |      1\n",
      "                 treebank|0.06 % |      1\n",
      "    word similarity tasks|0.06 % |      1\n",
      "related work on temporal relation modeling|0.06 % |      1\n",
      "       semantic functions|0.06 % |      1\n",
      "   information extraction|0.06 % |      1\n",
      "modeling ambiguous pronouns|0.06 % |      1\n",
      "related works and conclusions|0.06 % |      1\n",
      "c-test difficulty prediction|0.06 % |      1\n",
      "     data and experiments|0.06 % |      1\n",
      "          data statistics|0.06 % |      1\n",
      "     rl phase performance|0.06 % |      1\n",
      "     word sense induction|0.06 % |      1\n",
      "datasets and graph construction|0.06 % |      1\n",
      "          overall results|0.06 % |      1\n",
      "clinical temporal relation extraction|0.06 % |      1\n",
      "fake news detection using textual information|0.06 % |      1\n",
      "general neural model for chinese word segmentation|0.06 % |      1\n",
      "              nmt systems|0.06 % |      1\n",
      "ucca's semantic structures|0.06 % |      1\n",
      "replicability analysis for nlp|0.06 % |      1\n",
      "          atis experiment|0.06 % |      1\n",
      "             architecture|0.06 % |      1\n",
      "implementation and training details|0.06 % |      1\n",
      "   results and discussion|0.06 % |      1\n",
      "               procedure:|0.06 % |      1\n",
      "background: disambiguation of prepositions and possessives|0.06 % |      1\n",
      " machine learning results|0.06 % |      1\n",
      "           nmt background|0.06 % |      1\n",
      "     quantitative results|0.06 % |      1\n",
      "bag generation algorithms|0.06 % |      1\n",
      "word-level polarity features|0.06 % |      1\n",
      "      fine-tuning details|0.06 % |      1\n",
      "        review of methods|0.06 % |      1\n",
      "implementation details for lace|0.06 % |      1\n",
      "results and error analysis|0.06 % |      1\n",
      "dependency-based algorithm (dba)|0.06 % |      1\n",
      "     major and minor keys|0.06 % |      1\n",
      "         restricted track|0.06 % |      1\n",
      "                  resnets|0.06 % |      1\n",
      "       sg-mcmc algorithms|0.06 % |      1\n",
      "             multi-hop rc|0.06 % |      1\n",
      "           tasks and data|0.06 % |      1\n",
      "english-czech translation|0.06 % |      1\n",
      "sequence tagging with bidirectional rnn|0.06 % |      1\n",
      "measuring attributional similarity|0.06 % |      1\n",
      "           viterbi e-step|0.06 % |      1\n",
      " bidirectional lstm layer|0.06 % |      1\n",
      "recognizing textual entailment|0.06 % |      1\n",
      "    conversational models|0.06 % |      1\n",
      "            final results|0.06 % |      1\n",
      "related work & conclusion|0.06 % |      1\n",
      "linguistic input features|0.06 % |      1\n",
      "lstm neural reordering model|0.06 % |      1\n",
      " attention models for nlp|0.06 % |      1\n",
      "         importance score|0.06 % |      1\n",
      "pens classification scheme|0.06 % |      1\n",
      "eye-tracking database for sarcasm analysis|0.06 % |      1\n",
      "     character-level nlp.|0.06 % |      1\n",
      "comparison with other work|0.06 % |      1\n",
      "   pltig and related work|0.06 % |      1\n",
      "“soft\" (em) joint training|0.06 % |      1\n",
      "          text classifier|0.06 % |      1\n",
      "          acknowledgments|0.06 % |      1\n",
      " distributional semantics|0.06 % |      1\n",
      "  additional related work|0.06 % |      1\n",
      "        data and settings|0.06 % |      1\n",
      "          hyperparameters|0.06 % |      1\n",
      "            related tasks|0.06 % |      1\n",
      "measuring domain and noise in data|0.06 % |      1\n",
      "                 features|0.06 % |      1\n",
      "   baselines & evaluation|0.06 % |      1\n",
      "reuters cross-lingual document classification|0.06 % |      1\n",
      "results on cross-sentence nn-ary relation extraction|0.06 % |      1\n",
      "singular value decomposition|0.06 % |      1\n",
      "                inference|0.06 % |      1\n",
      " negation scope detection|0.06 % |      1\n",
      "         mean+max pooling|0.06 % |      1\n",
      "         captioning model|0.06 % |      1\n",
      "           representation|0.06 % |      1\n",
      "    applications of focus|0.06 % |      1\n",
      "dataset and evaluation criteria|0.06 % |      1\n",
      "multi-space variational autoencoders|0.06 % |      1\n",
      "related work and conclusion|0.06 % |      1\n",
      "                materials|0.06 % |      1\n",
      "experimental setup and results|0.06 % |      1\n",
      "                babi task|0.06 % |      1\n",
      "        domain adaptation|0.06 % |      1\n",
      "formalizing inflectional morphology|0.06 % |      1\n",
      "approaches to identifying dogmatism|0.06 % |      1\n",
      "    neural-based approach|0.06 % |      1\n",
      "tree structure enhanced neural machine translation|0.06 % |      1\n",
      "evaluation of pronoun translation|0.06 % |      1\n",
      "introduction and related work|0.06 % |      1\n",
      "      overall performance|0.06 % |      1\n",
      "feature-engineering models|0.06 % |      1\n",
      "background: attention-based neural machine translation|0.06 % |      1\n",
      "dialogue act classification|0.06 % |      1\n",
      "         experiment setup|0.06 % |      1\n",
      "    part-of-speech tagger|0.06 % |      1\n",
      "         network training|0.06 % |      1\n",
      "features of collaborative negotiation|0.06 % |      1\n",
      "similar, associated, and both|0.06 % |      1\n",
      "performance improvement from answer re-ranking|0.06 % |      1\n",
      "   knowledge distillation|0.06 % |      1\n",
      "      evaluation settings|0.06 % |      1\n",
      "pre-training the projection layer|0.06 % |      1\n",
      "flexible sense distinctions|0.06 % |      1\n",
      "introduction and motivation|0.06 % |      1\n",
      "          neural networks|0.06 % |      1\n",
      "          proposed models|0.06 % |      1\n",
      "           transfer tasks|0.06 % |      1\n",
      "combinatory category grammar supertagging|0.06 % |      1\n",
      "       speech recognition|0.06 % |      1\n",
      "                  derinet|0.06 % |      1\n",
      "multilingual sentence embeddings|0.06 % |      1\n",
      "related work and conclusions|0.06 % |      1\n",
      "          temporal search|0.06 % |      1\n",
      "          coding features|0.06 % |      1\n",
      "comparison with related work|0.06 % |      1\n",
      "          model debiasing|0.06 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   geocoding reddit users|0.06 % |      1\n",
      "      data representation|0.06 % |      1\n",
      "   ed baselines & results|0.06 % |      1\n",
      "span-attribute tagging (sa-t) model|0.06 % |      1\n",
      "residual vq-vae for unsupervised monolingual paraphrasing|0.06 % |      1\n",
      "asymptotic normality and statistical efficiency|0.06 % |      1\n",
      "experimental data, setup and results|0.06 % |      1\n",
      "revisiting the feature augmentation method|0.06 % |      1\n",
      "       sentiment baseline|0.06 % |      1\n",
      "shallow parsers with hand-written rules|0.06 % |      1\n",
      "      extended levi graph|0.06 % |      1\n",
      "        analysis of depnn|0.06 % |      1\n",
      "large dataset for training and validation|0.06 % |      1\n",
      "           classification|0.06 % |      1\n",
      "training text classifiers |0.06 % |      1\n",
      "deep convolutional networks|0.06 % |      1\n",
      "word embedding based models|0.06 % |      1\n",
      "distributed word representations|0.06 % |      1\n",
      "   methods not considered|0.06 % |      1\n",
      "                  systems|0.06 % |      1\n",
      "approaches using prosodic cues|0.06 % |      1\n",
      "related work on grammar induction|0.06 % |      1\n",
      "              hocus pocus|0.06 % |      1\n",
      "models for temporal relations extraction|0.06 % |      1\n",
      "       the annotated data|0.06 % |      1\n",
      "neural machine translation background|0.06 % |      1\n",
      "         baseline methods|0.06 % |      1\n",
      "recurrent models for text classification|0.06 % |      1\n",
      "             nli datasets|0.06 % |      1\n"
     ]
    }
   ],
   "source": [
    "print(\"%25s|%10s |%7s\"%('название секции','% от всех','кол-во '))\n",
    "print(50*'-')\n",
    "for ind,x,y in zip(df[df.rw_exact == 1].max_title.value_counts().index,df[df.rw_exact == 1].max_title.value_counts()/len(df[df.rw_exact == 1])*100,df[df.rw_exact == 1].max_title.value_counts()):\n",
    "    print(\"%25s|%2.2f %% |%7d\" % (ind,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          название секции| % от всех |кол-во \n",
      "--------------------------------------------------\n",
      "             introduction|33.47 % |   1224\n",
      "             related work|20.95 % |    766\n",
      "          acknowledgments|5.06 % |    185\n",
      "         acknowledgements|2.30 % |     84\n",
      "               conclusion|1.91 % |     70\n",
      "              conclusions|0.63 % |     23\n",
      "              experiments|0.52 % |     19\n",
      "                 sections|0.49 % |     18\n",
      "                  results|0.44 % |     16\n",
      "               discussion|0.44 % |     16\n",
      "       experimental setup|0.36 % |     13\n",
      "                     data|0.33 % |     12\n",
      "                 datasets|0.33 % |     12\n",
      "                  dataset|0.30 % |     11\n",
      "    experimental settings|0.27 % |     10\n",
      "conclusion and future work|0.27 % |     10\n",
      "                baselines|0.27 % |     10\n",
      "          acknowledgement|0.25 % |      9\n",
      "               references|0.25 % |      9\n",
      "               motivation|0.25 % |      9\n",
      "               evaluation|0.22 % |      8\n",
      "              methodology|0.22 % |      8\n",
      "   implementation details|0.22 % |      8\n",
      "                    model|0.19 % |      7\n",
      "                   models|0.16 % |      6\n",
      "                    setup|0.14 % |      5\n",
      "     experimental setting|0.14 % |      5\n",
      "                 overview|0.14 % |      5\n",
      "         training details|0.14 % |      5\n",
      "           acknowledgment|0.14 % |      5\n",
      "conclusions and future work|0.11 % |      4\n",
      "            introduction |0.11 % |      4\n",
      "                  methods|0.11 % |      4\n",
      "          baseline models|0.11 % |      4\n",
      "                 training|0.11 % |      4\n",
      "          hyperparameters|0.11 % |      4\n",
      "       model architecture|0.11 % |      4\n",
      "                  systems|0.08 % |      3\n",
      "     experimental results|0.08 % |      3\n",
      "          reproducibility|0.08 % |      3\n",
      "       evaluation metrics|0.08 % |      3\n",
      "          word embeddings|0.08 % |      3\n",
      "  zero pronoun resolution|0.05 % |      2\n",
      "   results and discussion|0.05 % |      2\n",
      "                  corpora|0.05 % |      2\n",
      "       speech recognition|0.05 % |      2\n",
      "       inferring entities|0.05 % |      2\n",
      "visual question answering|0.05 % |      2\n",
      "background and preliminaries|0.05 % |      2\n",
      "discussion and conclusions|0.05 % |      2\n",
      "assigning grammatical function labels|0.05 % |      2\n",
      "  experiments and results|0.05 % |      2\n",
      "        data and settings|0.05 % |      2\n",
      "  quantitative evaluation|0.05 % |      2\n",
      "      previous approaches|0.05 % |      2\n",
      "      experiment settings|0.05 % |      2\n",
      "introduction and motivation|0.05 % |      2\n",
      "               the parser|0.05 % |      2\n",
      "  unsupervised clustering|0.05 % |      2\n",
      "         other approaches|0.05 % |      2\n",
      "                 analysis|0.05 % |      2\n",
      "       quality estimation|0.05 % |      2\n",
      "        appendix overview|0.05 % |      2\n",
      "   supplementary material|0.05 % |      2\n",
      "         task description|0.05 % |      2\n",
      "    neural network models|0.05 % |      2\n",
      "  illustrative experiment|0.05 % |      2\n",
      "           baseline model|0.05 % |      2\n",
      "                attention|0.05 % |      2\n",
      "  data-to-text generation|0.05 % |      2\n",
      "     automatic evaluation|0.05 % |      2\n",
      "      multi-task learning|0.05 % |      2\n",
      "                 settings|0.05 % |      2\n",
      "  related and future work|0.05 % |      2\n",
      "             architecture|0.05 % |      2\n",
      "      system architecture|0.05 % |      2\n",
      "experimental results and discussion|0.05 % |      2\n",
      "       question answering|0.05 % |      2\n",
      "      data and evaluation|0.05 % |      2\n",
      "motivation and related work|0.05 % |      2\n",
      "implicit discourse relation recognition|0.05 % |      2\n",
      "           model training|0.05 % |      2\n",
      "discussion and future work|0.05 % |      2\n",
      "         acknowledgments.|0.05 % |      2\n",
      "    reading comprehension|0.05 % |      2\n",
      "       sentiment analysis|0.05 % |      2\n",
      "   pltig and related work|0.03 % |      1\n",
      "comparison with state of the arts|0.03 % |      1\n",
      "baselines and implementation details|0.03 % |      1\n",
      "dataset and experimental setup|0.03 % |      1\n",
      "general properties of parlai|0.03 % |      1\n",
      "    related work and data|0.03 % |      1\n",
      "       multitask learning|0.03 % |      1\n",
      "dialogue act classification|0.03 % |      1\n",
      "               experiment|0.03 % |      1\n",
      "  evaluating with sembleu|0.03 % |      1\n",
      "concept extraction and term similarity|0.03 % |      1\n",
      "implementation and training details|0.03 % |      1\n",
      "     collocations and lfs|0.03 % |      1\n",
      "clinical temporal relation extraction|0.03 % |      1\n",
      "semeval10 shared task 8 dataset|0.03 % |      1\n",
      "        existing datasets|0.03 % |      1\n",
      "        language typology|0.03 % |      1\n",
      "semantic or task-specific similarity?|0.03 % |      1\n",
      "        de-identification|0.03 % |      1\n",
      "identifying narrative paragraphs from three text corpora|0.03 % |      1\n",
      "related work and datasets|0.03 % |      1\n",
      "          data collection|0.03 % |      1\n",
      "compositionality in distributional models|0.03 % |      1\n",
      "          metric learning|0.03 % |      1\n",
      "            on evaluation|0.03 % |      1\n",
      "training text classifiers |0.03 % |      1\n",
      "tools for semantic construction|0.03 % |      1\n",
      "neural relation extraction|0.03 % |      1\n",
      "wsj treebank inconsistencies|0.03 % |      1\n",
      "    cross-lingual mapping|0.03 % |      1\n",
      "when intonation and centering collide|0.03 % |      1\n",
      "     multi-encoder models|0.03 % |      1\n",
      "model advice generation details|0.03 % |      1\n",
      "joint multimodal processing|0.03 % |      1\n",
      "      em-based clustering|0.03 % |      1\n",
      "language model pre-training|0.03 % |      1\n",
      "improving low-resource neural machine translation|0.03 % |      1\n",
      "     results & discussion|0.03 % |      1\n",
      "        syntactic islands|0.03 % |      1\n",
      "language model construction|0.03 % |      1\n",
      "natural language inference|0.03 % |      1\n",
      "word embedding based models|0.03 % |      1\n",
      "abstractive summarization|0.03 % |      1\n",
      "morphology in language modeling|0.03 % |      1\n",
      "    an incremental parser|0.03 % |      1\n",
      "deriving training data from umls|0.03 % |      1\n",
      "             applications|0.03 % |      1\n",
      "          maximum entropy|0.03 % |      1\n",
      "the type-token relationship (ttr)|0.03 % |      1\n",
      "use-case – adaptive text simplification using crowdsourcing|0.03 % |      1\n",
      "influence in interactions|0.03 % |      1\n",
      "         data and methods|0.03 % |      1\n",
      "long-short term memory network|0.03 % |      1\n",
      "approaches using prosodic cues|0.03 % |      1\n",
      "          two-level rules|0.03 % |      1\n",
      "future works and conclusions|0.03 % |      1\n",
      "     word-sense induction|0.03 % |      1\n",
      " analyzing trained models|0.03 % |      1\n",
      "          semantic parser|0.03 % |      1\n",
      "test suite administration|0.03 % |      1\n",
      "analysis of complex models|0.03 % |      1\n",
      "          the lhip system|0.03 % |      1\n",
      "       what's morphology?|0.03 % |      1\n",
      "syntactically rich language generation|0.03 % |      1\n",
      "background and motivation|0.03 % |      1\n",
      "bi-directional reconstruction|0.03 % |      1\n",
      "text classification with deep learning|0.03 % |      1\n",
      "  coordination in the ptb|0.03 % |      1\n",
      "experiment 1: optimizing text-based machine translation|0.03 % |      1\n",
      "comparison of model performance and error analysis|0.03 % |      1\n",
      "results and error analysis|0.03 % |      1\n",
      "      task and evaluation|0.03 % |      1\n",
      "     bi-directional lstms|0.03 % |      1\n",
      "paragraph-level sequence labeling|0.03 % |      1\n",
      "support-vector machine method|0.03 % |      1\n",
      "              evaluations|0.03 % |      1\n",
      "  restricting the grammar|0.03 % |      1\n",
      "          the source data|0.03 % |      1\n",
      "               pet system|0.03 % |      1\n",
      "document-level sentiment classification|0.03 % |      1\n",
      "composition in distributional models|0.03 % |      1\n",
      "            preliminaries|0.03 % |      1\n",
      "    speaker role modeling|0.03 % |      1\n",
      "  application and outlook|0.03 % |      1\n",
      "approaches to identifying dogmatism|0.03 % |      1\n",
      "          graph-based mds|0.03 % |      1\n",
      "emoji sentence representations|0.03 % |      1\n",
      "    proposed architecture|0.03 % |      1\n",
      "definite clause characterization of filtering|0.03 % |      1\n",
      "         reproduciblility|0.03 % |      1\n",
      "a lemmatizer: p(ℓ i ∣m i ,w i )p(\\ell _i \\mid m_i, w_i)|0.03 % |      1\n",
      "         table extraction|0.03 % |      1\n",
      "subtask a - sdqc support/ rumour stance classification|0.03 % |      1\n",
      "           viterbi e-step|0.03 % |      1\n",
      "                babi task|0.03 % |      1\n",
      "english-czech translation|0.03 % |      1\n",
      "         proof of lemma 1|0.03 % |      1\n",
      "  implementations details|0.03 % |      1\n",
      "results on cross-sentence nn-ary relation extraction|0.03 % |      1\n",
      "related work and conclusion|0.03 % |      1\n",
      "             full parsing|0.03 % |      1\n",
      "    the igtree algorithms|0.03 % |      1\n",
      "             rnn variants|0.03 % |      1\n",
      "            interrogative|0.03 % |      1\n",
      "                alignment|0.03 % |      1\n",
      "     directed replacement|0.03 % |      1\n",
      "l k,p 2 \\mathord {l^2_{k,p}}—the monadic second-order language of trees|0.03 % |      1\n",
      "         ethical approval|0.03 % |      1\n",
      "bag generation algorithms|0.03 % |      1\n",
      "relevance detection model|0.03 % |      1\n",
      "   defining the objective|0.03 % |      1\n",
      "             cube pruning|0.03 % |      1\n",
      "       creating instances|0.03 % |      1\n",
      " stochastic top-k listnet|0.03 % |      1\n",
      "       lexical challenges|0.03 % |      1\n",
      "feature-engineering models|0.03 % |      1\n",
      "     argumentation mining|0.03 % |      1\n",
      "         b-type sequences|0.03 % |      1\n",
      "knowledge graph embeddings|0.03 % |      1\n",
      "       concluding remarks|0.03 % |      1\n",
      "    composition functions|0.03 % |      1\n",
      "      duluth38 background|0.03 % |      1\n",
      "        the grammar model|0.03 % |      1\n",
      "word embedding feature models|0.03 % |      1\n",
      "cross-lingual word representations|0.03 % |      1\n",
      " maximum mean discrepancy|0.03 % |      1\n",
      "                 modeling|0.03 % |      1\n",
      "interaction with the dialogue manager|0.03 % |      1\n",
      " datasets & architectures|0.03 % |      1\n",
      "     learning experiments|0.03 % |      1\n",
      "background: disambiguation of prepositions and possessives|0.03 % |      1\n",
      "            video channel|0.03 % |      1\n",
      "word vector-based tree edit distance|0.03 % |      1\n",
      "       sentence selection|0.03 % |      1\n",
      "towards a classification scheme: linguistic theories of definite descriptions|0.03 % |      1\n",
      "            skip-thoughts|0.03 % |      1\n",
      "model selection criterion|0.03 % |      1\n",
      "multilingual nmt versus bilingual nmt|0.03 % |      1\n",
      "the task of grading lexical entailment|0.03 % |      1\n",
      "potential shortcomings of shallow generation methods|0.03 % |      1\n",
      "   additional experiments|0.03 % |      1\n",
      "                embedding|0.03 % |      1\n",
      "rethinking what constitutes abuse|0.03 % |      1\n",
      "evaluating the informative translator|0.03 % |      1\n",
      "modeling ambiguous pronouns|0.03 % |      1\n",
      "      data representation|0.03 % |      1\n",
      "            models tested|0.03 % |      1\n",
      "            unboundedness|0.03 % |      1\n",
      "      learning techniques|0.03 % |      1\n",
      "distributed word representations|0.03 % |      1\n",
      "  a tool for phonologists|0.03 % |      1\n",
      "         proposed methods|0.03 % |      1\n",
      "cross-lingual sentiment analysis|0.03 % |      1\n",
      "        proposed approach|0.03 % |      1\n",
      "revisiting the feature augmentation method|0.03 % |      1\n",
      "abstractive text summarization|0.03 % |      1\n",
      "     extrinsic evaluation|0.03 % |      1\n",
      "hierarchy label graph construction|0.03 % |      1\n",
      "         mean+max pooling|0.03 % |      1\n",
      "discourse information for completing incomplete parses|0.03 % |      1\n",
      "shallow parsers with hand-written rules|0.03 % |      1\n",
      "singular value decomposition|0.03 % |      1\n",
      "                   mctest|0.03 % |      1\n",
      "             autoencoding|0.03 % |      1\n",
      " issues with tokenization|0.03 % |      1\n",
      "    exploiting rationales|0.03 % |      1\n",
      "mdp versus two stage interpretation|0.03 % |      1\n",
      "overlapping mention recognition|0.03 % |      1\n",
      "existing corpora annotated with argumentation structures|0.03 % |      1\n",
      "neural language model architectures|0.03 % |      1\n",
      "sentence compression using multi-task deep bi-lstms|0.03 % |      1\n",
      "the corpus and intercoder reliability study|0.03 % |      1\n",
      "       tree-based encoder|0.03 % |      1\n",
      "            co-occurrence|0.03 % |      1\n",
      "              hocus pocus|0.03 % |      1\n",
      "                subtask a|0.03 % |      1\n",
      "a case study: model-building for predicting entailment|0.03 % |      1\n",
      "heuristic rules for demonstratives|0.03 % |      1\n",
      "        three srl systems|0.03 % |      1\n",
      "             2. algorithm|0.03 % |      1\n",
      "undirected mst with the boruvka algorithm|0.03 % |      1\n",
      "joint model for two tasks|0.03 % |      1\n",
      " applications and results|0.03 % |      1\n",
      "accounting for linguistic structure|0.03 % |      1\n",
      "      multiple choice cnn|0.03 % |      1\n",
      "      experiments on swda|0.03 % |      1\n",
      "   geocoding reddit users|0.03 % |      1\n",
      "phenomena treated by two-level rules in the czech lexicon|0.03 % |      1\n",
      "constructing modality training data|0.03 % |      1\n",
      "     annotated paragraphs|0.03 % |      1\n",
      "               robustness|0.03 % |      1\n",
      "     extended combinators|0.03 % |      1\n",
      "span-attribute tagging (sa-t) model|0.03 % |      1\n",
      "                  parsing|0.03 % |      1\n",
      "discourse marker applications|0.03 % |      1\n",
      "          neural networks|0.03 % |      1\n",
      "architecture of the grapheme-to-phoneme converter|0.03 % |      1\n",
      "lexical resources for nlp|0.03 % |      1\n",
      "            low-rank grus|0.03 % |      1\n",
      "differences between the two systems|0.03 % |      1\n",
      "neural machine translation background|0.03 % |      1\n",
      "experimental settings and detail|0.03 % |      1\n",
      "           word embedding|0.03 % |      1\n",
      "multimodal machine translation with embedding prediction|0.03 % |      1\n",
      "        annotation scheme|0.03 % |      1\n",
      "contrast to phrase-based smt|0.03 % |      1\n",
      "       keystroke dynamics|0.03 % |      1\n",
      "model descriptions and training details|0.03 % |      1\n",
      "  breaking news detection|0.03 % |      1\n",
      "neural network for sentiment classification|0.03 % |      1\n",
      "grammar extraction algorithm|0.03 % |      1\n",
      "              performance|0.03 % |      1\n",
      "nce as a matrix factorization|0.03 % |      1\n",
      "   argument structure svm|0.03 % |      1\n",
      "     speech-to-text model|0.03 % |      1\n",
      "  experimental evaluation|0.03 % |      1\n",
      "     nist chinese-english|0.03 % |      1\n",
      "related cognitive work on metaphor aptness|0.03 % |      1\n",
      "        ranking framework|0.03 % |      1\n",
      "          overall results|0.03 % |      1\n",
      "the right frontier constraint in sdrt|0.03 % |      1\n",
      "naive bayesian classifiers|0.03 % |      1\n",
      "                     code|0.03 % |      1\n",
      "semantic relation classification|0.03 % |      1\n",
      "evaluating existing approaches|0.03 % |      1\n",
      "intra attention mechanism|0.03 % |      1\n",
      "    linguistic hypotheses|0.03 % |      1\n",
      "comparison with direct assessment|0.03 % |      1\n",
      "         systems and data|0.03 % |      1\n",
      "      word representation|0.03 % |      1\n",
      "mechanisms for switching control|0.03 % |      1\n",
      "aggregation readers and explicit reference readers|0.03 % |      1\n",
      "   cross-lingual analysis|0.03 % |      1\n",
      "               ir engines|0.03 % |      1\n",
      "parsing strategies with spp|0.03 % |      1\n",
      "     preparing your paper|0.03 % |      1\n",
      "            tree lowering|0.03 % |      1\n",
      "measuring pragmatic variation|0.03 % |      1\n",
      "graph neural networks (gnns)|0.03 % |      1\n",
      "    dialog state tracking|0.03 % |      1\n",
      "supervised learning of linguistic generalizations|0.03 % |      1\n",
      "         network training|0.03 % |      1\n",
      "    part-of-speech tagger|0.03 % |      1\n",
      " conclusion & future work|0.03 % |      1\n",
      "     typed co-occurrences|0.03 % |      1\n",
      "introductionthis paper also appears in the proceedings of the sixth international conference on applied natural language processing, seattle, wa, april 2000.|0.03 % |      1\n",
      "     transformation stage|0.03 % |      1\n",
      "      similarity features|0.03 % |      1\n",
      "existing methods and datasets for word similarity evaluation|0.03 % |      1\n",
      "lexical rules in categorial grammar|0.03 % |      1\n",
      "cohesiveness of a word list|0.03 % |      1\n",
      "convolution and max-over-time pooling|0.03 % |      1\n",
      "evaluation: noun-compound interpretation tasks|0.03 % |      1\n",
      "  evidence identification|0.03 % |      1\n",
      " recurrent model and data|0.03 % |      1\n",
      "dimensions of cca and kcca projections.|0.03 % |      1\n",
      "methodology of extracting semantic shifts from data|0.03 % |      1\n",
      "    non-task-oriented sds|0.03 % |      1\n",
      "keyphrase extraction and generation|0.03 % |      1\n",
      "referential property constraint|0.03 % |      1\n",
      "nlp event representations|0.03 % |      1\n",
      "     adversarial heatmaps|0.03 % |      1\n",
      "  linear sequential model|0.03 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application to other formalisms|0.03 % |      1\n",
      "          structural cues|0.03 % |      1\n",
      "spanish, dutch and german|0.03 % |      1\n",
      "related work & background|0.03 % |      1\n",
      "       on genre detection|0.03 % |      1\n",
      "     top-down propagation|0.03 % |      1\n",
      "                 examples|0.03 % |      1\n",
      "          unbounded model|0.03 % |      1\n",
      "arabic dialect identification|0.03 % |      1\n",
      "    datasets and settings|0.03 % |      1\n",
      "             online abuse|0.03 % |      1\n",
      "                  summary|0.03 % |      1\n",
      "       evaluation details|0.03 % |      1\n",
      "               base model|0.03 % |      1\n",
      "       dual-coding theory|0.03 % |      1\n",
      "step 3: measure collocational distributions|0.03 % |      1\n",
      "subset approximation by transforming the grammar|0.03 % |      1\n",
      "phrase-based machine translation|0.03 % |      1\n",
      "            fact-checking|0.03 % |      1\n",
      "          morphophonology|0.03 % |      1\n",
      " fixed confidence by ttts|0.03 % |      1\n",
      "    left-to-right parsing|0.03 % |      1\n",
      "   traditional approaches|0.03 % |      1\n",
      "     experimental details|0.03 % |      1\n",
      "           model settings|0.03 % |      1\n",
      "         restricted track|0.03 % |      1\n",
      "     rl phase performance|0.03 % |      1\n",
      "            text matching|0.03 % |      1\n",
      "headword percolation and binarization|0.03 % |      1\n",
      "natural language adversarial examples|0.03 % |      1\n",
      "  the notion of placeness|0.03 % |      1\n",
      "        literature review|0.03 % |      1\n",
      "       data and baselines|0.03 % |      1\n",
      "regular tree grammars of derivations|0.03 % |      1\n",
      "                data sets|0.03 % |      1\n",
      "  the conversational game|0.03 % |      1\n",
      "        review of methods|0.03 % |      1\n",
      "syntax marginal inference for dependency paths|0.03 % |      1\n",
      "automatic discourse tagging|0.03 % |      1\n",
      "multilingual nlp and the role of typologies|0.03 % |      1\n",
      "           gold standards|0.03 % |      1\n",
      " supervised and `pseudo-supervised' methods|0.03 % |      1\n",
      "word similarity evaluation|0.03 % |      1\n",
      "       dataset properties|0.03 % |      1\n",
      "interactive attention vs. coverage model|0.03 % |      1\n",
      "       additional results|0.03 % |      1\n",
      "multilingual sentence embeddings|0.03 % |      1\n",
      "multimodal machine translation|0.03 % |      1\n",
      "mmd vs gan in distribution matching|0.03 % |      1\n",
      "  verb-noun constructions|0.03 % |      1\n",
      "      convolutional layer|0.03 % |      1\n",
      "political ideology detection|0.03 % |      1\n",
      "            pp attachment|0.03 % |      1\n",
      " rnn-based language model|0.03 % |      1\n",
      "learning across representations|0.03 % |      1\n",
      "evidence for limited attention from anaphoric processing|0.03 % |      1\n",
      "test data maintenance and retrieval|0.03 % |      1\n",
      "previous non-nlp benchmarks|0.03 % |      1\n",
      "automatic keyphrase extraction|0.03 % |      1\n",
      "background and task definition|0.03 % |      1\n",
      "techniques for gans for text|0.03 % |      1\n",
      "temporal information processing|0.03 % |      1\n",
      "      linguistic features|0.03 % |      1\n",
      "        word-level models|0.03 % |      1\n",
      "    binary classification|0.03 % |      1\n",
      "      clustering with mdl|0.03 % |      1\n",
      "representation and length formulæ|0.03 % |      1\n",
      "         data exploration|0.03 % |      1\n",
      "                 the data|0.03 % |      1\n",
      "       graph-based parser|0.03 % |      1\n",
      "graph convolutional networks|0.03 % |      1\n",
      "implementation details for lace|0.03 % |      1\n",
      "applying probability measures to tree adjoining languages|0.03 % |      1\n",
      "           neural readers|0.03 % |      1\n",
      "appendix i: hyper-parameter details|0.03 % |      1\n",
      "classifying speech segments in isolation|0.03 % |      1\n",
      "centering in japanese discourse|0.03 % |      1\n",
      "          timex templates|0.03 % |      1\n",
      "              future work|0.03 % |      1\n",
      "     a novel architecture|0.03 % |      1\n",
      "sequence tagging with bidirectional rnn|0.03 % |      1\n",
      "             multi-hop rc|0.03 % |      1\n",
      "       sg-mcmc algorithms|0.03 % |      1\n",
      "                  resnets|0.03 % |      1\n",
      "sandhi splitting challenges|0.03 % |      1\n",
      "  discourse embellishment|0.03 % |      1\n",
      "document retrieval and reranking|0.03 % |      1\n",
      "   text generation models|0.03 % |      1\n",
      "                datasets |0.03 % |      1\n",
      "new applications and algorithmic extensions in active learning|0.03 % |      1\n",
      "    hyperparameter tuning|0.03 % |      1\n",
      "           common setting|0.03 % |      1\n",
      "evidence-based models of context|0.03 % |      1\n",
      "                 approach|0.03 % |      1\n",
      "       model improvements|0.03 % |      1\n",
      "      embeddings training|0.03 % |      1\n",
      "          product reviews|0.03 % |      1\n",
      "related work & conclusion|0.03 % |      1\n",
      "          basic algorithm|0.03 % |      1\n",
      "       dual decomposition|0.03 % |      1\n",
      "       improving coverage|0.03 % |      1\n",
      "   japanese `classifiers'|0.03 % |      1\n",
      "       response generator|0.03 % |      1\n",
      "           related works |0.03 % |      1\n",
      "recurrent neural network models|0.03 % |      1\n",
      "         vhmsg components|0.03 % |      1\n",
      "machine translation of varieties|0.03 % |      1\n",
      "         related research|0.03 % |      1\n",
      "conditioned language model|0.03 % |      1\n",
      "        model description|0.03 % |      1\n",
      "      evaluation measures|0.03 % |      1\n",
      "generating training data from raw text|0.03 % |      1\n",
      "graph representation of vector embeddings|0.03 % |      1\n",
      "         extracting edits|0.03 % |      1\n",
      "         story generation|0.03 % |      1\n",
      "             baseline nmt|0.03 % |      1\n",
      "network training and hyper-parameters|0.03 % |      1\n",
      "          problem setting|0.03 % |      1\n",
      " existing related methods|0.03 % |      1\n",
      "     clustering algorithm|0.03 % |      1\n",
      "      similarity measures|0.03 % |      1\n",
      "   the learning algorithm|0.03 % |      1\n",
      "neural language generation|0.03 % |      1\n",
      "                 framenet|0.03 % |      1\n",
      "    next token prediction|0.03 % |      1\n",
      "measuring semantic textual similarity|0.03 % |      1\n",
      "range concatenation grammar as a pivot formalism|0.03 % |      1\n",
      "     coreference in lasie|0.03 % |      1\n",
      "       sentence embedding|0.03 % |      1\n",
      "            the algorithm|0.03 % |      1\n",
      "   methods not considered|0.03 % |      1\n",
      "       the annotated data|0.03 % |      1\n",
      "            memory module|0.03 % |      1\n",
      "         the sense tagger|0.03 % |      1\n",
      "                 seq-rnns|0.03 % |      1\n",
      "           classification|0.03 % |      1\n",
      "large dataset for training and validation|0.03 % |      1\n",
      "        analysis of depnn|0.03 % |      1\n",
      "       nn-gram experiment|0.03 % |      1\n",
      "            final results|0.03 % |      1\n",
      "porting grammars and lexica between closely related languages|0.03 % |      1\n",
      "  the bible: 62 languages|0.03 % |      1\n",
      "implications for annotation|0.03 % |      1\n",
      "       sentiment lexicons|0.03 % |      1\n",
      "multi-channel convolutional layer|0.03 % |      1\n",
      "description of heuristics baseline|0.03 % |      1\n",
      " attention models for nlp|0.03 % |      1\n",
      "     empirical comparison|0.03 % |      1\n",
      "neural sequence-to-sequence model|0.03 % |      1\n",
      "   measures of similarity|0.03 % |      1\n",
      "parameter estimation and inference via posterior regularization|0.03 % |      1\n",
      "reuters cross-lingual document classification|0.03 % |      1\n",
      "augmenting data using word2vec|0.03 % |      1\n",
      "      learning algorithms|0.03 % |      1\n",
      "grammatical error correction|0.03 % |      1\n",
      "other vision and language tasks|0.03 % |      1\n",
      "     candidate generation|0.03 % |      1\n",
      "statistical inference in bayesum|0.03 % |      1\n",
      "    the learning approach|0.03 % |      1\n",
      "      the test collection|0.03 % |      1\n",
      "word vectors and wordless word vectors|0.03 % |      1\n",
      "               simulation|0.03 % |      1\n",
      "          state generator|0.03 % |      1\n",
      "          data statistics|0.03 % |      1\n",
      "using randomization for precision and f-score|0.03 % |      1\n",
      "dropped pronoun recovery.|0.03 % |      1\n",
      "    applications of focus|0.03 % |      1\n",
      "a basic sequence-to-sequence approach|0.03 % |      1\n",
      "application of relaxation labeling to nlp|0.03 % |      1\n",
      "       domain differences|0.03 % |      1\n",
      "      attention mechanism|0.03 % |      1\n",
      "                  derinet|0.03 % |      1\n",
      "datasets and graph construction|0.03 % |      1\n",
      "    comparing the parsers|0.03 % |      1\n",
      "   geometry of embeddings|0.03 % |      1\n",
      " measuring dialogue costs|0.03 % |      1\n",
      "measuring attributional similarity|0.03 % |      1\n",
      "     bio-based srl models|0.03 % |      1\n",
      "comparison against baselines|0.03 % |      1\n",
      "categorization of hate speech|0.03 % |      1\n",
      "                  duluth2|0.03 % |      1\n",
      "          temporal search|0.03 % |      1\n",
      "       datasets and tasks|0.03 % |      1\n",
      "                  tagging|0.03 % |      1\n",
      "background: cross-lingual embeddings|0.03 % |      1\n",
      "   abstraction approaches|0.03 % |      1\n",
      "recurrent models for text classification|0.03 % |      1\n",
      " online dialogue features|0.03 % |      1\n",
      "          document reader|0.03 % |      1\n",
      "textual clues: object oriented description|0.03 % |      1\n",
      "rte classification experiments for contradiction and disagreeing reply detection|0.03 % |      1\n",
      "further discussion: specialising semantic spaces|0.03 % |      1\n",
      "many tasks one sequence to sequence|0.03 % |      1\n",
      "       sentence rewriting|0.03 % |      1\n",
      "incremental grammar development|0.03 % |      1\n",
      "             introduction|0.03 % |      1\n",
      "     persistence diagrams|0.03 % |      1\n",
      "              pos tagging|0.03 % |      1\n",
      "       training the model|0.03 % |      1\n",
      "         design solutions|0.03 % |      1\n",
      "                     hnmt|0.03 % |      1\n",
      " table-to-text generation|0.03 % |      1\n",
      "      training algorithms|0.03 % |      1\n",
      "formalizing inflectional morphology|0.03 % |      1\n",
      "                 corpuses|0.03 % |      1\n",
      "fine-grained entity typing|0.03 % |      1\n",
      "combinatory category grammar supertagging|0.03 % |      1\n",
      "            related tasks|0.03 % |      1\n",
      "experimental setup and results|0.03 % |      1\n",
      "semi-supervised wake-sleep|0.03 % |      1\n",
      "    supervised evaluation|0.03 % |      1\n",
      "bridging nlp models and neurolinguistics|0.03 % |      1\n",
      "phrase-based and neural machine translation|0.03 % |      1\n",
      "gated graph neural networks|0.03 % |      1\n",
      "semi-supervised learning using multilingual data|0.03 % |      1\n",
      "     reframing entailment|0.03 % |      1\n",
      "           proposed model|0.03 % |      1\n",
      "          text collection|0.03 % |      1\n",
      "sentiment analysis and text categorization|0.03 % |      1\n",
      "software engineering vs. grammar engineering|0.03 % |      1\n",
      "             segmentation|0.03 % |      1\n",
      "a probabilistically-derived measure for unithood determination|0.03 % |      1\n",
      "      overall performance|0.03 % |      1\n",
      "introduction and related work|0.03 % |      1\n",
      "          lambek calculus|0.03 % |      1\n",
      "subtask 1: mining semantic fields|0.03 % |      1\n",
      "    neural-based approach|0.03 % |      1\n",
      "                 features|0.03 % |      1\n",
      "residual vq-vae for unsupervised monolingual paraphrasing|0.03 % |      1\n",
      "        domain adaptation|0.03 % |      1\n",
      "encoding structured data of kbs|0.03 % |      1\n",
      "deep learning on extra-linguistic features|0.03 % |      1\n",
      "        task descriptions|0.03 % |      1\n",
      " semi-supervised learning|0.03 % |      1\n",
      "             paraphrasing|0.03 % |      1\n",
      "results and interpretation|0.03 % |      1\n",
      "       rnn-based students|0.03 % |      1\n",
      "distributional semantic word representation|0.03 % |      1\n",
      "nmt with relation networks|0.03 % |      1\n",
      "       spanish taxonomies|0.03 % |      1\n",
      "  shared task competition|0.03 % |      1\n",
      "        the grammar (set)|0.03 % |      1\n",
      "        naive translation|0.03 % |      1\n",
      "statistical machine translation|0.03 % |      1\n",
      "    finite-state calculus|0.03 % |      1\n",
      "      system descriptions|0.03 % |      1\n",
      "  response selection task|0.03 % |      1\n",
      "              integration|0.03 % |      1\n",
      "multilingual attentional nmt|0.03 % |      1\n",
      "               estimation|0.03 % |      1\n",
      "contextualized word embeddings|0.03 % |      1\n",
      " a feature-based approach|0.03 % |      1\n",
      "         preliminary test|0.03 % |      1\n",
      "a nonstationary language model|0.03 % |      1\n",
      "deep learning for sentiment analysis|0.03 % |      1\n",
      "      sentence similarity|0.03 % |      1\n",
      "              environment|0.03 % |      1\n",
      "introduction and related works|0.03 % |      1\n",
      "extending annotation coverage|0.03 % |      1\n",
      "        neural benchmarks|0.03 % |      1\n",
      "dependency-based algorithm (dba)|0.03 % |      1\n",
      "unicon: an implementation|0.03 % |      1\n",
      "sequence labeling neural models|0.03 % |      1\n",
      "                  setting|0.03 % |      1\n",
      "        annotation graphs|0.03 % |      1\n",
      "                 decoding|0.03 % |      1\n",
      "conditional variational autoencoder |0.03 % |      1\n",
      "     baseline system (bl)|0.03 % |      1\n",
      "near threshold structure in j/ψ→γpp ¯j/\\psi \\rightarrow \\gamma p \\bar{p}|0.03 % |      1\n",
      "       semantic functions|0.03 % |      1\n",
      "      a dynamic framework|0.03 % |      1\n",
      "hashing under the nvi framework|0.03 % |      1\n",
      "   treebank concatenation|0.03 % |      1\n",
      "       transfer algorithm|0.03 % |      1\n",
      "human evaluation of machine translation|0.03 % |      1\n",
      "sparse representations for expansion|0.03 % |      1\n",
      "sequence to sequence learning|0.03 % |      1\n",
      "neural network approaches|0.03 % |      1\n",
      "        nmt architectures|0.03 % |      1\n",
      "  features and agreements|0.03 % |      1\n",
      "cues for tracking initiative|0.03 % |      1\n",
      " technical implementation|0.03 % |      1\n",
      "               appendix a|0.03 % |      1\n",
      "learning base noun phrases by machine|0.03 % |      1\n",
      "pairwise system agreement|0.03 % |      1\n",
      "                   inputs|0.03 % |      1\n",
      "     monolingual baseline|0.03 % |      1\n",
      "enhanced sequential inference model|0.03 % |      1\n",
      "transfer learning for nmt|0.03 % |      1\n",
      "background and related work|0.03 % |      1\n",
      "                 resource|0.03 % |      1\n",
      "   subword-unit-based nmt|0.03 % |      1\n",
      "learning as search optimization|0.03 % |      1\n",
      "               appendices|0.03 % |      1\n",
      "conditional training (ct)|0.03 % |      1\n",
      "non-dl learning algorithms|0.03 % |      1\n",
      "       system description|0.03 % |      1\n",
      "       emotion annotation|0.03 % |      1\n",
      "features of individual documents|0.03 % |      1\n",
      "manifestation of highly contrasting word pairs in text|0.03 % |      1\n",
      "impacts of integrating the simple ppdb|0.03 % |      1\n",
      "       out-of-domain data|0.03 % |      1\n",
      "lexical semantic collocations|0.03 % |      1\n",
      "   distance-based methods|0.03 % |      1\n",
      "multiple timescale gated recurrent unit|0.03 % |      1\n",
      "universal language model fine-tuning|0.03 % |      1\n",
      "     external comparisons|0.03 % |      1\n",
      "         experiment setup|0.03 % |      1\n",
      "neural machine translation: conditional language modelling|0.03 % |      1\n",
      "   multi-source neural mt|0.03 % |      1\n",
      "neural embedding methods for relational learning|0.03 % |      1\n",
      "             exp-ii: absa|0.03 % |      1\n",
      "     word sense induction|0.03 % |      1\n",
      "distributional word representations|0.03 % |      1\n",
      "limitations of the grammar|0.03 % |      1\n",
      "compilation of sorted feature terms|0.03 % |      1\n",
      "    modality and negation|0.03 % |      1\n",
      "summary, comparisons, and ongoing work|0.03 % |      1\n",
      "mildly context-sensitive language recognition|0.03 % |      1\n",
      "                   set-up|0.03 % |      1\n",
      "               classifier|0.03 % |      1\n",
      "distributional language embeddings|0.03 % |      1\n",
      "            collaboration|0.03 % |      1\n",
      "      contextual encoding|0.03 % |      1\n",
      "              gap dataset|0.03 % |      1\n",
      "             encoder: gru|0.03 % |      1\n",
      "grounding kernels in natural language dictionaries|0.03 % |      1\n",
      "transduction with soft attention|0.03 % |      1\n",
      "         image captioning|0.03 % |      1\n",
      "                    tasks|0.03 % |      1\n",
      "related works and conclusions|0.03 % |      1\n",
      "approximation by dendroid distribution|0.03 % |      1\n",
      "            task overview|0.03 % |      1\n",
      "    word similarity tasks|0.03 % |      1\n",
      "communicative function: a common thread in generation resources|0.03 % |      1\n",
      "encoder-decoder models for structured output prediction|0.03 % |      1\n",
      "datasets and experimental setup|0.03 % |      1\n",
      "      anatomy of a parser|0.03 % |      1\n",
      "evaluating sentence representations|0.03 % |      1\n",
      "modeling constraint quality|0.03 % |      1\n",
      "     ibm computer manuals|0.03 % |      1\n",
      "hedging as a sign of scientific discourse|0.03 % |      1\n",
      "   reinforcement learning|0.03 % |      1\n",
      "  additional related work|0.03 % |      1\n",
      "              medical ner|0.03 % |      1\n",
      "           nmt background|0.03 % |      1\n",
      "            penn treebank|0.03 % |      1\n",
      "               procedure:|0.03 % |      1\n",
      "      the pointer softmax|0.03 % |      1\n",
      "      classifier settings|0.03 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "construction of an s-type transducer |0.03 % |      1\n",
      " predicting dialogue acts|0.03 % |      1\n",
      " augmented word embedding|0.03 % |      1\n",
      "   typed feature grammars|0.03 % |      1\n",
      "            the formalism|0.03 % |      1\n",
      "  generation architecture|0.03 % |      1\n",
      "further details on evaluation dataset|0.03 % |      1\n",
      "                algorithm|0.03 % |      1\n",
      "          text classifier|0.03 % |      1\n",
      "lstm neural reordering model|0.03 % |      1\n",
      "   training and test data|0.03 % |      1\n",
      "existing models of attachment|0.03 % |      1\n",
      "      extended levi graph|0.03 % |      1\n",
      "    conversational models|0.03 % |      1\n",
      "the task: base np chunking|0.03 % |      1\n",
      "       maximizing metrics|0.03 % |      1\n",
      "deriving chunks from treebank parses|0.03 % |      1\n",
      "                     glue|0.03 % |      1\n",
      "   gated-attention reader|0.03 % |      1\n",
      "asymptotic normality and statistical efficiency|0.03 % |      1\n",
      "similar, associated, and both|0.03 % |      1\n",
      "experimental evaluation: bucc shared task on mining bitexts|0.03 % |      1\n",
      " other similarity methods|0.03 % |      1\n",
      "     encoder pre-training|0.03 % |      1\n",
      "             nli datasets|0.03 % |      1\n",
      "related work on grammar induction|0.03 % |      1\n",
      "improving over wmt2017 systems|0.03 % |      1\n",
      "         captioning model|0.03 % |      1\n",
      "     quantitative results|0.03 % |      1\n",
      "neural machine translation|0.03 % |      1\n",
      "   long-short term memory|0.03 % |      1\n",
      "“soft\" (em) joint training|0.03 % |      1\n",
      "the problem of vp ellipsis|0.03 % |      1\n",
      "       mlstm-based system|0.03 % |      1\n",
      "an overview of tutorialbank|0.03 % |      1\n",
      "            topic vectors|0.03 % |      1\n",
      "linguistic input features|0.03 % |      1\n",
      "   basic parsing paradigm|0.03 % |      1\n",
      "           ampere dataset|0.03 % |      1\n",
      " distributional semantics|0.03 % |      1\n",
      "experiment and discussion|0.03 % |      1\n",
      "multi-label classification|0.03 % |      1\n",
      "adapting esa to 2011 wikipedia|0.03 % |      1\n",
      "            system setup |0.03 % |      1\n",
      "interpretable paraphrase generation|0.03 % |      1\n",
      "retrieval of relevant documents and sentences|0.03 % |      1\n",
      "graph convolutional network encoders|0.03 % |      1\n",
      "phrase-based baseline systems|0.03 % |      1\n",
      "dialogue generation and visual dialogue|0.03 % |      1\n",
      "             model design|0.03 % |      1\n",
      "     representation model|0.03 % |      1\n",
      "deep convolutional networks|0.03 % |      1\n",
      "            arbitrariness|0.03 % |      1\n",
      "              translation|0.03 % |      1\n",
      "                 chunking|0.03 % |      1\n",
      "eye-tracking database for sarcasm analysis|0.03 % |      1\n",
      "          stanford reader|0.03 % |      1\n",
      "           string kernels|0.03 % |      1\n",
      "        distance matrices|0.03 % |      1\n",
      "  semantic representation|0.03 % |      1\n",
      "consonant co-occurrence network|0.03 % |      1\n",
      "applying error-correcting codes|0.03 % |      1\n",
      "      results on ace 2005|0.03 % |      1\n",
      "               the corpus|0.03 % |      1\n",
      "         neural mt system|0.03 % |      1\n",
      "       sememes and hownet|0.03 % |      1\n",
      "comparison to state-of-the-art supervised methods|0.03 % |      1\n",
      "pre-training the projection layer|0.03 % |      1\n",
      "exposure bias and error propagation|0.03 % |      1\n",
      "deep learning in summarization|0.03 % |      1\n",
      "         baseline methods|0.03 % |      1\n",
      "using multilayer annotations|0.03 % |      1\n",
      "    theoretical framework|0.03 % |      1\n",
      "models for temporal relations extraction|0.03 % |      1\n",
      "  vae for text generation|0.03 % |      1\n",
      "stylistic similarity evaluation|0.03 % |      1\n",
      "generalization to n>2n > 2|0.03 % |      1\n",
      "         multi-task model|0.03 % |      1\n",
      "detailed analysis for english-french|0.03 % |      1\n",
      "adding adjoining constraints|0.03 % |      1\n",
      "                reranking|0.03 % |      1\n",
      "   baselines & evaluation|0.03 % |      1\n",
      "measuring domain and noise in data|0.03 % |      1\n",
      "      methods and results|0.03 % |      1\n",
      "      fine-tuning details|0.03 % |      1\n",
      "stir: strongly incremental repair detection|0.03 % |      1\n",
      "bilm derived substitutions|0.03 % |      1\n",
      "        utterance encoder|0.03 % |      1\n",
      "    gender identification|0.03 % |      1\n",
      "            related work |0.03 % |      1\n",
      "performance improvement from answer re-ranking|0.03 % |      1\n",
      "adversarial domain adaptation and domain generation|0.03 % |      1\n",
      "   information extraction|0.03 % |      1\n",
      "        transfer learning|0.03 % |      1\n",
      "patterns extraction and analysis|0.03 % |      1\n",
      "neural auto-regressive topic model|0.03 % |      1\n",
      "               parameters|0.03 % |      1\n",
      "inter-annotator agreement|0.03 % |      1\n",
      "stack long short-term memory (lstm)|0.03 % |      1\n",
      "topic-based cross-referencing|0.03 % |      1\n",
      "     word representations|0.03 % |      1\n",
      "building the benchmark corpus|0.03 % |      1\n",
      "previous systems built on acl anthology|0.03 % |      1\n",
      "an alternative: evaluation via gap-filling|0.03 % |      1\n",
      "   application-evaluation|0.03 % |      1\n",
      "    error detection model|0.03 % |      1\n",
      "applications of phonetic vectors|0.03 % |      1\n",
      " iterated response models|0.03 % |      1\n",
      "language modeling baselines|0.03 % |      1\n",
      "monolingual and bilingual embeddings|0.03 % |      1\n",
      "     network architecture|0.03 % |      1\n",
      "      modifying particles|0.03 % |      1\n",
      "          attention layer|0.03 % |      1\n",
      "comparison with other work|0.03 % |      1\n",
      "          system overview|0.03 % |      1\n",
      "        sentence matching|0.03 % |      1\n",
      " advcls: adversarial classifier|0.03 % |      1\n",
      "   common topic inference|0.03 % |      1\n",
      "          required skills|0.03 % |      1\n",
      "background: attention-based neural machine translation|0.03 % |      1\n",
      "arabic morphological analyzer/generator|0.03 % |      1\n",
      "machine reading comprehension|0.03 % |      1\n",
      "dataset and evaluation criteria|0.03 % |      1\n",
      "   noun compound analysis|0.03 % |      1\n",
      "       temporal reasoning|0.03 % |      1\n",
      "multi-class classification|0.03 % |      1\n",
      "        parallel datasets|0.03 % |      1\n",
      "features of collaborative negotiation|0.03 % |      1\n",
      "experiments with large margin loss|0.03 % |      1\n",
      "degree adverbs in linguistics|0.03 % |      1\n",
      "word-synchronous beam search|0.03 % |      1\n",
      "  short text conversation|0.03 % |      1\n",
      "neural machine translation evaluation|0.03 % |      1\n",
      "evaluation of pronoun translation|0.03 % |      1\n",
      "story generation with planning|0.03 % |      1\n",
      "    training and decoding|0.03 % |      1\n",
      "word embedding recommendations|0.03 % |      1\n",
      "                materials|0.03 % |      1\n",
      "       entity recognition|0.03 % |      1\n",
      "actions and attributes dataset|0.03 % |      1\n",
      "recurrent layer adaptation|0.03 % |      1\n",
      "       sentiment features|0.03 % |      1\n",
      "  document classification|0.03 % |      1\n",
      "    content determination|0.03 % |      1\n",
      "datasets and representation of ideas|0.03 % |      1\n",
      "assigning thesaurus categories|0.03 % |      1\n",
      "syntactic features in text classification|0.03 % |      1\n",
      "   affective text dataset|0.03 % |      1\n",
      "analysis of frequency and polysemy|0.03 % |      1\n",
      "    hate speech detection|0.03 % |      1\n",
      "variational encoder-decoder (ved)|0.03 % |      1\n",
      "selection of the closest variant|0.03 % |      1\n",
      "             dataset used|0.03 % |      1\n",
      "estimating p(k j |c i )p(k_j|c_i)|0.03 % |      1\n",
      "static and contextualized words embeddings|0.03 % |      1\n",
      "translation lexicon extraction|0.03 % |      1\n",
      "       lda baseline model|0.03 % |      1\n",
      "memory augmented neural networks|0.03 % |      1\n",
      "        ans model fitting|0.03 % |      1\n",
      " hyper-parameter settings|0.03 % |      1\n",
      "         final evaluation|0.03 % |      1\n",
      "comparing statistical parsers|0.03 % |      1\n",
      "               validation|0.03 % |      1\n",
      "dual conditional cross-entropy filtering|0.03 % |      1\n",
      "true label prediction in crowdsourcing|0.03 % |      1\n",
      "           representation|0.03 % |      1\n",
      "   sequence tagging model|0.03 % |      1\n",
      "   ed baselines & results|0.03 % |      1\n",
      "measuring grammar quality|0.03 % |      1\n",
      "   comparison of coverage|0.03 % |      1\n",
      "   the statistical tagger|0.03 % |      1\n",
      "word and sense interconnectivity|0.03 % |      1\n",
      "results on public datasets|0.03 % |      1\n",
      "   end-to-end nlg systems|0.03 % |      1\n",
      "   relevant previous work|0.03 % |      1\n",
      "                     arae|0.03 % |      1\n",
      "              file format|0.03 % |      1\n",
      "         dialogue systems|0.03 % |      1\n",
      " a specification language|0.03 % |      1\n",
      "    conclusion and future|0.03 % |      1\n",
      "  confidence and salience|0.03 % |      1\n",
      "     readability measures|0.03 % |      1\n",
      "                 ace 2005|0.03 % |      1\n",
      "analysis of translation errors|0.03 % |      1\n",
      "      result and thoughts|0.03 % |      1\n",
      "            existing work|0.03 % |      1\n",
      "        nmt configuration|0.03 % |      1\n",
      "rules of sampling sql queries|0.03 % |      1\n",
      "tree structure enhanced neural machine translation|0.03 % |      1\n",
      "   semantic role labeling|0.03 % |      1\n",
      "     empirical evaluation|0.03 % |      1\n",
      "implementing hpsg in a clp framework|0.03 % |      1\n",
      "operationalizing irregularity|0.03 % |      1\n",
      "              equivalence|0.03 % |      1\n",
      "gender stereotypes in text|0.03 % |      1\n",
      "search-based structured prediction|0.03 % |      1\n",
      "           related models|0.03 % |      1\n",
      "historical spelling normalization|0.03 % |      1\n",
      "introductionthe author is currently at texas instruments and all inquiries should be addressed to rajeev@csc.ti.com.|0.03 % |      1\n",
      "    inflection generation|0.03 % |      1\n",
      "the elements of the olac metadata set|0.03 % |      1\n",
      "      evaluation datasets|0.03 % |      1\n",
      "fake news detection using textual information|0.03 % |      1\n",
      "   maximum-entropy method|0.03 % |      1\n",
      "     model initialization|0.03 % |      1\n",
      "       model and training|0.03 % |      1\n",
      "handling of unknown words|0.03 % |      1\n",
      "     major and minor keys|0.03 % |      1\n",
      " semantic representations|0.03 % |      1\n",
      "              the problem|0.03 % |      1\n",
      "  summary and future work|0.03 % |      1\n",
      "      experimental set-up|0.03 % |      1\n",
      "  deterministic inference|0.03 % |      1\n",
      "recognizing textual entailment|0.03 % |      1\n",
      " bidirectional lstm layer|0.03 % |      1\n",
      "   the tagger and corpora|0.03 % |      1\n",
      "                   system|0.03 % |      1\n",
      "           tasks and data|0.03 % |      1\n",
      "experimental data, setup and results|0.03 % |      1\n",
      "      when to communicate|0.03 % |      1\n",
      "          coding features|0.03 % |      1\n",
      "integrating language technology with machine learning|0.03 % |      1\n",
      "          data for task 2|0.03 % |      1\n",
      "    experimental datasets|0.03 % |      1\n",
      "action recognition models|0.03 % |      1\n",
      "       weka configuration|0.03 % |      1\n",
      "compared against other models|0.03 % |      1\n",
      "a long time ago, in a galaxy far, far away|0.03 % |      1\n",
      "related work and existing datasets|0.03 % |      1\n",
      "visualization of perceptron weights.|0.03 % |      1\n",
      "frame tracking: an extension of state tracking|0.03 % |      1\n",
      "current sign language research|0.03 % |      1\n",
      "supervised review summary generation|0.03 % |      1\n",
      "         importance score|0.03 % |      1\n",
      "pens classification scheme|0.03 % |      1\n",
      "3 examples of defaults in vm–gen|0.03 % |      1\n",
      "            visualization|0.03 % |      1\n",
      "semi-supervised spoken language understanding|0.03 % |      1\n",
      "ensembles and model diversity|0.03 % |      1\n",
      "               text model|0.03 % |      1\n",
      "       sentiment baseline|0.03 % |      1\n",
      "relation to knowledge graph embedding|0.03 % |      1\n",
      "                  tag set|0.03 % |      1\n",
      "      problem formulation|0.03 % |      1\n",
      "symmetry between conjuncts|0.03 % |      1\n",
      "english–finnish and finnish–english|0.03 % |      1\n",
      "  a grammar for discourse|0.03 % |      1\n",
      "coupled-lstms for strong sentence interaction|0.03 % |      1\n",
      "lexicalization and the probability model|0.03 % |      1\n",
      "   combination techniques|0.03 % |      1\n",
      "       feature importance|0.03 % |      1\n",
      "       keyword extraction|0.03 % |      1\n",
      "outline of the generation process|0.03 % |      1\n",
      "selectional preference and sense ambiguity|0.03 % |      1\n",
      "       bidirectional lstm|0.03 % |      1\n",
      " machine learning results|0.03 % |      1\n",
      "       syntactic analysis|0.03 % |      1\n",
      "c-test difficulty prediction|0.03 % |      1\n",
      "       fluency evaluation|0.03 % |      1\n",
      "replicability analysis for nlp|0.03 % |      1\n",
      "ucca's semantic structures|0.03 % |      1\n",
      "sampling argument scrambling via syntactic transformations|0.03 % |      1\n",
      "              nmt systems|0.03 % |      1\n",
      " data augmentation in nlp|0.03 % |      1\n",
      " link function of bigrams|0.03 % |      1\n",
      "general neural model for chinese word segmentation|0.03 % |      1\n",
      "              test corpus|0.03 % |      1\n",
      "          proposed models|0.03 % |      1\n",
      "         paragraph reader|0.03 % |      1\n",
      "flexible sense distinctions|0.03 % |      1\n",
      "debiasing word embeddings|0.03 % |      1\n",
      "    alignment-based model|0.03 % |      1\n",
      "           simulated data|0.03 % |      1\n",
      "baseline rnn-based encdec model|0.03 % |      1\n",
      "  topic modeling analysis|0.03 % |      1\n",
      "conditional probability model|0.03 % |      1\n",
      "the dynamic programming table|0.03 % |      1\n",
      "  verb frame alternations|0.03 % |      1\n",
      "                inference|0.03 % |      1\n",
      "compositionality in vector space|0.03 % |      1\n",
      "          atis experiment|0.03 % |      1\n",
      "         centering theory|0.03 % |      1\n",
      "word-level polarity features|0.03 % |      1\n",
      "   learning and inference|0.03 % |      1\n",
      "       learning algorithm|0.03 % |      1\n",
      "the proposed doc architecture|0.03 % |      1\n",
      "naive bayes and logistic regression|0.03 % |      1\n",
      "   knowledge distillation|0.03 % |      1\n",
      "multi-space variational autoencoders|0.03 % |      1\n",
      "scientific paper datasets|0.03 % |      1\n",
      "       rule-based systems|0.03 % |      1\n",
      "       evaluation dataset|0.03 % |      1\n",
      " negation scope detection|0.03 % |      1\n",
      "parallelism and inference|0.03 % |      1\n",
      "           transfer tasks|0.03 % |      1\n",
      "independent normalisation|0.03 % |      1\n",
      "related work and conclusions|0.03 % |      1\n",
      "applications and future work|0.03 % |      1\n",
      "statistics on collected data (mturk subset)|0.03 % |      1\n",
      "     data and experiments|0.03 % |      1\n",
      "      semi-supervised nmt|0.03 % |      1\n",
      "  alignment visualization|0.03 % |      1\n",
      "the glr parsing algorithm|0.03 % |      1\n",
      "              pos-tagging|0.03 % |      1\n",
      "      evaluation settings|0.03 % |      1\n",
      "related work on temporal relation modeling|0.03 % |      1\n",
      "                 treebank|0.03 % |      1\n",
      "shortcomings of current summarization models|0.03 % |      1\n",
      "sexual predator detection|0.03 % |      1\n",
      "dataset and experimental settings|0.03 % |      1\n",
      "            the treebanks|0.03 % |      1\n",
      "motivation: robust nlp systems|0.03 % |      1\n",
      "         the second model|0.03 % |      1\n",
      "lexical replacement in phylogenetics|0.03 % |      1\n",
      "bootstrapping a first-person sentiment corpus|0.03 % |      1\n",
      "            dag gru model|0.03 % |      1\n",
      "     character-level nlp.|0.03 % |      1\n",
      "data intelligence methods|0.03 % |      1\n",
      " targeted feature dropout|0.03 % |      1\n",
      "underspecified semantic tagging|0.03 % |      1\n",
      "training and hyperparameter tuning|0.03 % |      1\n",
      "                 appendix|0.03 % |      1\n",
      "          model debiasing|0.03 % |      1\n",
      "comparison with related work|0.03 % |      1\n",
      "predicting ordinal judgments|0.03 % |      1\n"
     ]
    }
   ],
   "source": [
    "print(\"%25s|%10s |%7s\"%('название секции','% от всех','кол-во '))\n",
    "print(50*'-')\n",
    "for ind,x,y in zip(df.max_title.value_counts().index,df.max_title.value_counts()/len(df)*100,df.max_title.value_counts()):\n",
    "    print(\"%25s|%2.2f %% |%7d\" % (ind,x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Смотрим из 2 максимумов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_2(x):\n",
    "    if x['max_title'] == 'related work' or x['max_title_2'] == 'related work':\n",
    "        return 'related work'\n",
    "    else:\n",
    "        return x['max_title']+'|'+x['max_title_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top_2'] = df.apply(top_2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>max_title</th>\n",
       "      <th>max_title_2</th>\n",
       "      <th>rw_exact</th>\n",
       "      <th>top_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10164018</td>\n",
       "      <td>introduction</td>\n",
       "      <td>reader-aware salience estimation</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction|reader-aware salience estimation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>introduction</td>\n",
       "      <td>the grammar</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction|the grammar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>189927790</td>\n",
       "      <td>introduction</td>\n",
       "      <td>relevance-based auxiliary task (rat)</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction|relevance-based auxiliary task (rat)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5084110</td>\n",
       "      <td>acknowledgments</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>acknowledgments|None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>126168169</td>\n",
       "      <td>experiment</td>\n",
       "      <td>introduction</td>\n",
       "      <td>0</td>\n",
       "      <td>experiment|introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>184488238</td>\n",
       "      <td>introduction</td>\n",
       "      <td>related work</td>\n",
       "      <td>1</td>\n",
       "      <td>related work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>85517799</td>\n",
       "      <td>introduction</td>\n",
       "      <td>related work</td>\n",
       "      <td>1</td>\n",
       "      <td>related work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16050464</td>\n",
       "      <td>related work</td>\n",
       "      <td>introduction</td>\n",
       "      <td>1</td>\n",
       "      <td>related work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52155342</td>\n",
       "      <td>exposure bias and error propagation</td>\n",
       "      <td>introduction</td>\n",
       "      <td>0</td>\n",
       "      <td>exposure bias and error propagation|introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52247458</td>\n",
       "      <td>introduction</td>\n",
       "      <td>related work</td>\n",
       "      <td>1</td>\n",
       "      <td>related work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5267356</td>\n",
       "      <td>introduction</td>\n",
       "      <td>related work</td>\n",
       "      <td>1</td>\n",
       "      <td>related work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>52097879</td>\n",
       "      <td>introduction</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>introduction|None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>51889492</td>\n",
       "      <td>related work</td>\n",
       "      <td>introduction</td>\n",
       "      <td>1</td>\n",
       "      <td>related work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>195791871</td>\n",
       "      <td>introduction</td>\n",
       "      <td>training setup</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction|training setup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29152549</td>\n",
       "      <td>text model</td>\n",
       "      <td>introduction</td>\n",
       "      <td>0</td>\n",
       "      <td>text model|introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>195218693</td>\n",
       "      <td>introduction</td>\n",
       "      <td>integrated gradients</td>\n",
       "      <td>1</td>\n",
       "      <td>introduction|integrated gradients</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3698344</td>\n",
       "      <td>acknowledgments</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>acknowledgments|None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3204901</td>\n",
       "      <td>references</td>\n",
       "      <td>aggregate markov models</td>\n",
       "      <td>0</td>\n",
       "      <td>references|aggregate markov models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>53096901</td>\n",
       "      <td>related work</td>\n",
       "      <td>evaluation metrics and baselines</td>\n",
       "      <td>1</td>\n",
       "      <td>related work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6385589</td>\n",
       "      <td>acknowledgments</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>acknowledgments|None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                            max_title  \\\n",
       "0    10164018                         introduction   \n",
       "1         488                         introduction   \n",
       "2   189927790                         introduction   \n",
       "3     5084110                      acknowledgments   \n",
       "5   126168169                           experiment   \n",
       "7   184488238                         introduction   \n",
       "8    85517799                         introduction   \n",
       "9    16050464                         related work   \n",
       "11   52155342  exposure bias and error propagation   \n",
       "12   52247458                         introduction   \n",
       "13    5267356                         introduction   \n",
       "14   52097879                         introduction   \n",
       "15   51889492                         related work   \n",
       "16  195791871                         introduction   \n",
       "17   29152549                           text model   \n",
       "18  195218693                         introduction   \n",
       "19    3698344                      acknowledgments   \n",
       "20    3204901                           references   \n",
       "21   53096901                         related work   \n",
       "22    6385589                      acknowledgments   \n",
       "\n",
       "                             max_title_2  rw_exact  \\\n",
       "0       reader-aware salience estimation         1   \n",
       "1                            the grammar         0   \n",
       "2   relevance-based auxiliary task (rat)         0   \n",
       "3                                   None         0   \n",
       "5                           introduction         0   \n",
       "7                           related work         1   \n",
       "8                           related work         1   \n",
       "9                           introduction         1   \n",
       "11                          introduction         0   \n",
       "12                          related work         1   \n",
       "13                          related work         1   \n",
       "14                                  None         0   \n",
       "15                          introduction         1   \n",
       "16                        training setup         1   \n",
       "17                          introduction         0   \n",
       "18                  integrated gradients         1   \n",
       "19                                  None         0   \n",
       "20               aggregate markov models         0   \n",
       "21      evaluation metrics and baselines         1   \n",
       "22                                  None         0   \n",
       "\n",
       "                                                top_2  \n",
       "0       introduction|reader-aware salience estimation  \n",
       "1                            introduction|the grammar  \n",
       "2   introduction|relevance-based auxiliary task (rat)  \n",
       "3                                acknowledgments|None  \n",
       "5                             experiment|introduction  \n",
       "7                                        related work  \n",
       "8                                        related work  \n",
       "9                                        related work  \n",
       "11   exposure bias and error propagation|introduction  \n",
       "12                                       related work  \n",
       "13                                       related work  \n",
       "14                                  introduction|None  \n",
       "15                                       related work  \n",
       "16                        introduction|training setup  \n",
       "17                            text model|introduction  \n",
       "18                  introduction|integrated gradients  \n",
       "19                               acknowledgments|None  \n",
       "20                 references|aggregate markov models  \n",
       "21                                       related work  \n",
       "22                               acknowledgments|None  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          название секции| % от всех |кол-во \n",
      "--------------------------------------------------\n",
      "             related work|59.42 % |   1060\n",
      "       introduction|setup|0.50 % |      9\n",
      "    introduction|datasets|0.39 % |      7\n",
      " introduction|experiments|0.39 % |      7\n",
      "     introduction|results|0.34 % |      6\n",
      "introduction|experimental setup|0.28 % |      5\n",
      "  introduction|evaluation|0.22 % |      4\n",
      "     introduction|dataset|0.22 % |      4\n",
      "       introduction|model|0.22 % |      4\n",
      "introduction|evaluation metrics|0.17 % |      3\n",
      "       setup|introduction|0.17 % |      3\n",
      "introduction|background and related work|0.17 % |      3\n",
      "    datasets|introduction|0.17 % |      3\n",
      "     dataset|introduction|0.17 % |      3\n",
      "  introduction|discussion|0.17 % |      3\n",
      "introduction|motivation and related work|0.11 % |      2\n",
      "       model|introduction|0.11 % |      2\n",
      "introduction|implementation details|0.11 % |      2\n",
      "introduction|task definition|0.11 % |      2\n",
      "  evaluation|introduction|0.11 % |      2\n",
      "      results|fine-tuning|0.11 % |      2\n",
      " introduction|conclusions|0.11 % |      2\n",
      "  conclusion|introduction|0.11 % |      2\n",
      "introduction|optimization algorithm|0.11 % |      2\n",
      "experimental settings|introduction|0.11 % |      2\n",
      "introduction|experimental setting|0.11 % |      2\n",
      "introduction|experimental results|0.11 % |      2\n",
      "    introduction|training|0.11 % |      2\n",
      "   introduction|data sets|0.11 % |      2\n",
      "introduction|related work and motivation|0.11 % |      2\n",
      " introduction|game theory|0.06 % |      1\n",
      "experimental settings|comparison of alignment methods|0.06 % |      1\n",
      "         results|datasets|0.06 % |      1\n",
      "viterbi e-step|introduction|0.06 % |      1\n",
      "introduction|subject-verb agreement|0.06 % |      1\n",
      "experimental datasets|baseline methods|0.06 % |      1\n",
      "introduction|crowdsourcing|0.06 % |      1\n",
      "task description|introduction|0.06 % |      1\n",
      "introduction|external information learning models|0.06 % |      1\n",
      "performance|semantic dependencies|0.06 % |      1\n",
      "introduction|motivation & background|0.06 % |      1\n",
      " introduction|translation|0.06 % |      1\n",
      "introduction|task description|0.06 % |      1\n",
      "introduction|predicate-specific encoding|0.06 % |      1\n",
      "background and task definition|factuality cues: from seeds to extended lists|0.06 % |      1\n",
      "neural machine translation: conditional language modelling|introduction|0.06 % |      1\n",
      "conditional probability model|top-down parsing and language modeling|0.06 % |      1\n",
      "experiments|interpreting of α\\alpha  and β\\beta |0.06 % |      1\n",
      "full parsing|introduction|0.06 % |      1\n",
      "knowledge distillation|cltc methods|0.06 % |      1\n",
      "contrast to phrase-based smt|data and methods|0.06 % |      1\n",
      "related and future work|background: abstract meaning representation and jamr|0.06 % |      1\n",
      "    introduction|unimorph|0.06 % |      1\n",
      "  introduction |optimizer|0.06 % |      1\n",
      "maximum entropy|support vector machines|0.06 % |      1\n",
      "introduction|model and evaluation|0.06 % |      1\n",
      "sg-mcmc algorithms|sentence classification|0.06 % |      1\n",
      "error detection model|machine translation|0.06 % |      1\n",
      "introduction|dataset and experimental settings|0.06 % |      1\n",
      "structural cues|character-level cues|0.06 % |      1\n",
      "text matching|introduction|0.06 % |      1\n",
      "related works and conclusions|hierarchical structure of complex sentence|0.06 % |      1\n",
      "temporal information processing|topic-based semantic representation|0.06 % |      1\n",
      "introduction|domain-specific poincaré embedding|0.06 % |      1\n",
      "introduction|word representation learning|0.06 % |      1\n",
      "language typology|introduction|0.06 % |      1\n",
      "      introduction|corpus|0.06 % |      1\n",
      "neural network approaches|domain adaptation|0.06 % |      1\n",
      "introduction|syntax-driven rule-based approaches|0.06 % |      1\n",
      "introduction|language model approximation|0.06 % |      1\n",
      "training details|overall performance|0.06 % |      1\n",
      "introduction|automatic evaluation|0.06 % |      1\n",
      "document-level sentiment classification|memory augmented recurrent models|0.06 % |      1\n",
      "coupled-lstms for strong sentence interaction|introduction|0.06 % |      1\n",
      "introduction|nmt settings|0.06 % |      1\n",
      "introduction|recursive neural network|0.06 % |      1\n",
      "analysis of depnn|experiments|0.06 % |      1\n",
      "sparse representations for expansion|set expansion|0.06 % |      1\n",
      "experimental setup|twitter results|0.06 % |      1\n",
      "introduction|implementation|0.06 % |      1\n",
      "introduction|distributed language representations|0.06 % |      1\n",
      "distributed word representations|baseline methods|0.06 % |      1\n",
      "hyperparameters|introduction|0.06 % |      1\n",
      "top-down propagation|detection function|0.06 % |      1\n",
      "background: disambiguation of prepositions and possessives|the snacs hierarchy|0.06 % |      1\n",
      "introduction|training algorithm|0.06 % |      1\n",
      "static and contextualized words embeddings|automatic heuristic evaluation metrics|0.06 % |      1\n",
      "text classifier|adversarial training|0.06 % |      1\n",
      "introduction|standard neural lms|0.06 % |      1\n",
      "glue|relation classification|0.06 % |      1\n",
      "introduction|dynamic meta-embeddings|0.06 % |      1\n",
      "      baselines|data sets|0.06 % |      1\n",
      "introduction|the attentional model of translation|0.06 % |      1\n",
      "baselines and implementation details|introduction|0.06 % |      1\n",
      "proposed models|introduction|0.06 % |      1\n",
      "ensembles and model diversity|translation system|0.06 % |      1\n",
      "introduction|inference and learning|0.06 % |      1\n",
      "a basic sequence-to-sequence approach|a probabilistic, semantic-based approach|0.06 % |      1\n",
      "introduction|final results|0.06 % |      1\n",
      "defining the objective|neural architecture|0.06 % |      1\n",
      "introduction|agreement in visim-400|0.06 % |      1\n",
      "tree structure enhanced neural machine translation|introduction|0.06 % |      1\n",
      "introduction|span representation|0.06 % |      1\n",
      "introduction|distilling the ensemble|0.06 % |      1\n",
      "   systems|logic variants|0.06 % |      1\n",
      "introduction|discussion and future work|0.06 % |      1\n",
      "introduction|comparison to other structured corpora|0.06 % |      1\n",
      "datasets and experimental setup|hyperparameters and training|0.06 % |      1\n",
      "   inference|base speaker|0.06 % |      1\n",
      "word embeddings|conclusion|0.06 % |      1\n",
      "fact-checking|stance detection|0.06 % |      1\n",
      "multiple timescale gated recurrent unit|introduction and related works|0.06 % |      1\n",
      "pp attachment|wordnet grounding|0.06 % |      1\n",
      "background and related work|introduction|0.06 % |      1\n",
      "         baselines|ranker|0.06 % |      1\n",
      "introduction|information extraction|0.06 % |      1\n",
      "data and experiments|inference|0.06 % |      1\n",
      "methods|evaluation metrics|0.06 % |      1\n",
      "introduction|relationship between word- and sentence-level evaluation tasks|0.06 % |      1\n",
      "introduction|hyperparameter settings|0.06 % |      1\n",
      "introduction|comparison with state-of-the-art models|0.06 % |      1\n",
      "syntactically rich language generation|introduction & related work|0.06 % |      1\n",
      "restricted track|introduction|0.06 % |      1\n",
      "introduction|integrated gradients|0.06 % |      1\n",
      "experiments|problem formulation|0.06 % |      1\n",
      "introduction|a theory-based characterization of nmt architectures|0.06 % |      1\n",
      "     discussion|data sets|0.06 % |      1\n",
      "bag generation algorithms|compiling connectivity domains|0.06 % |      1\n",
      "related tasks|text to scene systems|0.06 % |      1\n",
      " methodology|introduction|0.06 % |      1\n",
      "introduction|paradigm projection|0.06 % |      1\n",
      "introduction|word similarity|0.06 % |      1\n",
      "bi-directional reconstruction|introduction|0.06 % |      1\n",
      "results on cross-sentence nn-ary relation extraction|results on sentence-level relation extraction|0.06 % |      1\n",
      "acknowledgments|introduction|0.06 % |      1\n",
      "feature-engineering models|the dataset|0.06 % |      1\n",
      " introduction|scene graph|0.06 % |      1\n",
      "results and discussion|introduction|0.06 % |      1\n",
      "variational encoder-decoder (ved)|seq2seq and attention mechanism|0.06 % |      1\n",
      "introduction|feature importance analysis|0.06 % |      1\n",
      "introduction and related works|datasets|0.06 % |      1\n",
      "human evaluation of machine translation|user trust in automation|0.06 % |      1\n",
      "preliminaries|introduction|0.06 % |      1\n",
      "   network training|model|0.06 % |      1\n",
      "verb frame alternations|pre-trained representations|0.06 % |      1\n",
      "transfer tasks|experiment setup|0.06 % |      1\n",
      "introduction|training and decoding|0.06 % |      1\n",
      "introduction|qualitative evaluation|0.06 % |      1\n",
      "model training|recurrent neural networks|0.06 % |      1\n",
      "experiments|experimental settings|0.06 % |      1\n",
      "introduction|knowledge aided self attention|0.06 % |      1\n",
      "introduction|lexical feature modelling|0.06 % |      1\n",
      "introduction|preservation of word semantics|0.06 % |      1\n",
      "experiments|finding sbleu-optimal hypotheses|0.06 % |      1\n",
      "introduction|observed feature models|0.06 % |      1\n",
      "introduction|supertagging model|0.06 % |      1\n",
      "word and sense interconnectivity|connecting words and senses in context|0.06 % |      1\n",
      "rnn variants|introduction|0.06 % |      1\n",
      "multilingual sentence embeddings|introduction|0.06 % |      1\n",
      "baseline methods|introduction|0.06 % |      1\n",
      "arabic dialect identification|string kernels|0.06 % |      1\n",
      "introduction|determining association strength|0.06 % |      1\n",
      "introduction|learning architectures|0.06 % |      1\n",
      "introduction|sampling-based approximation to the nmt models|0.06 % |      1\n",
      "introduction|setting for model comparisons|0.06 % |      1\n",
      "multimodal machine translation with embedding prediction|word embedding|0.06 % |      1\n",
      "     introduction|scoring|0.06 % |      1\n",
      "introduction|hyper-parameters|0.06 % |      1\n",
      "introduction|episodic logic|0.06 % |      1\n",
      "introduction|matrix factorization|0.06 % |      1\n",
      "        introduction|lstm|0.06 % |      1\n",
      "introduction|comparative argument mining|0.06 % |      1\n",
      "introduction|bert as a cross-lingual encoder|0.06 % |      1\n",
      "nmt background|re-scoring results|0.06 % |      1\n",
      "cube pruning|introduction|0.06 % |      1\n",
      "introduction|testbed tasks|0.06 % |      1\n",
      "introduction|exploiting monolingual corpora for machine translation|0.06 % |      1\n",
      "baseline models|introduction|0.06 % |      1\n",
      "implementation and training details|the extractor|0.06 % |      1\n",
      "related work on grammar induction|induction strategies|0.06 % |      1\n",
      "datasets|model architecture and optimization|0.06 % |      1\n",
      "attention models for nlp|sarcasm detection|0.06 % |      1\n",
      "experiments|introduction and related work|0.06 % |      1\n",
      "measuring domain and noise in data|more related work|0.06 % |      1\n",
      "    data|rouge evaluation|0.06 % |      1\n",
      "introduction|datasets and preprocessing|0.06 % |      1\n",
      "introduction|sentence matching/paraphrase|0.06 % |      1\n",
      "negation scope detection|conversational negation corpus |0.06 % |      1\n",
      "dataset and evaluation criteria|deep learning to rank|0.06 % |      1\n",
      "  methodology|conclusions|0.06 % |      1\n",
      "word similarity tasks|word translation|0.06 % |      1\n",
      "introduction|translation results|0.06 % |      1\n",
      "introduction|related work |0.06 % |      1\n",
      "language model construction|tree generation|0.06 % |      1\n",
      "introduction|experiments design|0.06 % |      1\n",
      "introduction|effects on long sentences|0.06 % |      1\n",
      "frame tracking: an extension of state tracking|learning protocol and metrics|0.06 % |      1\n",
      "neural-based approach|introduction|0.06 % |      1\n",
      "introduction|common setup|0.06 % |      1\n",
      "  introduction|conclusion|0.06 % |      1\n",
      "introduction|limitations of previous methods|0.06 % |      1\n",
      "semi-supervised nmt|model and hyperparameters|0.06 % |      1\n",
      "bootstrapping a first-person sentiment corpus|introduction|0.06 % |      1\n",
      " introduction|performance|0.06 % |      1\n",
      "     overview|experiments|0.06 % |      1\n",
      "references|task and formulation|0.06 % |      1\n",
      "ed baselines & results|local model with neural attention|0.06 % |      1\n",
      "background: cross-lingual embeddings|experiment setup|0.06 % |      1\n",
      "motivation and related work|automatic evaluation metric|0.06 % |      1\n",
      "debiasing word embeddings|models|0.06 % |      1\n",
      "experimental setup|evaluation|0.06 % |      1\n",
      "introduction|approximations|0.06 % |      1\n",
      "composition functions|conclusion|0.06 % |      1\n",
      "introduction|conditional vae for dialog|0.06 % |      1\n",
      "introduction|neural editor architecture|0.06 % |      1\n",
      "next token prediction|implementation details|0.06 % |      1\n",
      "representation|introduction|0.06 % |      1\n",
      "introduction|binary violation results|0.06 % |      1\n",
      "statistical machine translation|neural machine translation|0.06 % |      1\n",
      "data sets|effect of translationese on direct assessment scores|0.06 % |      1\n",
      "       references|methods|0.06 % |      1\n",
      "methods not considered|introduction|0.06 % |      1\n",
      "introduction|theoretical background and related work|0.06 % |      1\n",
      "introduction|modularity for research|0.06 % |      1\n",
      "introduction|logic in text|0.06 % |      1\n",
      "conclusion and future|type prediction module|0.06 % |      1\n",
      "introduction|empirical evaluation|0.06 % |      1\n",
      "introduction|data preparation|0.06 % |      1\n",
      "introduction|predicting pronunciation using logograph input|0.06 % |      1\n",
      "  model|proposed approach|0.06 % |      1\n",
      "   introduction|asr model|0.06 % |      1\n",
      "       introduction|tasks|0.06 % |      1\n",
      "english-czech translation|feature set|0.06 % |      1\n",
      "abstractive text summarization|english gigaword|0.06 % |      1\n",
      "introduction|sense-type classification of verbs|0.06 % |      1\n",
      "experiment setup|motivation|0.06 % |      1\n",
      "test data maintenance and retrieval|query and retrieval: an example|0.06 % |      1\n",
      "introduction and related work|gold standard data on armed conflicts|0.06 % |      1\n",
      "introduction|cross-language embeddings|0.06 % |      1\n",
      "nmt with relation networks|attention-based nmt|0.06 % |      1\n",
      "mean+max pooling|tying word embeddings and word prediction layer|0.06 % |      1\n",
      "introduction|reinforcement learning for distant supervision|0.06 % |      1\n",
      "introduction|hyperparameters|0.06 % |      1\n",
      "collaboration|planning and referring|0.06 % |      1\n",
      "introduction|why use jamo letters?|0.06 % |      1\n",
      "morphology in language modeling|experiments and results|0.06 % |      1\n",
      "introduction|conclusion & future work|0.06 % |      1\n",
      "model description|introduction|0.06 % |      1\n",
      "semi-supervised wake-sleep|our variational family|0.06 % |      1\n",
      "introduction|training settings|0.06 % |      1\n",
      "simulated data|contaminated data|0.06 % |      1\n",
      "introduction|learning dynamics|0.06 % |      1\n",
      "introduction|results for english|0.06 % |      1\n",
      "reuters cross-lingual document classification|p@k word translation|0.06 % |      1\n",
      "introduction|topic-sensitive representations|0.06 % |      1\n",
      "experimental setting|appendix b.|0.06 % |      1\n",
      "introduction|baseline convolutional neural net|0.06 % |      1\n",
      "results on public datasets|introduction|0.06 % |      1\n",
      "results|learning approach|0.06 % |      1\n",
      "conclusion and future work|materials|0.06 % |      1\n",
      "introduction|results on english-french translation|0.06 % |      1\n",
      "introduction|word sense disambiguation (wsd)|0.06 % |      1\n",
      "final results|constituent hierarchy prediction|0.06 % |      1\n",
      "applications of focus|abstraction and focus|0.06 % |      1\n",
      " supervised and `pseudo-supervised' methods|introduction|0.06 % |      1\n",
      "introduction|unsupervised parsing|0.06 % |      1\n",
      " introduction|methodology|0.06 % |      1\n",
      "fine-tuning details|single-model performance|0.06 % |      1\n",
      "neural networks|datasets and experiment preparation|0.06 % |      1\n",
      "   methodology|motivation|0.06 % |      1\n",
      "background and preliminaries|other related work|0.06 % |      1\n",
      "related work and conclusions|the basic algorithm|0.06 % |      1\n",
      "models|multi-task learning|0.06 % |      1\n",
      "introduction|word sequence representations|0.06 % |      1\n",
      "introduction|background: saliency|0.06 % |      1\n",
      "sequence tagging model|implementation|0.06 % |      1\n",
      "introduction|syntactic scaffolds|0.06 % |      1\n",
      "introduction|multi-sense skip-gram (mssg) model|0.06 % |      1\n",
      "introduction|word alignments|0.06 % |      1\n",
      "baselines & evaluation|original centroid-based method|0.06 % |      1\n",
      "introduction|generative adversarial training for distant supervision relation extraction|0.06 % |      1\n",
      "        data|introduction|0.06 % |      1\n",
      "joint model for two tasks|recurrent neural network for slot filling|0.06 % |      1\n",
      "introduction|researchers should prioritize computationally efficient hardware and algorithms.|0.06 % |      1\n",
      "introduction|byte pair encoding (bpe)|0.06 % |      1\n",
      "results and interpretation|datasets|0.06 % |      1\n",
      "         framenet|wordnet|0.06 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         setting|datasets|0.06 % |      1\n",
      "introduction|the webquestionssp dataset|0.06 % |      1\n",
      "introduction|experimental framework|0.06 % |      1\n",
      "      datasets|conclusion|0.06 % |      1\n",
      "introduction|dip-dqn baseline|0.06 % |      1\n",
      "conclusions and future work|recurrent networks and lstms|0.06 % |      1\n",
      "introduction|results of triad model|0.06 % |      1\n",
      "introduction|reverse dictionary|0.06 % |      1\n",
      "word vector-based tree edit distance|word vector-based dynamic time warping|0.06 % |      1\n",
      "phrase-based machine translation|evaluation|0.06 % |      1\n",
      "relevant previous work|datasets and relevant knowledge graphs|0.06 % |      1\n",
      "gated-attention reader|introduction|0.06 % |      1\n",
      "introduction|approximately boolean entity tuples|0.06 % |      1\n",
      "introduction|a model for editor identification|0.06 % |      1\n",
      "transfer learning for nmt|introduction|0.06 % |      1\n",
      "    training|introduction|0.06 % |      1\n",
      "introduction|compositional similarity|0.06 % |      1\n",
      "similar, associated, and both|toefl synonyms|0.06 % |      1\n",
      "        introduction|bert|0.06 % |      1\n",
      "domain adaptation|learning from partial annotation|0.06 % |      1\n",
      "neural network models|datasets|0.06 % |      1\n",
      "data and baselines|experimental results|0.06 % |      1\n",
      "performance improvement from answer re-ranking|answer re-ranking|0.06 % |      1\n",
      "introduction|learning basque agreement|0.06 % |      1\n",
      "dependency-based algorithm (dba)|introduction|0.06 % |      1\n",
      "step 3: measure collocational distributions|problem description|0.06 % |      1\n",
      "introduction|rnn encoder-decoder|0.06 % |      1\n",
      "proposed model|experimental results|0.06 % |      1\n",
      "related work and data|introduction|0.06 % |      1\n",
      "end-to-end nlg systems|introduction|0.06 % |      1\n",
      "dag gru model|randomized splits|0.06 % |      1\n",
      "   introduction|generator|0.06 % |      1\n",
      "coordination in the ptb|introduction|0.06 % |      1\n",
      "introduction|comparison to previous work|0.06 % |      1\n",
      "introduction|details on shared vocabulary|0.06 % |      1\n",
      "text collection|why another machine comprehension dataset on script knowledge?|0.06 % |      1\n",
      "      overview|discussion|0.06 % |      1\n",
      "     baselines|motivation|0.06 % |      1\n",
      "related work |evaluation metrics |0.06 % |      1\n",
      "conversational models|passage selection|0.06 % |      1\n",
      "replicability analysis for nlp|data|0.06 % |      1\n",
      "the learning approach|generating features|0.06 % |      1\n",
      "quality estimation|extensions to other tasks|0.06 % |      1\n",
      "introduction|comparison to other approaches|0.06 % |      1\n",
      "            model|dropout|0.06 % |      1\n",
      "geocoding reddit users|introduction|0.06 % |      1\n",
      "out-of-domain data|introduction|0.06 % |      1\n",
      " introduction|discussions|0.06 % |      1\n",
      "gender stereotypes in text|introduction|0.06 % |      1\n",
      "table-to-text generation|low resource natural language generation|0.06 % |      1\n",
      "introduction|style transfer tasks|0.06 % |      1\n",
      "metric learning|introduction|0.06 % |      1\n",
      "related work & conclusion|performance of filters on nucle and weather data|0.06 % |      1\n",
      "introduction|seq2seq dialogue architecture|0.06 % |      1\n",
      "atis experiment|related work on considering sentence-level information|0.06 % |      1\n",
      "introduction|embedding language corpus|0.06 % |      1\n",
      "introduction|experiment settings|0.06 % |      1\n",
      "introduction|oracle word selection|0.06 % |      1\n",
      "introduction|conclusions and future work|0.06 % |      1\n",
      "implementation details|introduction|0.06 % |      1\n",
      "discussion|initialization by context-dependent bilingual word embeddings|0.06 % |      1\n",
      "introduction|inter-annotator agreement|0.06 % |      1\n",
      "         conclusion|model|0.06 % |      1\n",
      "experimental setup|word similarity|0.06 % |      1\n",
      "extended levi graph|main results on amr-to-text generation|0.06 % |      1\n",
      "introduction|user simulator|0.06 % |      1\n",
      "introduction|ner and chunking|0.06 % |      1\n",
      "semi-supervised learning|data|0.06 % |      1\n",
      "introduction|lexicon features|0.06 % |      1\n",
      "training text classifiers |properties of the algorithms|0.06 % |      1\n",
      "multi-space variational autoencoders|introduction|0.06 % |      1\n",
      "conclusion and future work|introduction|0.06 % |      1\n",
      "datasets|linguistically motivated features|0.06 % |      1\n",
      "review of methods|top 3 submissions|0.06 % |      1\n",
      "evidence identification|results|0.06 % |      1\n",
      "additional related work|estimating model with gumbel-sinkhorn|0.06 % |      1\n",
      "character-level nlp.|alignment-based distant supervision.|0.06 % |      1\n",
      "  evaluation dataset|data|0.06 % |      1\n",
      "introduction|main results|0.06 % |      1\n",
      "introduction|head-phrase structure grammars|0.06 % |      1\n",
      "        treebank|training|0.06 % |      1\n",
      "introduction|minimum risk training for neural machine translation|0.06 % |      1\n",
      "related work and conclusion|introduction|0.06 % |      1\n",
      "general neural model for chinese word segmentation|incorporating adversarial training for shared layer|0.06 % |      1\n",
      "semantic functions|incorporation with dependency minimal recursion semantics|0.06 % |      1\n",
      "modeling ambiguous pronouns|introduction|0.06 % |      1\n",
      "results|optimizing the number of latent states|0.06 % |      1\n",
      "introduction|molecular structure-based ddi classification|0.06 % |      1\n",
      "machine learning results|introduction|0.06 % |      1\n",
      "deep convolutional networks|representation learning|0.06 % |      1\n",
      "introduction|related work: syntactic linearization|0.06 % |      1\n",
      "introduction|word prediction|0.06 % |      1\n",
      "introduction|on the evaluation of relational models|0.06 % |      1\n",
      "     methods|introduction|0.06 % |      1\n",
      "duluth38 background|introduction|0.06 % |      1\n",
      "sequence tagging with bidirectional rnn|conditional random field|0.06 % |      1\n",
      "introduction|labeled claim data|0.06 % |      1\n",
      "quantitative results|variational auto-encoder|0.06 % |      1\n",
      "large dataset for training and validation|data crawling and preprocessing|0.06 % |      1\n",
      "introduction|preprocessing alignment supervision|0.06 % |      1\n",
      "recurrent models for text classification|introduction|0.06 % |      1\n",
      "conclusions and future work|prosodic features|0.06 % |      1\n",
      "nli datasets|training details|0.06 % |      1\n",
      "analysis of frequency and polysemy|textual entailment experiment|0.06 % |      1\n",
      "introduction|reader-aware salience estimation|0.06 % |      1\n",
      "introduction|grammar induction as search|0.06 % |      1\n",
      "datasets and representation of ideas|introduction|0.06 % |      1\n",
      "introduction|expert and automated generation|0.06 % |      1\n",
      "introduction|distantly supervised data collection|0.06 % |      1\n",
      "shallow parsers with hand-written rules|testing methodology|0.06 % |      1\n",
      "introduction|machine translation|0.06 % |      1\n",
      "babi task|robust dnc training|0.06 % |      1\n",
      "introduction|learning to track entities|0.06 % |      1\n",
      "introduction|model comparison|0.06 % |      1\n",
      "introduction|sentence model|0.06 % |      1\n",
      "dialogue act classification|utterance-level rnn|0.06 % |      1\n",
      "lstm neural reordering model|introduction|0.06 % |      1\n",
      "introduction|quantitive assessment|0.06 % |      1\n",
      "introduction|limitations and future work|0.06 % |      1\n",
      "word vectors and wordless word vectors|solution: compositional models|0.06 % |      1\n",
      "de-identification|language modeling|0.06 % |      1\n",
      "introduction|speech translation|0.06 % |      1\n",
      "multi-hop rc|knowledge graph qa|0.06 % |      1\n",
      "introduction|pure memory-augmented neural network|0.06 % |      1\n",
      "evaluation details|sentence extraction|0.06 % |      1\n",
      "experimental results|background: model theory, neo-davidsonian events, and situations|0.06 % |      1\n",
      "related work & background|introduction|0.06 % |      1\n",
      "“soft\" (em) joint training|convolutional neural network|0.06 % |      1\n",
      "word sense induction|comparing on semeval-2010|0.06 % |      1\n",
      "related work on temporal relation modeling|introduction|0.06 % |      1\n",
      "attention mechanism|argument identification|0.06 % |      1\n",
      "long-short term memory network|introduction|0.06 % |      1\n",
      "the grammar (set)|the parser|0.06 % |      1\n",
      "    corpuses|introduction|0.06 % |      1\n",
      "measuring semantic textual similarity|introduction|0.06 % |      1\n",
      "the source data|introduction|0.06 % |      1\n",
      "word embeddings|introduction|0.06 % |      1\n",
      "    results|model details|0.06 % |      1\n",
      "introduction|input and output representations|0.06 % |      1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a lemmatizer: p(ℓ i ∣m i ,w i )p(\\ell _i \\mid m_i, w_i)|morphological tagger: p(𝐦∣𝐰)p(\\mathbf {m}\\mid \\mathbf {w})|0.06 % |      1\n",
      "   introduction|inference|0.06 % |      1\n",
      "eye-tracking database for sarcasm analysis|the sarcasm classifier|0.06 % |      1\n",
      "captioning model|introduction|0.06 % |      1\n",
      "      hocus pocus|dataset|0.06 % |      1\n",
      "ir engines|relevance feedback performance|0.06 % |      1\n",
      "evaluation settings|introduction|0.06 % |      1\n",
      "introduction|extensibility|0.06 % |      1\n",
      "    baselines|experiments|0.06 % |      1\n",
      "network training and hyper-parameters|regularization|0.06 % |      1\n",
      "implementation details|graph lstms|0.06 % |      1\n",
      "related work and datasets|introduction|0.06 % |      1\n",
      "model debiasing|introduction|0.06 % |      1\n",
      "introduction|the summarization approach|0.06 % |      1\n",
      "data statistics|baseline model|0.06 % |      1\n",
      "cross-lingual mapping|introduction|0.06 % |      1\n",
      "            data|features|0.06 % |      1\n",
      "introduction|linguistic constraints via regularization|0.06 % |      1\n",
      "introduction|context encoders|0.06 % |      1\n",
      "introduction|areal linguistics|0.06 % |      1\n",
      "comparison against baselines|model hyperparameters|0.06 % |      1\n",
      "introduction|choice of verb pairs and coverage|0.06 % |      1\n",
      "datasets|pre-training decoder|0.06 % |      1\n",
      "unsupervised clustering|introduction|0.06 % |      1\n",
      "experimental data, setup and results|relation extraction experiments|0.06 % |      1\n",
      "manifestation of highly contrasting word pairs in text|co-occurrence|0.06 % |      1\n",
      "introduction|multi-task learning|0.06 % |      1\n",
      "flexible sense distinctions|experimental evaluation of the method|0.06 % |      1\n",
      "major and minor keys|introduction|0.06 % |      1\n",
      "  methodology|experiments|0.06 % |      1\n",
      "speech recognition|text retrieval|0.06 % |      1\n",
      "introduction|background: pointer networks|0.06 % |      1\n",
      "    introduction|analysis|0.06 % |      1\n",
      "background and motivation|frames and well-being|0.06 % |      1\n",
      "asymptotic normality and statistical efficiency|the algorithms in previous work|0.06 % |      1\n",
      "quantitative evaluation|introduction|0.06 % |      1\n",
      "introduction|model design|0.06 % |      1\n",
      "introduction|multi-head attention|0.06 % |      1\n",
      "fake news detection using textual information|task settings of fake news detection|0.06 % |      1\n",
      "neural network for sentiment classification|effects of word embeddings|0.06 % |      1\n",
      "experimental setup|argus dataset|0.06 % |      1\n",
      "dataset|about polysynthetic languages|0.06 % |      1\n",
      "experiments|reinforcement learning|0.06 % |      1\n",
      "revisiting the feature augmentation method|adaptation between ms coco and flickr30k|0.06 % |      1\n",
      "approaches using prosodic cues|approaches based on discourse and combined cues|0.06 % |      1\n",
      "monolingual baseline|data|0.06 % |      1\n",
      "pltig and related work|introduction|0.06 % |      1\n",
      "subword-unit-based nmt|introduction|0.06 % |      1\n",
      "      evaluation|training|0.06 % |      1\n",
      "pens classification scheme|types and properties|0.06 % |      1\n",
      "influence in interactions|influence in social networks|0.06 % |      1\n",
      "models|sentence-level rnn language models|0.06 % |      1\n",
      "  introduction|comparison|0.06 % |      1\n",
      "     corpora|introduction|0.06 % |      1\n",
      "a probabilistically-derived measure for unithood determination|introduction|0.06 % |      1\n",
      "introduction|estimating item difficulty|0.06 % |      1\n",
      "formalizing inflectional morphology|introduction|0.06 % |      1\n",
      "introduction|architecture|0.06 % |      1\n",
      "background: attention-based neural machine translation|training|0.06 % |      1\n",
      "introduction|experimental set-up|0.06 % |      1\n",
      "neural machine translation background|computational performance|0.06 % |      1\n",
      "introduction|preservation of linguistic similarity|0.06 % |      1\n",
      "introduction|image captioning task|0.06 % |      1\n",
      "introduction|analysis of long sentence translation|0.06 % |      1\n",
      "fixed confidence by ttts|motivating example|0.06 % |      1\n",
      "           features|model|0.06 % |      1\n",
      "automatic discourse tagging|related work on discourse relations for summarization|0.06 % |      1\n",
      "contextualized word embeddings|introduction|0.06 % |      1\n",
      "dataset|implementation detail|0.06 % |      1\n",
      "experimental settings|corpus sorting methods|0.06 % |      1\n",
      "introduction|results on english-german translation|0.06 % |      1\n",
      "distributional semantics|frame-based semantics|0.06 % |      1\n",
      "   introduction|baselines|0.06 % |      1\n",
      "introduction|training details|0.06 % |      1\n",
      "introduction|evaluation using automatic metrics|0.06 % |      1\n",
      "introduction|pre-training of hyperdef|0.06 % |      1\n",
      "architecture|keyterm extraction|0.06 % |      1\n",
      "introduction|designing the translation experience|0.06 % |      1\n",
      "enhanced sequential inference model|recognizing textual entailment|0.06 % |      1\n",
      "information extraction|graph convolution network|0.06 % |      1\n",
      "introduction|shortcomings of generative modeling|0.06 % |      1\n",
      "    introduction|approach|0.06 % |      1\n",
      "introduction|training details and submitted runs|0.06 % |      1\n",
      "introduction|training data|0.06 % |      1\n",
      "introduction|natural language inference|0.06 % |      1\n",
      "        introduction|data|0.06 % |      1\n",
      "background and preliminaries|datasets|0.06 % |      1\n",
      "models for temporal relations extraction|data|0.06 % |      1\n",
      "compositionality in vector space|introduction|0.06 % |      1\n",
      "introduction|results on the spmrl datasets|0.06 % |      1\n",
      "introduction|acknowledgments|0.06 % |      1\n",
      "c-test difficulty prediction|evaluation of the manipulation system|0.06 % |      1\n",
      "modality and negation|simt results|0.06 % |      1\n",
      "introduction|base text encoder and explanation generator|0.06 % |      1\n",
      "introduction|augmentation with word substitution|0.06 % |      1\n",
      "introduction|dataset and evaluation|0.06 % |      1\n",
      "stanford reader|introduction|0.06 % |      1\n",
      "search-based structured prediction|introduction|0.06 % |      1\n",
      "introduction|related work not already discussed|0.06 % |      1\n",
      "introduction|model 1: mil|0.06 % |      1\n",
      "introduction|topic generation|0.06 % |      1\n",
      "introduction|lexical embeddings|0.06 % |      1\n",
      "speech-to-text model|introduction & related work|0.06 % |      1\n",
      "ucca's semantic structures|evaluation protocol|0.06 % |      1\n",
      "  introduction|motivation|0.06 % |      1\n",
      "model architecture|introduction|0.06 % |      1\n",
      "overall performance|introduction|0.06 % |      1\n",
      "mildly context-sensitive language recognition|introduction|0.06 % |      1\n",
      "       introduction|clevr|0.06 % |      1\n",
      "application to other formalisms|multiple-pass parsing|0.06 % |      1\n",
      "introduction|associative memory rnns for dual sequence modeling|0.06 % |      1\n",
      "naive translation|introduction|0.06 % |      1\n",
      "introduction|comparison with other segmentation algorithms|0.06 % |      1\n",
      "introduction|joint neural model|0.06 % |      1\n",
      "the conversational game|interactive language acquisition via joint imitation and reinforcement|0.06 % |      1\n",
      "introduction|recurrent neural networks|0.06 % |      1\n",
      "generating training data from raw text|evaluation in english|0.06 % |      1\n",
      "introduction|background – poincaré embeddings|0.06 % |      1\n",
      "introduction|convolutional neural networks|0.06 % |      1\n",
      "introduction|knowledge grounded conversation|0.06 % |      1\n",
      "introduction|character-level models|0.06 % |      1\n",
      "creating instances|rnns for relation classification|0.06 % |      1\n",
      "introduction|approaches to semantic multilayering|0.06 % |      1\n",
      "introduction|our approach|0.06 % |      1\n",
      "identifying narrative paragraphs from three text corpora|narrative cloze|0.06 % |      1\n",
      "sexual predator detection|comparison with related work|0.06 % |      1\n",
      "related work and existing datasets|introduction|0.06 % |      1\n",
      "finite-state calculus|an approximation algorithm|0.06 % |      1\n",
      "problem formulation|aspect-based autoencoder|0.06 % |      1\n",
      "     introduction|extract|0.06 % |      1\n",
      "introduction|data preprocessing|0.06 % |      1\n",
      "introduction|source language tokenization|0.06 % |      1\n",
      "introduction|prospects for field studies|0.06 % |      1\n",
      "word embedding|multimodal neural machine translation|0.06 % |      1\n",
      "  data|model architecture|0.06 % |      1\n",
      "experimental details|datasets|0.06 % |      1\n",
      "introduction|model implementation|0.06 % |      1\n",
      "introduction|joint parsing and disfluency detection|0.06 % |      1\n",
      "clinical temporal relation extraction|multi-task learning|0.06 % |      1\n",
      "introduction|limitations of flat mrs|0.06 % |      1\n",
      "experimental evaluation: bucc shared task on mining bitexts|multilingual sentence embeddings|0.06 % |      1\n",
      "introduction|distributional models|0.06 % |      1\n",
      "evaluation metrics|introduction|0.06 % |      1\n",
      "categorization of hate speech|inter annotator agreement|0.06 % |      1\n",
      "introduction|targeted features|0.06 % |      1\n",
      "document retrieval and reranking|other related work|0.06 % |      1\n",
      "stir: strongly incremental repair detection|our approach|0.06 % |      1\n",
      "introduction|data collection|0.06 % |      1\n",
      "overall results|introduction|0.06 % |      1\n",
      "introduction|the algorithm|0.06 % |      1\n",
      "a feature-based approach|lexical cohesion|0.06 % |      1\n",
      "evaluation of pronoun translation|introduction|0.06 % |      1\n",
      "references| conclusions and outlook|0.06 % |      1\n",
      "convolutional layer|integration with word embeddings|0.06 % |      1\n",
      "introduction|returnn features|0.06 % |      1\n",
      "introduction|deep transition rnn|0.06 % |      1\n",
      "introduction|action structure: aligning words to percepts|0.06 % |      1\n",
      "data collection|introduction|0.06 % |      1\n",
      "introduction|experimental procedure|0.06 % |      1\n",
      "introduction|pre-trained word embeddings|0.06 % |      1\n",
      "     introduction|methods|0.06 % |      1\n",
      "introduction|coherence discriminator: d coherence d_{\\text{coherence}}|0.06 % |      1\n",
      " experiments|introduction|0.06 % |      1\n",
      "        models|conclusion|0.06 % |      1\n",
      "introduction|background: rewrite rules and two-level constraints|0.06 % |      1\n",
      "combinatory category grammar supertagging|introduction|0.06 % |      1\n",
      "experimental setting|introduction|0.06 % |      1\n",
      "      classifier|features|0.06 % |      1\n",
      "recognizing textual entailment|introduction|0.06 % |      1\n",
      "materials|morphological patterns discovered by the algorithm|0.06 % |      1\n",
      "graph representation of vector embeddings|introduction|0.06 % |      1\n",
      "nmt configuration|introduction|0.06 % |      1\n",
      "existing corpora annotated with argumentation structures|discourse analysis|0.06 % |      1\n",
      "introduction|entity linking|0.06 % |      1\n",
      "introduction|surface analysis of the story cloze task|0.06 % |      1\n",
      "part-of-speech tagger|introduction|0.06 % |      1\n",
      "the tagger and corpora|the effect of the initial conditions|0.06 % |      1\n",
      "stochastic top-k listnet|introduction|0.06 % |      1\n",
      "introduction|np-annotated sms corpus|0.06 % |      1\n",
      "online dialogue features|neural network models|0.06 % |      1\n",
      "data representation|introduction|0.06 % |      1\n",
      "             tag set|note|0.06 % |      1\n",
      "comparison with related work|introduction|0.06 % |      1\n",
      "introduction|unsupervised dialogue embeddings|0.06 % |      1\n",
      "introduction|models and hyperparameters|0.06 % |      1\n",
      "      set-up|introduction|0.06 % |      1\n",
      "introduction|analysis on inter-sentence attention|0.06 % |      1\n",
      "datasets|baseline systems|0.06 % |      1\n",
      "system architecture|evaluation|0.06 % |      1\n",
      "introduction|system details|0.06 % |      1\n",
      "introduction|word sense disambiguation|0.06 % |      1\n",
      "stack long short-term memory (lstm)|introduction|0.06 % |      1\n",
      "    introduction|modeling|0.06 % |      1\n",
      "implementation details for lace|introduction|0.06 % |      1\n",
      "introduction|leveraging monolingual data for nmt|0.06 % |      1\n",
      "introduction|dynamic syntax and type theory with records|0.06 % |      1\n",
      "introduction|backward rnn reranking|0.06 % |      1\n",
      "     results|introduction|0.06 % |      1\n",
      "introduction|training setups|0.06 % |      1\n",
      "temporal search|word dynamics|0.06 % |      1\n",
      "baseline model|introduction|0.06 % |      1\n",
      "  the parser|introduction|0.06 % |      1\n",
      "hashing under the nvi framework|introduction|0.06 % |      1\n",
      "coding features|the corpus|0.06 % |      1\n",
      "  derinet|word embeddings|0.06 % |      1\n",
      "pre-training the projection layer|pre-training the recurrent layer|0.06 % |      1\n",
      "introduction|experiments and evaluation|0.06 % |      1\n",
      "introduction|discontinuity in scheduled sampling|0.06 % |      1\n",
      "introduction|compute event representations.|0.06 % |      1\n",
      "comparison with other work|training and test data|0.06 % |      1\n",
      "introduction|training setup|0.06 % |      1\n",
      "importance score|cnn model|0.06 % |      1\n",
      "decoding|training procedure|0.06 % |      1\n",
      "abstractive summarization|introduction|0.06 % |      1\n",
      "syntax marginal inference for dependency paths|semantic role assignment|0.06 % |      1\n",
      "introduction|text classification|0.06 % |      1\n",
      "introduction|human-human data collection|0.06 % |      1\n",
      "introduction|dataset and task|0.06 % |      1\n",
      "evaluation metrics|data preparation|0.06 % |      1\n",
      "introduction|results from indicator cues|0.06 % |      1\n",
      "introduction|performance comparison with baseline models|0.06 % |      1\n",
      "the pointer softmax|summarization|0.06 % |      1\n",
      "introduction|improving question generation|0.06 % |      1\n",
      "introduction|experiment setting for direct unsupervised nmt|0.06 % |      1\n",
      "introduction|the neural language generator|0.06 % |      1\n",
      "introduction|mapping of contextualized embeddings|0.06 % |      1\n",
      "measuring attributional similarity|measuring attributional similarity with the vector space model|0.06 % |      1\n",
      "introduction|character-based models of words|0.06 % |      1\n",
      "introduction|experimental settings|0.06 % |      1\n",
      "baseline models|model correlation|0.06 % |      1\n",
      "related works |experiments |0.06 % |      1\n",
      "span-attribute tagging (sa-t) model|introduction|0.06 % |      1\n",
      "introduction|evaluation and experiments|0.06 % |      1\n",
      "motivation and related work|architecture comparison|0.06 % |      1\n"
     ]
    }
   ],
   "source": [
    "print(\"%25s|%10s |%7s\"%('название секции','% от всех','кол-во '))\n",
    "print(50*'-')\n",
    "for ind,x,y in zip(df[df.rw_exact == 1].top_2.value_counts().index,df[df.rw_exact == 1].top_2.value_counts()/len(df[df.rw_exact == 1])*100,df[df.rw_exact == 1].top_2.value_counts()):\n",
    "    print(\"%25s|%2.2f %% |%7d\" % (ind,x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_with_sect_latex = dict()\n",
    "\n",
    "for article in all_articles:\n",
    "    \n",
    "    if article['latex_parse'] and article['latex_parse']['body_text']: \n",
    "        article_dict= dict()\n",
    "        for sections in article['latex_parse']['body_text']:\n",
    "            if sections['section'] or sections['section']!=None:\n",
    "                if sections['section'] in article_dict:\n",
    "                    article_dict[sections['section']] += len(sections['cite_spans'])\n",
    "                else:\n",
    "                    article_dict[sections['section']] = len(sections['cite_spans'])\n",
    "                        \n",
    "        article_with_sect_latex[article['paper_id']] = article_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10164018': {'Introduction': 13,\n",
       "  'Overview': 1,\n",
       "  'Reader-Aware Salience Estimation': 6,\n",
       "  'Summary Construction': 6,\n",
       "  'Data Description': 0,\n",
       "  'Background': 0,\n",
       "  'Data Collection': 0,\n",
       "  'Data Properties': 0,\n",
       "  'Dataset and Metrics': 1,\n",
       "  'Comparative Methods': 6,\n",
       "  'Experimental Settings': 2,\n",
       "  'Results on Our Dataset': 0,\n",
       "  'Further Investigation of Our Framework ': 2,\n",
       "  'Case Study': 0,\n",
       "  'Conclusions': 0},\n",
       " '488': {'Introduction': 6,\n",
       "  'Random Field Models': 1,\n",
       "  'RFM Estimation and Selection of the Informative Sample ': 2,\n",
       "  'The Grammar': 1,\n",
       "  'Modelling the Grammar ': 0,\n",
       "  'Experiments': 0,\n",
       "  'Testing the Various Sampling Strategies ': 0,\n",
       "  'Larger Scale Evaluation': 1,\n",
       "  'Comments': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '189927790': {'Introduction': 9,\n",
       "  'Balanced Translation Approach': 1,\n",
       "  'NMT and Transformer': 2,\n",
       "  'Relevance-based Auxiliary Task (RAT)': 6,\n",
       "  'Multi-task NMT Architecture': 3,\n",
       "  'Results and Analysis': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Loss Function and Validation Performance Analysis': 0},\n",
       " '5084110': {'Acknowledgments': 0},\n",
       " '198922003': {},\n",
       " '126168169': {'Introduction': 8,\n",
       "  'Linear sentence encoders': 6,\n",
       "  'Continual learning for linear sentence encoders': 1,\n",
       "  'Matrix conceptors': 3,\n",
       "  'Using conceptors to continually learn sentence representations': 0,\n",
       "  'Experiment': 11,\n",
       "  'Conclusions and future work': 3,\n",
       "  'Acknowledgement': 0,\n",
       "  'The split STS datasets': 0,\n",
       "  'CA compared with incremental-deletion SIF': 0,\n",
       "  'CA without stop word initialization': 0,\n",
       "  'CA with the reverse-ordered sequence of training corpora': 0,\n",
       "  'Experiment using other word embedding brands': 2},\n",
       " '1238927': {},\n",
       " '184488238': {'Introduction': 19,\n",
       "  'Related Work': 48,\n",
       "  'Data Collection and Annotation': 1,\n",
       "  'Data Gathering': 1,\n",
       "  'Visual Action Annotation': 1,\n",
       "  'Discussion': 0,\n",
       "  'Identifying Visible Actions in Videos': 0,\n",
       "  'Data Processing and Representations': 14,\n",
       "  'Baselines': 4,\n",
       "  'Multimodal Model': 1,\n",
       "  'Evaluation and Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '85517799': {'Introduction': 12,\n",
       "  'Related Works': 23,\n",
       "  'Model': 11,\n",
       "  'Reinforcement Learning': 6,\n",
       "  'Adversarial Training': 0,\n",
       "  'Experiments': 7,\n",
       "  'Evaluation': 9,\n",
       "  'Results': 2,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Sequence Sampling in Reinforcement Learning': 0},\n",
       " '16050464': {'Introduction': 12,\n",
       "  'Related Work': 15,\n",
       "  'Multimodal document retrieval': 1,\n",
       "  'Data, Knowledge and Features': 0,\n",
       "  'Language Models for Ranking': 1,\n",
       "  'Feature Weights': 1,\n",
       "  'Example': 0,\n",
       "  'Datasets': 8,\n",
       "  'Experiments': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Appendix': 0},\n",
       " '173990592': {},\n",
       " '52155342': {'Introduction': 25,\n",
       "  'Exposure Bias and Error Propagation': 13,\n",
       "  'Tackling Accuracy Drop': 9,\n",
       "  'Error Propagation is Not the Only Cause': 5,\n",
       "  'The Influence of Error Propagation': 1,\n",
       "  'Language Branching Matters': 7,\n",
       "  'Correlation between Language Branching and Accuracy Drop': 2,\n",
       "  'N-gram Statistics': 0,\n",
       "  'Dependency Statistics': 0,\n",
       "  'Extended Analyses and Discussions': 0,\n",
       "  'More Languages on Left-Branching': 0,\n",
       "  'Other Model Structures': 0,\n",
       "  'Other Sequence Generation Tasks': 0,\n",
       "  'Conclusion': 0,\n",
       "  'NMT Datasets': 4,\n",
       "  'NMT Models': 3,\n",
       "  'Dependency Parsing Results': 1},\n",
       " '52247458': {'Introduction': 13,\n",
       "  'Related Work': 29,\n",
       "  'Zero-Shot Open Entity Typing': 0,\n",
       "  'Initial Concept Candidate Generation': 2,\n",
       "  'Context-Consistent Re-Ranking': 1,\n",
       "  'Surface-Based Concept Generation': 0,\n",
       "  'Type Inference': 0,\n",
       "  'Experiments': 0,\n",
       "  'Fine-Grained Entity Typing ': 0,\n",
       "  'Coarse Entity Typing': 0,\n",
       "  'Typing of Unseen Types within Domain': 1,\n",
       "  'Biology Entity Typing': 1,\n",
       "  'Ablation Study': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '5267356': {'Introduction': 14,\n",
       "  'Joint Embedding of Words and Entities': 0,\n",
       "  'Skip-gram Model for Word Similarity': 0,\n",
       "  'Extending the Skip-gram Model': 2,\n",
       "  'Training': 2,\n",
       "  'Named Entity Disambiguation Using Embedding': 0,\n",
       "  'Mention Disambiguation': 4,\n",
       "  'Experiments': 0,\n",
       "  'Training for the Proposed Embedding': 1,\n",
       "  'Entity Relatedness': 5,\n",
       "  'Named Entity Disambiguation': 14,\n",
       "  'Related Work': 18,\n",
       "  'Conclusions': 0},\n",
       " '52097879': {'Introduction': 3},\n",
       " '51889492': {'Introduction': 1,\n",
       "  'Related Work': 3,\n",
       "  'Data': 0,\n",
       "  'Data Pre-processing': 1,\n",
       "  'Features': 3,\n",
       "  'Experimental Settings': 0,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '195791871': {'Introduction': 6,\n",
       "  'Proposed Architectures': 2,\n",
       "  'Parallel MTL Architecture': 2,\n",
       "  'Serial MTL Architecture': 0,\n",
       "  'Serial MTL Architecture with Highway Connections': 0,\n",
       "  'Serial MTL Architecture with Highway Connections and Feature Swapping': 0,\n",
       "  'An Example of Encoder-Decoder Architecture for a Single Task': 7,\n",
       "  'Dataset': 2,\n",
       "  'Baselines': 0,\n",
       "  'Training Setup': 6,\n",
       "  'Benchmark data': 2,\n",
       "  'Alexa data': 1,\n",
       "  'Result Analysis': 2,\n",
       "  'Related Work': 10,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '29152549': {'Introduction': 17,\n",
       "  'Model': 0,\n",
       "  'Text Model': 9,\n",
       "  'Sponsor Metadata': 5,\n",
       "  'Dataset': 0,\n",
       "  'Experiments': 0,\n",
       "  'Results': 0,\n",
       "  'In-session Results': 0,\n",
       "  'Out-of-session Results': 0,\n",
       "  'Overall Analysis': 0,\n",
       "  'Future Work': 0,\n",
       "  'Conclusion': 0},\n",
       " '195218693': {'Introduction': 22,\n",
       "  'Feature Attribution': 5,\n",
       "  'Integrated Gradients': 3,\n",
       "  'Incorporating Priors': 0,\n",
       "  'Experiments': 0,\n",
       "  'Discussion and Related Work': 0,\n",
       "  'Conclusion and Future Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '3698344': {'Acknowledgments': 0},\n",
       " '3204901': {'Introduction': 0,\n",
       "  'Aggregate Markov models': 0,\n",
       "  'Mixed-order Markov models': 0,\n",
       "  'Smoothing': 0,\n",
       "  'Discussion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'References': 0},\n",
       " '53096901': {'Introduction': 7,\n",
       "  'Related Work': 8,\n",
       "  'Variational Natural Language Generator': 5,\n",
       "  'Variational CNN-DCNN Model': 0,\n",
       "  'Training VNLG Model': 1,\n",
       "  'Training Variational CNN-DCNN Model': 0,\n",
       "  'Joint Training Dual VAE Model': 0,\n",
       "  'Joint Cross Training Dual VAE Model': 0,\n",
       "  'Experiments': 2,\n",
       "  'Evaluation Metrics and Baselines': 4,\n",
       "  'Experimental Setups': 0,\n",
       "  'Results and Analysis': 0,\n",
       "  'Integrating Variational Inference': 0,\n",
       "  'Ablation Studies': 0,\n",
       "  'Model comparison on unseen domain': 0,\n",
       "  'Domain Adaptation': 0,\n",
       "  'Comparison on Generated Outputs': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '6385589': {'Acknowledgments': 0},\n",
       " '52118895': {},\n",
       " '28556787': {'Introduction': 4,\n",
       "  'Model': 5,\n",
       "  'Parameter Learning': 3,\n",
       "  'Setup': 0,\n",
       "  'Results on English': 0,\n",
       "  'Multilingual Results': 1,\n",
       "  'Conclusion': 0},\n",
       " '53981714': {'Acknowledgment': 0},\n",
       " '51862727': {'Acknowledgments': 0},\n",
       " '868799': {},\n",
       " '102353905': {'Acknowledgements': 0},\n",
       " '8174613': {'Introduction': 8,\n",
       "  'Background: Neural Models for Sequence-to-sequence Learning': 1,\n",
       "  'RNN Encoder-Decoder': 4,\n",
       "  'The Attention Mechanism': 2,\n",
       "  'CopyNet': 0,\n",
       "  'Model Overview': 2,\n",
       "  'Prediction with Copying and Generation': 2,\n",
       "  'State Update': 0,\n",
       "  'Hybrid Addressing of ': 2,\n",
       "  'Learning': 0,\n",
       "  'Experiments': 0,\n",
       "  'Synthetic Dataset': 0,\n",
       "  'Text Summarization': 8,\n",
       "  'Single-turn Dialogue': 3,\n",
       "  'Related Work': 4,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '21840890': {'Introduction': 4,\n",
       "  'Low Resource g2p': 4,\n",
       "  'Multilingual Neural NLP': 2,\n",
       "  'Grapheme-to-Phoneme': 0,\n",
       "  'Encoder–Decoder Models': 3,\n",
       "  'Training Multilingual Models': 0,\n",
       "  'Data': 2,\n",
       "  'Experiments': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Baseline': 0,\n",
       "  'Training': 1,\n",
       "  'Adapted Results': 0,\n",
       "  'High Resource Results': 0,\n",
       "  'Results on Unseen Languages': 0,\n",
       "  'Language ID Tokens': 0,\n",
       "  'Language Embeddings': 0,\n",
       "  'Phoneme Embeddings': 0,\n",
       "  'Future Work': 0},\n",
       " '53224724': {'Introduction': 6,\n",
       "  'Transformer Decoder': 1,\n",
       "  'Proposed Strategies': 1,\n",
       "  'Experiments': 5,\n",
       "  'Multimodal Translation': 7,\n",
       "  'Multi-Source MT': 2,\n",
       "  'Results': 2,\n",
       "  'Related Work': 8,\n",
       "  'Conclusions': 4,\n",
       "  'Acknowledgments': 0},\n",
       " '29245285': {'Introduction': 4,\n",
       "  'Data Set': 2,\n",
       "  'Baseline Decoder': 1,\n",
       "  'Decoder Speed Improvements': 1,\n",
       "  '16-Bit Matrix Multiplication': 1,\n",
       "  'Pre-Compute Embeddings': 1,\n",
       "  'Pre-Compute Attention': 0,\n",
       "  'SSE & Lookup Tables': 0,\n",
       "  'Merge Recurrent States': 0,\n",
       "  'Speedup Results': 0,\n",
       "  'Model Improvements': 8,\n",
       "  'Model Results': 3},\n",
       " '2768698': {'Introduction': 2,\n",
       "  'SuperPivot: Description of method': 4,\n",
       "  'Data': 1,\n",
       "  'Experiments': 4,\n",
       "  'Evaluation': 1,\n",
       "  'A map of past tense': 1,\n",
       "  'Related work': 45,\n",
       "  'Parallel corpora and annotation projection': 3,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Future directions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2753602': {},\n",
       " '100300': {'Finite-state approximations': 1,\n",
       "  'Finite-state calculus': 5,\n",
       "  'An approximation algorithm': 1,\n",
       "  'A small example': 0,\n",
       "  'Computational complexity': 0,\n",
       "  'Results with a larger grammar': 0,\n",
       "  'Comparison with previous work': 0,\n",
       "  'Discussion and conclusions': 0},\n",
       " '24216565': {'Introduction': 7,\n",
       "  'Related Work': 2,\n",
       "  'Atomic Concept Trees': 0,\n",
       "  'Atomic-Concepts Based Slot Filling': 1,\n",
       "  'Combinatory Concepts Extending': 0,\n",
       "  'Experiments': 0,\n",
       "  'Semantic Slot Refinement': 2,\n",
       "  'Domain Adaptation': 1,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '3047902': {'Introduction': 4,\n",
       "  'Previous work': 2,\n",
       "  'Challenges and Approach': 0,\n",
       "  'Our approach': 10,\n",
       "  'STIR: Strongly Incremental Repair detection': 4,\n",
       "  'Enriched incremental language models': 5,\n",
       "  'Individual classifiers': 1,\n",
       "  'Classifier pipeline': 2,\n",
       "  'Experimental set-up': 0,\n",
       "  'Incremental evaluation metrics': 3,\n",
       "  'Results and Discussion': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '28841086': {'Introduction': 9,\n",
       "  'Contributions': 1,\n",
       "  'Approach': 2,\n",
       "  'SIFT and Color Features': 2,\n",
       "  'Convolutional Neural Network Feature Representations': 2,\n",
       "  'Representations from Cross-Lingual Word-Embeddings': 1,\n",
       "  'Combined Representations': 0,\n",
       "  'Similarity Computation': 3,\n",
       "  'Evaluation Metrics': 0,\n",
       "  'Experiments and Results': 0,\n",
       "  'Results': 1,\n",
       "  'Analysis': 2,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0},\n",
       " '131774178': {},\n",
       " '11003493': {'Introduction': 1,\n",
       "  'Background': 5,\n",
       "  'Method': 4,\n",
       "  'Model configuration': 1,\n",
       "  'Results and Analysis': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '118673171': {'Introduction': 6,\n",
       "  'Definitions': 0,\n",
       "  'LDA regression model': 2,\n",
       "  'Regression model and number of topics': 3,\n",
       "  'Introducing new documents': 2,\n",
       "  'sLDA regression model': 3,\n",
       "  'HMTM regression model': 5,\n",
       "  'Testing the topic regression models': 0,\n",
       "  'Word count model': 3,\n",
       "  'Topic regression models': 0,\n",
       "  'Incorporating language structure': 0,\n",
       "  'Discussion and further research': 0,\n",
       "  'Text cleaning': 1,\n",
       "  'Topic model inference': 1},\n",
       " '2311790': {'Introduction': 3,\n",
       "  'Finite State Calculus': 0,\n",
       "  'A finite state method for grapheme to phoneme conversion': 0,\n",
       "  'The Conversion Rules': 0,\n",
       "  'Test results and discussion': 1,\n",
       "  'Transformation-based grapheme to phoneme conversion': 0,\n",
       "  'Alignment': 2,\n",
       "  'The experiments': 1,\n",
       "  'Concluding remarks': 0},\n",
       " '52097259': {'Introduction': 5,\n",
       "  'Preprocessing': 1,\n",
       "  'Architecture': 4,\n",
       "  'Implementation Details and Hyperparameters': 5,\n",
       "  'Ensembles': 1,\n",
       "  'Experiments and Analyses': 0,\n",
       "  'Ablation Study': 2,\n",
       "  'Error Analysis': 0,\n",
       "  'Effect of the Amount of Training Data': 0,\n",
       "  'Effect of Emoji and Hashtags': 0,\n",
       "  'Conclusions and Future Work': 1,\n",
       "  'Acknowledgements': 0},\n",
       " '7193975': {},\n",
       " '2558': {'Introduction': 5,\n",
       "  'Alternative Avenues Towards Robustness': 11,\n",
       "  'MDP versus Two Stage Interpretation': 11,\n",
       "  'The Two Stage Interpretation Process': 0,\n",
       "  'The Partial Parsing Stage': 2,\n",
       "  'The Combination Stage': 2,\n",
       "  'The Genetic Programming Combination Process In-Depth': 2,\n",
       "  'Constructing Alternative Hypotheses': 0,\n",
       "  'Applying the Genetic Programming Paradigm to Repair': 0,\n",
       "  'Training a Fitness Function': 0,\n",
       "  'A Comparative Analysis in a Large Scale Practical Setting': 2,\n",
       "  'Evaluation': 0,\n",
       "  'Conclusions': 0},\n",
       " '165163629': {'Acknowledgements': 0},\n",
       " '174798321': {'Appendix A': 0},\n",
       " '53168791': {'Introduction': 13,\n",
       "  'Vector Similarity': 14,\n",
       "  'Predictive Similarity': 1,\n",
       "  'Experiments': 2,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Left wing news sources': 0},\n",
       " '5294994': {'Acknowledgements': 0},\n",
       " '21387831': {'Introduction': 7,\n",
       "  'Related work': 18,\n",
       "  'Bag of objects': 1,\n",
       "  'Image captioning model': 2,\n",
       "  'Visual representations': 1,\n",
       "  'Experiments': 4,\n",
       "  'Analysis': 0,\n",
       "  'Spatial information on instances': 0,\n",
       "  'Spatial representation': 0,\n",
       "  'Importance of different categories': 1,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Hyperparmeter Settings': 0,\n",
       "  'Full Experimental Results': 0,\n",
       "  'Example captions for different models': 0,\n",
       "  'Example nearest neighbours for test images': 0},\n",
       " '44140055': {'Conclusion and Future Work': 0},\n",
       " '58094': {'Annotation Graphs': 4,\n",
       "  'The Formalism': 1,\n",
       "  'LDC Telephone Speech Transcripts': 1,\n",
       "  'Dialogue Annotation in COCONUT': 2,\n",
       "  'Coreference Annotation in MUC-7': 2,\n",
       "  'Hybrid Annotations': 1,\n",
       "  'DAMSL annotation of TRAINS': 2,\n",
       "  'Multiple annotations of the BU corpus': 3,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '28479851': {'Introduction': 9,\n",
       "  'Related Work': 2,\n",
       "  'The Proposed Approach': 0,\n",
       "  ' Model': 0,\n",
       "  'SimCluster Algorithm': 1,\n",
       "  'Alignment': 0,\n",
       "  'Experiments on Synthetic Dataset': 1,\n",
       "  'Evaluation and Results': 1,\n",
       "  'Description and preprocessing of dataset': 2,\n",
       "  'Results': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52112656': {'Introduction': 12,\n",
       "  'Task': 0,\n",
       "  'Dataset': 0,\n",
       "  'Phenomena present in the commands': 0,\n",
       "  'Retrieval-based model': 0,\n",
       "  'Embedding-based model': 2,\n",
       "  'Alignment-based model': 5,\n",
       "  'Experiments': 1,\n",
       "  'Ablation analysis': 0,\n",
       "  'Error analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52896891': {'Acknowledgements': 0},\n",
       " '53108063': {'Introduction': 5,\n",
       "  'Information Theory': 1,\n",
       "  'Brown Clustering': 5,\n",
       "  'Objectives': 0,\n",
       "  'Generalized Brown Objective': 1,\n",
       "  'Variational Lower Bound': 2,\n",
       "  'Analysis': 0,\n",
       "  'Setting': 0,\n",
       "  'Result': 0,\n",
       "  'Experiments': 6,\n",
       "  'Definition of (X,Y)(X,Y)': 0,\n",
       "  'Architecture': 0,\n",
       "  'Baselines': 8,\n",
       "  'Results': 6,\n",
       "  'Related Work': 17,\n",
       "  'Acknowledgments': 0,\n",
       "  'Proof of Theorem 4.1': 0},\n",
       " '196198090': {'Introduction': 11,\n",
       "  'Problem Formulation': 1,\n",
       "  'Method': 0,\n",
       "  'Candidate Generation': 5,\n",
       "  'Concept Classification': 9,\n",
       "  'Game-based Optimization': 1,\n",
       "  'Datasets': 1,\n",
       "  'Experiment Settings': 8,\n",
       "  'Overall Evaluation': 0,\n",
       "  'Result Analysis': 0,\n",
       "  'Online Evaluation': 0,\n",
       "  'Related Works': 15,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Probe into the different performance across datasets': 0},\n",
       " '6922975': {'Introduction': 4,\n",
       "  'Discounting and Redistribution': 5,\n",
       "  'The Similarity Model': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Related Work': 1,\n",
       "  'Further Research': 2,\n",
       "  'Conclusions': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '102351836': {'Introduction': 6,\n",
       "  'Model': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Dataset': 1,\n",
       "  'Results': 0,\n",
       "  'Model ensemble results': 0,\n",
       "  'Error analysis': 0,\n",
       "  'Unsubmitted models': 2,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '29151018': {'Conclusion': 0},\n",
       " '30344': {'Introduction': 9,\n",
       "  'Distributional models of meaning': 7,\n",
       "  'Syntactic awareness': 2,\n",
       "  'Compositionality in distributional models': 8,\n",
       "  'Disambiguation in composition': 5,\n",
       "  'Syntax-based generic training': 0,\n",
       "  'From words to senses': 0,\n",
       "  'Task-specific dynamic disambiguation': 0,\n",
       "  'A siamese network for paraphrase detection': 7,\n",
       "  'Experiments': 0,\n",
       "  'Model pre-training': 3,\n",
       "  'Qualitative evaluation of the word vectors': 0,\n",
       "  'Word similarity': 1,\n",
       "  'Paraphrase detection': 4,\n",
       "  'Conclusion and future work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '196171551': {'Synthetic topic recovery experiment details': 0,\n",
       "  'Additional TU and NPMI Plots for W-LDA': 0,\n",
       "  'Document classification': 0,\n",
       "  'MMD vs GAN in distribution matching': 1,\n",
       "  'Topic Words': 0,\n",
       "  'Topic words on 20NG': 0,\n",
       "  'Topic words on NYTimes:': 0,\n",
       "  'Topic words on Wikitext-103:': 0,\n",
       "  'AGnews': 0,\n",
       "  'DBPedia': 0,\n",
       "  'Yelp Review Polarity': 0},\n",
       " '51940055': {'Introduction': 21,\n",
       "  'Related Work': 6,\n",
       "  'Variational Domain-Adaptation Neural Language Generator': 1,\n",
       "  'Variational Neural Encoder': 0,\n",
       "  'Variational Neural Inferer': 0,\n",
       "  'Neural Posterior Approximator': 1,\n",
       "  'Neural Prior Model': 1,\n",
       "  'Variational Neural Decoder': 1,\n",
       "  'Critics': 0,\n",
       "  'Text similarity critic': 1,\n",
       "  'Domain critic': 1,\n",
       "  'Training Domain Adaptation Model': 0,\n",
       "  'Training Critics': 0,\n",
       "  'Training Variational Generator': 0,\n",
       "  'Adversarial Training': 0,\n",
       "  'Experiments': 3,\n",
       "  'Integrating Variational Inference': 0,\n",
       "  'Ablation Studies': 0,\n",
       "  'Distance of Dataset Pairs': 0,\n",
       "  'Adaptation vs. All Training Scenario': 0,\n",
       "  'Unsupervised Domain Adaptation': 1,\n",
       "  'Comparison on Generated Outputs': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Appendix A. Experimental Setups': 3,\n",
       "  'Appendix B. Generated Utterances': 0},\n",
       " '86813509': {'Introduction': 20,\n",
       "  'Background and Notation': 3,\n",
       "  'Completion Robustness and Interpretability via Adversarial Graph Edits ()': 0,\n",
       "  'Removing a fact ()': 2,\n",
       "  'Adding a new fact ()': 0,\n",
       "  'Challenges': 0,\n",
       "  'Efficiently Identifying the Modification': 0,\n",
       "  'First-order Approximation of Influence': 0,\n",
       "  'Continuous Optimization for Search': 0,\n",
       "  'Experiments': 0,\n",
       "  'Influence Function vs ': 0,\n",
       "  'Robustness of Link Prediction Models': 0,\n",
       "  'Interpretability of Models': 1,\n",
       "  'Finding Errors in Knowledge Graphs': 0,\n",
       "  'Related Work': 28,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Appendix': 0,\n",
       "  \"Modifications of the Form 〈s,r ' ,o ' 〉\\\\langle s, r^{\\\\prime }, o^{\\\\prime } \\\\rangle \": 0,\n",
       "  \"Modifications of the Form 〈s,r ' ,o〉\\\\langle s, r^{\\\\prime }, o \\\\rangle \": 0,\n",
       "  'First-order Approximation of the Change For TransE': 1,\n",
       "  'Sample Adversarial Attacks': 0},\n",
       " '21677829': {'Introduction': 13,\n",
       "  'Analysis of Complex Models': 9,\n",
       "  'Behavior Analysis': 4,\n",
       "  'Natural Language Inference': 5,\n",
       "  'SNLI Dataset': 1,\n",
       "  'Models': 2,\n",
       "  'Methods': 0,\n",
       "  'Basic Procedure and Statistical Analyses': 4,\n",
       "  'Transformation and Word Pairs': 0,\n",
       "  'Experimental Conditions': 0,\n",
       "  'Test Sets': 2,\n",
       "  'Factors Under Study': 0,\n",
       "  'Experiments and Results': 0,\n",
       "  'Experiment 1: Swapping Antonyms in In Situ Instances': 0,\n",
       "  'Experiment 2: Swapping Antonyms in Ex Situ Instances': 0,\n",
       "  'Experiment 3: Swapping Hypernyms and Hyponyms in In Situ Instances': 1,\n",
       "  'Experiment 4: Swapping Hypernyms and Hyponyms in Ex Situ Instances': 0,\n",
       "  'Discussion and Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '29160079': {'Introduction': 13,\n",
       "  'Related Work': 20,\n",
       "  'Incorporating Glosses into Neural Word Sense Disambiguation': 0,\n",
       "  'Architecture of GAS': 0,\n",
       "  'Context Module': 0,\n",
       "  'Gloss Module': 0,\n",
       "  'Memory Module': 0,\n",
       "  'Scoring Module': 0,\n",
       "  'Dataset': 7,\n",
       "  'Implementation Details': 1,\n",
       "  'Systems to be Compared': 7,\n",
       "  'Results and Discussion': 3,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5448554': {'Introduction': 9,\n",
       "  'Derivation Grammar and CF Parse Forest': 1,\n",
       "  'Linear Indexed Grammars': 2,\n",
       "  'Linear Derivation Grammar': 4,\n",
       "  'LIG parsing': 5,\n",
       "  'First Example': 0,\n",
       "  'Second Example': 0,\n",
       "  'Conclusion': 0},\n",
       " '174797858': {},\n",
       " '195699929': {'Introduction': 9,\n",
       "  'Formalizing Inflectional Morphology': 1,\n",
       "  'Operationalizing Irregularity': 14,\n",
       "  'Modeling Morphological Inflection': 0,\n",
       "  'A Lemma-Based Model': 6,\n",
       "  'Handling Syncretism': 1,\n",
       "  'Handling Derived Forms': 0,\n",
       "  'Measuring Irregularity': 0,\n",
       "  'Studies of Irregularity': 0,\n",
       "  'Simulations': 2,\n",
       "  'Validation and Accuracy': 4,\n",
       "  'Irregularity across Languages': 1,\n",
       "  'Irregularity and Frequency': 16,\n",
       "  'Conclusion': 5,\n",
       "  'Acknowledgments': 0},\n",
       " '174797750': {'Introduction': 14,\n",
       "  'RCN: The Proposed Model': 0,\n",
       "  'Related Work': 18,\n",
       "  'Setup': 2,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '44128838': {'Introduction': 21,\n",
       "  'Method': 1,\n",
       "  'EM Training': 1,\n",
       "  'Unified Bidirectional Training': 0,\n",
       "  'Training Details': 3,\n",
       "  'Datasets': 3,\n",
       "  'Baselines': 4,\n",
       "  'Overall Results': 0,\n",
       "  'The Effect of Extra Monolingual Data': 0,\n",
       "  'EM Training Curves': 0,\n",
       "  'Reinforcement Learning Mechanism in Our Method': 0,\n",
       "  'Related Work': 7,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0,\n",
       "  'Implementation details': 2},\n",
       " '6182054': {'Introduction': 5,\n",
       "  'Related Work': 10,\n",
       "  'Methodology': 1,\n",
       "  'Training details and submitted runs': 5,\n",
       "  'Results': 0,\n",
       "  'Discussion': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1703535': {'Introduction': 13,\n",
       "  'Related Work': 19,\n",
       "  'Our Previous Results': 2,\n",
       "  'Boundary Classification': 0,\n",
       "  'Coding of Linguistic Features': 12,\n",
       "  'Evaluation': 4,\n",
       "  'Hand Tuning': 2,\n",
       "  'Machine Learning': 3,\n",
       "  'Conclusion': 0},\n",
       " '549981': {'Introduction': 9,\n",
       "  'Related Work': 12,\n",
       "  'System Overview': 11,\n",
       "  'Overview of the proof strategy': 3,\n",
       "  'Proving entailment relations': 0,\n",
       "  'Proving the contradiction': 0,\n",
       "  'Description of the Features': 0,\n",
       "  'Logic-based Features': 0,\n",
       "  'Non-logic-based Features': 3,\n",
       "  'Experimental Conditions': 8,\n",
       "  'Results': 0,\n",
       "  'Positive examples and error analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '195069071': {'Deep ER Hyperparameters': 0, 'Non-DL Learning Algorithms': 1},\n",
       " '46938018': {},\n",
       " '67855637': {},\n",
       " '43977290': {'Introduction': 16,\n",
       "  'Method': 7,\n",
       "  'Geometry of Phrasal Verbs': 1,\n",
       "  'Downstream Application: Preposition Selection': 4,\n",
       "  'Downstream Application: Prepositional Attachment': 5,\n",
       "  'Related Work': 18,\n",
       "  'Conclusion': 1,\n",
       "  'Roster of Prepositions': 0},\n",
       " '14170854': {'Introduction': 10,\n",
       "  'Task': 0,\n",
       "  'Compositionalization': 0,\n",
       "  'Example': 1,\n",
       "  'General technique': 0,\n",
       "  'Compositional training': 0,\n",
       "  'Other composable models': 4,\n",
       "  'Implementation': 3,\n",
       "  'Datasets': 0,\n",
       "  'Base datasets': 2,\n",
       "  'Path query datasets': 0,\n",
       "  'Main results': 1,\n",
       "  'Analysis': 0,\n",
       "  'Why does compositional training improve path query answering?': 0,\n",
       "  'Why does compositional training improve knowledge base completion?': 2,\n",
       "  'Discussion': 0},\n",
       " '2478928': {},\n",
       " '29165442': {},\n",
       " '52193282': {'Introduction': 14,\n",
       "  'Related Work': 16,\n",
       "  'Model': 0,\n",
       "  'Representation of Han logographs': 1,\n",
       "  'Neural pronunciation prediction models': 2,\n",
       "  'Multimodal neural pronunciation model of logographs': 1,\n",
       "  'Experiments': 1,\n",
       "  'Data': 0,\n",
       "  'Predicting pronunciation using logograph input': 1,\n",
       "  'Predicting pronunciation using multimodal input': 0,\n",
       "  'Discussion': 2,\n",
       "  'The Ideographic Description algorithm': 0},\n",
       " '49358911': {},\n",
       " '53114840': {'Introduction': 4,\n",
       "  'Political Ideology Detection': 2,\n",
       "  'Sentiment Analysis for Controversy Detection': 0,\n",
       "  'Using Topic Sentiments for Ideology Detection': 2,\n",
       "  'Determining Topic-specific Sentiments': 1,\n",
       "  'Nearest TSM Classification': 0,\n",
       "  'Logistic Regression Classification': 0,\n",
       "  'Dataset': 2,\n",
       "  'Methods': 1,\n",
       "  'Results': 0,\n",
       "  'Discussion': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6716178': {'Introduction': 32,\n",
       "  'Neural Machine Translation': 1,\n",
       "  'Decoding': 4,\n",
       "  'Many Decoding Objectives': 3,\n",
       "  'Trainable Greedy Decoding': 2,\n",
       "  'Learning and Challenges': 2,\n",
       "  'Deterministic Policy Gradient  for Trainable Greedy Decoding': 2,\n",
       "  'Critic-Aware Actor Learning': 2,\n",
       "  'Experimental Settings': 2,\n",
       "  'Model Architectures and Learning': 4,\n",
       "  'Results and Analysis': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '14037609': {'Introduction': 3,\n",
       "  'Related work': 10,\n",
       "  'Task definition': 0,\n",
       "  'Data collection': 1,\n",
       "  'Model': 2,\n",
       "  '360°\\\\! \\\\! Stance Detection Demo': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '197865': {'Conclusions': 0, 'Acknowledgment': 0},\n",
       " '371926': {},\n",
       " '6415280': {'Introduction': 1,\n",
       "  'Related work': 1,\n",
       "  'Splitting compounds for SMT': 0,\n",
       "  'Semantic compositionality': 0,\n",
       "  'Unsupervised morphology induction from word embeddings': 0,\n",
       "  'Compound words and the semantic vector space': 0,\n",
       "  'Compound extraction': 0,\n",
       "  'Implementation considerations': 0,\n",
       "  'Compound splitting': 1,\n",
       "  'Conclusion': 0},\n",
       " '182953200': {'Introduction': 23,\n",
       "  'Related Work': 25,\n",
       "  'Methodology': 0,\n",
       "  'NRE Models': 1,\n",
       "  'STAGE 1: Deep Pattern Extraction': 7,\n",
       "  'STAGE 2: Pattern Refinement': 0,\n",
       "  'STAGE 3: Weak Label Fusion': 4,\n",
       "  'Experiments': 1,\n",
       "  'Experimental Setup': 10,\n",
       "  'Performance Comparisons': 5,\n",
       "  'Diagnosis Results': 0,\n",
       "  'Example Patterns & Instances': 0,\n",
       "  'Conclusion and Future Work': 1},\n",
       " '102352421': {'Introduction': 2,\n",
       "  'Related Work': 16,\n",
       "  'Discerning Relevant Context for DST': 5,\n",
       "  'Experimental Setup': 4,\n",
       "  'Implementation Details': 4,\n",
       "  'Results': 3,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '682772': {'Introduction': 0,\n",
       "  'Regularization Based on a Generative Model': 0,\n",
       "  'The GeneRe Algorithm': 0,\n",
       "  'Illustrative Experiment': 1,\n",
       "  'Aspect-Based Sentiment Analysis': 0,\n",
       "  'Appendix': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2840197': {'Evaluation on Visual Question Answering': 0,\n",
       "  'Datasets': 14,\n",
       "  'Experimental Setup': 2,\n",
       "  'Ablation Results': 0,\n",
       "  'Comparison to State-of-the-Art': 1,\n",
       "  'Evaluation on Visual Grounding': 0,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '102351546': {},\n",
       " '15213991': {'Visualization of perceptron weights.': 0},\n",
       " '2411': {'Introduction': 12,\n",
       "  'Application of Relaxation Labeling to nlp': 6,\n",
       "  'Algorithm Description': 2,\n",
       "  'Application to taxonomy mapping': 0,\n",
       "  'The Constraints': 0,\n",
       "  'Experiments and Results': 0,\n",
       "  'Coincidence of Both Mappings': 0,\n",
       "  'Conclusions & Further Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '63744': {'Introduction': 1,\n",
       "  'Method': 0,\n",
       "  'Task descriptions': 3,\n",
       "  'Machine Learning Techniques': 10,\n",
       "  'Combination techniques': 3,\n",
       "  'Results': 0,\n",
       "  'Chunking': 2,\n",
       "  'NP chunking': 1,\n",
       "  'NP bracketing': 0,\n",
       "  'Prospects': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '1325523': {'Multimodal Machine Translation': 0,\n",
       "  'Multimodal Image Description Generation': 0,\n",
       "  'Multimodal NMT System': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5050182': {'Introduction': 8,\n",
       "  'Emoji sentence representations': 4,\n",
       "  'Methodology & Emoji Dataset': 3,\n",
       "  'Evaluation': 0,\n",
       "  'Emotional word vectors (EVEC)': 2,\n",
       "  'Methodology': 3,\n",
       "  'Hashtag Dataset': 2,\n",
       "  'Features': 2,\n",
       "  'Preprocessing': 0,\n",
       "  'Regression and Ordinal Classification': 0,\n",
       "  'Multi-label Classification': 3,\n",
       "  'Experiments & Results': 2,\n",
       "  'Competition dataset': 1,\n",
       "  'Regression: Subtask 1a & 3a': 1,\n",
       "  'Ordinal Classification: Subtask 2a& 4a': 0,\n",
       "  'Multi-label Classification: Subtask 5a': 0,\n",
       "  \"Analysis of system's gender/racial biases\": 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '16273304': {'Introduction': 18,\n",
       "  'Implicit Discourse Relation Recognition': 17,\n",
       "  'Adversarial Networks': 6,\n",
       "  'Adversarial Method': 0,\n",
       "  'Model Architecture': 0,\n",
       "  'Training Procedure': 0,\n",
       "  'Component Structures': 2,\n",
       "  'Experiments': 0,\n",
       "  'Experiment Setup': 6,\n",
       "  'Implicit Relation Classification': 8,\n",
       "  'Qualitative Analysis': 1,\n",
       "  'Discussions': 2,\n",
       "  'Model architectures and training configurations': 0,\n",
       "  'One-vs-all Classifications': 0},\n",
       " '196180216': {'Introduction': 4,\n",
       "  'Data-to-Text Generation': 19,\n",
       "  'Memory modules': 12,\n",
       "  'Data': 2,\n",
       "  'Saliency-Aware Text Generation': 0,\n",
       "  'Initialization': 0,\n",
       "  'Saliency transition': 0,\n",
       "  'Selection and tracking': 1,\n",
       "  'Summary generation': 0,\n",
       "  'Incorporating writer information': 0,\n",
       "  'Learning objective': 0,\n",
       "  'Experimental settings': 4,\n",
       "  'Models to be compared': 8,\n",
       "  'Evaluation metrics': 2,\n",
       "  'Results and Discussions': 0,\n",
       "  'Saliency tracking-based model': 0,\n",
       "  'Qualitative analysis of entity embedding': 1,\n",
       "  'Duplicate ratios of extracted relations': 3,\n",
       "  'Qualitative analysis of output examples': 3,\n",
       "  'Use of writer information': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Algorithm': 0},\n",
       " '184487659': {'Introduction': 33,\n",
       "  'Basic Model': 2,\n",
       "  'Self-supervised Pre-training Methods': 0,\n",
       "  'Experiment': 4,\n",
       "  'On CNN/DM Dataset': 5,\n",
       "  'Ablation Study': 0,\n",
       "  'Conclusion': 1,\n",
       "  'Rouge-1 and Rouge-L results': 0},\n",
       " '53082498': {'Acknowledgments': 0},\n",
       " '993603': {'Introduction': 25,\n",
       "  'Preliminaries': 0,\n",
       "  'Parsing strategies': 2,\n",
       "  'Correct-prefix property': 2,\n",
       "  'Strong predictiveness': 6,\n",
       "  'Parsing strategies with SPP': 19},\n",
       " '17511008': {'Introduction': 4,\n",
       "  'Continuous domains vs. mixtures of discrete domains': 3,\n",
       "  'Dialogue modeling as a test bed for investigating domains': 4,\n",
       "  'Representing continuous domains': 10,\n",
       "  'Investigating language change': 4,\n",
       "  'Evaluation': 1,\n",
       "  'Conclusion': 0},\n",
       " '195847811': {'Introduction': 8,\n",
       "  'Related Work': 19,\n",
       "  'RINANTE': 0,\n",
       "  'Rule Mining Algorithm': 2,\n",
       "  'Neural Model': 2,\n",
       "  'Experiments': 1,\n",
       "  'Datasets': 3,\n",
       "  'Experimental Setting': 3,\n",
       "  'Performance Comparison': 14,\n",
       "  'Mined Rule Results': 2,\n",
       "  'Case Study': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Experimental Results with BERT': 0},\n",
       " '52114454': {},\n",
       " '10422112': {'Introduction': 1, 'Partial Instantiation': 0},\n",
       " '3076756': {'Introduction': 10,\n",
       "  'Target relation instances': 1,\n",
       "  'Annotation guideline': 0,\n",
       "  'Annotation procedure': 0,\n",
       "  'Encoder for Relational Patterns': 0,\n",
       "  'Baseline methods without supervision': 1,\n",
       "  'Recurrent Neural Network': 3,\n",
       "  'RNN variants': 9,\n",
       "  'Gated Additive Composition (GAC)': 0,\n",
       "  'Parameter estimation: Skip-gram model': 2,\n",
       "  'Experiments': 0,\n",
       "  'Relational pattern similarity': 1,\n",
       "  'Relation classification': 2,\n",
       "  'Related Work': 12,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '3101294': {'Introduction': 13,\n",
       "  'Materials': 0,\n",
       "  'MEG experiment': 1,\n",
       "  'Modeling of MEG data': 1,\n",
       "  'Results': 0,\n",
       "  'Discussion': 1},\n",
       " '3161327': {'Acknowledgements': 0},\n",
       " '5740960': {},\n",
       " '52157228': {'Introduction': 5,\n",
       "  'Related Work': 21,\n",
       "  'Notations and Model Overview': 0,\n",
       "  'Feature Adaptation': 6,\n",
       "  'Domain Adaptive Semi-supervised Learning (DAS)': 5,\n",
       "  'CNN Encoder Implementation': 3,\n",
       "  'Datasets and Experimental Settings': 6,\n",
       "  'Selection of Development Set': 0,\n",
       "  'Training Details and Hyper-parameters': 3,\n",
       "  'Models for Comparison': 5,\n",
       "  'Main Results': 0,\n",
       "  'Further Analysis': 0,\n",
       "  'CNN Filter Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Results on Amazon Benchmark': 6,\n",
       "  'Numerical Results of Figure ': 0,\n",
       "  'CNN Filter Analysis Full Results': 0},\n",
       " '2145766': {'Introduction': 5,\n",
       "  'Related Work': 16,\n",
       "  'Dataset and Preprocessing': 1,\n",
       "  'Topic Extraction': 1,\n",
       "  'Review Segmentation and Sentiment Analysis': 0,\n",
       "  'User Segmentation': 0,\n",
       "  'Rating Prediction': 0,\n",
       "  'Experiments and Analysis': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '190000077': {'Conclusion': 0, 'Acknowledgments': 0},\n",
       " '52073201': {},\n",
       " '5312054': {'Introduction': 5,\n",
       "  'Dependent disjunctions': 5,\n",
       "  'Contexted constraints': 0,\n",
       "  'Dependent disjunctions as contexted constraints': 0,\n",
       "  'Modularization': 0,\n",
       "  'Free combination elimination': 0,\n",
       "  'Conclusion': 0},\n",
       " '410080': {'Introduction': 8,\n",
       "  'Problem statement': 0,\n",
       "  'Computation of Semantic Similarity': 6,\n",
       "  'Disambiguation Algorithm': 2,\n",
       "  'Examples': 0,\n",
       "  'Distributionally derived groupings': 5,\n",
       "  'Thesaurus Classes': 0,\n",
       "  'Formal Evaluation': 0,\n",
       "  'Conclusions and Future Work': 3},\n",
       " '17268102': {'Introduction': 8,\n",
       "  'Related Work': 25,\n",
       "  'Models': 0,\n",
       "  'Neural Scorer': 3,\n",
       "  'Search': 0,\n",
       "  'Training Criteria': 5,\n",
       "  'Datasets and Settings': 3,\n",
       "  'Model Analysis': 1,\n",
       "  'Main Results': 7,\n",
       "  'Conclusion': 0},\n",
       " '44151916': {'Introduction': 21,\n",
       "  'Related Work': 0,\n",
       "  \"UCCA's Semantic Structures\": 7,\n",
       "  'The SAMSA Metric': 0,\n",
       "  'Matching Scenes to Sentences': 1,\n",
       "  'Score Computation': 1,\n",
       "  'Evaluation Protocol': 10,\n",
       "  'Human Score Computation': 0,\n",
       "  'Inter-annotator Agreement': 1,\n",
       "  'Experimental Setup': 4,\n",
       "  'Correlation with Human Evaluation': 3,\n",
       "  'Discussion': 0,\n",
       "  'Evaluation on the QATS Benchmark': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '228172': {'Introduction': 26,\n",
       "  'Model': 0,\n",
       "  'Evaluation Dataset': 6,\n",
       "  'Models': 0,\n",
       "  'Implementation Details': 4,\n",
       "  'Results and Discussions': 1,\n",
       "  'Related Work': 18,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '36737547': {'Introduction': 0,\n",
       "  'Neural MT System': 7,\n",
       "  'Experiments': 0,\n",
       "  'Corpora': 2,\n",
       "  'Training Details': 5,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '1762731': {'Acknowledgments': 0},\n",
       " '6431039': {'Introduction': 19,\n",
       "  'Definition and Construction of the Networks': 0,\n",
       "  'Definition of PlaNet': 0,\n",
       "  'Data Source': 5,\n",
       "  'Construction of the Networks': 0,\n",
       "  'The Growth Model for the Networks': 4,\n",
       "  'Experiments and Results': 2,\n",
       "  'Control Experiment': 0,\n",
       "  'Correlation between Families': 0,\n",
       "  'Conclusion': 4},\n",
       " '162168938': {'Introduction': 5,\n",
       "  'Background': 10,\n",
       "  'Methods': 3,\n",
       "  'Joint Neural Structural Correspondence Learning': 3,\n",
       "  'Pivot Selection for Neural Systems': 1,\n",
       "  'Evaluation': 3,\n",
       "  'Discussion and Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '753195': {'Introduction': 10,\n",
       "  'From Word Forms to Senses': 4,\n",
       "  'Selecting a Sense': 3,\n",
       "  'Experiments': 1,\n",
       "  'Inspection of nearest neighbors': 0,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '174797737': {'Introduction': 21,\n",
       "  'Related Work': 13,\n",
       "  'Our Approach': 1,\n",
       "  'Background': 0,\n",
       "  'Graph Attention Networks (GATs)': 3,\n",
       "  'Relations are Important': 3,\n",
       "  'Training Objective': 1,\n",
       "  'Decoder': 1,\n",
       "  'Datasets': 9,\n",
       "  'Training Protocol': 3,\n",
       "  'Evaluation Protocol': 3,\n",
       "  'Results and Analysis': 0,\n",
       "  'Ablation Study': 0,\n",
       "  'Conclusion and Future Work': 4,\n",
       "  'Acknowledgments': 0},\n",
       " '5079594': {'Introduction': 12,\n",
       "  'Annotation': 3,\n",
       "  'Background: Tweebank': 1,\n",
       "  'Tokenization': 2,\n",
       "  'Part-of-Speech Annotation': 4,\n",
       "  'Universal Dependencies Applied to Tweets': 3,\n",
       "  'Comparison to PoSTWITA-UD': 5,\n",
       "  'Tweebank v2': 1,\n",
       "  'Parsing Pipeline': 0,\n",
       "  'Tokenizer': 7,\n",
       "  'Part-of-Speech Tagger': 17,\n",
       "  'Parser': 18,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52054464': {'Introduction': 4,\n",
       "  'Related Work': 8,\n",
       "  'Noise-Information Trade-Off in Document Retrieval': 1,\n",
       "  'Adaptive Document Retrieval': 0,\n",
       "  'Threshold-Based Retrieval': 0,\n",
       "  'Ordinal Regression': 0,\n",
       "  'Experiments': 1,\n",
       "  'Overall Performance': 5,\n",
       "  'Sensitivity: Adaptive QA to Corpus Size': 0,\n",
       "  'Robustness Check': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Reproducibility': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '173990530': {'Introduction': 4,\n",
       "  'Framework Overview': 0,\n",
       "  'Dialogue Generation': 3,\n",
       "  'Strategy Evaluation': 7,\n",
       "  'Strategy Evolution': 2,\n",
       "  'Settings': 4,\n",
       "  'Experimental Results': 3,\n",
       "  'Discussions': 4,\n",
       "  'Related Work': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2821908': {},\n",
       " '15359942': {},\n",
       " '4547415': {'Introduction': 9,\n",
       "  'Basic usages': 0,\n",
       "  'Components': 1,\n",
       "  'Evaluation': 0,\n",
       "  'Named entity recognition': 5,\n",
       "  'Dependency parsing': 2,\n",
       "  'Conclusion': 0},\n",
       " '21662980': {'Introduction': 20,\n",
       "  'Related Work': 6,\n",
       "  'Partial Orders and Lattices': 0,\n",
       "  'Order Embeddings (OE)': 5,\n",
       "  'Probabilistic Order Embeddings (POE)': 1,\n",
       "  'Probabilistic Asymmetric Transitive Relations': 1,\n",
       "  'Method': 0,\n",
       "  'Correlations from Cone Measures': 0,\n",
       "  'Box Lattices': 0,\n",
       "  'Learning': 0,\n",
       "  'Warmup: 2D Embedding of a Toy Lattice': 0,\n",
       "  'WordNet': 3,\n",
       "  'Flickr Entailment Graph': 3,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Queries with Negated Variables': 0,\n",
       "  'Properties of the Box Lattice': 0,\n",
       "  'Non-Distributivity': 2},\n",
       " '298504': {'Introduction': 0, 'Approach': 0},\n",
       " '870921': {'Introduction': 10,\n",
       "  'Word-based method': 1,\n",
       "  'Method based on hard clustering': 1,\n",
       "  'Finite Mixture Model': 1,\n",
       "  'Estimation and Hypothesis Testing': 0,\n",
       "  'Creating clusters': 0,\n",
       "  'Estimating P(w|k j )P(w|k_j)': 0,\n",
       "  'Estimating P(k j |c i )P(k_j|c_i)': 5,\n",
       "  'Testing': 0,\n",
       "  'Advantages of FMM': 1,\n",
       "  'Preliminary Experimental Results': 2,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '759831': {'Introduction': 1,\n",
       "  'Notations and Definitions': 0,\n",
       "  'Link Function of Text': 0,\n",
       "  'Link Function of Bigrams': 2,\n",
       "  'Link Function of a Text Window': 1,\n",
       "  'Generative Process and Likelihood': 0,\n",
       "  'Learning Objective': 0,\n",
       "  'Learning VV as Low Rank PSD Approximation': 0,\n",
       "  'Online Blockwise Regression of VV': 13},\n",
       " '2103785': {'Introduction': 0,\n",
       "  'Semantic Similarity of Words and Compositional Phrases': 0,\n",
       "  'Semantic Network Model': 6,\n",
       "  'Distributional Similarity Model': 2,\n",
       "  'Evaluation': 2,\n",
       "  'Semantic Compositionality in Context': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '173188232': {'Introduction': 4,\n",
       "  'Related Work': 17,\n",
       "  'Approach': 3,\n",
       "  'Data sets': 25,\n",
       "  'Results': 3,\n",
       "  'Discussion': 3,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '53217693': {'Introduction': 5,\n",
       "  'Related Work': 14,\n",
       "  'Sequence-to-Sequence Model': 0,\n",
       "  'Topical Hierarchical Recurrent Encoder Decoder': 1,\n",
       "  'Message Encoder': 1,\n",
       "  'Message Attention': 1,\n",
       "  'Context-Level Encoder': 0,\n",
       "  'Context-Topic Joint Attention': 3,\n",
       "  'Decoder': 1,\n",
       "  'Datasets': 2,\n",
       "  'Experiments': 4,\n",
       "  'Quantitative Evaluation': 3,\n",
       "  'Semantic Similarity': 4,\n",
       "  'Response Echo Index': 1,\n",
       "  'Degree of Diversity & Perplexity': 1,\n",
       "  'Human Evaluation': 3,\n",
       "  'Comparing Datasets': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Experimental Setup': 2,\n",
       "  'Human Evaluation Procedure': 0},\n",
       " '10396966': {'Introduction': 21,\n",
       "  'Related Work': 12,\n",
       "  'Model': 0,\n",
       "  'GRU Introduction': 2,\n",
       "  'Representation Learning for Phrases': 0,\n",
       "  'Attentive Pooling': 0,\n",
       "  'The Whole Architecture': 0,\n",
       "  'Experiments': 0,\n",
       "  'Common Setup': 5,\n",
       "  'Textual Entailment': 4,\n",
       "  'Answer Selection': 4,\n",
       "  'Visual Analysis': 0,\n",
       "  'Effects of Pooling Size kk': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '44173285': {'Introduction': 4,\n",
       "  'Related Work': 12,\n",
       "  'Dataset': 1,\n",
       "  'Feature Representation': 2,\n",
       "  'Model Description': 1,\n",
       "  'Post-processing': 0,\n",
       "  'Experimental Setup': 2,\n",
       "  'Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1958200': {'Abstract': 0,\n",
       "  'Introduction': 3,\n",
       "  'The Type Hierarchy of Japanese Particles': 6,\n",
       "  'Case Particles': 7,\n",
       "  'The Complementizer to': 1,\n",
       "  'Modifying Particles': 8,\n",
       "  'Omitted Particles': 4,\n",
       "  'ga-Adjuncts': 2,\n",
       "  'Conclusion': 2},\n",
       " '19237342': {'Introduction': 6,\n",
       "  'Related Work': 10,\n",
       "  'Data': 1,\n",
       "  'Rubric Features': 1,\n",
       "  'Word Embedding Feature Extraction': 4,\n",
       "  'Experimental Setup': 6,\n",
       "  'Results and Discussion': 1,\n",
       "  'Conclusion and Future Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '116880070': {'Introduction': 18,\n",
       "  'Background': 1,\n",
       "  'Our Approach': 0,\n",
       "  'Synchronous Bidirectional Beam Search': 0,\n",
       "  'Synchronous Bidirectional Attention': 0,\n",
       "  'Integrating Synchronous Bidirectional Attention into NMT': 0,\n",
       "  'Training': 5,\n",
       "  'Experiments': 0,\n",
       "  'Dataset': 2,\n",
       "  'Setting': 0,\n",
       "  'Baselines': 5,\n",
       "  'Results on Chinese-English Translation': 0,\n",
       "  'Results on English-German Translation': 5,\n",
       "  'Results on Russian-English Translation': 0,\n",
       "  'Analysis': 1,\n",
       "  'Related Work': 17,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '3942023': {'Introduction': 23,\n",
       "  'Experimental Setup': 11,\n",
       "  'Judgements of Multiple Criteria': 3,\n",
       "  'Consistency and Use of Scales': 6,\n",
       "  'Ranking vs Direct Assessment': 4,\n",
       "  'Relative comparisons of many outputs': 3,\n",
       "  'Conclusion and Discussion': 4,\n",
       "  'Acknowledgements': 0},\n",
       " '48356442': {'Introduction': 2,\n",
       "  'Languages overview': 3,\n",
       "  'Corpus and digital resources': 16,\n",
       "  'Morphological segmentation and analyses': 29,\n",
       "  'Machine Translation': 22,\n",
       "  'Multilinguality and Code-Switching': 3,\n",
       "  'Language Tools': 6,\n",
       "  'Discussion': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Appendix A. Language Families': 1},\n",
       " '170079259': {},\n",
       " '16839291': {'Introduction': 13,\n",
       "  'Graph Convolutional Networks': 1,\n",
       "  'Syntactic GCNs': 0,\n",
       "  'Incorporating directions and labels': 0,\n",
       "  'Edge-wise gating': 2,\n",
       "  'Complementarity of GCNs and LSTMs': 4,\n",
       "  'Syntax-Aware Neural SRL Encoder': 2,\n",
       "  'Word representations': 0,\n",
       "  'Bidirectional LSTM layer': 7,\n",
       "  'Graph convolutional layer': 0,\n",
       "  'Semantic role classifier': 0,\n",
       "  'Datasets and parameters': 2,\n",
       "  'Results and discussion': 0,\n",
       "  'Related Work': 11,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '1438450': {'Introduction': 0,\n",
       "  'Choice of Programming Language': 2,\n",
       "  'Design Criteria': 0,\n",
       "  'Requirements': 0,\n",
       "  'Non-Requirements': 0,\n",
       "  'Modules': 0,\n",
       "  'Parsing Modules': 0,\n",
       "  'Tagging Modules': 0,\n",
       "  'Finite State Automata': 0,\n",
       "  'Type Checking': 0,\n",
       "  'Visualization': 0,\n",
       "  'Text Classification': 0,\n",
       "  'Documentation': 0,\n",
       "  'Assignments': 0,\n",
       "  'Class demonstrations': 0,\n",
       "  'Advanced Projects': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Other Approaches': 12,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '15128214': {'Introduction': 19,\n",
       "  'Related Work': 26,\n",
       "  'System': 3,\n",
       "  'Training Stage': 0,\n",
       "  'Inference Stage': 0,\n",
       "  'Experiments': 1,\n",
       "  'Setup': 1,\n",
       "  'Morpho Challenge Dataset': 0,\n",
       "  'Semantically Driven Dataset': 6,\n",
       "  'Handling Compositionality': 1,\n",
       "  'Results': 0,\n",
       "  'Discussion': 0,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '309476': {'Introduction': 2,\n",
       "  'Related Work': 22,\n",
       "  'Task': 0,\n",
       "  'Approach Overview': 0,\n",
       "  'Corpus': 5,\n",
       "  'Model': 2,\n",
       "  'Experimental Results': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '44278': {'Introduction': 6,\n",
       "  'Our Solution': 1,\n",
       "  'Assignment of Descriptors': 2,\n",
       "  'The Role of the Neural Network': 1,\n",
       "  'Heuristics': 0,\n",
       "  'Experiments and Results': 2,\n",
       "  'Discussion and Future Work': 0},\n",
       " '7669927': {'Introduction': 8,\n",
       "  'Facebook reactions as labels': 0,\n",
       "  'Emotion datasets': 0,\n",
       "  'Affective Text dataset': 7,\n",
       "  'Fairy Tales dataset': 3,\n",
       "  'ISEAR': 2,\n",
       "  'Overview of datasets and emotions': 0,\n",
       "  'Model': 1,\n",
       "  'Selecting Facebook pages': 1,\n",
       "  'Features': 6,\n",
       "  'Results on development set': 1,\n",
       "  'Results': 0,\n",
       "  'Discussion, conclusions and future work': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '53081403': {'Introduction': 4,\n",
       "  'Background': 4,\n",
       "  'Self-Attention Model': 3,\n",
       "  'Motivation': 0,\n",
       "  'Localness Modeling': 0,\n",
       "  'Localness Modeling as a Gaussian Bias': 0,\n",
       "  'Central Position Prediction': 0,\n",
       "  'Window Size Prediction': 1,\n",
       "  'Incorporating into Transformer': 1,\n",
       "  'Setup': 7,\n",
       "  'Ablation Study': 0,\n",
       "  'Main Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Compatibility with Multi-Head Attention': 0,\n",
       "  'Analysis on Multi-Layer Attention': 0,\n",
       "  'Analysis on Phrasal Pattern': 0,\n",
       "  'Related Work': 2,\n",
       "  'Conclusion': 7,\n",
       "  'Acknowledgments': 0},\n",
       " '189927986': {'Introduction': 17,\n",
       "  'Preliminaries': 1,\n",
       "  'Background': 19,\n",
       "  'Rhythmic Syncope': 0,\n",
       "  'Synchronized Strictly Local Functions': 0,\n",
       "  'Properties of TSSL Functions': 0,\n",
       "  'Relation to TIOSL Functions': 0,\n",
       "  'Non-Onward TSSL SFSTs': 0,\n",
       "  'Rhythmic Syncope in Phonology': 3,\n",
       "  'Conclusion': 2,\n",
       "  'Proof of Theorem 16': 0},\n",
       " '46929672': {'Introduction': 20,\n",
       "  'NCE as a Matrix Factorization': 2,\n",
       "  'The NCE Self-Normalization property': 0,\n",
       "  'Explicit Self-normalization': 0,\n",
       "  'Conclusions': 0},\n",
       " '190000105': {'Introduction': 15,\n",
       "  'Quantifying Bias in BERT': 0,\n",
       "  'Correlation with Human Biases': 2,\n",
       "  'Baseline: WEAT for BERT': 0,\n",
       "  'Proposed: Log Probability Bias Score': 0,\n",
       "  'Comparison Results': 1,\n",
       "  'Real World Implications': 3,\n",
       "  'Related Work': 16,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52131263': {'Introduction': 3,\n",
       "  'Multiple-utterance problem in goal-oriented dialog': 0,\n",
       "  'Issues with the present methods': 0,\n",
       "  'Proposed Method': 2,\n",
       "  'Permuted bAbI dialog tasks': 2,\n",
       "  'Experiments and Results': 3,\n",
       "  'Baseline model: memN2N': 2,\n",
       "  'Mask End-to-End Memory Network: Mask-memN2N': 1,\n",
       "  'Model comparison': 1,\n",
       "  'Ablation study': 0,\n",
       "  'Related Work': 17,\n",
       "  'Conclusion': 0},\n",
       " '52816033': {'Introduction': 15,\n",
       "  'Approach': 0,\n",
       "  'Word Representation': 4,\n",
       "  'Syntactic Graph Convolution Network': 3,\n",
       "  'Self-Attention Trigger Classification': 0,\n",
       "  'Argument Classification': 0,\n",
       "  'Biased Loss Function': 0,\n",
       "  'Experiment Settings': 18,\n",
       "  'Overall Performance': 0,\n",
       "  'Effect on Extracting Multiple Events': 2,\n",
       "  'Analysis of Self-Attention Mechanism': 0,\n",
       "  'Related Work': 17,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '3502581': {'Acknowledgments': 0},\n",
       " '52897773': {'Introduction': 11,\n",
       "  'Iterated Response Models': 11,\n",
       "  'Global Pragmatics': 0,\n",
       "  'Incremental Pragmatics': 2,\n",
       "  'An Utterance-level Incremental Speaker': 0,\n",
       "  'Relating the Global and Incremental Models': 2,\n",
       "  'Application to Prior Experiments': 0,\n",
       "  'Over-informative Referring Expressions': 4,\n",
       "  'Anticipatory Implicatures': 1,\n",
       "  'Experimental Validation with TUNA': 1,\n",
       "  'Data': 0,\n",
       "  'Methods': 0,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 5,\n",
       "  'Acknowledgments': 0},\n",
       " '102350959': {'Acknowledgments': 0},\n",
       " '8140780': {'Introduction': 4,\n",
       "  'Translation System': 7,\n",
       "  'Data Sets': 0,\n",
       "  'Background': 14,\n",
       "  'Results and Analysis': 0,\n",
       "  'Ensembles and Model Diversity': 10,\n",
       "  'Experimental Setup': 3,\n",
       "  'Experimental Results': 2,\n",
       "  'Conclusion': 0},\n",
       " '51867574': {'Introduction': 22,\n",
       "  'Machine Reading Comprehension': 4,\n",
       "  'Attention-based Neural Networks': 21,\n",
       "  'Task Description': 1,\n",
       "  'Encode-Interaction-Pointer Framework': 0,\n",
       "  'Hierarchical Attention Fusion Network': 1,\n",
       "  ' Language Model & Encoder Layer': 3,\n",
       "  'Hierarchical Attention & Fusion Layer': 6,\n",
       "  'Model & Output Layer': 0,\n",
       "  'Experiments': 2,\n",
       "  'Dataset': 0,\n",
       "  'Training Details': 1,\n",
       "  'Main Results': 1,\n",
       "  'Ablations': 0,\n",
       "  'Fusion Functions': 0,\n",
       "  'Attention Hierarchy and Function': 0,\n",
       "  'Experiments on TriviaQA': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2174668': {'Introduction': 15,\n",
       "  'Transition-based parsing': 0,\n",
       "  'Dependency parsing': 1,\n",
       "  'Constituent parsing': 0,\n",
       "  'Generalization': 0,\n",
       "  'Baseline': 2,\n",
       "  'Model': 0,\n",
       "  'Encoder': 1,\n",
       "  'Vanilla decoder': 0,\n",
       "  'Stack-Queue decoder': 0,\n",
       "  'Training': 0,\n",
       "  'Data': 2,\n",
       "  'Hyper-parameters': 0,\n",
       "  'Development experiments': 1,\n",
       "  'Comparison to stack-LSTM': 1,\n",
       "  'Attention visualization': 0,\n",
       "  'Final results': 3,\n",
       "  'Related work': 0,\n",
       "  'Conclusion and Future work': 3},\n",
       " '1225426': {'Introduction': 13,\n",
       "  'Binary paragraph vector models': 6,\n",
       "  'Experiments': 11,\n",
       "  'Transfer learning': 1,\n",
       "  'Retrieval with Real-Binary models': 1,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0,\n",
       "  'Visualization of Binary PV codes': 2},\n",
       " '13687638': {'Introduction': 20,\n",
       "  'Profiling with Abstract Features': 4,\n",
       "  'Experiments': 1,\n",
       "  'Lexical vs Bleached Models': 4,\n",
       "  'Human Evaluation': 2,\n",
       "  'Related Work': 11,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '174798375': {'Supplementary Material': 0},\n",
       " '182953237': {'Introduction': 6,\n",
       "  'Methods': 0,\n",
       "  'Dataset Collection': 1,\n",
       "  'BioELMo': 1,\n",
       "  'Abbreviation-specific biLSTM Classifiers': 0,\n",
       "  'Training': 1,\n",
       "  'Inference': 0,\n",
       "  'Baseline Settings': 1,\n",
       "  'Subset Settings': 1,\n",
       "  'Evaluation Metrics': 1,\n",
       "  'Results': 1,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '2756620': {'Introduction': 21,\n",
       "  'Related Work': 0,\n",
       "  'Chatbots': 25,\n",
       "  'Text Matching': 18,\n",
       "  'Problem Formalization': 0,\n",
       "  'A Framework for the Existing Models': 15,\n",
       "  'Sequential Matching Framework': 0,\n",
       "  'Utterance-Response Matching': 0,\n",
       "  'Matching Accumulation': 0,\n",
       "  'Matching Prediction': 1,\n",
       "  'Model Training': 2,\n",
       "  'Experiments': 0,\n",
       "  'Data Sets': 4,\n",
       "  'Baselines': 8,\n",
       "  'Evaluation Metrics': 3,\n",
       "  'Parameter Tuning': 3,\n",
       "  'Evaluation Results': 0,\n",
       "  'Further Analysis': 1,\n",
       "  'Error analysis and future work': 1,\n",
       "  'Conclusion': 0},\n",
       " '711424': {'Introduction': 12,\n",
       "  'Problem definition, notation, datasets': 0,\n",
       "  'Datasets': 0,\n",
       "  'Overview and intuition': 2,\n",
       "  'The Extractor': 3,\n",
       "  'The Reasoner': 0,\n",
       "  'Combining components': 0,\n",
       "  'Related Work': 3,\n",
       "  'Implementation and training details': 3,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '49559642': {'Introduction': 4,\n",
       "  'About Polysynthetic Languages': 22,\n",
       "  'Previous Work': 13,\n",
       "  'Morpheme Alignment Between a Polysynthetic and a Fusional Language': 1,\n",
       "  'Experiments': 1,\n",
       "  'Languages': 1,\n",
       "  'Dataset': 5,\n",
       "  'Results and Discussion': 3,\n",
       "  'Information Loss in Translation Between Polysynthetic and Fusional Languages': 2,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5802722': {'Introduction': 14,\n",
       "  'Related Work': 15,\n",
       "  'Deep Learning Semantic Decoder': 0,\n",
       "  'Sentence Representation': 1,\n",
       "  'Context Representation': 1,\n",
       "  'Combining Sentence and Context': 0,\n",
       "  'Experimental Evaluation': 0,\n",
       "  'Corpora': 3,\n",
       "  'Hyperparameters and Training': 3,\n",
       "  'Experiments': 0,\n",
       "  'Evaluation Metrics': 1,\n",
       "  'Results and Discussion': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1423962': {'Introduction': 9,\n",
       "  'Latent Graph Parser': 2,\n",
       "  'Word Representation': 0,\n",
       "  'POS Tagging Layer': 1,\n",
       "  'Dependency Parsing Layer': 3,\n",
       "  'NMT with Latent Graph Parser': 1,\n",
       "  'Encoder with Dependency Composition': 4,\n",
       "  'Decoder with Attention Mechanism': 10,\n",
       "  'Data': 5,\n",
       "  'Parameter Optimization and Translation': 6,\n",
       "  'Pre-Training of Latent Graph Parser': 2,\n",
       "  'Model Configurations': 0,\n",
       "  'Results on Small and Medium Datasets': 0,\n",
       "  'Small Training Dataset': 1,\n",
       "  'Medium Training Dataset': 0,\n",
       "  'Results on Large Dataset': 7,\n",
       "  'Analysis on Translation Examples': 0,\n",
       "  'Analysis on Learned Latent Graphs': 3,\n",
       "  'Related Work': 20,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52113744': {'Introduction': 5,\n",
       "  'Neural-Symbolic Learning': 0,\n",
       "  'Inputs': 3,\n",
       "  'Modules': 4,\n",
       "  'Aggregator Network': 3,\n",
       "  'Experiments': 5,\n",
       "  'Results': 2,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Related Work': 9,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '53064621': {'Acknowledgements': 0},\n",
       " '7872859': {'Introduction': 10,\n",
       "  'Related Work': 34,\n",
       "  'Problem Formulation': 0,\n",
       "  'Scoring Stories': 0,\n",
       "  'Thematicity': 0,\n",
       "  'Syntactic compatibility': 0,\n",
       "  'Semantic Coherence': 1,\n",
       "  'Decoding': 0,\n",
       "  'Data Collection': 0,\n",
       "  'Setup': 12,\n",
       "  'Results': 0,\n",
       "  'Qualitative Examples': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '160009896': {'Introduction': 5,\n",
       "  'TRADE Model': 1,\n",
       "  'Utterance Encoder': 1,\n",
       "  'State Generator': 6,\n",
       "  'Slot Gate': 2,\n",
       "  'Optimization': 0,\n",
       "  'Unseen Domain DST': 1,\n",
       "  'Zero-shot DST': 0,\n",
       "  'Expanding DST for Few-shot Domain': 3,\n",
       "  'Dataset': 3,\n",
       "  'Training Details': 4,\n",
       "  'Results': 6,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52115505': {'Introduction': 1,\n",
       "  'Related Work': 14,\n",
       "  'Term Extraction and Representation': 7,\n",
       "  'Multi-Context Term Similarity': 1,\n",
       "  'Implementation and Evaluation': 0,\n",
       "  'System Workflow and Application': 0,\n",
       "  'Field Use Cases': 0,\n",
       "  'Conclusion': 0},\n",
       " '10643243': {},\n",
       " '3444808': {'Introduction': 8,\n",
       "  'Related Work': 16,\n",
       "  'Model': 3,\n",
       "  'Fact Encoder': 2,\n",
       "  'Textual Context Encoder': 1,\n",
       "  'Decoder': 2,\n",
       "  'Attention': 0,\n",
       "  'Part-Of-Speech Copy Actions': 3,\n",
       "  'Textual contexts dataset': 1,\n",
       "  'Collection of Textual Contexts': 3,\n",
       "  'Generation of Special tokens': 1,\n",
       "  'Zero-Shot Setups': 1,\n",
       "  'Baselines': 7,\n",
       "  'Training & Implementation Details': 4,\n",
       "  'Automatic Evaluation Metrics': 3,\n",
       "  'Human Evaluation': 1,\n",
       "  'Conclusion': 0},\n",
       " '5019682': {'Models': 0, 'Conclusion': 0, 'Acknowledgements': 0},\n",
       " '52077772': {'Introduction': 8,\n",
       "  'Logic Rules in Sentiment Classification': 0,\n",
       "  'A Reanalysis': 0,\n",
       "  'Importance of Averaging': 0,\n",
       "  'Performance of hu2016harnessing': 0,\n",
       "  'Contextualized Word Embeddings': 3,\n",
       "  'Crowdsourced Experiments': 0,\n",
       "  'Conclusion': 1,\n",
       "  'Crowdsourcing Details': 0},\n",
       " '2162860': {'Introduction': 13,\n",
       "  'Model': 0,\n",
       "  'Convolutional Semantic Similarity': 0,\n",
       "  'Integrating with a Sparse Model': 6,\n",
       "  'Experimental Results': 2,\n",
       "  'Multiple Granularities of Convolution': 0,\n",
       "  'Embedding Vectors': 2,\n",
       "  'Analysis of Learned Convolutions': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '174797834': {'Introduction and background': 57,\n",
       "  'Data': 9,\n",
       "  'Methodology and Resources': 37,\n",
       "  'Ethical considerations': 21,\n",
       "  'Impact and conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '174798142': {'Introduction': 6,\n",
       "  'Models and Data': 2,\n",
       "  'Named entity recognition experiments': 2,\n",
       "  'Part of speech tagging experiments': 2,\n",
       "  'Vocabulary Memorization ': 0,\n",
       "  'Effect of vocabulary overlap': 0,\n",
       "  'Generalization across scripts': 0,\n",
       "  'Encoding Linguistic Structure ': 0,\n",
       "  'Effect of language similarity': 2,\n",
       "  'Generalizing across typological features ': 1,\n",
       "  'Code switching and transliteration': 3,\n",
       "  'Multilingual characterization of the feature space ': 0,\n",
       "  'Experimental Setup': 1,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgements': 0,\n",
       "  'Model Parameters': 0},\n",
       " '1915951': {'Introduction': 4,\n",
       "  'Model': 1,\n",
       "  'Network architecture': 2,\n",
       "  'Training & decoding': 2,\n",
       "  'Results & discussion': 0,\n",
       "  'Conclusion': 0},\n",
       " '67856324': {'Conclusion and Future Work': 0},\n",
       " '27418157': {'Introduction': 20,\n",
       "  'Related Work ': 24,\n",
       "  'Learning How to Learn Visual Attributes: an Adaptive Dialogue Agent': 0,\n",
       "  'Overall System Architecture': 1,\n",
       "  'Adaptive Learning Agent with Hierarchical MDP ': 3,\n",
       "  'Human-Human Dialogue Corpus: BURCHAK ': 1,\n",
       "  'Experiment Setup ': 1,\n",
       "  'Evaluation Metrics ': 1,\n",
       "  'User Simulation': 1,\n",
       "  'Results ': 0,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion & Future Work': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '14342526': {'Introduction': 10,\n",
       "  'Related work': 5,\n",
       "  'Non-linear Translation': 1,\n",
       "  'Decoding': 5,\n",
       "  'Two-layer Neural Networks': 0,\n",
       "  'Features': 1,\n",
       "  'Non-linear Learning Framework': 1,\n",
       "  'Training Criteria': 3,\n",
       "  'Training Objective': 0,\n",
       "  'Training Procedure': 4,\n",
       "  'Structure of the Network': 0,\n",
       "  'Network with two-degree Hidden Layer': 0,\n",
       "  'Network with Grouped Features': 0,\n",
       "  'General Settings': 3,\n",
       "  'Experiments of Training Criteria': 2,\n",
       "  'Experiments of Network Structures': 0,\n",
       "  'Conclusion': 0},\n",
       " '174801285': {'Acknowledgements': 0},\n",
       " '370914': {'Introduction': 6,\n",
       "  'Related work': 10,\n",
       "  'Tensors and multilinear maps': 2,\n",
       "  'Tensor-based predicate calculi': 0,\n",
       "  'Logical connectives with tensors': 0,\n",
       "  'Quantifiers and non-linearity': 0,\n",
       "  'Conclusions and future work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '48358107': {'Conclusions': 0, 'Acknowledgments': 0},\n",
       " '52160439': {'Introduction': 18,\n",
       "  'Method': 3,\n",
       "  'Data sets': 6,\n",
       "  'Preprocessing': 5,\n",
       "  'Model architecture for ASR and ST': 12,\n",
       "  'Evaluation': 2,\n",
       "  'ASR results': 0,\n",
       "  'Spanish-English ST': 0,\n",
       "  'Using English ASR to improve ST': 1,\n",
       "  'Analysis': 10,\n",
       "  'Using French ASR to improve Spanish-English ST': 0,\n",
       "  'Mboshi-French ST': 0,\n",
       "  'Conclusion': 9,\n",
       "  'Acknowledgments': 0},\n",
       " '27246259': {'Introduction': 10,\n",
       "  'A Brief Survey of Datasets': 10,\n",
       "  'Aggregation Readers and Explicit Reference Readers': 7,\n",
       "  'Aggregation Readers': 4,\n",
       "  'Explicit Reference Readers': 3,\n",
       "  'Emergent Predication Structure': 0,\n",
       "  'Pointer Annotation Readers': 0,\n",
       "  'Discussion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Experiment Details': 8,\n",
       "  'Heat Maps for Stanford Reader for Different Answer Candidates': 0,\n",
       "  'Heat Maps for Other Readers': 0},\n",
       " '195316637': {'Introduction': 13,\n",
       "  'Adapting a NMT system via online learning': 0,\n",
       "  'Architecture': 0,\n",
       "  'Machine Translation Engine': 1,\n",
       "  'Translation Server': 0,\n",
       "  'User Interface': 0,\n",
       "  'Logging': 1,\n",
       "  'Summary': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '29164993': {},\n",
       " '15600925': {'Introduction': 4,\n",
       "  'Neural Machine Translation': 2,\n",
       "  'NMT Training with Monolingual Training Data': 1,\n",
       "  'Dummy Source Sentences': 1,\n",
       "  'Synthetic Source Sentences': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Data and Methods': 13,\n",
       "  'Results': 2,\n",
       "  'Contrast to Phrase-based SMT': 4,\n",
       "  'Analysis': 0,\n",
       "  'Related Work': 7,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5067886': {'Multilingual Machine Translation': 4,\n",
       "  'Our Contributions': 1,\n",
       "  'Model Architecture': 2,\n",
       "  'Experiments': 0,\n",
       "  'Model Training': 1,\n",
       "  'Language Rotation During Training': 0,\n",
       "  'Multilingual NMT versus Bilingual NMT': 2,\n",
       "  'Zero-shot Multilingual Classification': 0,\n",
       "  'Direct Zero-shot Translation': 2,\n",
       "  'Interlingua Visualization': 0,\n",
       "  'Networks with Language-specific Encoders and Decoders': 1,\n",
       "  'Universal Encoder-Decoder Networks': 1,\n",
       "  'Zero-shot Translation': 1,\n",
       "  'Conclusion': 0},\n",
       " '184488087': {'Introduction': 10,\n",
       "  'Related Work': 25,\n",
       "  'Conversational Negation Corpus ': 7,\n",
       "  'Annotation Guidelines': 5,\n",
       "  'Negation Cue and Scope Detection Experiments': 0,\n",
       "  'Negation Cue Detection': 4,\n",
       "  'Negation Scope Detection': 15,\n",
       "  'Negation Scope Detection Evaluation': 1,\n",
       "  'Sentiment with Negation Detection Pipeline ': 0,\n",
       "  'Experimental Method ': 7,\n",
       "  'Neural Network Evaluation': 6,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '119105191': {},\n",
       " '195750688': {'Introduction and Background': 8,\n",
       "  'Motivating example': 6,\n",
       "  'Problem Statement': 3,\n",
       "  'Algorithms': 1,\n",
       "  'Fixed Budget by Sequential Halving': 5,\n",
       "  'Fixed Confidence by TTTS': 14,\n",
       "  'Batch Fixed Confidence by BTS': 1,\n",
       "  'Experiments': 11,\n",
       "  'Fixed Budget Model Selection': 0,\n",
       "  'Fixed Confidence Model Selection': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Characterising the Difficulty of a Model Selection Problem': 1},\n",
       " '10086161': {'Introduction': 3,\n",
       "  'Neural Machine Translation': 5,\n",
       "  'Integrating Lexicons into NMT': 0,\n",
       "  'Converting Lexicon Probabilities into Conditioned Predictive Proabilities': 0,\n",
       "  'Combining Predictive Probabilities': 0,\n",
       "  'Constructing Lexicon Probabilities': 0,\n",
       "  'Automatically Learned Lexicons': 5,\n",
       "  'Manual Lexicons': 0,\n",
       "  'Hybrid Lexicons': 1,\n",
       "  'Experiment & Result': 0,\n",
       "  'Settings': 15,\n",
       "  'Effect of Integrating Lexicons': 3,\n",
       "  'Comparison of Integration Methods': 1,\n",
       "  'Additional Experiments': 1,\n",
       "  'Related Work': 8,\n",
       "  'Conclusion & Future Work': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '21460834': {'Introduction': 2,\n",
       "  'Mini-batches for NMT': 1,\n",
       "  'Mini-batch Creation Strategies': 0,\n",
       "  'Mini-batch Size': 0,\n",
       "  'Sentence vs. Word Mini-batching': 2,\n",
       "  'Corpus Sorting Methods': 7,\n",
       "  'Experiments': 0,\n",
       "  'Experimental Settings': 6,\n",
       "  'Experimental Results and Analysis': 0,\n",
       "  'Experiments with a Different Toolkit': 1,\n",
       "  'Related Work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '16631020': {'Introduction': 3,\n",
       "  'NMT Background': 7,\n",
       "  'Transfer Learning': 4,\n",
       "  'Experiments': 3,\n",
       "  'Transfer Results': 1,\n",
       "  'Re-scoring Results': 5,\n",
       "  'Analysis': 0,\n",
       "  'Different Parent Languages': 0,\n",
       "  'Effects of having Similar Parent Language': 0,\n",
       "  'Ablation Analysis': 0,\n",
       "  'Learning Curve': 0,\n",
       "  'Dictionary Initialization': 1,\n",
       "  'Different Parent Models': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '102486945': {},\n",
       " '1222212': {'Introduction': 4,\n",
       "  'Related Work': 16,\n",
       "  'Task': 0,\n",
       "  'Model': 0,\n",
       "  'Inference': 0,\n",
       "  'Learning': 2,\n",
       "  'Experiments': 1,\n",
       "  'Hyperparameters': 4,\n",
       "  'Ensembling': 0,\n",
       "  'Results': 0,\n",
       "  'Coreference Results': 0,\n",
       "  'Ablations': 0,\n",
       "  'Comparing Span Pruning Strategies': 1,\n",
       "  'Analysis': 0,\n",
       "  'Mention Recall': 1,\n",
       "  'Mention Precision': 0,\n",
       "  'Head Agreement': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '11492268': {'Introduction': 4,\n",
       "  'Probabilistic Models for QE': 9,\n",
       "  'Gaussian Process Regression': 0,\n",
       "  'Matèrn Kernels': 1,\n",
       "  'Warped Gaussian Processes': 1,\n",
       "  'Intrinsic Uncertainty Evaluation': 1,\n",
       "  'Experimental Settings': 6,\n",
       "  'Results and Discussion': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Asymmetric Risk Scenarios': 1,\n",
       "  'Bayes Risk for Asymmetric Losses': 1,\n",
       "  'Related Work': 8,\n",
       "  'Conclusions': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '2638551': {'Introduction': 18,\n",
       "  'Related Work': 25,\n",
       "  'Corpus Creation': 6,\n",
       "  'Experimental Results': 0,\n",
       "  'RQs vs. Information-Seeking Qs': 4,\n",
       "  'Sarcastic vs. Other Uses of RQs': 11,\n",
       "  'Linguistic Characteristics of RQs by Class and Domain': 4,\n",
       "  'Conclusions': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '44170166': {'Acknowledgments': 0},\n",
       " '5072513': {'Introduction': 12,\n",
       "  'Related work': 30,\n",
       "  'Proposed model': 0,\n",
       "  'Fused Bifocal Attention Mechanism': 0,\n",
       "  'Gated Orthogonalization for Modeling Stay-On and Never Look Back behaviour': 0,\n",
       "  'Experimental setup': 0,\n",
       "  'Datasets': 0,\n",
       "  'Models compared': 4,\n",
       "  'Hyperparameter tuning': 3,\n",
       "  'Results and Discussions': 0,\n",
       "  'Comparison of different models': 1,\n",
       "  'Human Evaluations': 0,\n",
       "  'Performance on different languages': 1,\n",
       "  'Visualizing Attention Weights': 0,\n",
       "  'Out of domain results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '44073804': {'Introduction': 2,\n",
       "  'Data': 3,\n",
       "  'Model': 3,\n",
       "  'Monolingual Baseline': 2,\n",
       "  'Simple Polyglot Sharing': 2,\n",
       "  'Language Identification': 0,\n",
       "  'Language-Specific LSTMs': 0,\n",
       "  'Experiments': 0,\n",
       "  'Related Work': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52205000': {},\n",
       " '52068673': {'Introduction': 2,\n",
       "  'Related Work': 10,\n",
       "  'Proposed Model': 2,\n",
       "  'Emotional Words Detection Model': 1,\n",
       "  'Sentiment-Memory Based Auto-Encoder': 1,\n",
       "  'Data Preprocessing': 1,\n",
       "  'Experiment Settings': 1,\n",
       "  'Baselines': 5,\n",
       "  'Results and Discussions': 1,\n",
       "  'Effectiveness of Sentiment-Memories': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '182952400': {'Introduction': 10,\n",
       "  'Related Work': 18,\n",
       "  'Pretraining Tasks': 0,\n",
       "  'Models and Training Procedures': 1,\n",
       "  'Results': 4,\n",
       "  'Conclusions': 0,\n",
       "  'Hyperparameters and Optimization Details': 0,\n",
       "  'Multitask Learning Methods': 0,\n",
       "  'Diagnostic Set Results': 1},\n",
       " '29149415': {'Introduction': 4,\n",
       "  'Related Work': 0,\n",
       "  'Unsupervised Knowledge-Free Interpretable WSD': 0,\n",
       "  'Induction of the WSD Models': 6,\n",
       "  'WSD API': 0,\n",
       "  'User Interface for Interpretable WSD': 0,\n",
       "  'Evaluation': 4,\n",
       "  'Dataset': 0,\n",
       "  'Evaluation Metrics': 0,\n",
       "  'Discussion of Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '497108': {'Introduction': 0,\n",
       "  'Related Work': 0,\n",
       "  'Semantic Parsing to SQL': 0,\n",
       "  'Benchmark Experiments': 0,\n",
       "  'Interactive Learning Experiments': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '534431': {'Introduction and Related Work': 10,\n",
       "  'System for Interpretable STS': 0,\n",
       "  'Type Prediction Module': 1,\n",
       "  'Score Prediction Module': 1,\n",
       "  'Experimental Results': 0,\n",
       "  'Conclusion and Future': 1},\n",
       " '48364557': {'Introduction': 9,\n",
       "  'Multi-Hop Path Reasoning Tasks': 0,\n",
       "  'Related Work': 7,\n",
       "  'Recurrent One-Hop Prediction': 2,\n",
       "  'Knowledge Base Completion (mh-KBC)': 10,\n",
       "  'Path Query Answering (mh-PQA)': 3,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgement': 0},\n",
       " '52068327': {'Introduction': 1,\n",
       "  'Related Work': 9,\n",
       "  'Data Sets': 3,\n",
       "  'Architecture Overview': 0,\n",
       "  'Word - Multichannel Neural Network': 4,\n",
       "  'Training': 4,\n",
       "  'Context - Bi-LSTM-CRF': 2,\n",
       "  'Evaluation': 1,\n",
       "  'Analysis & Discussion': 0,\n",
       "  'Conclusion & Future Work': 2},\n",
       " '15986631': {'Conclusion': 0, 'Acknowledgments': 0},\n",
       " '4380496': {'Conclusion': 0, 'Acknowledgment': 0},\n",
       " '3641676': {'Introduction': 18,\n",
       "  'The Proposed DOC Architecture': 7,\n",
       "  'CNN and Feed Forward Layers of DOC': 2,\n",
       "  '1-vs-Rest Layer of DOC': 7,\n",
       "  'Reducing Open Space Risk Further': 0,\n",
       "  'Datasets': 3,\n",
       "  'Test Settings and Evaluation Metrics': 3,\n",
       "  'Baselines': 4,\n",
       "  'Hyperparameter Setting': 0,\n",
       "  'Result Analysis': 0,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '52116412': {'Introduction': 5,\n",
       "  'Background': 20,\n",
       "  'Method': 0,\n",
       "  'Residualized Control Prediction': 1,\n",
       "  'Factor Adaptation': 3,\n",
       "  'Residualized Factor Adaptation': 0,\n",
       "  'Evaluation Setup': 5,\n",
       "  'Data Set': 6,\n",
       "  'Baselines': 5,\n",
       "  'Comparison of RC, FA, and RFA': 4,\n",
       "  'Feature Selection': 0,\n",
       "  'Increasing Factors and Factor Selection': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '189898467': {'Introduction': 13,\n",
       "  'Definition and Training': 1,\n",
       "  'Application': 0,\n",
       "  'Architecture': 2,\n",
       "  'Evaluation': 3,\n",
       "  'Data': 0,\n",
       "  'English→\\\\rightarrow German': 2,\n",
       "  'English→\\\\rightarrow Romanian': 2,\n",
       "  'Example Output': 0,\n",
       "  'Discussion': 0,\n",
       "  'Related Work': 11,\n",
       "  'Conclusion': 0},\n",
       " '189998202': {'Introduction': 12,\n",
       "  'WALS: A Typological Database': 1,\n",
       "  'Two Typological Experiments': 0,\n",
       "  'Predicting Typological Features': 9,\n",
       "  'Discovering Typological Implications': 3,\n",
       "  'Related Work': 18,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '21665312': {'Introduction': 7,\n",
       "  'Methodology': 1,\n",
       "  'GAN': 2,\n",
       "  'ARAE': 1,\n",
       "  'Conditional ARAE': 0,\n",
       "  'Demonstration': 0,\n",
       "  'Conclusion': 0},\n",
       " '198962066': {'Introduction': 6,\n",
       "  'Proposed method': 8,\n",
       "  'Experimental settings': 4,\n",
       "  'Results and discussion': 2,\n",
       "  'Related work': 19,\n",
       "  'Conclusions and future work': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '17953812': {},\n",
       " '52310274': {'Introduction': 4,\n",
       "  'Datasets and Tasks': 6,\n",
       "  'Wikipedia Talk Pages dataset': 0,\n",
       "  'Twitter dataset': 1,\n",
       "  'Common Challenges': 0,\n",
       "  'Methods and Ensemble': 0,\n",
       "  'Logistic Regression': 1,\n",
       "  'Recurrent Neural Networks': 2,\n",
       "  'Convolutional Neural Networks': 2,\n",
       "  '(Sub-)Word Embeddings': 3,\n",
       "  'Ensemble Learning': 0,\n",
       "  'Experimental Study': 0,\n",
       "  'Setup': 1,\n",
       "  'Correlation Analysis': 0,\n",
       "  'Experimental Results': 0,\n",
       "  'Detailed Error Analysis': 6,\n",
       "  'Error Classes of False Negatives': 9,\n",
       "  'Error Classes of False Positives': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '852013': {'Introduction': 0,\n",
       "  'Component taggers': 0,\n",
       "  'The data': 0,\n",
       "  'Potential for improvement': 0,\n",
       "  'Simple Voting': 0,\n",
       "  'Pairwise Voting': 0,\n",
       "  'Stacked classifiers': 0,\n",
       "  'The value of combination': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '56895551': {},\n",
       " '3536772': {'Introduction': 5,\n",
       "  'Resources': 0,\n",
       "  'Sentiment Lexicons': 7,\n",
       "  'Embeddings': 3,\n",
       "  'System Description': 0,\n",
       "  'Preprocessing and Normalization': 4,\n",
       "  'Features ُExtraction': 8,\n",
       "  'Classifier': 1,\n",
       "  'Results': 1,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgment': 0},\n",
       " '9371149': {'Introduction': 5,\n",
       "  'Related Work': 0,\n",
       "  'Information Extraction': 14,\n",
       "  'Construction of Knowledge Sources for Generation': 1,\n",
       "  'Creation of a Database of Profiles': 0,\n",
       "  'Extraction of entity names from old newswire': 3,\n",
       "  'Extraction of descriptions': 0,\n",
       "  'Categorization of descriptions': 0,\n",
       "  'Organization of descriptions in a database of profiles': 0,\n",
       "  'Generation': 0,\n",
       "  'Transformation of descriptions into Functional Descriptions': 1,\n",
       "  'Lexicon creation': 0,\n",
       "  'Coverage and Limitations': 0,\n",
       "  'Coverage': 0,\n",
       "  'Limitations': 0,\n",
       "  'Current Directions': 1,\n",
       "  'Contributions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '311594': {'Introduction': 0},\n",
       " '52290656': {'Acknowledgments': 0},\n",
       " '6256345': {'Introduction': 17,\n",
       "  'Word Sense Induction': 8,\n",
       "  'Sense Embedding for WSI': 4,\n",
       "  'Experimental Setup and baselines': 4,\n",
       "  'Comparing on SemEval-2010': 4,\n",
       "  'Related Work': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '67856793': {'Introduction': 12,\n",
       "  'Models': 1,\n",
       "  'Experiments': 2,\n",
       "  'Results': 0,\n",
       "  'Related Work': 17,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '22253718': {'Introduction': 12,\n",
       "  'Cross-Lingual Word Embeddings ': 14,\n",
       "  'Cross-Lingual Sentiment Analysis': 5,\n",
       "  'Data': 12,\n",
       "  'Translation Matrix Technique': 1,\n",
       "  'Models': 1,\n",
       "  'Translation Accuracy': 3,\n",
       "  'Binary Word Sentiment Classification': 0,\n",
       "  'Fine-Grained Sentiment Analysis': 0,\n",
       "  'Sentiment Classification of Reviews': 3,\n",
       "  'Discussion and Future Work': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '5673925': {'Introduction': 8,\n",
       "  'Related Work': 16,\n",
       "  'Dataset': 0,\n",
       "  'Proposed model': 0,\n",
       "  'Diversity based attention model': 0,\n",
       "  'Baseline Methods': 1,\n",
       "  'Experimental Setup': 1,\n",
       "  'Discussions': 0,\n",
       "  'Conclusion': 0},\n",
       " '14179380': {'Introduction': 1},\n",
       " '5071138': {'Introduction': 10,\n",
       "  'Related Work': 12,\n",
       "  'Dataset': 0,\n",
       "  'Models': 7,\n",
       "  'Experimental Setup': 0,\n",
       "  'Results and Discussions': 0,\n",
       "  'Conclusion': 0},\n",
       " '7038187': {'Introduction & related work': 20,\n",
       "  'Problems of real-world datasets': 0,\n",
       "  'Issues with crowdsourced real-world data': 3,\n",
       "  'The Clever Hans effect': 2,\n",
       "  'Guiding principles for DNN evaluation': 0,\n",
       "  'Automatic generation of language data': 1,\n",
       "  'Abstract microworlds': 0,\n",
       "  'Syntactically rich language generation': 5,\n",
       "  'Quantification example': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '81977022': {'Conclusion and Future Work': 0, 'Acknowledgment': 0},\n",
       " '11077725': {'Introduction': 6,\n",
       "  'Implementation': 2,\n",
       "  'Combining Multiple Sources of Vectors': 1,\n",
       "  'Vocabulary Selection': 0,\n",
       "  'Dimensionality Reduction': 1,\n",
       "  \"Don't Take “OOV” for an Answer\": 1,\n",
       "  'Results': 0,\n",
       "  'The Submitted System: ConceptNet + word2vec + GloVe': 0,\n",
       "  'Variant A: Adding Polyglot Embeddings': 1,\n",
       "  'Variant B: Adding Parallel Text from OpenSubtitles': 2,\n",
       "  'Comparison of Results': 1,\n",
       "  'Discussion': 2},\n",
       " '14974': {'Introduction': 2,\n",
       "  'Related Work': 16,\n",
       "  'Creating a Sentiment Lexicon for Opposing Polarity Phrases': 11,\n",
       "  'Sentiment Composition Patterns': 0,\n",
       "  'Automatically Predicting Sentiment': 0,\n",
       "  'Baseline Classifiers': 0,\n",
       "  'Supervised Classifiers': 2,\n",
       "  'Results': 0,\n",
       "  'Conclusions': 0},\n",
       " '10726920': {'Introduction': 1,\n",
       "  'Related Work': 8,\n",
       "  'NLI Shared Task 2013': 2,\n",
       "  'Overlap with Dialect Identification': 18,\n",
       "  'Methods': 0,\n",
       "  'Data': 0,\n",
       "  'Approach': 4,\n",
       "  'Results': 2,\n",
       "  'Most Informative Features': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '232808': {'Introduction': 0,\n",
       "  'Lexical rules in Categorial Grammar': 0,\n",
       "  'The Lemma Table proof procedure': 0,\n",
       "  'A worked example': 0},\n",
       " '3933075': {'Introduction': 3,\n",
       "  'Chinese Reading Comprehension Datasets': 0,\n",
       "  'Existing Cloze-style Datasets': 0,\n",
       "  \"People Daily and Children's Fairy Tale Datasets\": 1,\n",
       "  'Consensus Attention Sum Reader': 1,\n",
       "  'Experimental Setups': 6,\n",
       "  'Results on Public Datasets': 3,\n",
       "  'Results on Chinese Reading Comprehension Datasets': 0,\n",
       "  'Related Work': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '52895758': {'Introduction': 13,\n",
       "  'Related work': 21,\n",
       "  'Problem Formulation': 1,\n",
       "  'The Behavioral Graph: A Knowledge Base For Navigation': 0,\n",
       "  'Approach': 5,\n",
       "  'Dataset': 1,\n",
       "  'Experiments': 0,\n",
       "  'Evaluation Metrics': 3,\n",
       "  'Models Used in the Evaluation': 3,\n",
       "  'Implementation Details': 2,\n",
       "  'Quantitative Evaluation': 0,\n",
       "  'Qualitative Evaluation': 0,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '1571038': {'Introduction': 14,\n",
       "  'A Greedy Incremental Generation Algorithm': 0,\n",
       "  'TNCBs': 1,\n",
       "  'Suitable Grammars': 1,\n",
       "  'The Generation Algorithm': 0,\n",
       "  'Initialising the Generator': 0,\n",
       "  'The Complexity of the Generator': 1,\n",
       "  'Conclusion': 1},\n",
       " '2862211': {'Introduction': 7,\n",
       "  'A computational grammar for Dutch': 1,\n",
       "  'Interaction with the dialogue manager': 2,\n",
       "  'Robust parsing': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Test set': 0,\n",
       "  'Efficiency': 1,\n",
       "  'Accuracy': 0,\n",
       "  'Concept Accuracy': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6210126': {'Introduction': 7,\n",
       "  'Framework': 3,\n",
       "  'Part-of-Speech Induction': 3,\n",
       "  'The Hidden Markov Model': 3,\n",
       "  'Additional Comparisons': 1,\n",
       "  'Neural HMM': 0,\n",
       "  'Producing Probabilities': 0,\n",
       "  'Training the Neural Network': 1,\n",
       "  'HMM and Neural HMM Equivalence': 0,\n",
       "  'Convolutions for Morphology': 2,\n",
       "  'Infinite Context with LSTMs': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Data and Parameters': 0,\n",
       "  'Results': 0,\n",
       "  'Parameter Ablation': 0,\n",
       "  'Future Work': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '52099223': {'Introduction': 7, 'Method': 0, 'biLM Derived Substitutions': 1},\n",
       " '2797612': {'Introduction': 6,\n",
       "  'Cross-sentence n-ary relation extraction': 3,\n",
       "  'Graph LSTMs': 5,\n",
       "  'Document Graph': 2,\n",
       "  'Backpropagation in Graph LSTMs': 3,\n",
       "  'The Basic Recurrent Propagation Unit': 2,\n",
       "  'Comparison with Prior LSTM Approaches': 3,\n",
       "  'Multi-task Learning with Sub-relations': 3,\n",
       "  'Implementation Details': 4,\n",
       "  'Domain: Molecular Tumor Boards': 0,\n",
       "  'Datasets': 5,\n",
       "  'Distant Supervision': 0,\n",
       "  'Automatic Evaluation': 7,\n",
       "  'PubMed-Scale Extraction': 0,\n",
       "  'Manual Evaluation': 0,\n",
       "  'Domain: Genetic Pathways': 3,\n",
       "  'Related Work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '195848205': {'Introduction': 5,\n",
       "  'System Details': 0,\n",
       "  'NMT Model': 5,\n",
       "  'Data Preprocessing': 9,\n",
       "  'Monolingual Data': 4,\n",
       "  'Placeholder': 1,\n",
       "  'Fine-tuning': 0,\n",
       "  'Experimental Settings': 9,\n",
       "  'Experimental Results': 0,\n",
       "  'Effect of Placeholders': 0,\n",
       "  'Effect of Fine-tuning': 0,\n",
       "  'Difficulties in Translating Social Media Texts': 0,\n",
       "  'Use of Contextual Information': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5656482': {'Introduction': 4,\n",
       "  'Problem Formulation and Notation': 0,\n",
       "  'Bayesian Optimization': 0,\n",
       "  'Acquisition Function': 4,\n",
       "  'Surrogate Model': 3,\n",
       "  'Implementation Details': 3,\n",
       "  'Experiments': 0,\n",
       "  'Setup': 2,\n",
       "  'Datasets': 7,\n",
       "  'Baselines': 0,\n",
       "  'Results': 8,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '21716001': {'Introduction': 8,\n",
       "  'Morphological Relations': 0,\n",
       "  'Reduplication': 0,\n",
       "  'Semi-affixation': 0,\n",
       "  'Semantic Relations': 0,\n",
       "  'Task of Chinese Analogical Reasoning': 2,\n",
       "  'Experiments': 0,\n",
       "  'Vector Representations': 4,\n",
       "  'Context Features': 3,\n",
       "  'Corpora': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52189218': {'Introduction': 0,\n",
       "  'Related Work': 10,\n",
       "  'Method': 0,\n",
       "  'Types of Noise': 0,\n",
       "  'Datasets and Document Classification Algorithms': 7,\n",
       "  'Experiments': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Conclusion': 0},\n",
       " '3085700': {'Introduction': 19,\n",
       "  'Contrastive Translation Pairs': 3,\n",
       "  'Evaluation': 0,\n",
       "  'Data and Methods': 4,\n",
       "  'Results': 4,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '7730925': {'Introduction': 7,\n",
       "  'Sequence-to-sequence Parsing Model': 1,\n",
       "  'Encoder-Decoder': 2,\n",
       "  'Parse Tree as Target Sequence': 1,\n",
       "  'AMR Linearization': 0,\n",
       "  'Dealing with the Data Sparsity Issue': 0,\n",
       "  'AMR Categorization': 0,\n",
       "  'Categorize Source Sequence': 0,\n",
       "  'Recovering AMR graph': 0,\n",
       "  'Supervised Attention Model': 2,\n",
       "  'Experiments': 2,\n",
       "  'Experiment Settings': 1,\n",
       "  'Baseline Model': 1,\n",
       "  'Impact of Re-Categorization': 0,\n",
       "  'Impact of Supervised Alignment': 2,\n",
       "  'Results': 3,\n",
       "  'Discussion': 5,\n",
       "  'Conclusion': 0},\n",
       " '44123113': {'Introduction': 17,\n",
       "  'Related Work': 3,\n",
       "  'Dependent Gated Reading': 0,\n",
       "  'Multi-hop Reading of Document and Query': 2,\n",
       "  'Ranking & Prediction': 2,\n",
       "  'Further Enhancements': 3,\n",
       "  'Datasets': 4,\n",
       "  'Training Details & Experimental Setup': 3,\n",
       "  'Results': 0,\n",
       "  'Ablation Study': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Rule-based Disambiguation Study': 0,\n",
       "  'Attention Study': 0},\n",
       " '950292': {'Introduction': 10,\n",
       "  'Related Work': 28,\n",
       "  'CMU-MOSI Dataset': 2,\n",
       "  'Tensor Fusion Network': 0,\n",
       "  'Modality Embedding Subnetworks': 19,\n",
       "  'Tensor Fusion Layer': 0,\n",
       "  'Sentiment Inference Subnetwork': 0,\n",
       "  'Experiments': 0,\n",
       "  'E1: Multimodal Sentiment Analysis': 5,\n",
       "  'E2: Tensor Fusion Evaluation': 0,\n",
       "  'E3: Modality Embedding Subnetworks Evaluation': 8,\n",
       "  'Methodology': 2,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '145055053': {'Translation': 0, 'Paraphrase Generation': 0},\n",
       " '52050424': {'Introduction': 14,\n",
       "  'Dialog State Tracking': 7,\n",
       "  'Cross-Lingual Transfer Learning': 6,\n",
       "  'Problem Definition': 0,\n",
       "  'Decoupled Neural Belief Tracker': 1,\n",
       "  'Cross-lingual Neural Belief Tracker': 0,\n",
       "  'Teacher-Student Framework': 0,\n",
       "  'Bilingual Corpus (XL-NBT-C)': 0,\n",
       "  'Bilingual Dictionary (XL-NBT-D)': 0,\n",
       "  'Dataset': 7,\n",
       "  'Results': 1,\n",
       "  'Discussion': 0,\n",
       "  'Ablation Test': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0,\n",
       "  'Translator Baseline': 0,\n",
       "  'NBT system setting': 0,\n",
       "  'Learning Curve': 0},\n",
       " '140080210': {'Introduction': 15,\n",
       "  'Adversarial Regularization': 1,\n",
       "  'Gradient Reversal Layer Scheduling': 1,\n",
       "  'Data': 5,\n",
       "  'Implementation': 2,\n",
       "  'Strengths of AdvReg': 1,\n",
       "  'Shortcomings of AdvReg': 3,\n",
       "  'Effect of GRL scheduling': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Quantitative Analysis': 3,\n",
       "  'Qualitative Analysis': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Implementation Details': 1,\n",
       "  'GRL Scheduling Details': 0},\n",
       " '32533948': {'Introduction': 7,\n",
       "  'Related works': 27,\n",
       "  'Introducing the Lifted Matrix-Space model': 2,\n",
       "  'Base model': 0,\n",
       "  'The Lifted Matrix-Space model augmented with TreeLSTM components': 0,\n",
       "  'Simplified Lifted Matrix-Space model augmented with TreeLSTM components': 0,\n",
       "  'NLI experiments': 0,\n",
       "  'Implementation details': 6,\n",
       "  'Test results and analysis': 3,\n",
       "  'Functional versus lexical meanings in lifted matrix representations': 0,\n",
       "  'Conclusion': 0},\n",
       " '24544277': {'Introduction': 11,\n",
       "  'Methodology': 2,\n",
       "  'Data': 3,\n",
       "  'Experimental Setup': 4,\n",
       "  'Results': 0,\n",
       "  'Effect of network depth': 5,\n",
       "  'Effect of target language': 1,\n",
       "  'Analysis at the semantic tag level': 1,\n",
       "  'Analyzing discourse relations': 0,\n",
       "  'Other architectural variants': 3,\n",
       "  'Related Work': 12,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '51878898': {'Acknowledgments': 0},\n",
       " '1387162': {'Introduction': 12,\n",
       "  'Multilingual Relation Extraction': 1,\n",
       "  'Relation Extraction in English': 3,\n",
       "  'Cross-lingual Relation Projection': 2,\n",
       "  'Experiments': 0,\n",
       "  'Related Work': 7,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '9712152': {'Introduction': 4,\n",
       "  'LR Parser Generation': 2,\n",
       "  'Lazy Parser Generation': 2,\n",
       "  'Incremental Parser Generation': 1,\n",
       "  'Conclusion': 1},\n",
       " '51938927': {'Introduction': 16,\n",
       "  'Dataset': 1,\n",
       "  'Linguistic features': 7,\n",
       "  'Model': 4,\n",
       "  'Experiments': 0,\n",
       "  'Noise modality improves performance': 0,\n",
       "  'Effectiveness of cooperative optimization': 0,\n",
       "  'Agreement among modalities is desirable': 0,\n",
       "  'Dividing features by their natural modalities is desirable': 0,\n",
       "  'Visualizing the representations': 0,\n",
       "  'Evaluation against benchmark algorithms': 3,\n",
       "  'Conclusion and future works': 0,\n",
       "  'Linguistic Features': 11},\n",
       " '15881253': {'Introduction': 7, 'Conclusion': 0, 'Acknowledgements': 0},\n",
       " '199442630': {'Introduction': 5,\n",
       "  'Background': 0,\n",
       "  'Neural-based Approach': 2,\n",
       "  'Experimental Framework': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '16043159': {'Introduction': 6,\n",
       "  'Unsupervised Dependency Parsing': 0,\n",
       "  'Reranking': 2,\n",
       "  'The IR Framework': 0,\n",
       "  'Treebank-oriented Greedy Search': 2,\n",
       "  'Iterated Reranking': 1,\n",
       "  'Multi-phase Iterated Reranking': 0,\n",
       "  \"le2014the's Reranker\": 0,\n",
       "  'The ∞\\\\infty -order Generative Model': 0,\n",
       "  'Estimation with the IORNN': 1,\n",
       "  'The Reranker': 0,\n",
       "  'Complete System': 1,\n",
       "  'Tuning Parser PP': 0,\n",
       "  'Tuning Reranker RR': 1,\n",
       "  'Tuning multi-phase IR': 0,\n",
       "  'Setting': 2,\n",
       "  'Results': 1,\n",
       "  'Analysis': 2,\n",
       "  'Discussion': 1,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '52178091': {},\n",
       " '174797955': {},\n",
       " '52164624': {'Introduction': 2,\n",
       "  'System Description': 0,\n",
       "  'Baseline': 0,\n",
       "  'Our system': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Ablation Study': 0,\n",
       "  'Error analysis': 1,\n",
       "  'MSD prediction': 0,\n",
       "  'Related Work': 11,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '8328845': {'Introduction': 9,\n",
       "  'Derivational Morphology': 1,\n",
       "  'Related Work': 5,\n",
       "  'Dataset': 1,\n",
       "  'Experiments': 0,\n",
       "  'Baseline': 0,\n",
       "  'Encoder–Decoder Model': 2,\n",
       "  'Settings': 2,\n",
       "  'Results': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '793385': {'Introduction': 5,\n",
       "  'Related Work': 2,\n",
       "  'Approach': 0,\n",
       "  'Dataset': 0,\n",
       "  'Model': 5,\n",
       "  'Experiments': 0,\n",
       "  'Unseen Entities': 0,\n",
       "  'Unseen Question Templates': 0,\n",
       "  'Unseen Relations': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '5201435': {'Introduction': 10,\n",
       "  'Model Description': 7,\n",
       "  'Automatic Post-Editing': 2,\n",
       "  'Related Work': 10,\n",
       "  'Experiments & Results': 0,\n",
       "  'Multimodal Translation': 4,\n",
       "  'Phrase-Based System': 2,\n",
       "  'Neural System': 2,\n",
       "  'Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '118684820': {'Introduction': 31,\n",
       "  'Background': 1,\n",
       "  'Related Work': 21,\n",
       "  'Methodology': 0,\n",
       "  'Soft-GAN': 1,\n",
       "  'LATEXT-GAN I': 1,\n",
       "  'LATEXT-GAN II': 1,\n",
       "  'LATEXT-GAN III': 1,\n",
       "  'Dataset and Experimental Procedures': 10,\n",
       "  'Quantitative Evaluation': 14,\n",
       "  'Human Evaluation': 1,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5054582': {'Introduction': 10,\n",
       "  'Data and pre-processing': 6,\n",
       "  'Proposed Approach': 0,\n",
       "  'Word representation': 2,\n",
       "  'Model architecture': 4,\n",
       "  'Experimental setup': 2,\n",
       "  'Results': 3,\n",
       "  'Conclusion': 0},\n",
       " '13840496': {'Introduction': 18,\n",
       "  'Experimental Method': 13,\n",
       "  'Text Processing': 0,\n",
       "  'Compute Event Representations': 0,\n",
       "  'Calculate Contingency Measures': 6,\n",
       "  'Web Search Refinement': 4,\n",
       "  'Evaluation and Results': 0,\n",
       "  'Mechanical Turk Contingent Pair Evaluations': 10,\n",
       "  'Results': 0,\n",
       "  'Discussion and Future Work': 14,\n",
       "  'Acknowledgments': 0},\n",
       " '47020134': {'Introduction': 15,\n",
       "  'Recurrent neural network grammars for incremental processing': 5,\n",
       "  'Word-synchronous beam search': 7,\n",
       "  'Complexity metrics': 4,\n",
       "  'Regression models of naturalistic EEG': 4,\n",
       "  'Language models for literary stimuli': 4,\n",
       "  'Results': 0,\n",
       "  'Whole-Head analysis': 0,\n",
       "  'Region of Interest analysis': 2,\n",
       "  'Discussion': 4,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '102354931': {'Introduction': 26,\n",
       "  'Background: Normalizing Flows': 6,\n",
       "  'Proposed Method': 0,\n",
       "  'Notation': 0,\n",
       "  'Density Estimation in Monolingual Space': 0,\n",
       "  'Density Matching': 1,\n",
       "  'Weak Orthogonality Constraint': 2,\n",
       "  'Weak Supervision with Identical Words': 0,\n",
       "  'Retrieval Method': 3,\n",
       "  'Iterative Procrustes Refinement': 1,\n",
       "  'Dataset and Task': 3,\n",
       "  'Implementation Details': 3,\n",
       "  'Main Results on BLI': 10,\n",
       "  'Morphologically Rich Language Results': 3,\n",
       "  'Cross-lingual Word Similarity': 2,\n",
       "  'Ablation Study': 0,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '195218635': {'Introduction': 10,\n",
       "  'Related Work': 21,\n",
       "  'Model': 0,\n",
       "  'Translation Candidate Ranking Task using a Dual Encoder': 8,\n",
       "  'Hierarchical Document Encoder (HiDE)': 0,\n",
       "  'Experiments': 0,\n",
       "  'Data': 1,\n",
       "  'Configuration': 1,\n",
       "  'Mining Translations and Evaluation': 3,\n",
       "  'Analysis': 0,\n",
       "  'Errors': 0,\n",
       "  'HiDE performance on Coarse Sentence-level Models': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '125545': {'Introduction': 3,\n",
       "  'Model Description': 2,\n",
       "  'Sequence Generation': 2,\n",
       "  'Training': 2,\n",
       "  'Related Work': 5,\n",
       "  'Experiments': 0,\n",
       "  'Baseline Methods': 6,\n",
       "  'Proposed Methods': 1,\n",
       "  'Datasets': 3,\n",
       "  'Metrics and Reward': 0,\n",
       "  'Implementation Details': 1,\n",
       "  'Results and Discussion': 0,\n",
       "  'Relevant Terms per Document': 0,\n",
       "  'Scalability: Number of Terms vs Recall': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Training and Inference Times': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '8622019': {'Acknowledgements': 0},\n",
       " '3262913': {'Introduction': 7,\n",
       "  'Overview': 0,\n",
       "  'NLP-based Extraction Method': 3,\n",
       "  'HTML-based Extraction Method': 0,\n",
       "  'Language Modeling for Filtering': 3,\n",
       "  'Clustering Term Descriptions': 1,\n",
       "  'Methodology': 1,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '15295411': {},\n",
       " '10324034': {'Introduction': 7,\n",
       "  'The Hypothesis Evaluation Task': 0,\n",
       "  'Argus Dataset': 3,\n",
       "  'AI2-8grade/CK12 Dataset': 0,\n",
       "  'MCTest Dataset': 2,\n",
       "  'Related Work': 11,\n",
       "  'Neural Model': 0,\n",
       "  'Sentence Embeddings': 12,\n",
       "  'Evidence Integration': 3,\n",
       "  'Experimental Setup': 3,\n",
       "  'Evaluation': 4,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '3295641': {'Introduction': 4,\n",
       "  'Motivation': 7,\n",
       "  'Challenges': 4,\n",
       "  'Universal Neural Machine Translation': 1,\n",
       "  'Universal Lexical Representation (ULR)': 3,\n",
       "  'Mixture of Language Experts (MoLE)': 1,\n",
       "  'Experiments': 0,\n",
       "  'Settings': 3,\n",
       "  'Back-Translation': 1,\n",
       "  'Preliminary Experiments': 1,\n",
       "  'Results': 0,\n",
       "  'Ablation Study': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Fine-tuning a Pre-trained Model': 2,\n",
       "  'Related Work': 0,\n",
       "  'Conclusion': 0},\n",
       " '90247557': {'Introduction': 11,\n",
       "  'Word Similarity and Relatedness': 7,\n",
       "  'Data': 9,\n",
       "  'Hyper-parameters': 2,\n",
       "  'Evaluation on Benchmarks': 1,\n",
       "  'Syntactic Interchangeability': 2,\n",
       "  'Interchangeability Analysis in Word Similarity Benchmarks': 0,\n",
       "  'Nearest Neighbor Analysis': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '174799399': {'Introduction': 10,\n",
       "  'Post-Norm and Pre-Norm Transformer': 0,\n",
       "  'Model Layout': 5,\n",
       "  'On the Importance of Pre-Norm for Deep Residual Network': 0,\n",
       "  'Dynamic Linear Combination of Layers': 5,\n",
       "  'Experimental Setup': 1,\n",
       "  'Datasets and Evaluation': 5,\n",
       "  'Model and Hyperparameters': 4,\n",
       "  'Results on the En-De Task': 4,\n",
       "  'Results on the Zh-En-Small Task': 0,\n",
       "  'Results on the Zh-En-Large Task': 0,\n",
       "  'Effect of Encoder Depth': 0,\n",
       "  'Effect of Decoder Depth': 0},\n",
       " '7021843': {'Introduction': 8,\n",
       "  'Single-Task Learning': 0,\n",
       "  'Multi-Task Learning': 0,\n",
       "  'Three Perspectives of Multi-Task Learning': 0,\n",
       "  'Methodology': 0,\n",
       "  'Recurrent Neural Network': 2,\n",
       "  'Multi-Task Label Embedding': 0,\n",
       "  'Model-I: Unsupervised': 2,\n",
       "  'Model-II: Supervised': 1,\n",
       "  'Model-III: Semi-Supervised': 0,\n",
       "  'Experiment': 0,\n",
       "  'Datasets': 5,\n",
       "  'Hyperparameters and Training': 2,\n",
       "  'Results of Model-I and Model-II': 2,\n",
       "  'Scaling and Transferring Capability of MTLE': 0,\n",
       "  'Multi-Task or Label Embedding': 1,\n",
       "  'Comparisons with State-of-the-art Models': 9,\n",
       "  'Related Work': 14,\n",
       "  'Conclusion': 0},\n",
       " '53234825': {'Introduction': 4,\n",
       "  'The Need for Data Efficiency': 3,\n",
       "  'Alexa Prize Ratings': 2,\n",
       "  'A neural ranker for open-domain conversation': 1,\n",
       "  'Ranker architecture': 3,\n",
       "  'Training method': 1,\n",
       "  'Baselines': 2,\n",
       "  'Handcrafted ranker': 3,\n",
       "  'Linear ranker': 2,\n",
       "  'Dual-encoder ranker': 1,\n",
       "  'Training data': 0,\n",
       "  'Evaluation and experimental setup': 0,\n",
       "  'Evaluation based on explicit user feedback': 0,\n",
       "  'Interim results': 1,\n",
       "  'Training on larger amounts of data': 0,\n",
       "  'Discussion and future work': 3,\n",
       "  'Related work': 17,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '14025': {'Introduction': 24,\n",
       "  'Related Work': 16,\n",
       "  'Modeling and Characterizing Texts as Complex Networks': 15,\n",
       "  'Datasets': 13,\n",
       "  'Features': 13,\n",
       "  'Classification Algorithms': 1,\n",
       "  'Experiments and Results': 3,\n",
       "  'Conclusions and Future Work ': 4,\n",
       "  'Acknowledgments': 0,\n",
       "  'Supplementary Material': 0,\n",
       "  'Examples of transcriptions': 0,\n",
       "  'Coh-Metrix-Dementia metrics': 0},\n",
       " '1282002': {'Introduction': 16,\n",
       "  'Problem Formulation': 0,\n",
       "  'Paraphrase Generation': 18,\n",
       "  'Paraphrase Scoring': 5,\n",
       "  'QA Models': 7,\n",
       "  'Training and Inference': 2,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 3,\n",
       "  'Implementation Details': 8,\n",
       "  'Paraphrase Statistics': 2,\n",
       "  'Comparison Systems': 0,\n",
       "  'Results': 5,\n",
       "  'Conclusions': 2},\n",
       " '247735': {'Introduction': 13},\n",
       " '2863491': {'Introduction': 14,\n",
       "  'Neural Machine Translation and Limited Vocabulary Problem': 1,\n",
       "  'Neural Machine Translation': 9,\n",
       "  'Limited Vocabulary Issue and Conventional Solutions': 7,\n",
       "  'Description': 1,\n",
       "  'Decoding': 0,\n",
       "  'Source Words for Unknown Words': 2,\n",
       "  'Experiments': 6,\n",
       "  'Settings': 3,\n",
       "  'Translation Performance': 5,\n",
       "  'Note on Ensembles': 1,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 4,\n",
       "  'Acknowledgments': 2},\n",
       " '3772816': {'Introduction': 31, 'Story Generation Framework': 13},\n",
       " '2411818': {'Background': 7,\n",
       "  'From Truth-Theoretic to Corpus-based Meaning': 7,\n",
       "  'Concrete Computations': 0,\n",
       "  'Different Grammatical Structures': 0,\n",
       "  'Ambiguous Words': 1,\n",
       "  'Related Work': 4},\n",
       " '5031672': {'Introduction': 12,\n",
       "  'Related Work': 11,\n",
       "  'Meaning representation': 1,\n",
       "  'Natural deduction and word abduction': 4,\n",
       "  'Graph illustration': 0,\n",
       "  'Phrase Abduction': 0,\n",
       "  'Phrase pair detection': 0,\n",
       "  'Graph alignments': 0,\n",
       "  'Non-basic formulas': 0,\n",
       "  'Dataset selection': 3,\n",
       "  'Experimental setup': 12,\n",
       "  'Extracted paraphrases': 0,\n",
       "  'RTE evaluation results': 0,\n",
       "  'Positive examples and error analysis': 1,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '1460': {'Introduction': 12,\n",
       "  'Background: rewrite rules and two-level constraints': 9,\n",
       "  'Optimality theory': 5,\n",
       "  'gen for syllabification': 1,\n",
       "  'Syllabification constraints': 1,\n",
       "  'Constraint application': 0,\n",
       "  'Lenient composition': 4},\n",
       " '6406465': {'Abstract': 0,\n",
       "  'Introduction': 0,\n",
       "  'A Tutorial System for Computational Semantics': 3,\n",
       "  'Comparing theories': 0,\n",
       "  'The Library': 1,\n",
       "  'Tools for Semantic Construction': 5,\n",
       "  'Modularisation Principles': 0,\n",
       "  'The graphical interface': 2,\n",
       "  'Availability of the system': 2,\n",
       "  'Conclusion': 0},\n",
       " '12607082': {'Introduction': 13,\n",
       "  'Background Knowledge and Related Works': 0,\n",
       "  'Word Representation Learning': 9,\n",
       "  'Improving Chinese Word Representation Learning': 3,\n",
       "  'Model': 0,\n",
       "  'Character Bitmap Feature Extraction': 1,\n",
       "  'Glyph-Enhanced Word Embedding (GWE)': 0,\n",
       "  'Directly Learn From Character Glyph Features': 4,\n",
       "  'Preprocessing': 0,\n",
       "  'Extracting Visual Features of Character Bitmap': 2,\n",
       "  'Training Details of Word Representations': 1,\n",
       "  'Word Similarity': 3,\n",
       "  'Word Analogy': 2,\n",
       "  'Case Study': 0,\n",
       "  'Conclusions': 3},\n",
       " '9387600': {},\n",
       " '186206602': {'Introduction': 3,\n",
       "  'Background and Problem Definition': 5,\n",
       "  'Incremental Dialogue System': 0,\n",
       "  'Dialogue Embedding': 2,\n",
       "  'Uncertainty Estimation': 5,\n",
       "  'Online Learning': 4,\n",
       "  'Construction of Experimental Data': 2,\n",
       "  'Data Preprocessing': 0,\n",
       "  'Baselines': 8,\n",
       "  'Measurements': 2,\n",
       "  'Implementation Details': 1,\n",
       "  'Robustness to Unconsidered User Actions': 2,\n",
       "  'Deploying without Initialization': 2,\n",
       "  'Frequency of Human Intervention': 0,\n",
       "  'Visual Analysis of Context Embedding': 1,\n",
       "  'Related Work': 27,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Dialogue Example': 0,\n",
       "  'Data Statistics': 0},\n",
       " '59599691': {'Introduction': 3,\n",
       "  'Related Work': 4,\n",
       "  'Attentive Marker Model': 2,\n",
       "  'Experiments': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Notes': 5,\n",
       "  'Hyperparameters & Preprocessing': 0,\n",
       "  'Data statistics': 0},\n",
       " '2955580': {'Acknowledgments': 0},\n",
       " '3152424': {'Introduction': 6,\n",
       "  'segmentation based on LM': 2,\n",
       "  'Iterative procedure to build LM': 3,\n",
       "  'Segmentation Accuracy': 4,\n",
       "  'Experiment of the iterative procedure': 0,\n",
       "  'Perplexity of the language model': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '16251657': {'Introduction': 10,\n",
       "  'Model': 9,\n",
       "  'Proposed Unsupervised Model': 6,\n",
       "  'Computational Efficiency': 3,\n",
       "  'Comparison to C-BOW': 3,\n",
       "  'Model Training': 4,\n",
       "  'Related Work': 3,\n",
       "  'Unsupervised Models Independent of Sentence Ordering': 12,\n",
       "  'Unsupervised Models Depending on Sentence Ordering': 6,\n",
       "  'Models requiring structured data': 1,\n",
       "  'Evaluation Tasks': 13,\n",
       "  'Results and Discussion': 10,\n",
       "  'Conclusion': 0,\n",
       "  'L1 regularization of models': 1},\n",
       " '166228427': {'Introduction': 13,\n",
       "  \"HPAC: The Harry Potter's Action prediction Corpus\": 2,\n",
       "  'Domain motivation': 0,\n",
       "  'Data crawling': 1,\n",
       "  'Models': 0,\n",
       "  'Machine learning models': 0,\n",
       "  'Sequential models': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowlegments': 0,\n",
       "  'Corpus distribution': 0},\n",
       " '2728774': {},\n",
       " '52104940': {'Introduction': 14,\n",
       "  'Background: Neural MT': 7,\n",
       "  'Output Layer parametrizations': 3,\n",
       "  'Challenges': 2,\n",
       "  'Structure-aware Output Layer for Neural Machine Translation': 0,\n",
       "  'Joint Input-Output Embedding': 0,\n",
       "  'Sampling-based Training': 3,\n",
       "  'Relation to Weight Tying': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Datasets and Metrics': 5,\n",
       "  'Model Configurations': 2,\n",
       "  'Translation Performance': 0,\n",
       "  'Ablation Analysis': 4,\n",
       "  'Effect of Embedding Size': 0,\n",
       "  'Effect of Output Frequency and Architecture Depth': 0,\n",
       "  'Related Work': 10,\n",
       "  'Conclusion and Perspectives': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '4690531': {'Introduction': 28,\n",
       "  'Dataset': 0,\n",
       "  'Our modeling approach': 0,\n",
       "  'Neural network model': 1,\n",
       "  'Features': 5,\n",
       "  'Implementation details': 1,\n",
       "  'Metrics': 1,\n",
       "  'Results for subtask 1': 0,\n",
       "  'Results for subtask 2': 0,\n",
       "  'Discussions': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52157376': {'Introduction': 13,\n",
       "  'Related Work': 9,\n",
       "  'Data': 0,\n",
       "  'Task Setup': 0,\n",
       "  'Baseline': 0,\n",
       "  'Submission Results': 0,\n",
       "  'Review of Methods': 1,\n",
       "  'Top 3 Submissions': 6,\n",
       "  'Error Analysis': 0,\n",
       "  'Comparison to Human Performance': 0,\n",
       "  'Conclusions & Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Results by emotion class': 0,\n",
       "  'Examples': 0},\n",
       " '634325': {'Introduction': 3,\n",
       "  'Distributional Semantic Word Representation': 6,\n",
       "  'Distributional Paragraph Representation': 4,\n",
       "  'Problem with Paragraph Vectors': 0,\n",
       "  'Hierarchical Product Categorization': 8,\n",
       "  'Graded Weighted Bag of Word Vectors': 0,\n",
       "  'Ensemble of Multitype Predictors': 1,\n",
       "  'Dataset': 0,\n",
       "  'Results': 0,\n",
       "  'Non-Book Data Result': 0,\n",
       "  'Book Data Result': 0,\n",
       "  'Ensemble Classification': 0,\n",
       "  'Conclusions': 1,\n",
       "  'Future Work': 0,\n",
       "  'Label Embedding Based Approach': 3,\n",
       "  'Example of gwBoWV Approach': 0,\n",
       "  'Quality of WordVec Clusters': 0,\n",
       "  'Two Level Classification Approach': 2,\n",
       "  'Confused Category Group Discovery': 0,\n",
       "  'Data Visualization Tree Maps': 0,\n",
       "  'More Results for Ensemble Approach': 0,\n",
       "  'Classification Example from Book Data': 0},\n",
       " '7887385': {'Acknowledgments': 0},\n",
       " '49210906': {'Introduction': 21,\n",
       "  'Related Work': 18,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1516982': {'Introduction': 13,\n",
       "  'Related Work': 0,\n",
       "  'Distant Supervision Relation Extraction': 15,\n",
       "  'Deep Learning to Rank': 4,\n",
       "  'Proposed Model': 0,\n",
       "  'Notation': 0,\n",
       "  'CNN for Sentence Embedding': 2,\n",
       "  'Learning Class Ties by Joint Extraction with Pairwise Ranking': 10,\n",
       "  'Relieving Impact of NR': 5,\n",
       "  'Dataset and Evaluation Criteria': 6,\n",
       "  'Experimental Settings': 3,\n",
       "  'Comparisons with Baselines': 5,\n",
       "  'Impact of Joint Extraction and Class Ties': 1,\n",
       "  'Comparisons of Variant Joint Extractions': 0,\n",
       "  'Impact of NR Relation': 0,\n",
       "  'Case Study': 0,\n",
       "  'Conclusion and Future Works': 4,\n",
       "  'Acknowledgments': 0},\n",
       " '5890185': {'Introduction': 9,\n",
       "  'Related Work': 29,\n",
       "  'Methodology': 0,\n",
       "  'Word Prediction': 12,\n",
       "  'Sentence Generation': 2,\n",
       "  'Answer Reasoning': 0,\n",
       "  'Experiment Setting': 6,\n",
       "  'Word-based VQA': 0,\n",
       "  'Sentence-based VQA': 0,\n",
       "  'Case Study': 0,\n",
       "  'Performance Comparison': 7,\n",
       "  'Discussions and Conclusions': 0},\n",
       " '139106285': {'Introduction': 9,\n",
       "  'Related Work': 7,\n",
       "  'Methodology and Data': 0,\n",
       "  'System Description': 7,\n",
       "  'Results': 0,\n",
       "  'Comparison to a Neural Model': 8,\n",
       "  'Conclusion and Future Work': 5,\n",
       "  'Acknowledgements': 0},\n",
       " '52113877': {},\n",
       " '21382535': {'Introduction': 4,\n",
       "  'Motivation': 5,\n",
       "  'Methodology': 3,\n",
       "  'Datasets': 0,\n",
       "  'Human Elicited': 2,\n",
       "  'Human Judged': 13,\n",
       "  'Automatically Recast': 2,\n",
       "  'Results': 0,\n",
       "  'Statistical Irregularities': 0,\n",
       "  'Can Labels be Inferred from Single Words?': 0,\n",
       "  'What are “Give-away” Words?': 2,\n",
       "  'On the Role of Grammaticality': 0},\n",
       " '14922772': {'Conclusion': 0},\n",
       " '2652169': {'Introduction': 0,\n",
       "  'The Systems Surveyed': 0,\n",
       "  'An Overview of the Consensus Architecture': 0,\n",
       "  'A More Detailed Examination of the Architecture': 0,\n",
       "  'Modularized Pipeline Architecture': 3,\n",
       "  'Content Determination': 3,\n",
       "  'Sentence Planning': 8,\n",
       "  'Surface Generation': 4,\n",
       "  'Morphology and Formatting': 0,\n",
       "  'A Controversial (?) View': 1,\n",
       "  'Acknowledgements': 0},\n",
       " '196177814': {},\n",
       " '3204831': {},\n",
       " '49557308': {'Introduction': 2,\n",
       "  'Related Work': 13,\n",
       "  'Noise Layer': 3,\n",
       "  'Dataset and Automatic Annotation': 3,\n",
       "  'Model Architectures and Training': 6,\n",
       "  'Experiments and Evaluation': 0,\n",
       "  'Model Comparison': 2,\n",
       "  'Amount of Noisy Data': 0,\n",
       "  'Learned Weights': 0,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '52117484': {},\n",
       " '1452429': {},\n",
       " '118683163': {'Introduction': 14,\n",
       "  'Discourse Coherence and Text–Image Presentations': 11,\n",
       "  'Annotation Effort': 1,\n",
       "  'Conclusions': 2,\n",
       "  'Acknowledgement': 0},\n",
       " '12245103': {},\n",
       " '80628357': {},\n",
       " '3502245': {'Introduction': 22,\n",
       "  'Related work': 0,\n",
       "  'Distributed language representations': 14,\n",
       "  'Typological features in the World Atlas of Language Structure': 1,\n",
       "  'Method': 1,\n",
       "  'Grapheme-to-phoneme': 3,\n",
       "  'Phonological reconstruction': 1,\n",
       "  'Morphological inflection': 1,\n",
       "  'Part-of-speech tagging': 3,\n",
       "  'Sequence-to-sequence modelling': 3,\n",
       "  'Sequence labelling': 5,\n",
       "  'Discussion and Conclusions': 1,\n",
       "  'RQ1: Encoding of typological features in task-specific language embeddings': 0,\n",
       "  'RQ2: Change in encoding of typological features': 0,\n",
       "  'RQ3: Language similarities': 1},\n",
       " '3266239': {'Introduction': 0,\n",
       "  'A Formalism for DOP': 0,\n",
       "  'Accounting for Linguistic Structure': 0,\n",
       "  'Coverage': 0,\n",
       "  'Discontinuous Constituents': 0,\n",
       "  'Non-Constituent Coordination': 0,\n",
       "  'Centre-Embedding': 0,\n",
       "  'Parsing': 0,\n",
       "  'Smoothing Probability Distributions': 0,\n",
       "  'Problems Arising from Smoothing': 0,\n",
       "  'Capturing Lexical Preferences': 0,\n",
       "  'Present Implementation': 0,\n",
       "  'Results': 0,\n",
       "  'Future Development': 0,\n",
       "  'References': 0},\n",
       " '29149810': {'Introduction': 11,\n",
       "  'Related Work': 0,\n",
       "  'Neural Networks': 11,\n",
       "  'Aspect based Sentiment Analysis': 13,\n",
       "  'Gated Convolutional Network with Aspect Embedding': 8,\n",
       "  'Gating Mechanisms': 6,\n",
       "  'GCAE on ATSA': 0,\n",
       "  'Datasets and Experiment Preparation': 9,\n",
       "  'Compared Methods': 6,\n",
       "  'Results and Analysis': 5,\n",
       "  'Training Time': 0,\n",
       "  'Visualization': 0,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '49330155': {'Introduction': 9,\n",
       "  'Tasks and Baseline Models': 0,\n",
       "  'Question Matching and Ranking': 2,\n",
       "  'Support Vector machines': 5,\n",
       "  'Injecting Structures in NNs': 0,\n",
       "  'NNs for question similarity': 2,\n",
       "  'Relational Information': 1,\n",
       "  'Learning NNs with structure': 1,\n",
       "  'Experiments': 0,\n",
       "  'Data': 0,\n",
       "  'NN setup': 1,\n",
       "  'Results on Quora': 0,\n",
       "  'Results on Qatar Living': 0,\n",
       "  'Related Work': 12},\n",
       " '85543217': {'Introduction': 12,\n",
       "  'Approach': 0,\n",
       "  'Step 1: Train': 5,\n",
       "  'Step 2: Sort': 0,\n",
       "  'Step 3: Explain': 11,\n",
       "  'Implementation': 1,\n",
       "  'Datasets and Experiments': 8,\n",
       "  'Demonstration and Observations': 0,\n",
       "  'Future Work': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '11913192': {'Introduction': 14,\n",
       "  'Baseline Parsing Model': 5,\n",
       "  'LSTM Character Composition Model': 2,\n",
       "  'CNN Character Composition Model': 1,\n",
       "  'Experimental Setup': 4,\n",
       "  'Internal Comparisons': 0,\n",
       "  'External Comparisons': 7,\n",
       "  'Discussion on CNN and LSTM': 0,\n",
       "  'Analyses on OOV and Morphology': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Character Input Preprocessing': 0,\n",
       "  'Hyper-Parameters': 0},\n",
       " '4875809': {},\n",
       " '174798262': {'Introduction': 29,\n",
       "  'LeafNATS Toolkithttps://github.com/tshi04/LeafNATS': 13,\n",
       "  'A Live System Demonstrationhttp://dmkdt3.cs.vt.edu/leafNATS': 0,\n",
       "  'Architecture': 0,\n",
       "  'Design of Frontend': 1,\n",
       "  'The Proposed Model': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1804978': {'Introduction': 12,\n",
       "  'Overview': 3,\n",
       "  'Modeling Zero Pronouns and Antecedents': 3,\n",
       "  'Our Probabilistic Model for Zero Pronoun Detection and Resolution': 0,\n",
       "  'Computing Certainty Score': 0,\n",
       "  'Methodology': 2,\n",
       "  'Comparative Experiments': 0,\n",
       "  'Related Work': 0,\n",
       "  'Conclusion': 0},\n",
       " '17297069': {'Introduction': 9,\n",
       "  'Related Work': 8,\n",
       "  'Convolutional Neural Networks (CNN)': 0,\n",
       "  'Input: Extended Middle Context': 0,\n",
       "  'Convolutional Layer': 3,\n",
       "  'Recurrent Neural Networks (RNN)': 2,\n",
       "  'Input of the RNNs': 0,\n",
       "  'Connectionist Bi-directional RNNs': 1,\n",
       "  'Word Representations': 2,\n",
       "  'Objective Function: Ranking Loss': 0,\n",
       "  'Experiments and Results': 3,\n",
       "  'Performance of CNNs': 0,\n",
       "  'Performance of RNNs': 1,\n",
       "  'Combination of CNNs and RNNs': 0,\n",
       "  'Comparison with State of the Art': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '607080': {'Introduction': 4,\n",
       "  'Entity Linking': 3,\n",
       "  'Measuring Entity–Hashtag Similarities': 2,\n",
       "  'Ranking Entity Prominence': 0,\n",
       "  'Similarity Measures': 0,\n",
       "  'Link-based Mention Similarity': 4,\n",
       "  'Ranking Framework': 18},\n",
       " '52144157': {'Introduction': 8,\n",
       "  'Related Work': 29,\n",
       "  'Background: Neural Summarization': 2,\n",
       "  'Bottom-Up Attention': 0,\n",
       "  'Content Selection': 4,\n",
       "  'Bottom-Up Copy Attention': 1,\n",
       "  'End-to-End Alternatives': 1,\n",
       "  'Inference': 6,\n",
       "  'Data and Experiments': 12,\n",
       "  'Results': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Domain Transfer Examples': 0},\n",
       " '17355': {'Introduction': 5,\n",
       "  'Related Work': 13,\n",
       "  'Proposed Model': 5,\n",
       "  'Empirical Study': 6,\n",
       "  'Conclusion and Future work': 0},\n",
       " '3411040': {'Introduction': 4,\n",
       "  'Method': 6,\n",
       "  'Implementation': 0,\n",
       "  'Settings': 9,\n",
       "  'Comparison on Training Speed': 0,\n",
       "  'Comparison on Translation Quality': 8,\n",
       "  'Conclusion': 2},\n",
       " '10870417': {'Introduction': 13,\n",
       "  'The Approach': 0,\n",
       "  'Long Short-Term Memory (LSTM)': 13,\n",
       "  'Target-Dependent LSTM (TD-LSTM)': 0,\n",
       "  'Target-Connection LSTM (TC-LSTM)': 2,\n",
       "  'Model Training': 0,\n",
       "  'Experiment': 0,\n",
       "  'Experimental Settings': 3,\n",
       "  'Comparison to Other Methods': 4,\n",
       "  'Effects of Word Embeddings': 5,\n",
       "  'Case Study': 0,\n",
       "  'Discussion': 3,\n",
       "  'Related Work': 0,\n",
       "  'Target-Dependent Sentiment Classification': 4,\n",
       "  'Neural Network for Sentiment Classification': 17,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '44133327': {'Introduction': 8,\n",
       "  'Task Formulation': 0,\n",
       "  'Model': 3,\n",
       "  'Experiments': 8,\n",
       "  'Related Work': 17,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '621025': {'Introduction': 4,\n",
       "  'Constraints on narrative continuations': 5,\n",
       "  'An hpsg implementation of a discourse grammar': 2,\n",
       "  'The algorithm': 0,\n",
       "  'An underspecified representation': 0,\n",
       "  'Conclusion': 0},\n",
       " '26397607': {},\n",
       " '13029170': {'Introduction': 1,\n",
       "  'The case for explanations': 6,\n",
       "  'Local Interpretable  Model-Agnostic Explanations': 0,\n",
       "  'Interpretable Data Representations': 0,\n",
       "  'Fidelity-Interpretability Trade-off': 1,\n",
       "  'Sampling for Local Exploration': 0,\n",
       "  'Sparse Linear Explanations': 2,\n",
       "  'Example 1: Text classification with SVMs': 0,\n",
       "  'Example 2: Deep networks for images': 1,\n",
       "  'Submodular Pick for Explaining Models': 4,\n",
       "  'Simulated User Experiments': 0,\n",
       "  'Experiment Setup': 4,\n",
       "  'Are explanations faithful to the model?': 0,\n",
       "  'Should I trust this prediction?': 0,\n",
       "  'Can I trust this model?': 0,\n",
       "  'Evaluation with human subjects': 0,\n",
       "  'Experiment setup': 0,\n",
       "  'Can users select the best classifier?': 0,\n",
       "  'Can non-experts improve a classifier?': 0,\n",
       "  'Do explanations lead to insights?': 1,\n",
       "  'Related Work': 23,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '2244226': {'Kernel embeddings of functions': 0,\n",
       "  'Embedding probability measures': 0,\n",
       "  'Maximum mean discrepancy': 1,\n",
       "  'Hilbert-Schmidt Independence Criterion': 0},\n",
       " '85556928': {'Introduction': 3,\n",
       "  'Design Principles and Challenges': 5,\n",
       "  'Dataset construction': 0,\n",
       "  'Statistics on the dataset': 0,\n",
       "  'Required skills': 2,\n",
       "  'Empirical Analysis': 0,\n",
       "  'Systems': 2,\n",
       "  'Evaluation metrics.': 0,\n",
       "  'Results': 0,\n",
       "  'Discussion': 6,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Statistics': 0,\n",
       "  'Measure of agreement': 1},\n",
       " '52157368': {'Introduction': 3,\n",
       "  'Related work': 6,\n",
       "  'Model': 1,\n",
       "  'Experiments': 5,\n",
       "  'Effect of Forgetting Factors': 0,\n",
       "  'Evaluation Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '53113020': {'Introduction': 3,\n",
       "  'Background: Morphological Inflection': 5,\n",
       "  'Two Schemata, Two Philosophies': 0,\n",
       "  'Universal Dependencies': 2,\n",
       "  'UniMorph': 9,\n",
       "  'Similarities in the annotation': 0,\n",
       "  'UD treebanks and UniMorph tables': 4,\n",
       "  'A Deterministic Conversion': 2,\n",
       "  'Experiments': 0,\n",
       "  'Intrinsic evaluation': 0,\n",
       "  'Extrinsic evaluation': 2,\n",
       "  'Results': 0,\n",
       "  'Related Work': 6,\n",
       "  'Conclusion and Future Work': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '13661068': {'Introduction': 5,\n",
       "  'Sequence-to-sequence models for NMT': 6,\n",
       "  'Chinese-English translation': 6,\n",
       "  'Encoding Chinese characters with Wubi': 1,\n",
       "  'Dataset': 2,\n",
       "  'Model descriptions and training details': 7,\n",
       "  'Quantitative evaluation': 1,\n",
       "  'Qualitative evaluation': 0,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '1765384': {'Introduction': 0,\n",
       "  'The Verbmobil Project': 1,\n",
       "  'Theoretical Background': 2,\n",
       "  'LUD-Representations': 0,\n",
       "  'Lexical Entries and Composition': 0,\n",
       "  'Related Work': 7,\n",
       "  'Grammar': 1,\n",
       "  'Interfaces to Other Components': 0,\n",
       "  'Implementation Status': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '75135250': {'Introduction': 16,\n",
       "  'Transition-based Parser': 4,\n",
       "  'Neural Transition-based Parser with Stack-LSTMs': 4,\n",
       "  'Transfer learning': 0,\n",
       "  'Data': 2,\n",
       "  'Experiments': 0,\n",
       "  'Attention': 2,\n",
       "  'Handling Sparsity': 0,\n",
       "  'Transfer Learning': 0,\n",
       "  'Related work': 23,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '3025759': {'Introduction': 7,\n",
       "  'Related Work': 2,\n",
       "  'Review of ListNet': 0,\n",
       "  'Stochastic Top-k ListNet': 2,\n",
       "  'Sampling methods for stochastic ListNet': 0,\n",
       "  'Gradients with linear networks': 0,\n",
       "  'Stochastic Top-k ListNet algorithm': 0,\n",
       "  'Data': 1,\n",
       "  'Experiment Setup': 0,\n",
       "  'Experimental results': 0,\n",
       "  'Discussion': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5152691': {'Introduction': 0,\n",
       "  'Lexical Semantics for Motion Verbs': 0,\n",
       "  'Lexical Semantics for Spatial Prepositions': 0,\n",
       "  'Compositional Semantics for Motion Complexes': 0,\n",
       "  'Conclusion': 0,\n",
       "  'References': 0},\n",
       " '131773929': {},\n",
       " '53549222': {'Introduction': 2,\n",
       "  'Related Work': 14,\n",
       "  'Discourse Embellishment': 7,\n",
       "  'Story Corpus': 2,\n",
       "  'Sentence Based: Lexical Embellishment': 3,\n",
       "  'Paragraph Based: Degradation': 1,\n",
       "  'Pair Based: Syntactical Embellishment': 0,\n",
       "  'Conclusion': 2,\n",
       "  'Appendix': 1},\n",
       " '52143204': {},\n",
       " '196174735': {'Introduction': 8,\n",
       "  'TweetQA': 0,\n",
       "  'Data Collection': 1,\n",
       "  'Task and Evaluation': 3,\n",
       "  'Analysis': 3,\n",
       "  'Experiments': 0,\n",
       "  'Query Matching Baseline': 1,\n",
       "  'Neural Baselines': 7,\n",
       "  'Overall Performance': 0,\n",
       "  'Performance Analysis over Human-Labeled Question Types': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Full results of Performance Analysis over Human-Labeled Question Types': 0,\n",
       "  'Performance Analysis over Automatically-Labeled Question Types': 0},\n",
       " '125952286': {'Introduction': 9,\n",
       "  'Data': 0,\n",
       "  'Model': 1,\n",
       "  'Overview': 2,\n",
       "  'Modeling Text with Predicates': 1,\n",
       "  'Reconstructing Text with Relation Embeddings': 2,\n",
       "  'Learning Objective and Summary': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Understanding Model Outputs': 0,\n",
       "  'Quantitative Evaluation': 0,\n",
       "  'Human Evaluation': 0,\n",
       "  'Further Exploration': 0,\n",
       "  'Context for Relations': 0,\n",
       "  'Regional Differences in News Coverage': 2,\n",
       "  'Related Work': 27,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Data: Aliases and Key Events': 0,\n",
       "  'Preprocessing': 3,\n",
       "  'Implementation Details': 1,\n",
       "  'Relation Descriptors Postprocessing': 0,\n",
       "  'Additional Temporal Relation Trends: US-Russia, US-India, US-Syria': 0,\n",
       "  'Additional Influential Background Words:US-Russia, US-India, US-Syria': 0,\n",
       "  'Robustness Check: Another Set of Key Events Annotations': 0,\n",
       "  'A Simpler Baseline: Term-frequency Trend of Verbal Predicates': 0},\n",
       " '52186890': {'Introduction': 10,\n",
       "  'Related Work': 39,\n",
       "  'Proposed Method': 1,\n",
       "  'Sinkhorn Distance': 2,\n",
       "  'Objective Function': 0,\n",
       "  'Wasserstein GAN Training for Good Initial Point': 4,\n",
       "  'Implementation': 0,\n",
       "  'Experiments': 0,\n",
       "  'Data': 6,\n",
       "  'Baseline Methods': 8,\n",
       "  'Results in Bilingual Lexicons Induction (Task 1)': 1,\n",
       "  'Results in Cross-lingual Word Similarity Prediction (Task 2) ': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '195848083': {'Introduction': 14,\n",
       "  'Datasets and Models': 9,\n",
       "  'Nearest Neighbors Analysis': 2,\n",
       "  'Hidden States vs Embeddings': 1,\n",
       "  'WordNet Coverage': 0,\n",
       "  'Syntactic Similarity': 7,\n",
       "  'Concentration of Nearest Neighbors': 0,\n",
       "  'Empirical Analyses': 0,\n",
       "  'Embedding Nearest Neighbors Coverage': 1,\n",
       "  'Direction-Wise Analyses': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '174801552': {'Introduction': 10,\n",
       "  'Task and Data': 1,\n",
       "  'Corpora': 3,\n",
       "  'DatasetsThe datasets are available in Appendix  and at https://github.com/Garrafao/LSCDetection. and Evaluation': 6,\n",
       "  'Meaning RepresentationsFind the hyperparameter settings in Appendix . The scripts for vector space creation, alignment, measuring LSC and evaluation are available at https://github.com/Garrafao/LSCDetection.': 0,\n",
       "  'Semantic Vector Spaces': 30,\n",
       "  'Topic Distributions': 1,\n",
       "  'LSC Detection Measures': 0,\n",
       "  'Similarity Measures': 5,\n",
       "  'Dispersion Measures': 3,\n",
       "  'Results and Discussions': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Model Overview': 0,\n",
       "  'Datasets': 0},\n",
       " '102351524': {'Introduction': 6,\n",
       "  'Proposed Method: PretRand': 0,\n",
       "  'Base Model': 1,\n",
       "  'Adding Random Branch': 2,\n",
       "  'Independent Normalisation': 2,\n",
       "  'Learnable Weighting Vectors': 0,\n",
       "  'Experiments': 0,\n",
       "  'Implementation Details': 1,\n",
       "  'Datasets': 3,\n",
       "  'Comparison Methods': 0,\n",
       "  'Results': 0,\n",
       "  'Analysis': 2,\n",
       "  'Conclusion': 0},\n",
       " '3160550': {'Introduction and related work': 6,\n",
       "  'Contributions': 0,\n",
       "  'Gold standard data on armed conflicts': 5,\n",
       "  'Predicting armed conflict participants': 0,\n",
       "  'Synchronic modeling': 5,\n",
       "  'Diachronic modeling': 2,\n",
       "  'Evaluation of diachronic models': 0,\n",
       "  'Conclusion': 0},\n",
       " '189999934': {'Introduction': 16,\n",
       "  'Methods': 18,\n",
       "  'Datasets': 6,\n",
       "  'Experimental results': 4,\n",
       "  'Related and Future Work': 4,\n",
       "  'Hyper-parameters': 2},\n",
       " '1871596': {'Introduction': 14,\n",
       "  'Heuristics for Genus Sense Disambiguation': 0,\n",
       "  'Heuristic 1: Monosemous Genus Term': 0,\n",
       "  'Heuristic 2: Entry Sense Ordering': 0,\n",
       "  'Heuristic 3: Explicit Semantic Domain': 0,\n",
       "  'Heuristic 4: Word Matching': 0,\n",
       "  'Heuristic 5: Simple Cooccurrence': 2,\n",
       "  'Heuristic 6: Cooccurrence Vectors': 1,\n",
       "  'Heuristic 7: Semantic Vectors': 2,\n",
       "  'Heuristic 8: Conceptual Distance': 2,\n",
       "  'Combining the heuristics: Summing': 0,\n",
       "  'Test Set': 0,\n",
       "  'Results': 1,\n",
       "  'Evaluation': 0,\n",
       "  'Cooccurrence Data': 1,\n",
       "  'Multilingual Data': 0,\n",
       "  'Comparison with Previous Work': 18,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '102487125': {'Introduction': 2,\n",
       "  'Related Work': 20,\n",
       "  'Task Description': 0,\n",
       "  'Datasets': 2,\n",
       "  'Experiments': 0,\n",
       "  'SegRNN': 2,\n",
       "  'Baselines': 4,\n",
       "  'Metrics': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '802243': {'Introduction': 3,\n",
       "  'Task Description': 2,\n",
       "  'Related Work': 0,\n",
       "  'Text to Scene Systems': 2,\n",
       "  'Related Tasks': 9,\n",
       "  'Dataset': 0,\n",
       "  'Model': 0,\n",
       "  'Learning lexical groundings': 0,\n",
       "  'Rule-based Model': 2,\n",
       "  'Combined Model': 0,\n",
       "  'Learned lexical groundings': 0,\n",
       "  'Experimental Results': 2,\n",
       "  'Qualitative Evaluation': 0,\n",
       "  'Human Evaluation': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Scene Similarity Metric': 0,\n",
       "  'Future Work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '102354361': {'Introduction': 3,\n",
       "  'Japanese PASA and ENASA Approaches': 1,\n",
       "  'PASA using neural networks': 0,\n",
       "  'Semantic Role Labeling': 8,\n",
       "  'Task Description': 7,\n",
       "  'End-to-end Single Model': 5,\n",
       "  'Multi-task Model': 8,\n",
       "  'Multi Input Layer': 0,\n",
       "  'Multi RNN Layer': 2,\n",
       "  'Multi Output Layer': 0,\n",
       "  'Dataset and Setting': 0,\n",
       "  'Hyperparameters': 0,\n",
       "  'Results': 4,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '102352181': {'Description of Heuristics Baseline': 0,\n",
       "  'Annotation Costing Details': 0,\n",
       "  'Additional Dataset Details': 0,\n",
       "  'Preprocessing Details': 0,\n",
       "  'Attention (and Attention Pretraining) Variants': 0,\n",
       "  'Validation Results and Variances': 0},\n",
       " '27914547': {'Introduction': 1,\n",
       "  'Preprocessing': 0,\n",
       "  'Feature Extraction': 18,\n",
       "  'Regression': 2,\n",
       "  'Parameter Optimization': 1,\n",
       "  'Experimental Results': 1,\n",
       "  'Feature Importance': 3,\n",
       "  'System Limitations': 0,\n",
       "  'Future Work & Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '4709462': {'Introduction': 12,\n",
       "  'Encoder-Decoder Models for Structured Output Prediction': 20,\n",
       "  'Domain Adaptation for Neural Network Models': 11,\n",
       "  'Model Architecture': 4,\n",
       "  'SHAPED: Shared-private encoder-decoder': 0,\n",
       "  'The Mix-SHAPED Model': 0,\n",
       "  'Model Instantiation': 2,\n",
       "  'Quantitative Experiments': 2,\n",
       "  'Headline-generation Setup': 8,\n",
       "  'Conclusion': 0},\n",
       " '47018994': {},\n",
       " '14257538': {'Introduction': 19,\n",
       "  'Background': 8,\n",
       "  'Agreement-based Learning': 0,\n",
       "  'Outer Agreement': 3,\n",
       "  'Inner Agreement': 1,\n",
       "  'Experiments': 0,\n",
       "  'Alignment Evaluation': 1,\n",
       "  'Translation Evaluation': 3,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '5509327': {'Introduction': 7,\n",
       "  'Related Work': 9,\n",
       "  'Interpreting structured prediction': 2,\n",
       "  'Perturbation Model': 5,\n",
       "  'Causal model': 1,\n",
       "  'Explanation Selection': 2,\n",
       "  'Training and optimization': 3,\n",
       "  'Recovering simple mappings': 1,\n",
       "  'Machine Translation': 0,\n",
       "  'A (mediocre) dialogue system': 1,\n",
       "  'Bias detection in parallel corpora': 3,\n",
       "  'Discussion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '53082704': {},\n",
       " '118680003': {},\n",
       " '11451871': {'Introduction': 10,\n",
       "  'CCG': 0,\n",
       "  'λ\\\\lambda PROLOG and Abstract Syntax': 2,\n",
       "  'Implementation of Coordination': 3,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '57189520': {'Introduction': 2,\n",
       "  'The meaning of “most”': 0,\n",
       "  'Generalized quantifiers and “most”': 0,\n",
       "  'Alternative characterization': 0,\n",
       "  'Two interpretation strategies': 1,\n",
       "  'Cognitive implications': 3,\n",
       "  'Experimental setup': 1,\n",
       "  'Training and evaluation data': 3,\n",
       "  'Model': 6,\n",
       "  'Training details': 0,\n",
       "  'Related work': 15,\n",
       "  'Conclusion': 3},\n",
       " '102350767': {'Introduction': 8,\n",
       "  'Adding Plans to Training Data': 1,\n",
       "  'Test-time Plan Selection': 0,\n",
       "  'Plan Realization': 0,\n",
       "  'Experimental Setup': 1,\n",
       "  'Automatic Metrics': 5,\n",
       "  'Manual Evaluation': 0,\n",
       "  'Plan Realization Consistency': 0,\n",
       "  'Related Work': 21,\n",
       "  'Conclusion': 0,\n",
       "  'Diverse Outputs': 0,\n",
       "  'Manual Evaluation Setup': 0,\n",
       "  'Fluency Evaluation by Crowd': 0,\n",
       "  'Faithfulness Evaluation by Expert': 0,\n",
       "  'Training Parameters': 2},\n",
       " '2973141': {'Related Work': 15,\n",
       "  'Comparing pruning schemes': 0,\n",
       "  'Pruning and retraining': 0,\n",
       "  'Starting with sparse models': 2,\n",
       "  'Storage size': 0,\n",
       "  'Distribution of redundancy in NMT': 1,\n",
       "  'Generalizability of our results': 1,\n",
       "  'Future Work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '52114019': {'Introduction': 9,\n",
       "  'An L2-L1 Parallel Corpus': 1,\n",
       "  'The Annotation Process': 1,\n",
       "  'Inter-annotator Agreement': 0,\n",
       "  'Three SRL Systems': 16,\n",
       "  'Main Results': 0,\n",
       "  'Analysis': 1,\n",
       "  'Enhancing SRL with L2-L1 Parallel Data': 0,\n",
       "  'The Method': 1,\n",
       "  'Experimental Setup': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '195316269': {'Introduction': 8,\n",
       "  'Methodology': 5,\n",
       "  'LASER Multilingual Representations': 3,\n",
       "  'Other Similarity Methods': 5,\n",
       "  'Ensemble': 1,\n",
       "  'Experimental Setup': 0,\n",
       "  'Preprocessing': 1,\n",
       "  'LASER Encoder Training': 2,\n",
       "  'Results': 1,\n",
       "  'Discussion': 0,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '4551282': {'Introduction': 0,\n",
       "  'Background': 25,\n",
       "  'Data': 0,\n",
       "  'Model Specification': 0,\n",
       "  'Experiment': 0,\n",
       "  'Data Preprocessing': 1,\n",
       "  'Inputs': 3,\n",
       "  'Model Setting': 1,\n",
       "  'Baseline': 1,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Topics in the Data': 0,\n",
       "  'AIM': 0,\n",
       "  'TFIDF Features': 0,\n",
       "  'Vulnerability Examples': 0,\n",
       "  'Topic Similarity between Sentences': 0,\n",
       "  'Top TFIDF nn-grams': 0},\n",
       " '6262432': {'Introduction': 11,\n",
       "  'Neural models for text classification': 2,\n",
       "  'Exploiting rationales': 6,\n",
       "  'Preliminaries: CNNs for text classification': 4,\n",
       "  'Rationale-Augmented CNN for Document Classification': 0,\n",
       "  'Modeling Document Structure': 0,\n",
       "  'RA-CNN': 2,\n",
       "  \"Rationales as `Supervised Attention'\": 1,\n",
       "  'Datasets': 0,\n",
       "  'Risk of Bias (RoB) Datasets': 2,\n",
       "  'Movie Review Dataset': 0,\n",
       "  'Baselines': 7,\n",
       "  'Implementation/Hyper-Parameter Details': 3,\n",
       "  'Quantitative Results': 2,\n",
       "  'Qualitative Results: Illustrative Rationales': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52112578': {'Introduction': 7,\n",
       "  'Unified DAG Format': 2,\n",
       "  'General Transition-based DAG Parser': 3,\n",
       "  'Transition Set': 0,\n",
       "  'Transition Classifier': 2,\n",
       "  'Constraints': 1,\n",
       "  'Training details': 2,\n",
       "  'Hyperparameters': 6,\n",
       "  'Small Treebanks': 0,\n",
       "  'Multilingual Model': 0,\n",
       "  'Out-of-domain Evaluation': 0,\n",
       "  'Results': 2,\n",
       "  'Evaluation on Enhanced Dependencies': 1,\n",
       "  'Ablation Experiments': 1,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '5040774': {'Introduction': 6,\n",
       "  'Motivation and Related Work': 24,\n",
       "  'Entity Linking Architecture': 0,\n",
       "  'Variable Context Granularity Network': 4,\n",
       "  'Global entity assignment': 2,\n",
       "  'Composite Loss Function': 0,\n",
       "  'Architecture comparison': 13},\n",
       " '71147690': {'Introduction': 16,\n",
       "  'Previous non-NLP Benchmarks': 7,\n",
       "  'The Proposed Lifelong Relation Detection Benchmarks': 3,\n",
       "  'Simple Episodic Memory Replay Algorithm for Lifelong Learning': 1,\n",
       "  'Episodic Memory Replay (EMR)': 1,\n",
       "  'Comparing EMR with State-of-the-art Memory-based Lifelong Algorithm': 4,\n",
       "  'Embedding Aligned EMR (EA-EMR)': 2,\n",
       "  'Embedding Alignment for Lifelong Learning': 3,\n",
       "  'Selective Storing Samples in Memory': 1,\n",
       "  'Experimental Setting': 5,\n",
       "  'Lifelong Relation Detection Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0,\n",
       "  'Experiment setting for MNIST and CIFAR': 1},\n",
       " '6488759': {'Introduction': 10,\n",
       "  'Multi-objective Reinforcement Learning with Gaussian Processes': 0,\n",
       "  'Reward Balancing using MORL': 0,\n",
       "  'Experiments and Results': 5,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0,\n",
       "  'Data': 0},\n",
       " '186206781': {'Conclusion': 0, 'Acknowledgments': 0, 'Training Details': 4},\n",
       " '13747354': {},\n",
       " '3132651': {'Introduction': 13,\n",
       "  'A Performance Model for Dialogue': 8,\n",
       "  'Tasks as Attribute Value Matrices': 0,\n",
       "  'Measuring Task Success': 7,\n",
       "  'Measuring Dialogue Costs': 10,\n",
       "  'Estimating a Performance Function': 1,\n",
       "  'Application to Subdialogues': 0,\n",
       "  'Summary': 0,\n",
       "  'Generality': 1,\n",
       "  'Discussion': 5,\n",
       "  'Acknowledgments': 0},\n",
       " '7033945': {'Introduction': 3,\n",
       "  'Relevance Scoring Methods': 0,\n",
       "  'Baseline methods': 0,\n",
       "  'TF-IDF': 1,\n",
       "  'Word2Vec': 1,\n",
       "  'IDF-Embeddings': 0,\n",
       "  'Skip-Thoughts': 3,\n",
       "  'Weighted-Embeddings': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Results': 0,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0},\n",
       " '20995314': {'Introduction': 8,\n",
       "  'Related Work': 39,\n",
       "  'Neural Variational Models': 0,\n",
       "  'Neural Variational Learning': 1,\n",
       "  'Piecewise Constant Distribution': 0,\n",
       "  'Latent Variable Parametrizations': 0,\n",
       "  'Gaussian Parametrization': 1,\n",
       "  'Piecewise Constant Parametrization': 0,\n",
       "  'Variational Text Modeling': 0,\n",
       "  'Document Model': 5,\n",
       "  'Dialogue Model': 3,\n",
       "  'Experiments': 2,\n",
       "  'Document Modeling': 10,\n",
       "  'Dialogue Modeling': 20,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Appendix: Inappropriate Gaussian Priors': 0,\n",
       "  'Appendix: Piecewise Constant Variable Derivations': 1,\n",
       "  'Appendix: NVDM Implementation': 1,\n",
       "  'Appendix: VHRED Implementation': 0,\n",
       "  'Appendix: Training Details': 0,\n",
       "  'Appendix: Additional Document Modeling Experiments': 0,\n",
       "  'Appendix: Additional Dialogue Modeling Experiments': 4},\n",
       " '85529973': {},\n",
       " '14401063': {'Introduction': 7,\n",
       "  'Standard Neural LMs': 7,\n",
       "  'Complexity Concerns of Neural LMs': 4,\n",
       "  'Related Work': 0,\n",
       "  'Our Proposed Model': 0,\n",
       "  'Sparse Representations of Words': 12},\n",
       " '90262493': {},\n",
       " '2213896': {'Introduction': 5,\n",
       "  'The Joint Many-Task Model': 2,\n",
       "  'Word Representations': 1,\n",
       "  'Word-Level Task: POS Tagging': 2,\n",
       "  'Word-Level Task: Chunking': 2,\n",
       "  'Syntactic Task: Dependency Parsing': 3,\n",
       "  'Semantic Task: Semantic relatedness': 3,\n",
       "  'Semantic Task: Textual entailment': 0,\n",
       "  'Training the JMT Model': 0,\n",
       "  'Pre-Training Word Representations': 1,\n",
       "  'Training the POS Layer': 0,\n",
       "  'Training the Chunking Layer': 0,\n",
       "  'Training the Dependency Layer': 0,\n",
       "  'Training the Relatedness Layer': 1,\n",
       "  'Training the Entailment Layer': 0,\n",
       "  'Related Work': 13,\n",
       "  'Datasets': 1,\n",
       "  'Training Details': 1,\n",
       "  'Results and Discussion': 0,\n",
       "  'Comparison with Published Results': 8,\n",
       "  'Analysis on the Model Architectures': 1,\n",
       "  'Discussion': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Details of Character NN-Gram Embeddings': 0,\n",
       "  'Pre-Training with Skip-Gram Objective': 1,\n",
       "  'Effectiveness on Unknown Words': 1,\n",
       "  'Analysis on Dependency Parsing': 0,\n",
       "  'Analysis on Semantic Tasks': 3,\n",
       "  'How Do Shared Embeddings Change': 0},\n",
       " '1157800': {'Introduction': 5, 'The Phenomena': 0},\n",
       " '3204935': {'Introduction': 14,\n",
       "  'Transforming Definitions': 0,\n",
       "  'Knowledge Integration': 2,\n",
       "  'Maximal Common Subgraph': 5,\n",
       "  'Maximal Join': 0,\n",
       "  'Integration process': 1,\n",
       "  'Example of integration': 0,\n",
       "  'Discussion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '14748840': {'Acknowledgments': 0},\n",
       " '102353644': {'Introduction': 10,\n",
       "  'Outlier Detection': 24,\n",
       "  'Data Collection': 6,\n",
       "  'Method': 5,\n",
       "  'Application: Error Detection': 0,\n",
       "  'Application: Uniqueness-driven Data Collection': 4,\n",
       "  'Experiments': 0,\n",
       "  'Error Detection': 4,\n",
       "  'Uniqueness-driven Data Collection': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '338760': {'Introduction': 5,\n",
       "  'Related Work': 5,\n",
       "  'Learning User Embeddings': 1,\n",
       "  'Proposed Model': 1,\n",
       "  'Experimental Setup': 0,\n",
       "  'Baselines': 0,\n",
       "  'Pre-Training Word and User Embeddings': 0,\n",
       "  'Model Training and Evaluation': 2,\n",
       "  'Classification Results': 0,\n",
       "  'User Embedding Analysis': 1,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52118297': {'Introduction': 18,\n",
       "  'Method': 3,\n",
       "  'Linguistic accuracy': 1,\n",
       "  'Modeling human expectations': 2,\n",
       "  'Does the model adapt to syntax?': 4,\n",
       "  'Reduced relative clauses': 4,\n",
       "  'The dative alternation': 4,\n",
       "  'Testing for catastrophic forgetting': 1,\n",
       "  'Discussion': 9,\n",
       "  'Language model': 1,\n",
       "  'Analysis of reading times': 1,\n",
       "  ' simulation': 0,\n",
       "  'Dative alternation simulation': 0},\n",
       " '537': {'Introduction': 7,\n",
       "  'The Empirical Evidence': 4,\n",
       "  'Exponential Models of Distance': 2,\n",
       "  'Estimating the Parameters': 1,\n",
       "  'A Nonstationary Language Model': 1,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '3263529': {'Introduction': 0,\n",
       "  'The Identification Algorithm': 0,\n",
       "  'The language model and the algorithm': 0,\n",
       "  'Training the model': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Broader applicability': 0,\n",
       "  'On decisiveness': 0,\n",
       "  'A further comparison': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Appendix': 0},\n",
       " '43939290': {'Introduction': 11,\n",
       "  'The amr-Covington algorithm': 3,\n",
       "  'Formalization': 0,\n",
       "  'Training the classifiers': 6,\n",
       "  'Running the system': 2,\n",
       "  'Results and discussion': 1,\n",
       "  'Conclusion': 4,\n",
       "  'Acknowledgments': 0,\n",
       "  'Supplemental Material': 0,\n",
       "  'Additional design decisions': 0},\n",
       " '198229567': {},\n",
       " '16557171': {'Introduction': 9,\n",
       "  'Related Work': 7,\n",
       "  'Clustering words in a hybrid morphological system': 8,\n",
       "  'Words removed from clusters': 0,\n",
       "  'Quality ratings of clusters': 0,\n",
       "  'Hybridity in clustering': 0,\n",
       "  'Classifying morphological properties': 0,\n",
       "  'The classification system': 1,\n",
       "  'Evaluation Results': 0,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '49363457': {'Introduction': 3,\n",
       "  'Meaning and composition': 1,\n",
       "  'Approach': 1,\n",
       "  'Classification tasks': 0,\n",
       "  'Means of control': 1,\n",
       "  'Generation system': 0,\n",
       "  'Event/sentence representations': 2,\n",
       "  'Event population': 2,\n",
       "  'Syntactic realization': 2,\n",
       "  'Sentence quality': 0,\n",
       "  'Implementation of lexical variability': 0,\n",
       "  'Surface tasks: word content and order': 0,\n",
       "  'Classification experiments': 0,\n",
       "  'Sentence encoding models': 13,\n",
       "  'Results and Discussion': 3,\n",
       "  'Related work': 11,\n",
       "  'Conclusions and future directions': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '7717275': {'Introduction': 19,\n",
       "  'Related Work': 4,\n",
       "  'Inferring Semantic Verb Classes': 1,\n",
       "  'German Subcategorization Lexicon': 1,\n",
       "  'Findings from Formal Semantics': 2,\n",
       "  'Mapping to Meaning Components': 0,\n",
       "  'Linking to Semantic Classes': 1,\n",
       "  'Analysis of Frequency and Polysemy': 3,\n",
       "  'Textual Entailment Experiment': 5,\n",
       "  'Results and Discussion': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Verb Lemma Frequency List': 1,\n",
       "  'Verb Sense Linking': 0},\n",
       " '19172224': {'Introduction': 1,\n",
       "  'Related Work': 13,\n",
       "  'Methods': 0,\n",
       "  'Word Embedding': 2,\n",
       "  'Sequence Encoder': 1,\n",
       "  'Composition Layer': 3,\n",
       "  'Top-layer Classifier': 1,\n",
       "  'Results': 5,\n",
       "  'Summary and Future Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '126147495': {'Introduction': 7,\n",
       "  'Related Work': 28,\n",
       "  'Dataset': 1,\n",
       "  'Experiments and Results': 0,\n",
       "  'Data Preprocessing': 0,\n",
       "  'Training Deep Learning Models': 1,\n",
       "  'Heuristics for Sub-task B': 0,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '5037669': {},\n",
       " '44167998': {'Introduction': 11,\n",
       "  'Related Work': 34,\n",
       "  'Problem Formulation': 0,\n",
       "  'Sketch Generation': 2,\n",
       "  'Meaning Representation Generation': 0,\n",
       "  'Training and Inference': 0,\n",
       "  'Semantic Parsing Tasks': 0,\n",
       "  'Natural Language to Logical Form': 4,\n",
       "  'Natural Language to Source Code': 4,\n",
       "  'Natural Language to SQL': 1,\n",
       "  'Experiments': 0,\n",
       "  'Experimental Setup': 8,\n",
       "  'Results and Analysis': 3,\n",
       "  'Conclusions': 0},\n",
       " '92994351': {'Acknowledgments': 0},\n",
       " '1031444': {'Introduction': 3,\n",
       "  'Gaussian Process Regression': 0,\n",
       "  'Tree Kernels': 0,\n",
       "  'Symbol-aware Subset Tree Kernel': 0,\n",
       "  'Kernel Gradients': 0,\n",
       "  'Kernel Normalization': 0,\n",
       "  'Other Extensions': 3,\n",
       "  'Synthetic Data Experiments': 0,\n",
       "  'SSTK Prior': 0,\n",
       "  'SASSTK Prior': 0,\n",
       "  'Performance Experiments': 0,\n",
       "  'NLP Experiments': 1,\n",
       "  'Emotion Analysis': 5,\n",
       "  'Quality Estimation': 9,\n",
       "  'Overfitting': 1,\n",
       "  'Extensions to Other Tasks': 4,\n",
       "  'Related Work': 16,\n",
       "  'Conclusions': 0,\n",
       "  'Details on SVM Baselines': 0},\n",
       " '49901898': {'Introduction': 21,\n",
       "  'Knowledge Graph Representation': 0,\n",
       "  'Multi-Graph Relational Learning': 0,\n",
       "  'Proposed Method: LinkNBed ': 0,\n",
       "  'Atomic Layer': 2,\n",
       "  'Contextual Layer': 1,\n",
       "  'Representation Layer': 0,\n",
       "  'Relational Score Function': 0,\n",
       "  'Objective Function': 2,\n",
       "  'Datasets': 0,\n",
       "  'Baselines': 5,\n",
       "  'Evaluation Scheme': 1,\n",
       "  'Predictive Analysis': 0,\n",
       "  'Neural Embedding Methods for Relational Learning': 19,\n",
       "  'Entity Resolution in Relational Data': 6,\n",
       "  'Learning across multiple graphs': 5,\n",
       "  'Concluding Remarks and Future Work': 1,\n",
       "  'Acknowledgments': 0,\n",
       "  'Discussion and Insights on Entity Linkage Task': 0,\n",
       "  'Additional Dataset Details': 0,\n",
       "  'Training Configurations': 0,\n",
       "  'Contextual Information Formulations': 0},\n",
       " '18449288': {'Acknowledgments': 0},\n",
       " '195345032': {},\n",
       " '8620787': {'Introduction': 1,\n",
       "  'Some Previous Work': 0,\n",
       "  'Text tiling': 1,\n",
       "  'Lexical cohesion': 2,\n",
       "  'Decision trees': 1,\n",
       "  'A Feature-Based Approach': 3,\n",
       "  'A short-range model of language': 1,\n",
       "  'A long-range model of language': 5,\n",
       "  'Language model “relevance” features': 0,\n",
       "  'Vocabulary features': 0,\n",
       "  'Feature Induction': 2,\n",
       "  'Feature Induction in Action': 1,\n",
       "  'WSJ features': 0,\n",
       "  'TDT features': 0,\n",
       "  'A Probabilistic Error Metric': 1,\n",
       "  'The new metric': 0,\n",
       "  'Quantitative results': 0,\n",
       "  'Qualitative results': 0,\n",
       "  'Conclusions': 1,\n",
       "  'Acknowledgements': 0},\n",
       " '26591736': {'Introduction': 38,\n",
       "  'Proposed Approach': 0,\n",
       "  'Notation': 0,\n",
       "  'Word2Vec and Doc2Vec': 2,\n",
       "  'Training for DocTag2Vec': 0,\n",
       "  'Prediction for DocTag2Vec': 0,\n",
       "  'Datasets': 0,\n",
       "  'Baselines and Hyperparameter Setting': 2,\n",
       "  'Results': 0,\n",
       "  'Case Study for NCT dataset': 0,\n",
       "  'Conclusions and Future Work': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '5112203': {'Acknowledgments': 0},\n",
       " '465': {'Introduction': 4,\n",
       "  'Bunsetsu identification problem': 0,\n",
       "  'Decision-tree method': 1,\n",
       "  'Maximum-entropy method': 6,\n",
       "  'Example-based method (use of similarity)': 1,\n",
       "  'Decision-list method (use of probability and frequency)': 3,\n",
       "  'Method 1 (use of category-exclusive rules)': 0,\n",
       "  'Method 2 (using category-exclusive rules with the highest similarity)': 0,\n",
       "  'Experiments and discussion': 4,\n",
       "  'Conclusion': 0},\n",
       " '739696': {'Introduction': 13,\n",
       "  'Related Work': 14,\n",
       "  'The Neural Language Generator': 3,\n",
       "  'Semantic Controlled LSTM cell': 6,\n",
       "  'The Deep Structure': 6,\n",
       "  'Backward LSTM reranking': 3,\n",
       "  'Training': 2,\n",
       "  'Decoding': 0,\n",
       "  'Experimental Setup': 4,\n",
       "  'Objective Evaluation': 1,\n",
       "  'Human Evaluation': 1,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '4891749': {'Introduction': 2,\n",
       "  'Higher-order Coreference Resolution': 2,\n",
       "  'Coarse-to-fine Inference': 0,\n",
       "  'Heuristic antecedent pruning': 0,\n",
       "  'Coarse-to-fine antecedent pruning': 0,\n",
       "  'Experimental Setup': 1,\n",
       "  'Results': 0,\n",
       "  'Related Work': 12,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '3432876': {'Introduction': 16,\n",
       "  'Data Collection': 9,\n",
       "  'The Resulting Corpus': 2,\n",
       "  'Baselines': 6,\n",
       "  'Overall Difficulty': 0,\n",
       "  'Analysis by Linguistic Phenomenon': 5,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '174798366': {'Introduction': 12,\n",
       "  'Rhetorical Structure Theory (RST)': 2,\n",
       "  'Models': 6,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 6,\n",
       "  'Settings': 0,\n",
       "  'Results': 1,\n",
       "  'Learning better structure': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1732735': {'Introduction': 10, 'System Description': 3},\n",
       " '12914628': {'Introduction': 8,\n",
       "  ' Extension': 6,\n",
       "  'Algorithm for centering and pronoun binding': 0,\n",
       "  'Discussion of the algorithm': 1,\n",
       "  'Future Research': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Appendix': 0},\n",
       " '52123817': {'Introduction': 22,\n",
       "  'System Combination': 12,\n",
       "  'Right-to-left Translation Models': 4,\n",
       "  'Data Selection': 5,\n",
       "  'Preprocessing': 1,\n",
       "  'Model Hyper-Parameters': 8,\n",
       "  'Training': 9,\n",
       "  'Decoding': 4,\n",
       "  'Results': 6,\n",
       "  'Related Work': 24,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '67855643': {'Credits': 0,\n",
       "  'Introduction': 0,\n",
       "  'General Instructions': 0,\n",
       "  'The Ruler': 0,\n",
       "  'Electronically-available resources': 0,\n",
       "  'Format of Electronic Manuscript': 0,\n",
       "  'Layout': 0,\n",
       "  'Fonts': 0,\n",
       "  'The First Page': 0,\n",
       "  'Sections': 18,\n",
       "  'Footnotes': 0,\n",
       "  'Graphics': 0,\n",
       "  'Accessibility': 0,\n",
       "  'Translation of non-English Terms': 0,\n",
       "  'Length of Submission': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '57916': {'Introduction': 9,\n",
       "  'Notation': 0,\n",
       "  'Overview': 1,\n",
       "  'Computing Prefix Probabilities': 0,\n",
       "  'General equations': 0,\n",
       "  'Terminating equations': 0,\n",
       "  'Off-line Equations': 0,\n",
       "  'Complexity and concluding remarks': 1},\n",
       " '8938702': {'Introduction': 4,\n",
       "  'Related Work': 5,\n",
       "  'Jamo Structure of the Korean Language': 0,\n",
       "  'Jamo Architecture': 1,\n",
       "  'Unicode Decomposition': 0,\n",
       "  'Why Use Jamo Letters?': 2,\n",
       "  'Discussion of Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1683643': {'Introduction': 5,\n",
       "  'Data': 1,\n",
       "  'Models': 1,\n",
       "  'The Flat Model': 0,\n",
       "  'The Hier Model': 0,\n",
       "  'Statistical Inference': 0,\n",
       "  'Data Preprocessing and Search': 1,\n",
       "  'Results': 0,\n",
       "  'Quantitative Evaluation': 0,\n",
       "  'Cross-model Comparison': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Multi-conditional Implications': 0,\n",
       "  'Discussion': 1},\n",
       " '119184444': {'Acknowledgements': 0},\n",
       " '7182393': {'Introduction': 9,\n",
       "  'Emotion Annotation': 11,\n",
       "  'Best–Worst Scaling': 10,\n",
       "  'Data': 1,\n",
       "  'Annotating with Best–Worst Scaling': 3,\n",
       "  'Reliability of Annotations': 2,\n",
       "  'The Task': 0,\n",
       "  'Training, development, and test sets': 0,\n",
       "  'Resources': 0,\n",
       "  'Official Submission to the Shared Task': 0,\n",
       "  'Evaluation': 0,\n",
       "  'System': 6,\n",
       "  'Experiments': 0,\n",
       "  'Official System Submissions to the Shared Task': 0,\n",
       "  'Results': 0,\n",
       "  'Machine Learning Setups': 0,\n",
       "  'Prayas: Rank 1': 2,\n",
       "  'IMS: Rank 2': 5,\n",
       "  'SeerNet: Rank 3': 1,\n",
       "  'Conclusions': 1,\n",
       "  'Acknowledgment': 0,\n",
       "  'Best–Worst Scaling Questionnaire used to Obtain Emotion Intensity Scores': 0,\n",
       "  'An Interactive Visualization to Explore the Tweet Emotion Intensity Dataset': 0,\n",
       "  'AffectiveTweets Weka Package: Implementation Details': 2},\n",
       " '7671180': {'Introduction': 0,\n",
       "  'Approach': 0,\n",
       "  'Simulation': 0,\n",
       "  'Results': 0,\n",
       "  'Discussion and prospects': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '257038': {'1. INTRODUCTION': 0,\n",
       "  '2. PART-OF-SPEECH TAG SEQUENCE GRAMMAR': 0,\n",
       "  '3. TEXT GRAMMAR AND PUNCTUATION': 0,\n",
       "  '4. THE INTEGRATED GRAMMAR': 0,\n",
       "  '5. PARSING THE SUSANNE AND SEC CORPORA': 0,\n",
       "  'COVERAGE AND AMBIGUITY': 0,\n",
       "  'Grammar Development & Refinement': 0,\n",
       "  'PARSE SELECTION': 0,\n",
       "  'Training Data Size and Accuracy': 0,\n",
       "  '6. CONCLUSIONS': 0,\n",
       "  'APPENDIX': 0},\n",
       " '102353614': {'Introduction': 31,\n",
       "  'Data Exploration': 5,\n",
       "  'Model Description': 0,\n",
       "  'Context Generation': 5,\n",
       "  'Waveform Synthesis': 0,\n",
       "  'Training': 2,\n",
       "  'Setup for Evaluation': 5,\n",
       "  'Analysis of Objective Metrics': 0,\n",
       "  'Analysis of MUSHRA Scores': 0,\n",
       "  'Effect of Contextual Word Embeddings on Prosody Modelling': 0,\n",
       "  'Analysis of Speech Tempo': 0,\n",
       "  'Conclusions': 0},\n",
       " '13745156': {'Introduction': 0,\n",
       "  'Background': 0,\n",
       "  'Dataset Ambiguity and Upperbound': 0,\n",
       "  'Approach': 0,\n",
       "  'Results': 3,\n",
       "  'Baseline Model': 0,\n",
       "  'Model Details': 4,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '52135124': {'Introduction': 10,\n",
       "  'Related Work': 27,\n",
       "  'Overview: Data-Driven NLG': 8,\n",
       "  'Background: Semi-Markov Models': 4,\n",
       "  'A Neural HSMM Decoder': 0,\n",
       "  'Parameterization': 4,\n",
       "  'Learning': 3,\n",
       "  'Extracting Templates and Generating': 0,\n",
       "  'Discussion': 0,\n",
       "  'Data and Methods': 0,\n",
       "  'Datasets': 10,\n",
       "  'Model and Training Details': 1,\n",
       "  'Results': 4,\n",
       "  'Qualitative Evaluation': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Additional Model and Training Details': 4,\n",
       "  'Additional Learned Templates': 0},\n",
       " '195584320': {'Introduction': 18,\n",
       "  'Probabilistic Context-Free Grammars': 0,\n",
       "  'Compound PCFGs': 10,\n",
       "  'Experimental Setup': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Related Work': 29,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Model Parameterization': 0,\n",
       "  'Corpus/Sentence F 1 F_1 by Sentence Length': 0,\n",
       "  'Experiments with RNNGs': 12,\n",
       "  'Nonterminal/Preterminal Alignments': 0,\n",
       "  'Subtree Analysis': 0},\n",
       " '4943905': {},\n",
       " '52056513': {'Introduction': 13,\n",
       "  'Learning Setup': 2,\n",
       "  'Data, Tasks, and Protected Attributes': 0,\n",
       "  'Mitigating Data Leakage': 0,\n",
       "  'Adversarial Training': 1,\n",
       "  'Strengthening the Adversarial Component': 0,\n",
       "  'Analysis': 0,\n",
       "  'Related Work': 23,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Standard Fairness Definitions and Guarded Classifiers': 0,\n",
       "  'Emojis Details': 0},\n",
       " '7312259': {'Introduction': 6,\n",
       "  'Related Work': 16,\n",
       "  'A Parallel Sarcastic Tweets Corpus': 0,\n",
       "  'Evaluation Measures': 0,\n",
       "  'Sarcasm Interpretations as MT': 0,\n",
       "  'The Sarcasm SIGN Algorithm': 2,\n",
       "  'Experiments and Results': 0},\n",
       " '47016219': {'Introduction': 7,\n",
       "  'Models': 0,\n",
       "  'Pattern-based Hypernym Detection': 1,\n",
       "  'Distributional Hypernym Detection': 11,\n",
       "  'Evaluation': 0,\n",
       "  'Tasks': 15,\n",
       "  'Experimental Setup': 3,\n",
       "  'Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '44081223': {'Introduction': 9,\n",
       "  'Inflectional Morphology': 3,\n",
       "  'Morphological Inflection': 0,\n",
       "  'Generating Sequences of Inflections': 0,\n",
       "  'Recurrent Neural Parameterization': 2,\n",
       "  'LSTM Language Models': 1,\n",
       "  'Our Conditional Distributions': 3,\n",
       "  'Semi-Supervised Wake-Sleep': 6,\n",
       "  'Data Requirements of Wake-Sleep': 0,\n",
       "  'The Sleep Phase': 1,\n",
       "  'The Wake Phase': 0,\n",
       "  'Adding Supervision to Wake-Sleep': 0,\n",
       "  'Our Variational Family': 4,\n",
       "  'Interpretation as an Autoencoder': 1,\n",
       "  'Related Work': 0,\n",
       "  'Experiments': 0,\n",
       "  'Low-Resource Inflection Generation': 0,\n",
       "  'Data': 2,\n",
       "  'Evaluation': 0,\n",
       "  'Baselines': 0,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Hyperparameters and Experimental Details': 0,\n",
       "  'Fake Data from the Sleep Phase': 0},\n",
       " '643470': {'Introduction': 5,\n",
       "  'Transformation-Based Learning': 1,\n",
       "  'Dialogue Act Tagging': 0,\n",
       "  'Limitations of TBL': 2,\n",
       "  'Experimental Results': 1,\n",
       "  'Discussion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '30758763': {'Introduction': 2,\n",
       "  'Dataset': 3,\n",
       "  'Shared Task Competition': 4,\n",
       "  'Results and Leaderboard': 6,\n",
       "  'Model Comparison': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Annotations': 0,\n",
       "  'Nearest Neighbors': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52074264': {},\n",
       " '4895777': {'Introduction': 24,\n",
       "  'Datasets': 6,\n",
       "  'Automatic Cognate Detection': 27,\n",
       "  'Bayesian Phylogenetic Inference': 3,\n",
       "  'Experiments': 7,\n",
       "  'GQD': 8,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Supplemental Material': 0},\n",
       " '199528340': {'Introduction': 10,\n",
       "  'Our Approach': 1,\n",
       "  'Our Model': 5,\n",
       "  'Joint Training with De-noising Encoder': 4,\n",
       "  'Experiments': 1,\n",
       "  'Data and Settings': 15,\n",
       "  'Results': 2,\n",
       "  'Related Work': 8,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '8075816': {'Discussion and Future Work': 4, 'Acknowledgments': 0},\n",
       " '52048852': {'Introduction': 0,\n",
       "  'Data collection': 0,\n",
       "  'Model and evaluation': 0,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Related work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '14472773': {'Introduction': 8,\n",
       "  'Our Approach': 0,\n",
       "  'Improving the weights of feature vectors': 1,\n",
       "  'Integrating the distributional lexical contrast into a skip-gram model': 2,\n",
       "  'Experiments': 0,\n",
       "  'Experimental Settings': 3,\n",
       "  'Distinguishing antonyms from synonyms': 2,\n",
       "  'Effects of distributional lexical contrast on word embeddings': 2,\n",
       "  'Conclusion': 0},\n",
       " '198229768': {'Introduction': 16,\n",
       "  'Post-Editor Action Sequences': 1,\n",
       "  'Editor Identification': 0,\n",
       "  'Data Preparation': 0,\n",
       "  'A Model for Editor Identification': 4,\n",
       "  'Baselines': 5,\n",
       "  'Editor Identification Accuracy': 0,\n",
       "  'Editor Representation': 3,\n",
       "  'Prediction of Post-Editing Time': 3,\n",
       "  'Related Work': 32,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '3262717': {'Introduction': 5,\n",
       "  'Probabilistic Dependencies': 3,\n",
       "  'Model A: Bigram lexical affinities': 5,\n",
       "  'Model B: Selectional preferences': 0,\n",
       "  'Model C: Recursive generation': 3,\n",
       "  'Bottom-Up Dependency Parsing': 1,\n",
       "  'Bottom-Up Probabilities': 0,\n",
       "  'Empirical Comparison': 1,\n",
       "  'Conclusions': 0},\n",
       " '459098': {'Introduction': 0,\n",
       "  'The data': 0,\n",
       "  'Mechanisms for switching control': 0,\n",
       "  'Control cues and global control': 0,\n",
       "  'Conclusions': 0},\n",
       " '174797747': {'Introduction': 13,\n",
       "  'Table-to-Text Generation': 2,\n",
       "  'PARENT': 0,\n",
       "  'Evaluation via Information Extraction': 4,\n",
       "  'Experiments & Results': 1,\n",
       "  'Data & Models': 3,\n",
       "  'Human Evaluation': 2,\n",
       "  'Compared Metrics': 5,\n",
       "  'Correlation Comparison': 2,\n",
       "  'Analysis': 1,\n",
       "  'WebNLG Dataset': 1,\n",
       "  'Related Work': 15,\n",
       "  'Conclusions': 1,\n",
       "  'Acknowledgements': 0,\n",
       "  'Information Extraction System': 3,\n",
       "  'Hyperparameters': 1,\n",
       "  'Sample Outputs': 0},\n",
       " '22272492': {'Introduction': 9,\n",
       "  'Related work': 26,\n",
       "  'Methodology': 0,\n",
       "  'Active learning as a decision process': 2,\n",
       "  'Stream-based learning': 6,\n",
       "  'Cross-lingual policy transfer': 0,\n",
       "  'Cold-start transfer ': 0,\n",
       "  'Experiments': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '53176182': {'Introduction': 2,\n",
       "  'Models': 13,\n",
       "  'Experiments': 2,\n",
       "  'Diagnosis Detection': 0,\n",
       "  'Prescription Reasons': 0,\n",
       "  'Penn Adverse Drug Reactions (ADR)': 9,\n",
       "  'Chemical–Disease Relations (CDR)': 1,\n",
       "  'Drug–Disease Relations': 1,\n",
       "  'Discussion': 0,\n",
       "  'The Role of Text Length': 0,\n",
       "  'Analysis of the CRF Potential Scores': 0,\n",
       "  'Major Improvements in Minor Categories': 0,\n",
       "  'Clinical Text Labeling': 6,\n",
       "  'Efficient Annotation': 2,\n",
       "  'Related Models': 9,\n",
       "  'Conclusion': 0},\n",
       " '19191579': {'Introduction': 20,\n",
       "  'The Existing Learning Approach': 5,\n",
       "  'A New Learning Method': 6,\n",
       "  'Experiment': 3,\n",
       "  'Implementation Details': 1,\n",
       "  'Single-turn Response Selection': 4,\n",
       "  'Multi-turn Response Selection': 4,\n",
       "  'Discussion': 2,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '52176506': {},\n",
       " '2496355': {'Introduction': 21,\n",
       "  'Attention-Based NMT': 7,\n",
       "  'Data Sets': 2,\n",
       "  'Extended Context Models': 0,\n",
       "  'Experiments and Results': 0,\n",
       "  '2+1: Extended Source Language Context': 0,\n",
       "  '2+2: Larger Translation Units': 0,\n",
       "  'Manual Evaluation': 0,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '14206024': {'Acknowledgments': 0},\n",
       " '28971531': {'Introduction': 8,\n",
       "  'Related work': 16,\n",
       "  'Approach': 0,\n",
       "  'The Natural Language Inference task': 0,\n",
       "  'Sentence encoder architectures': 8,\n",
       "  'Training details': 0,\n",
       "  'Evaluation of sentence representations': 2,\n",
       "  'Empirical results': 0,\n",
       "  'Architecture impact': 0,\n",
       "  'Task transfer': 10,\n",
       "  'Conclusion': 0},\n",
       " '7169761': {'The Italian pronominal system': 2,\n",
       "  'Centering theory': 8,\n",
       "  'Centering and Italian pronouns': 0,\n",
       "  'Other phenomena': 0,\n",
       "  'Purpose of an utterance': 0,\n",
       "  'Null subject referring to a whole discourse segment': 2,\n",
       "  'Conclusions': 0},\n",
       " '52941580': {'Introduction': 12,\n",
       "  'reddit: the front page of the internet': 4,\n",
       "  'Geocoding reddit Users': 7,\n",
       "  'Labeling Procedure': 3,\n",
       "  'Geographic Representation': 0,\n",
       "  'Label Accuracy': 0,\n",
       "  'Geolocation Inference': 0,\n",
       "  'Related Work': 14,\n",
       "  'Location Estimation Model': 2,\n",
       "  'Within-Domain Evaluation': 1,\n",
       "  'Results': 3,\n",
       "  'Cross-Domain Evaluation': 3,\n",
       "  'Discussion and Future Work': 0},\n",
       " '982761': {'Introduction': 31,\n",
       "  'Notation and Previous Models': 12,\n",
       "  'Model': 1,\n",
       "  'Block Iterative Optimization': 0,\n",
       "  'Corrupted Sample Generating Method': 4,\n",
       "  'Setup': 3,\n",
       "  'Implementation Details': 13,\n",
       "  'Results & Analysis': 0,\n",
       "  'Analysis on Sparseness': 0,\n",
       "  'Related Work': 10,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Domain Sampling Probability': 0,\n",
       "  'Performance on individual relations of WN18': 0},\n",
       " '947703': {'Introduction': 7,\n",
       "  'Lexicalized DMV': 2,\n",
       "  'Lexicalized NDMV': 2,\n",
       "  'Model Initialization': 4,\n",
       "  'Experimental Setup': 3,\n",
       "  'Results on English': 0,\n",
       "  'Results on Chinese': 0,\n",
       "  'Effect of Grammar Rule Probability Initialization': 1,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '67856013': {'Introduction': 3,\n",
       "  'Base Architecture': 1,\n",
       "  'Copying Mechanism': 3,\n",
       "  'Pre-training': 0,\n",
       "  'Denoising Auto-encoder': 3,\n",
       "  'Pre-training Decoder': 6,\n",
       "  'Multi-Task Learning': 4,\n",
       "  'Token-level Labeling Task': 0,\n",
       "  'Sentence-level Copying Task': 0,\n",
       "  'Datasets': 11,\n",
       "  'Model and Training Settings': 4,\n",
       "  'Experimental Results': 10,\n",
       "  'Ablation Study': 0,\n",
       "  'Attention Visualization': 1,\n",
       "  'Recall on Different Error Types': 1,\n",
       "  'Related Work': 10,\n",
       "  'Conclusions': 0},\n",
       " '5033484': {'Introduction': 5,\n",
       "  'Approach': 0,\n",
       "  'Conversational Learning Task using Input-Response Prediction': 2,\n",
       "  'Model Architecture': 0,\n",
       "  'Encoders': 5,\n",
       "  'Multitask Encoder': 1,\n",
       "  'Dataset': 0,\n",
       "  'Experiments': 1,\n",
       "  'Experiment Configuration': 0,\n",
       "  'Response Prediction': 0,\n",
       "  'SNLI': 3,\n",
       "  'STS Benchmark': 8,\n",
       "  'How Much Data for the Supervised Task?': 0,\n",
       "  'CQA Subtask B': 3,\n",
       "  'Related Work': 10,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2373337': {'Introduction': 5,\n",
       "  'Phrase Structure and Grammatical Relations': 2,\n",
       "  'Core syntactic phrases': 3,\n",
       "  'Grammatical relations': 5,\n",
       "  'Semantic interpretation': 1,\n",
       "  'The Processing Model': 1,\n",
       "  'The Data': 2,\n",
       "  'Parameter Settings for Training': 0,\n",
       "  'The Results': 0,\n",
       "  'Comparison with Other Work': 7,\n",
       "  'Summary, Discussion, and Speculation': 3,\n",
       "  'Appendix: Examples from Test Results': 0},\n",
       " '67855320': {'Introduction': 12,\n",
       "  'Parsing as sequence labeling': 9,\n",
       "  'Encoding of trees and labels': 0,\n",
       "  'Model': 0,\n",
       "  'Experiments': 5,\n",
       "  'Encoding evaluation and model selection': 3,\n",
       "  'Results and discussion': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Model parameters': 0},\n",
       " '557620': {'Introduction': 6,\n",
       "  'Related Work': 19,\n",
       "  'Task Definition and Data Introduction': 2,\n",
       "  'Entity Linking': 1,\n",
       "  'Fact Selection': 0,\n",
       "  'Framework of CNN-Maxpooling': 0,\n",
       "  'AMPCNN: CNN-Attentive-Maxpooling': 0,\n",
       "  'Training Setup': 2,\n",
       "  'SimpleQuestions': 4,\n",
       "  'Effect of Attentive Maxpooling (AMP)': 3,\n",
       "  'Conclusion': 0},\n",
       " '195658138': {'Introduction': 14,\n",
       "  'Datasets': 3,\n",
       "  'Background': 5,\n",
       "  'The Model': 4,\n",
       "  'Performance Comparison': 0,\n",
       "  'Model Boosting ': 2,\n",
       "  'Discussion and Conclusion': 0},\n",
       " '17731007': {'Introduction': 8,\n",
       "  'Combining word embedding models': 1,\n",
       "  'Solution with the ordinary least squares (SOLS)': 0,\n",
       "  'Solution to the Orthogonal Procrustes problem (SOPP)': 1,\n",
       "  'Experiments': 1,\n",
       "  'Results': 0,\n",
       "  'Synonym ranks': 1,\n",
       "  'Analogy tests': 2,\n",
       "  'Analysis': 0,\n",
       "  'Distribution of mean squared distances': 0,\n",
       "  'Word pair similarities': 0,\n",
       "  'Discussion and future work': 3,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '102352338': {'Introduction': 20,\n",
       "  'Model': 5,\n",
       "  'State Tracking': 2,\n",
       "  'Location Tracking': 0,\n",
       "  'Learning and Inference': 0,\n",
       "  'Experiments': 1,\n",
       "  'Task 1: Sentence Level': 0,\n",
       "  'Task 2: Document Level': 0,\n",
       "  'Model Ablations': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2404341': {},\n",
       " '28335546': {'Introduction': 9,\n",
       "  'Transition-based Dependency Parsing': 3,\n",
       "  'Non-local Transitions with arc-swift': 2,\n",
       "  'Data and Model': 4,\n",
       "  'Results': 5,\n",
       "  'Linguistic Analysis': 0,\n",
       "  'Related Work': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Model and Training Details': 5},\n",
       " '10867850': {'Introduction': 4,\n",
       "  'Related Work': 21,\n",
       "  'The Watset Method': 3,\n",
       "  'Learning Word Embeddings': 0,\n",
       "  'Construction of a Synonymy Graph': 1,\n",
       "  'Local Clustering: Word Sense Induction': 0,\n",
       "  'Disambiguation of Neighbors': 0,\n",
       "  'Global Clustering: Synset Induction': 3,\n",
       "  'Local-Global Fuzzy Graph Clustering': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Gold Standard Datasets': 4,\n",
       "  'Evaluation Metrics': 3,\n",
       "  'Word Embeddings': 3,\n",
       "  'Input Dictionary of Synonyms': 1,\n",
       "  'Results': 0,\n",
       "  'Impact of Graph Weighting Schema': 0,\n",
       "  'Comparative Analysis': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6246116': {'Credits': 0,\n",
       "  'Introduction': 0,\n",
       "  'General Instructions': 0,\n",
       "  'The Ruler': 0,\n",
       "  'Electronically-available resources': 0,\n",
       "  'Format of Electronic Manuscript': 0,\n",
       "  'Layout': 0,\n",
       "  'Fonts': 0,\n",
       "  'The First Page': 0,\n",
       "  'Sections': 11,\n",
       "  'Footnotes': 0,\n",
       "  'Graphics': 0,\n",
       "  'Accessibility': 0,\n",
       "  'XML conversion and supported  packages': 0,\n",
       "  'Translation of non-English Terms': 0,\n",
       "  'Length of Submission': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Supplemental Material': 0,\n",
       "  'Multiple Appendices': 0},\n",
       " '13052370': {'Introduction': 10},\n",
       " '182953250': {'Introduction': 17,\n",
       "  'Keyphrase Extraction and Generation': 18,\n",
       "  'Reinforcement Learning for Text Generation': 8,\n",
       "  'Problem Definition': 0,\n",
       "  'Keyphrase Generation Model': 6,\n",
       "  'Reinforcement Learning Formulation': 3,\n",
       "  'New Evaluation Method': 0,\n",
       "  'Name Variation Extraction': 2,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 8,\n",
       "  'Evaluation Metrics': 7,\n",
       "  'Baseline and Deep Reinforced Models': 6,\n",
       "  'Implementation Details': 7,\n",
       "  'Main Results': 1,\n",
       "  'Number of Generated Keyphrases': 0,\n",
       "  'Ablation Study': 0,\n",
       "  'Analysis of New Evaluation Method': 0,\n",
       "  'Conclusion and Future Work': 6,\n",
       "  'Acknowledgments': 0},\n",
       " '195316904': {'Introduction': 2,\n",
       "  'Related work': 20,\n",
       "  'Online learning from NMT post-edits': 7,\n",
       "  'Adaption from post-edits via online learning': 0,\n",
       "  'Experimental framework': 0,\n",
       "  'NMT systems': 8,\n",
       "  'Translation environment': 1,\n",
       "  'Tasks and evaluation': 8,\n",
       "  'Results': 0,\n",
       "  'Adaptation with simulated users': 1,\n",
       "  'Adaptation with human post-editors': 0,\n",
       "  'User perceptions and opinions': 1,\n",
       "  'Conclusions and future work': 4,\n",
       "  'Acknowledgements': 0},\n",
       " '84842': {'Introduction': 15,\n",
       "  'Related Work': 12,\n",
       "  'Framework': 0,\n",
       "  'POS projection via word alignments': 4,\n",
       "  'BiLSTM with bias layer': 1,\n",
       "  'Experiments': 0,\n",
       "  'Evaluation Corpora': 7,\n",
       "  'Setup and baselines': 0,\n",
       "  'Results': 0},\n",
       " '18860664': {'Introduction': 7,\n",
       "  'Derivational Morphology': 7,\n",
       "  'A Joint Model': 5,\n",
       "  'Transduction Factor': 4,\n",
       "  'Segmentation Factor': 3,\n",
       "  'Composition Factor': 1,\n",
       "  'Inference and Learning': 1,\n",
       "  'Inference by Importance Sampling': 1,\n",
       "  'Learning': 2,\n",
       "  'Decoding': 0,\n",
       "  'Related Work': 8,\n",
       "  'Experiments and Results': 1,\n",
       "  'Experiment 1: Canonical Segmentation': 2,\n",
       "  'Experiment 2: Vector Approximation': 16,\n",
       "  'Experiment 3: Derivational Productivity': 1,\n",
       "  'Conclusion': 0},\n",
       " '1924466': {'Introduction': 3,\n",
       "  'Notation': 0,\n",
       "  'Applying probability measures to Tree Adjoining Languages': 2,\n",
       "  'Conditions for Consistency': 8,\n",
       "  'TAG Derivations and Branching Processes': 4,\n",
       "  'Conclusion': 0},\n",
       " '174799390': {'Introduction': 16,\n",
       "  'Related Work': 21,\n",
       "  'Multi-News Dataset': 1,\n",
       "  'Statistics and Analysis': 1,\n",
       "  'Diversity': 0,\n",
       "  'Other Datasets': 0,\n",
       "  'Preliminaries': 0,\n",
       "  'Pointer-generator Network': 3,\n",
       "  'Transformer': 4,\n",
       "  'MMR': 2,\n",
       "  'Hi-MAP Model': 0,\n",
       "  'Sentence representations': 0,\n",
       "  'MMR-Attention': 0,\n",
       "  'MMR-attention Pointer-generator': 0,\n",
       "  'Experiments': 0,\n",
       "  'Baseline and Extractive Methods': 2,\n",
       "  'Neural Abstractive Methods': 2,\n",
       "  'Experimental Setting': 6,\n",
       "  'Analysis and Discussion': 5,\n",
       "  'Conclusion': 0},\n",
       " '119303681': {'Introduction': 11,\n",
       "  'Related Work': 9,\n",
       "  'Corpus Creation': 2,\n",
       "  'Experiment': 5,\n",
       "  'Results': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Error Types': 1,\n",
       "  'Errors between domains': 1,\n",
       "  'Errors between segmenters': 0,\n",
       "  'Errors between annotators and segmenters': 0,\n",
       "  'Next Steps': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '67856167': {'Introduction': 8,\n",
       "  'Quality Estimation': 9,\n",
       "  'Datasets': 0,\n",
       "  'Implemented Systems': 0,\n",
       "  'Benchmark Experiments': 3,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2183942': {'Introduction': 18,\n",
       "  'Related Work': 20,\n",
       "  'RNN as Bayesian Predictive Models': 0,\n",
       "  'RNN Architectures': 4,\n",
       "  'Applications': 3,\n",
       "  'The Pitfall of Stochastic Optimization': 2,\n",
       "  'Large-scale Bayesian Learning': 1,\n",
       "  'SG-MCMC Algorithms': 15,\n",
       "  'Understanding SG-MCMC': 4,\n",
       "  'Experiments': 1,\n",
       "  'Language Modeling': 4,\n",
       "  'Image Caption Generation': 10,\n",
       "  'Sentence Classification': 7,\n",
       "  'Discussion': 3,\n",
       "  'Conclusion': 0},\n",
       " '102350797': {'Acknowledgments': 0},\n",
       " '3666178': {'Introduction': 7,\n",
       "  'Definitions and notations': 0,\n",
       "  'Consistency': 0,\n",
       "  'Highest-weighted string': 1,\n",
       "  'Equivalence': 4,\n",
       "  'Minimization': 2,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0,\n",
       "  'Appendix': 0},\n",
       " '53109320': {'Introduction': 7,\n",
       "  'Related Work': 14,\n",
       "  'Problem Definition': 0,\n",
       "  'Method': 0,\n",
       "  'Encoder': 2,\n",
       "  'Graph Module': 1,\n",
       "  'Decoder': 2,\n",
       "  'Adaptation to Word-level Graphs': 0,\n",
       "  'Experimental Setup': 0,\n",
       "  'Task 1: Textual Information Extraction': 5,\n",
       "  'Task 2: Social Media Information Extraction': 7,\n",
       "  'Task 3: Visual Information Extraction': 0,\n",
       "  'Baseline and Our Method': 2,\n",
       "  'Implementation Details': 3,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5754528': {'Introduction': 18,\n",
       "  'Background': 1,\n",
       "  'Constituent Trees': 22,\n",
       "  'Dependency Trees': 9,\n",
       "  'Head-Ordered Dependency Trees': 0,\n",
       "  'Strictly Ordered Dependency Trees': 1,\n",
       "  'Weakly Ordered Dependency Trees': 0,\n",
       "  'Continuous and Projective Trees': 1,\n",
       "  'Reduction-Based Constituent Parsers': 2,\n",
       "  'Dependency Encoding': 1,\n",
       "  'Training the Labeled Dependency Parser': 5,\n",
       "  'Decoding into Unaryless Constituents': 0,\n",
       "  'Recovery of Unary Nodes': 0,\n",
       "  'Experiments': 0,\n",
       "  'Results on the English PTB': 4,\n",
       "  'Results on the SPMRL Datasets': 7,\n",
       "  'Results on the Discontinuous Treebanks': 7,\n",
       "  'Related Work': 9,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '14623890': {'Introduction': 0,\n",
       "  'Experimental setup': 5,\n",
       "  'Experimental results': 0,\n",
       "  'Time course of tagging accuracy': 0,\n",
       "  'Related work': 10,\n",
       "  'Conclusion': 0},\n",
       " '10341744': {'Introduction': 18,\n",
       "  'Mining Topic Preferences of Users': 0,\n",
       "  'Mining Linguistic Patterns of Agreement and Disagreement': 3,\n",
       "  'Extracting Instances of Topic Preferences': 0,\n",
       "  'Matrix Factorization': 4,\n",
       "  'Determining the Dimension Parameter kk': 0,\n",
       "  'Predicting Missing Topic Preferences': 0,\n",
       "  'Inter-topic Preferences': 0,\n",
       "  'Related Work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '6376814': {},\n",
       " '49907944': {},\n",
       " '1290310': {'Introduction': 5,\n",
       "  'Structured Linear Models': 4,\n",
       "  'Dual Decomposition for Global Constraints': 5,\n",
       "  'Soft Constraints in Dual Decomposition': 1,\n",
       "  'Learning Penalties': 2,\n",
       "  'Citation Extraction Data': 0,\n",
       "  'Constraint Templates': 0,\n",
       "  'Singleton Constraints': 0,\n",
       "  'Pairwise Constraints': 0,\n",
       "  'Hierarchical Equality Constraints': 0,\n",
       "  'Local constraints': 0,\n",
       "  'Constraint Pruning': 0,\n",
       "  'Related Work': 14,\n",
       "  'Experimental Results': 0,\n",
       "  'Examples of learned constraints': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '4889691': {},\n",
       " '5848469': {'Introduction': 0,\n",
       "  'Beam Thresholding': 1,\n",
       "  'Global Thresholding': 0,\n",
       "  'Global Thresholding Algorithm': 0,\n",
       "  'Multiple-Pass Parsing': 1,\n",
       "  'Multiple-Pass Speech Recognition': 0,\n",
       "  'Multiple Parameter Optimization': 0,\n",
       "  'Comparison to Previous Work': 3,\n",
       "  'The Parser and Data': 0,\n",
       "  'The Grammar': 0,\n",
       "  'What we measured': 0,\n",
       "  'Experiments in Beam Thresholding': 0,\n",
       "  'Experiments in Global Thresholding': 0,\n",
       "  'Experiments combining Global Thresholding and Beam Thresholding': 0,\n",
       "  'Experiments in Multiple-Pass Parsing': 0,\n",
       "  'Application to Other Formalisms': 5,\n",
       "  'Conclusions': 0},\n",
       " '8822680': {'Introduction': 6,\n",
       "  'Experimental Setup': 0,\n",
       "  'Neural Machine Translation': 6,\n",
       "  'Statistical Machine Translation': 6,\n",
       "  'Data Conditions': 1,\n",
       "  'Domain Mismatch': 3,\n",
       "  'Amount of Training Data': 2,\n",
       "  'Rare Words': 9,\n",
       "  'Long Sentences': 2,\n",
       "  'Word Alignment': 7,\n",
       "  'Beam Search': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '91183987': {'Introduction': 11,\n",
       "  'Method': 9,\n",
       "  'Data Augmentation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Lematus Training': 5,\n",
       "  'Data Preparation': 1},\n",
       " '21428581': {'Conclusion': 0, 'Acknowledgments': 0},\n",
       " '2965282': {'Introduction': 2,\n",
       "  'Stochastic Unification-based Grammars': 0,\n",
       "  'Stochastic Lexical Functional Grammar': 0,\n",
       "  'Estimating stochastic unification-based grammars': 0,\n",
       "  'Auxiliary distributions': 0,\n",
       "  'Lexical selectional preferences': 2,\n",
       "  'Empirical evaluation': 0,\n",
       "  'Conclusion': 0},\n",
       " '53222270': {'Introduction': 2,\n",
       "  'Machine Translation of Varieties': 8,\n",
       "  'Dialect Identification': 5,\n",
       "  'Multilingual NMT': 0,\n",
       "  'NMT into Language Varieties': 0,\n",
       "  'Dataset and Preprocessing': 1,\n",
       "  'Experimental Settings': 3,\n",
       "  'Language Variety Identification': 2,\n",
       "  'Results and Discussion': 0,\n",
       "  'Low-resource setting': 0,\n",
       "  'High-resource setting': 0,\n",
       "  'Unsupervised Multilingual Models': 0,\n",
       "  'Translation Examples': 0,\n",
       "  'Conclusions': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '46936578': {'Conclusion': 0, 'Acknowledgment': 0},\n",
       " '725590': {'Introduction': 5,\n",
       "  'Text Chunking': 0,\n",
       "  'Existing Chunk Identification Techniques': 1,\n",
       "  'Deriving Chunks from Treebank Parses': 1,\n",
       "  'The Transformation-based Learning Paradigm': 0,\n",
       "  'Transformational Text Chunking': 0,\n",
       "  'Encoding Choices': 0,\n",
       "  'Baseline System': 1,\n",
       "  'Rule Templates': 1,\n",
       "  'Algorithm Design Issues': 0,\n",
       "  'Organization of the Computation': 1,\n",
       "  'Indexing Static Rule Elements': 1,\n",
       "  'Heuristic Disabling of Unlikely Rules': 0,\n",
       "  'Results': 0,\n",
       "  'Analysis of Initial Rules': 0,\n",
       "  'Contribution of Lexical Templates': 0,\n",
       "  'Frequent Error Classes': 0,\n",
       "  'Future Directions': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '14944414': {'Introduction': 18,\n",
       "  'Motivation and Background': 6,\n",
       "  'Learning Approach': 5,\n",
       "  'Learning model and parsing': 1,\n",
       "  'The inverse λ\\\\lambda  operators': 1,\n",
       "  'Generalization': 0,\n",
       "  'Trivial inverse solutions': 0,\n",
       "  'The overall learning algorithm.': 0,\n",
       "  'The data': 3,\n",
       "  'Results': 7,\n",
       "  'Analysis': 2,\n",
       "  'Conclusions and Discussion': 0},\n",
       " '27131087': {'Introduction': 2,\n",
       "  'Related Work': 13,\n",
       "  'Proposed Model': 3,\n",
       "  'Experiments and Results': 1,\n",
       "  'Quantitative Evaluation': 2,\n",
       "  'Qualitative Evaluation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '969555': {'Introduction': 5,\n",
       "  'Related Work': 15,\n",
       "  'Neural Machine Translation Model with Attention': 5,\n",
       "  'The Pointer Softmax': 3,\n",
       "  'Basic Components of the Pointer Softmax': 0,\n",
       "  'Experiments': 1,\n",
       "  'The Rarest Word Detection': 1,\n",
       "  'Summarization': 7,\n",
       "  'Neural Machine Translation': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 1},\n",
       " '10147973': {'Introduction': 3,\n",
       "  'Two-Step NMT': 1,\n",
       "  'Modeling Czech Morphology': 2,\n",
       "  'Modeling German Morphology': 0,\n",
       "  'Linguistic Resources': 2,\n",
       "  'German Inflectional Features': 0,\n",
       "  'Building the stemmed representation': 0,\n",
       "  'Reduction of Vocabulary Size': 0,\n",
       "  'Simple Compound Handling': 0,\n",
       "  'Experimental Evaluation': 0,\n",
       "  'Czech': 4,\n",
       "  'German': 1,\n",
       "  'Related Work': 6,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '16152426': {'Introduction': 2,\n",
       "  'Related work': 3,\n",
       "  'Distributed word representations': 10,\n",
       "  'Sparse coding': 3,\n",
       "  'Sequence labeling framework': 3,\n",
       "  'Experiments': 1,\n",
       "  'Baseline methods': 6,\n",
       "  'POS tagging experiments': 4,\n",
       "  'Named entity recognition experiments': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '1707814': {},\n",
       " '44096233': {'Introduction': 7,\n",
       "  'Related Work': 29,\n",
       "  'Datasets': 5,\n",
       "  'Dynamic Spatial Memory Network': 5,\n",
       "  'DSMN with or without intermediate visual supervision': 0,\n",
       "  'Experiments': 6},\n",
       " '24129906': {'Conclusion': 0},\n",
       " '2616243': {'The GLR Parsing Algorithm': 2,\n",
       "  'The GLR* Parser': 1,\n",
       "  'The Parse Evaluation Heuristics': 0,\n",
       "  'The Parse Evaluation Features': 1,\n",
       "  'The Parse Quality Heuristic': 0,\n",
       "  'Parsing of Spontaneous Speech Using GLR*': 0},\n",
       " '739426': {},\n",
       " '3543642': {'Introduction': 13,\n",
       "  'Methodology': 2,\n",
       "  'Interface': 0,\n",
       "  'Results': 1,\n",
       "  'Italian-speaking participants': 0,\n",
       "  'Non-Italian speaking participants': 2,\n",
       "  'Overall discussion': 1,\n",
       "  'Averaging the acoustic transcriptions': 4,\n",
       "  'Conclusion': 0},\n",
       " '174798239': {'Introduction': 15,\n",
       "  'Related Work': 17,\n",
       "  'Model Description': 0,\n",
       "  'Unsupervised Relation Vector Learning': 9,\n",
       "  'Learning Relational Word Vectors': 0,\n",
       "  'Experimental Setting': 1,\n",
       "  'Experimental Results': 6,\n",
       "  'Relation Classification': 6,\n",
       "  'Lexical Feature Modelling': 6,\n",
       "  'Analysis': 0,\n",
       "  'Word Embeddings: Nearest Neighbours': 0,\n",
       "  'Word Relation Encoding': 1,\n",
       "  'Lexical Memorization': 2,\n",
       "  'Conclusions': 2},\n",
       " '81982064': {'Introduction': 8,\n",
       "  'Related Work': 15,\n",
       "  'Proposed Model': 0,\n",
       "  'Encoder-Decoder': 1,\n",
       "  'Adding sentential context': 3,\n",
       "  'Improved sentence-level features': 1,\n",
       "  'Experiments': 1,\n",
       "  'Datasets': 9,\n",
       "  'Models': 6,\n",
       "  'Results': 0,\n",
       "  'Historical languages': 2,\n",
       "  'Standard languages': 0,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgments': 0,\n",
       "  'Dataset Statistics': 0},\n",
       " '189897682': {'Introduction': 20,\n",
       "  'Standard coreference resolution': 13,\n",
       "  'Gender bias in standard coreference resolution': 10,\n",
       "  '“Difficult\" coreference resolution': 5,\n",
       "  'Hard-CoRe Coreference Task': 0,\n",
       "  'Dataset Creation': 3,\n",
       "  'Task Characteristics': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Experiments and Results': 5,\n",
       "  'Analysis by Switching Entities': 4,\n",
       "  'Conclusion': 0},\n",
       " '16124522': {'Introduction': 2,\n",
       "  'Related Work': 0,\n",
       "  'Neural Language Models': 2,\n",
       "  'Training': 1,\n",
       "  'Results and Discussion': 0,\n",
       "  'Word Comparisons': 0,\n",
       "  'Periods of Change': 1,\n",
       "  'Limitations': 0,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '15902640': {'Introduction': 8,\n",
       "  'Model': 1,\n",
       "  'Hierarchical Character-level Language Model ( HCLM \\\\mathrm {HCLM})': 1,\n",
       "  'Continuous cache component': 0,\n",
       "  'Character-level Neural Cache Language Model': 0,\n",
       "  'Training objective': 0,\n",
       "  'Datasets': 0,\n",
       "  'Penn Tree Bank (PTB)': 0,\n",
       "  'WikiText-2': 0,\n",
       "  'Multilingual Wikipedia Corpus (MWC)': 0,\n",
       "  'Experiments': 0,\n",
       "  'Results': 2,\n",
       "  'Analysis': 2,\n",
       "  'Discussion': 0,\n",
       "  'Related Work': 9,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Corpus Statistics': 0},\n",
       " '52157814': {'Machine translation for gisting': 0,\n",
       "  'Evaluation of MT for gisting': 15,\n",
       "  'Reading comprehension questionnaires': 12,\n",
       "  'An alternative: evaluation via gap-filling': 15,\n",
       "  'Data and informants': 6,\n",
       "  'Reading comprehension questionnaire task': 0,\n",
       "  'Gap filling task': 3,\n",
       "  'Results': 0,\n",
       "  'Reading comprehension questionnaire scores': 0,\n",
       "  'Gap-filling': 0,\n",
       "  'Correlation between GF and RCQ': 0,\n",
       "  'Concluding remarks': 0},\n",
       " '1998416': {'Acknowledgment': 0, 'Alignment Visualization': 2},\n",
       " '2315102': {'Introduction': 6,\n",
       "  'Related Work': 23,\n",
       "  'SIMT Motivation': 2,\n",
       "  'Modality and Negation': 5,\n",
       "  'The Modality/Negation Annotation Scheme': 1,\n",
       "  'Anatomy of Modality/Negation in Sentences': 0,\n",
       "  'Linguistic Simplifications for Efficient Operationalization': 0,\n",
       "  'The English Modality/Negation Lexicon': 1,\n",
       "  'Automatic Modality/Negation Annotation': 0,\n",
       "  'The String-based English Modality/Negation Tagger': 1,\n",
       "  'The Structure-based English Modality/Negation Tagger': 1,\n",
       "  'Evaluating the Effectiveness of Structure-based MN Tagging': 1,\n",
       "  'Semantically-Informed Syntactic MT': 0,\n",
       "  'Refinement of Translation Grammars with Semantic Categories': 3,\n",
       "  'Tree-Grafting Algorithm': 0,\n",
       "  'SIMT Results': 6,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 1},\n",
       " '6751341': {'Introduction': 3,\n",
       "  'Error Generation Methods': 0,\n",
       "  'Machine Translation': 9,\n",
       "  'Pattern Extraction': 2,\n",
       "  'Error Detection Model': 4,\n",
       "  'Evaluation': 5,\n",
       "  'Related Work': 0,\n",
       "  'Conclusion': 0},\n",
       " '59599633': {'Introduction': 7,\n",
       "  'Related work': 18,\n",
       "  'Principled unsupervised SMT': 0,\n",
       "  'Initial phrase-table': 4,\n",
       "  'Adding subword information': 3,\n",
       "  'Unsupervised tuning': 6,\n",
       "  'Joint refinement': 0,\n",
       "  'NMT hybridization': 2,\n",
       "  'Experiments and results': 5,\n",
       "  'Main results': 2,\n",
       "  'Comparison with supervised systems': 2,\n",
       "  'Qualitative results': 4,\n",
       "  'Conclusions and future work': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '85499972': {'Introduction': 11,\n",
       "  'The Supervised Translation Problem': 0,\n",
       "  'Existing Solution Methods': 14,\n",
       "  'The Unsupervised Translation Problem': 8,\n",
       "  'The Effect of Noise': 1,\n",
       "  'Noise-aware Model': 0,\n",
       "  'Bilingual Word Embedding': 4,\n",
       "  'Diachronic (Historical) Word Embedding': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '4311819': {'Introduction': 9},\n",
       " '173990455': {'Introduction': 14,\n",
       "  'BiLSTM': 1,\n",
       "  'Large Margin Cosine Loss (LMCL)': 1,\n",
       "  'Local Outlier Factor (LOF)': 1,\n",
       "  'Datasets': 1,\n",
       "  'Baselines': 2,\n",
       "  'Experimental Settings': 4,\n",
       "  'Results and Discussion': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '359042': {'Introduction': 17,\n",
       "  'Background: Memory Network': 2,\n",
       "  'Deep Memory Network for Aspect Level Sentiment Classification': 0,\n",
       "  'Task Definition and Notation': 2,\n",
       "  'An Overview of the Approach': 2,\n",
       "  'Content Attention': 1,\n",
       "  'Location Attention': 0,\n",
       "  'The Need for Multiple Hops': 1,\n",
       "  'Aspect Level Sentiment Classification': 11,\n",
       "  'Experiment': 0,\n",
       "  'Experimental Setting': 1,\n",
       "  'Comparison to Other Methods': 3,\n",
       "  'Runtime Analysis': 0,\n",
       "  'Effects of Location Attention': 0,\n",
       "  'Visualize Attention Models': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Related Work': 0,\n",
       "  'Compositionality in Vector Space': 16,\n",
       "  'Attention and Memory Networks': 7,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '22336489': {'Introduction': 3,\n",
       "  'Network Architecture': 8,\n",
       "  'Alternative Methods': 0,\n",
       "  'Labeling Through Backpropagation': 3,\n",
       "  'Relative Frequency Baseline': 0,\n",
       "  'Supervised Sequence Labeling': 0,\n",
       "  'Datasets': 0,\n",
       "  'CoNLL 2010 Uncertainty Detection': 1,\n",
       "  'FCE Error Detection': 4,\n",
       "  'SemEval Sentiment Detection in Twitter': 3,\n",
       "  'Implementation Details': 5,\n",
       "  'Evaluation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '51977305': {'Introduction': 0,\n",
       "  'Linguistically Interpreted Corpora': 0,\n",
       "  'Existing Treebank Formats': 9,\n",
       "  'Language-Specific Features': 4,\n",
       "  'Annotating Argument Structure': 1,\n",
       "  'Architecture': 0,\n",
       "  'Argument Structure': 0,\n",
       "  'Grammatical Functions': 0,\n",
       "  'Structure Sharing': 0,\n",
       "  'Treatment of Selected Phenomena': 0,\n",
       "  'Noun Phrases': 0,\n",
       "  'Attachment Ambiguities': 0,\n",
       "  'Coordination': 0,\n",
       "  'Requirements': 0,\n",
       "  'Implementation': 1,\n",
       "  'Automation': 0,\n",
       "  'Assigning Grammatical Function Labels': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '102354561': {'Introduction': 4,\n",
       "  'Related work': 13,\n",
       "  'Background: Reinforcement learning': 3,\n",
       "  'Training': 1,\n",
       "  'Evaluation': 0,\n",
       "  'Proposed method': 0,\n",
       "  'Supervised learning': 1,\n",
       "  'Ternary reward structure': 1,\n",
       "  'Experimental setup': 5,\n",
       "  'Results': 0,\n",
       "  'Conclusions': 0},\n",
       " '196202286': {'Introduction': 2,\n",
       "  'Learning Head-Tail Distribution': 0,\n",
       "  'Formal Definition of Fact Distribution': 0,\n",
       "  'Neural architecture design': 0,\n",
       "  'Training': 1,\n",
       "  'Quantifying Similarity': 0,\n",
       "  'Relations as Distributions': 0,\n",
       "  'Defining Similarity': 0,\n",
       "  'Calculating Similarity': 0,\n",
       "  'Relationship with other metrics': 4,\n",
       "  'Dataset Construction': 0,\n",
       "  'Wikidata': 1,\n",
       "  'ReVerb Extractions': 1,\n",
       "  'FB15K and TACRED': 2,\n",
       "  'Human Judgments': 5,\n",
       "  'Redundant Relation Removal': 7,\n",
       "  'Toy Experiment': 0,\n",
       "  'Real World Experiment': 0,\n",
       "  'Error Analysis for Relational Classification': 0,\n",
       "  'Relation Prediction': 2,\n",
       "  'Relation Extraction': 1,\n",
       "  'Results': 0,\n",
       "  'Similarity and Negative Sampling': 2,\n",
       "  'Similarity and Softmax-Margin Loss': 2,\n",
       "  'Related Works': 30,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Proofs to theorems in the paper': 1,\n",
       "  'Chinese Restaurant Process': 0,\n",
       "  'Training Details': 0,\n",
       "  'Training Details on Negative Sampling': 0,\n",
       "  'Training Details on Softmax-Margin Loss': 0,\n",
       "  'Recall Standard Deviation': 0,\n",
       "  'Negative Samplilng with Relation Type Constraints': 0,\n",
       "  'Add Up Two Uniform Distribution': 0,\n",
       "  'Add Weight': 0,\n",
       "  'Wikidata annotation guidance': 0},\n",
       " '11744937': {'Introduction': 14,\n",
       "  'Background: Language Modeling': 1,\n",
       "  'Methodology': 1,\n",
       "  'Experimental Setup': 5,\n",
       "  'Discussion': 6,\n",
       "  'Conclusions': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '3915390': {'Introduction': 15,\n",
       "  'Related Work': 17,\n",
       "  'Experimental Setup': 2,\n",
       "  'How well does each word embedding model allow us to predict neural activation patterns in human brain?': 3,\n",
       "  'What is the best word embedding model for predicting brain activation for different (classes of) nouns? ': 4,\n",
       "  '25 features vs experiential ': 0,\n",
       "  'GloVe vs Dependency-based word2vec': 0,\n",
       "  'Experiential vs Dependency-based word2vec': 0,\n",
       "  'Which are the most predictable voxels in the brain for each word embedding model? ': 0,\n",
       "  'Discussion and Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '10636826': {'Introduction': 8,\n",
       "  'Dataset and Task': 0,\n",
       "  'Models': 0,\n",
       "  'Bi-Directional LSTMs': 11,\n",
       "  'Baselines': 6,\n",
       "  'Experiments and Evaluation': 0,\n",
       "  'First Experiment': 2,\n",
       "  'Second Experiment': 1,\n",
       "  'Conclusions': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '6444309': {'Introduction': 8,\n",
       "  'Background': 1,\n",
       "  'Elements of Coordination Structure': 2,\n",
       "  'Coordinations in the Penn Tree Bank': 0},\n",
       " '14330405': {'Introduction': 0,\n",
       "  'Motivation': 3,\n",
       "  'The Bootstrap Method': 0,\n",
       "  'Experimental Setup': 0,\n",
       "  'Data': 1,\n",
       "  'Learning Algorithms': 4,\n",
       "  'Sampling Method': 0,\n",
       "  'Experiments': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Further Observations': 0,\n",
       "  'Summary and Further Research': 2},\n",
       " '29168700': {'Introduction': 16,\n",
       "  'Task-Oriented Dialogue': 0,\n",
       "  'Motivation and Related Work': 20,\n",
       "  'Neural User Simulator': 0,\n",
       "  'Goal Generator': 3,\n",
       "  'Feature Extractor': 0,\n",
       "  'Sequence-To-Sequence Model': 1,\n",
       "  'Training': 5,\n",
       "  'Experimental Setup': 0,\n",
       "  'Testing with a simulated user': 1,\n",
       "  'Testing with real users': 1,\n",
       "  'Cross-Model Evaluation': 0,\n",
       "  'Human Evaluation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '173188413': {'Introduction': 16,\n",
       "  'Related Work': 5,\n",
       "  'Models': 1,\n",
       "  'Explanations': 0,\n",
       "  'Dataset': 0,\n",
       "  'Classification Accuracy': 2,\n",
       "  'Explanation Evaluation Methods': 0,\n",
       "  'HIT Design': 0,\n",
       "  'Quality Control': 2,\n",
       "  'Explanation Evaluation Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '174798228': {'Introduction': 3,\n",
       "  'Related Work': 12,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Appendix: Languages and Datasets': 2,\n",
       "  'Appendix: Model Parameters and Training': 1,\n",
       "  'Appendix: Additional Results': 0},\n",
       " '2238015': {'Introduction': 5,\n",
       "  'Related Work': 11,\n",
       "  'Model': 3,\n",
       "  'Formulation': 2,\n",
       "  'Algorithm': 0,\n",
       "  'Fixed point continuation for DRMC-b': 1,\n",
       "  'Fixed point continuation for DRMC-1': 0,\n",
       "  'Experiments': 4,\n",
       "  'Dataset': 0,\n",
       "  'Parameter setting': 1,\n",
       "  'Rank estimation': 0,\n",
       "  'Method Comparison': 5,\n",
       "  'Discussion': 1,\n",
       "  'Conclusion and Future Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '4812047': {'Introduction': 3},\n",
       " '2389139': {'Introduction': 19,\n",
       "  'Related Work': 4,\n",
       "  'Attention Models': 0,\n",
       "  'Comparing Attention with Alignment': 0,\n",
       "  'Measuring Attention-Alignment Accuracy': 0,\n",
       "  'Measuring Attention Concentration': 0,\n",
       "  'Empirical Analysis of Attention Behaviour': 2,\n",
       "  'Impact of Attention Mechanism': 4,\n",
       "  'Alignment Quality Impact on Translation': 4,\n",
       "  'Attention Concentration': 0,\n",
       "  'Attention Distribution': 1,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '52155319': {'Introduction': 10,\n",
       "  'Related Works': 21,\n",
       "  'Scheme': 3,\n",
       "  'Dataset': 3,\n",
       "  'Annotation Design': 4,\n",
       "  'Annotation Result': 4,\n",
       "  'Discriminating ACC Type, Inner-Post Relation and Inter-Post Interaction': 0,\n",
       "  'Thread Representation as a Sequence': 3,\n",
       "  'Parallel Constrained Pointer Architecture': 8,\n",
       "  'Experimental Settings': 9,\n",
       "  'Experimental Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '500600': {'Introduction': 7,\n",
       "  'Task Decomposition': 0,\n",
       "  'A Novel NER++ Method': 0,\n",
       "  'Results': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '51972934': {'Introduction': 12,\n",
       "  'Model': 0,\n",
       "  'Character-based Input Representation': 1,\n",
       "  'Semi-Markov CRF': 6,\n",
       "  'Experiments and Analysis': 2,\n",
       "  'Multilingual Experiments on Clean Data': 12,\n",
       "  'Analysis on Noisy Data': 0,\n",
       "  'Related Work': 23,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52112703': {},\n",
       " '30617432': {'Introduction': 13,\n",
       "  'Related Work': 18,\n",
       "  'Context Selection: Methodology': 1,\n",
       "  'Context Configuration Space': 9,\n",
       "  'Class-Specific Configuration Search': 0,\n",
       "  'Implementation Details': 16,\n",
       "  'Training and Evaluation': 4,\n",
       "  'Baselines': 6,\n",
       "  'Main Evaluation Setup': 1,\n",
       "  'Generalisation: Configuration Transfer': 6,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52153975': {'Introduction': 12,\n",
       "  'Background': 1,\n",
       "  'Dialogue ontology': 6,\n",
       "  'Natural language understanding': 9,\n",
       "  'Related work': 11,\n",
       "  'CBT ontology': 4,\n",
       "  'Thinking errors': 0,\n",
       "  'Emotions': 1,\n",
       "  'Situations': 0,\n",
       "  'The corpus': 2,\n",
       "  'Annotation': 0,\n",
       "  'Distributed representations': 8,\n",
       "  'Convolutional neural network model': 1,\n",
       "  'Gated recurrent unit model': 0,\n",
       "  'Training set-up': 4,\n",
       "  'Baselines': 3,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '49480783': {'Introduction': 11,\n",
       "  'Preliminaries': 4,\n",
       "  'Generating contrastive adversarial samples': 3,\n",
       "  'Intra-pair hard negative mining': 1,\n",
       "  'Experiments': 0,\n",
       "  'Adversarial attacks': 2,\n",
       "  'Saliency visualization': 0,\n",
       "  'Correlate words and objects': 5,\n",
       "  'Concept to word': 0,\n",
       "  'Discussion and conclusion': 2,\n",
       "  'Semantic Overlap Table of Frequent Prepositions': 0,\n",
       "  'Training Details of VSE-C': 2},\n",
       " '52099643': {'Introduction': 18},\n",
       " '186206925': {'Introduction': 1,\n",
       "  'Core Design Structure': 0,\n",
       "  'Learned Classifiers': 1,\n",
       "  'Candidate Retrieval': 0,\n",
       "  'Minimal Perspective Discovery': 1,\n",
       "  'Utilizing user feedback': 0,\n",
       "  'Related Work': 7,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '30908': {'Introduction': 7,\n",
       "  'Related Work': 3,\n",
       "  'Word Classes': 1,\n",
       "  'Smoothing Models': 0,\n",
       "  'Mapping All Words at Once (map-all)': 0,\n",
       "  'Mapping Each Word at a Time (map-each)': 0,\n",
       "  'Setup': 6,\n",
       "  'Comparison of Vocabularies': 3,\n",
       "  'Comparison of Smoothing Models': 0,\n",
       "  'Comparison of Training Data Size': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '174801377': {'Introduction': 22,\n",
       "  'Related Work': 24,\n",
       "  'Model Architecture': 0,\n",
       "  'Dual Encoders': 2,\n",
       "  'Scoring': 0,\n",
       "  'Training': 1,\n",
       "  'Whitelist Generation': 0,\n",
       "  'Experiments and Results': 0,\n",
       "  'Data': 0,\n",
       "  'Model Details': 3,\n",
       "  'Results and Analysis': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '23144639': {'Acknowledgments': 1},\n",
       " '49430686': {'Introduction': 5,\n",
       "  'Neural Graph-to-Sequence Model': 0,\n",
       "  'Gated Graph Neural Networks': 3,\n",
       "  'Using GGNNs in attentional encoder-decoder models': 2,\n",
       "  'Bidirectionality and positional embeddings': 0,\n",
       "  'Levi Graph Transformation': 4,\n",
       "  'Generation from AMR Graphs': 1,\n",
       "  'Experimental setup': 9,\n",
       "  'Results and analysis': 1,\n",
       "  'Syntax-based Neural Machine Translation': 0,\n",
       "  'Discussion and Conclusion': 1,\n",
       "  'Acknowledgements': 0,\n",
       "  'Simplification and Anonymisation for AMR graphs': 3,\n",
       "  'Model Hyperparameters': 1,\n",
       "  'Vocabulary': 0,\n",
       "  'Model': 2,\n",
       "  'Training': 3,\n",
       "  'Decoding': 1},\n",
       " '14817153': {'Introduction': 1,\n",
       "  'Related Work': 4,\n",
       "  'Empirical Replication Study': 2,\n",
       "  'Frequency-Based Algorithm (FBA)': 5,\n",
       "  'Dependency-Based Algorithm (DBA)': 2,\n",
       "  'Translation-based Algorithm (TBA)': 2,\n",
       "  'Preliminary Results': 0,\n",
       "  'Discussion and further directions': 1,\n",
       "  'Conclusions': 0},\n",
       " '174802484': {'Introduction': 14,\n",
       "  'Related Work': 12,\n",
       "  'The New Dataset': 1,\n",
       "  'Limitations and Future Work': 7,\n",
       "  'Acknowledgements': 0,\n",
       "  'eurlex57k statistics': 1,\n",
       "  'Hyper-paramater tuning': 2,\n",
       "  'Evaluation Measures': 0,\n",
       "  'Experimental Results': 0},\n",
       " '119186140': {'Acknowledgements': 0},\n",
       " '184488364': {'Conclusions': 0, 'Acknowledgement': 0},\n",
       " '51985973': {'Introduction': 5,\n",
       "  'Short Text Conversation': 10,\n",
       "  'Evaluation for Conversational Systems': 7,\n",
       "  'Addressee and Response Selection': 8,\n",
       "  'Cross-Lingual Conversation': 0,\n",
       "  'Multilingual Addressee and Response Selection': 0,\n",
       "  'Motivative Situations': 0,\n",
       "  'Task Overview': 0,\n",
       "  'Formal Task Definition': 0,\n",
       "  'Methods': 2,\n",
       "  '(a) Multilingual Embedding Replacement': 0,\n",
       "  'Extended Methods': 0,\n",
       "  '(b) Fine-Tuning with Target Language Data': 1,\n",
       "  '(c) Joint Loss Training': 0,\n",
       "  '(d) Multi-Language Adversarial Training': 2,\n",
       "  'Corpus and Dataset': 1,\n",
       "  'Experimental Setup': 0,\n",
       "  '(a) Single-Language Adaptation': 0,\n",
       "  '(b) Multi-Language Adaptation': 1,\n",
       "  'Results': 0,\n",
       "  'Data Augmentation with NMT': 0,\n",
       "  'Analysis of Number of Agents': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Statics of Dataset': 0,\n",
       "  'Results for each language': 0},\n",
       " '121176': {'Theoretical Background': 0,\n",
       "  'The Language Acquisition Device': 0,\n",
       "  'The Grammar (set)': 0,\n",
       "  'The Parser': 0,\n",
       "  'The Parameter Setting Algorithm': 0,\n",
       "  'The Simulation Model': 0,\n",
       "  'Effectiveness of Learning Procedures': 0,\n",
       "  'Evolution of Learning Procedures': 0,\n",
       "  'Emergence of Language and Learners': 0,\n",
       "  'Conclusions': 0},\n",
       " '135465038': {'Introduction': 16,\n",
       "  'Data': 4,\n",
       "  'Graph linearization': 2,\n",
       "  'Model': 6,\n",
       "  'Semi-supervised training': 4,\n",
       "  'Results': 2,\n",
       "  'Baselines': 5,\n",
       "  'Out of domain evaluation': 0,\n",
       "  'Attribute ablations': 1,\n",
       "  'Comparison with AMR generation': 2,\n",
       "  'Error analysis': 2},\n",
       " '437687': {'Introduction': 7,\n",
       "  'Background': 9,\n",
       "  'Neural Belief Tracker': 0,\n",
       "  'Representation Learning': 5,\n",
       "  'Semantic Decoding': 0,\n",
       "  'Context Modelling': 0,\n",
       "  'Belief State Update Mechanism': 0,\n",
       "  'Datasets': 2,\n",
       "  'Models': 2,\n",
       "  'Belief Tracking Performance': 0,\n",
       "  'The Importance of Word Vector Spaces': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '49657219': {'Introduction': 6,\n",
       "  'Methodology': 1,\n",
       "  'Replicated, Multi-and-Cross-Level, Multi-Channel Siamese LSTM': 12,\n",
       "  'Neural Auto-Regressive Topic Model': 10,\n",
       "  'Multi-Channel Manhattan Metric': 0,\n",
       "  'Evaluation and Analysis': 0,\n",
       "  'Industrial Dataset for Ticketing System': 0,\n",
       "  'Experimental Setup: Unsupervised': 0,\n",
       "  'Experimental Setup: Supervised': 4,\n",
       "  'Results: State-of-the-art Comparisons': 0,\n",
       "  'Success Rate: End-User Evaluation': 0,\n",
       "  'Qualitative Inspections for STS and IR': 0,\n",
       "  'Related Work': 13,\n",
       "  'Conclusion and Discussion': 3,\n",
       "  'Acknowledgements': 0},\n",
       " '49215220': {'Introduction': 1,\n",
       "  'Open IE Systems': 0,\n",
       "  'Learning-based Systems': 4,\n",
       "  'Rule-based Systems': 6,\n",
       "  'Clause-based Systems': 1,\n",
       "  'Systems Capturing Inter-Proposition Relationships': 12,\n",
       "  'Evaluation': 0},\n",
       " '170078653': {'Introduction': 12,\n",
       "  'Related Work & Background': 21,\n",
       "  'Dialog Act Representation': 0,\n",
       "  'Model': 0,\n",
       "  'Automatic Evaluation': 6,\n",
       "  'Human Evaluation': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Details of Model Implementation': 0,\n",
       "  'Baseline Implementation': 0,\n",
       "  'Human Evaluation Interface': 0,\n",
       "  'Controllability Evaluation': 0,\n",
       "  'Enumeration of all the Dialog Acts': 0},\n",
       " '52286383': {'Introduction': 9,\n",
       "  'Related Work': 0,\n",
       "  'Neural Network Models': 17,\n",
       "  'Incorporating User and Product Information': 2,\n",
       "  'The DUPMN Model': 1,\n",
       "  'Task Definition': 0,\n",
       "  'Document Embedding': 1,\n",
       "  'Memory Network Structure': 3,\n",
       "  'Experiment and Result Analysis': 2,\n",
       "  'Datasets': 4,\n",
       "  'Baseline Methods': 9,\n",
       "  'Performance Evaluation': 5,\n",
       "  'Case Analysis': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '19751968': {'Acknowledgments': 0},\n",
       " '431360': {'Introduction': 5,\n",
       "  'System Overview': 17,\n",
       "  'Overview': 0,\n",
       "  'Compound Word Translation': 3,\n",
       "  'Transliteration': 1,\n",
       "  'Evaluation': 1,\n",
       "  'Evaluation of compound word translation': 0,\n",
       "  'Evaluation of transliteration': 0,\n",
       "  'Evaluation of the overall performance': 0,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '201070451': {'Introduction': 5,\n",
       "  'Related Work': 6,\n",
       "  'Data': 0,\n",
       "  'Pre-processing': 2,\n",
       "  'System Architecture - The Transference Model': 1,\n",
       "  'Experiments': 0,\n",
       "  'Experiment Setup': 3,\n",
       "  'Hyper-parameter Setup': 4,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '33865525': {'Introduction': 17,\n",
       "  'Related Work': 11,\n",
       "  'Word Embedding Pre-training': 3,\n",
       "  'AA Model': 0,\n",
       "  'Experimental Setup': 7,\n",
       "  'Results and Discussion': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion and Future Work': 1},\n",
       " '6985080': {'Introduction': 15,\n",
       "  'The Collins Structured Perceptron': 1,\n",
       "  'The Structured Weighted Violations Perceptron (SWVP)': 0,\n",
       "  'Basic Concepts': 0,\n",
       "  'Algorithm': 0,\n",
       "  'Theory': 0,\n",
       "  'Convergence for Linearly Separable Data': 1,\n",
       "  'Convergence Properties': 1,\n",
       "  'Mistake and Generalization Bounds': 0,\n",
       "  'Passive Aggressive SWVP': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Proof Observation 1.': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '14232249': {'Introduction': 8,\n",
       "  'Related Work': 24,\n",
       "  'New Data and Annotation': 0,\n",
       "  'Improved Phrase Summarization': 1,\n",
       "  'Candidate Phrase Extraction': 1,\n",
       "  'Similarity Learning': 8,\n",
       "  'Phrase Clustering': 6,\n",
       "  'Summary Evaluation': 1,\n",
       "  'ROUGE': 5,\n",
       "  'A New Metric based on Color Matching': 0,\n",
       "  'Example Summaries': 1,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '6160155': {'Introduction': 20,\n",
       "  'System Description': 0,\n",
       "  'The Reverse Map': 0,\n",
       "  'Connectivity Matrix of the Reverse Map': 1,\n",
       "  'The Node-Graph Architecture': 0,\n",
       "  'The Similarity Measure': 0,\n",
       "  'System Summary': 0,\n",
       "  'Graph exploration': 2,\n",
       "  'Performance Analysis': 4,\n",
       "  'Performance Test': 0,\n",
       "  'Performance results': 1,\n",
       "  'Recommendations': 3,\n",
       "  'Concluding Remarks': 11},\n",
       " '11839232': {},\n",
       " '2267179': {'Introduction': 7,\n",
       "  'Intent Detection': 4,\n",
       "  'Slot Filling': 6,\n",
       "  'RNN Language Model': 3,\n",
       "  'RNN for Intent Detection and Slot Filling': 2,\n",
       "  'Method': 0,\n",
       "  'Model': 0,\n",
       "  'Next Step Prediction': 0,\n",
       "  'Training': 0,\n",
       "  'Inference': 2,\n",
       "  'Model Variations': 0,\n",
       "  'Data': 5,\n",
       "  'Training Procedure': 4,\n",
       "  'Results and Discussions': 0,\n",
       "  'Conclusion': 0},\n",
       " '72940921': {'Introduction': 18,\n",
       "  'General methods': 10,\n",
       "  'Experimental methodology': 2,\n",
       "  'Models tested': 7,\n",
       "  'Subordinate clauses': 5,\n",
       "  'Maintenance and degradation of syntactic state': 1,\n",
       "  'Garden path effects': 5,\n",
       "  'NP/Z Ambiguity': 5,\n",
       "  'Main Verb/Reduced Relative Ambiguity': 0,\n",
       "  'General Discussion and Conclusion': 0},\n",
       " '49541428': {'Introduction': 4,\n",
       "  'Word Embeddings': 6,\n",
       "  'Word Embedding Recommendations': 9,\n",
       "  'Multilingual Cultural Heritage Data': 3,\n",
       "  'Goal and Approach': 0,\n",
       "  'The DualNeighbors Algorithm': 1,\n",
       "  'FSA-OWI Captions': 2,\n",
       "  'News Twitter Reports': 2,\n",
       "  'Connectivity': 3,\n",
       "  'Relevance': 0,\n",
       "  'Implementation': 0,\n",
       "  'Conclusions': 4,\n",
       "  'Acknowledgements': 0},\n",
       " '6179947': {'Conclusion': 0, 'Acknowledgements': 0},\n",
       " '52908627': {'Introduction': 14,\n",
       "  'Data Collection Procedure': 4,\n",
       "  'Data Statistics': 1,\n",
       "  'Systems in the Competition': 3,\n",
       "  'Word-overlap Metrics': 2,\n",
       "  'Results of Human Evaluation': 6,\n",
       "  'Conclusion': 5,\n",
       "  'Acknowledgements': 0},\n",
       " '15360530': {'Project idea': 0,\n",
       "  'Outline of the generation process': 3,\n",
       "  'The Knowledge Base': 1,\n",
       "  'Document Representation Using RST': 2,\n",
       "  'Event simulation in the knowledge base': 0,\n",
       "  'Towards a document authoring tool': 0,\n",
       "  'From text generation to a multimodal information system': 0,\n",
       "  'Implementation Note': 1,\n",
       "  'Acknowledgements': 0,\n",
       "  'Bibliographical note': 0},\n",
       " '6143824': {'Introduction': 12,\n",
       "  'Related Work': 18,\n",
       "  'The Proposed Model': 0,\n",
       "  'The 4-Level Hierarchical Model': 3,\n",
       "  'Linguistic Features': 9,\n",
       "  'Experiment and Evaluation': 0,\n",
       "  'Dataset': 3,\n",
       "  'Implementation Detail': 2,\n",
       "  'Performance of Satirical News Detection': 8,\n",
       "  'Word Level Analysis': 0,\n",
       "  'Analysis of Weighted Linguistic Features': 5,\n",
       "  'Visualization of Attended Paragraph': 0,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '30651544': {'Introduction': 13,\n",
       "  'Related Work': 8,\n",
       "  'Dataset Description': 1,\n",
       "  'Proposed framework': 0,\n",
       "  \"Mitra's Method\": 2,\n",
       "  \"McCarthy's Method\": 0,\n",
       "  \"Lau's Method\": 1,\n",
       "  'Evaluation Framework and Results': 0,\n",
       "  'Discussion': 0,\n",
       "  'Parameter Tuning': 0,\n",
       "  'Conclusions and future work': 0},\n",
       " '4897319': {'Introduction': 8,\n",
       "  'Related Work': 17,\n",
       "  'Method': 0,\n",
       "  'Partition Unlabeled Data': 38},\n",
       " '51882837': {},\n",
       " '6519154': {'Acknowledgments': 0},\n",
       " '2879445': {'Introduction': 12,\n",
       "  'Definition of calibration': 3,\n",
       "  'Empirical calibration analysis': 0,\n",
       "  'Adaptive binning procedure': 3,\n",
       "  'Confidence interval estimation': 1,\n",
       "  'Visualizing calibration': 0,\n",
       "  'Calibration for classification and tagging models': 0,\n",
       "  'Naive Bayes and logistic regression': 7,\n",
       "  'Hidden Markov models and conditional random fields': 2,\n",
       "  'Coreference resolution': 0,\n",
       "  'Antecedent selection model': 2,\n",
       "  'Sampling-based inference': 3,\n",
       "  'Calibration analysis': 0,\n",
       "  'Entity-syntactic event aggregation': 5,\n",
       "  'Results': 2,\n",
       "  'Conclusion': 9,\n",
       "  'Acknowledgments': 0},\n",
       " '15314162': {'Introduction': 3,\n",
       "  'Keystroke dynamics': 1,\n",
       "  'From keystroke logs to auxiliary labels': 0},\n",
       " '102351489': {'Introduction': 6,\n",
       "  'Overview': 2,\n",
       "  'Implementation': 1,\n",
       "  'Validation': 7,\n",
       "  'Case study: refugee bias on talk radio': 3,\n",
       "  'Dataset and analyses': 3,\n",
       "  'Analysis 1: refugee bias over time': 2,\n",
       "  'Analysis 2: refugee bias by city': 0,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '52100401': {'Introduction': 7,\n",
       "  'Related Works': 16,\n",
       "  'Skip-Thought Generative Adversarial Network (STGAN)': 2,\n",
       "  'Skip-Thought Vectors': 2,\n",
       "  'Generative Adversarial Networks': 2,\n",
       "  'Model Architecture': 4,\n",
       "  'Improving Training and Loss': 7,\n",
       "  'Conditional Generation of Sentences.': 5,\n",
       "  'Language Generation.': 5,\n",
       "  'Proposed Oracle Experiment.': 0,\n",
       "  'Further Work': 4},\n",
       " '260001': {'Introduction': 4,\n",
       "  'Related Work': 11,\n",
       "  'LSTM Noisy Channel Model': 0,\n",
       "  'Channel Model': 2,\n",
       "  'Language Model': 5,\n",
       "  'Reranker': 0,\n",
       "  'Corpora for Language Modelling': 3,\n",
       "  'Results and Discussion': 1,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '11365698': {'Introduction': 6,\n",
       "  'Architecture': 12,\n",
       "  'Data': 1,\n",
       "  'Training': 3,\n",
       "  'Metrics': 4,\n",
       "  'Results': 4,\n",
       "  'Discussion and Conclusion': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '49210174': {'Introduction': 3,\n",
       "  'Historical Spelling Normalization': 10,\n",
       "  'Neural Machine Translation': 9,\n",
       "  'NMT Models': 0,\n",
       "  'Hypotheses': 2,\n",
       "  'Models': 0,\n",
       "  'Data': 0,\n",
       "  'Experimental Settings': 2,\n",
       "  'Results': 0,\n",
       "  'Word Accuracy': 0,\n",
       "  'CER': 0,\n",
       "  'NMT versus SMT': 0,\n",
       "  'Different NMT Models': 0,\n",
       "  'Conclusions and Future Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '1805495': {},\n",
       " '174802490': {'Introduction': 1,\n",
       "  'Related work': 4,\n",
       "  'Data': 0,\n",
       "  'DeriNet': 11,\n",
       "  'Word Embeddings': 11,\n",
       "  'Annotation of Derivational Relations': 0,\n",
       "  'Unsupervised Clustering': 0},\n",
       " '10361075': {},\n",
       " '52054914': {'Introduction': 13,\n",
       "  'The MedNLI dataset': 0,\n",
       "  'Premise sampling and hypothesis generation': 2,\n",
       "  'Annotation collection': 3,\n",
       "  'Dataset statistics': 2,\n",
       "  'Models': 2,\n",
       "  'Transfer learning': 3,\n",
       "  'Word embeddings': 7,\n",
       "  'Knowledge integration': 10,\n",
       "  'Implementation details': 4,\n",
       "  'Baselines': 0,\n",
       "  'Error analysis': 2,\n",
       "  'Limitations': 0,\n",
       "  'Applications': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '174801821': {'Introduction': 7},\n",
       " '5169126': {},\n",
       " '53218215': {'Introduction': 15,\n",
       "  'Related Work': 0,\n",
       "  'Multi-hop RC': 10,\n",
       "  'Semi-structured QA': 5,\n",
       "  'Knowledge Graph QA': 9,\n",
       "  'Approach Overview': 0,\n",
       "  'Path Extraction': 0,\n",
       "  'Model': 0,\n",
       "  'Embedding and Encoding': 2,\n",
       "  'Path Encoding': 1,\n",
       "  'Path Scoring': 0,\n",
       "  'Experiments': 0,\n",
       "  'Settings': 3,\n",
       "  'Results': 7,\n",
       "  'Conclusion': 0},\n",
       " '14029756': {'Introduction': 17,\n",
       "  'Sentence Pair Modeling with Subwords': 10,\n",
       "  'Pairwise Word Interaction (PWI) Model ': 3,\n",
       "  'Embedding Subwords in PWI Model': 2,\n",
       "  'Auxiliary Language Modeling (LM)': 1,\n",
       "  'Datasets': 4,\n",
       "  'Settings': 5,\n",
       "  'Results': 2,\n",
       "  'Combining Word and Subword Representations': 2,\n",
       "  'Model Ablations': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 1},\n",
       " '2958363': {'Introduction': 12,\n",
       "  'Background': 31,\n",
       "  'Algorithm': 4,\n",
       "  'Similarity measure': 1,\n",
       "  'Ranking': 1,\n",
       "  'Clustering': 4,\n",
       "  'Speed optimisation': 0,\n",
       "  'Evaluation': 4,\n",
       "  'Experiment procedure': 5,\n",
       "  'Experiment 1 - Baseline': 1,\n",
       "  'Experiment 2 - TextTiling': 1,\n",
       "  'Experiment 3 - DotPlot': 2,\n",
       "  'Experiment 4 - Segmenter': 1,\n",
       "  'Experiment 5 - Our algorithm, C99C99': 0,\n",
       "  'Summary': 0,\n",
       "  'Conclusions and future work': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '102353817': {'Introduction': 7,\n",
       "  'WiC: the Word-in-Context dataset': 0,\n",
       "  'Construction': 12,\n",
       "  'Quality check': 0,\n",
       "  'Statistics': 0,\n",
       "  'Experiments': 0,\n",
       "  'Results': 0,\n",
       "  'Related work': 4,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '8951658': {'Introduction': 13,\n",
       "  'Related Work': 8,\n",
       "  'Probabilistic KB Lookup': 0,\n",
       "  'Entity-Centric Knowledge Base (EC-KB)': 1,\n",
       "  'Notations and Assumptions': 0,\n",
       "  'Soft-KB Lookup': 0,\n",
       "  'Towards an End-to-End-KB-InfoBot': 0,\n",
       "  'Overview': 7,\n",
       "  'Belief Trackers': 3,\n",
       "  'Soft-KB Lookup + Summary': 1,\n",
       "  'Dialogue Policy': 3,\n",
       "  'Training': 3,\n",
       "  'Experiments and Results': 0,\n",
       "  'KB-InfoBot versions': 0,\n",
       "  'User Simulator': 6,\n",
       "  'Movies-KB': 0,\n",
       "  'Simulated User Evaluation': 1,\n",
       "  'Human Evaluation': 4,\n",
       "  'Conclusions and Discussion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Posterior Derivation': 0,\n",
       "  'Gated Recurrent Units': 1,\n",
       "  'REINFORCE updates': 2,\n",
       "  'Hyperparameters': 0},\n",
       " '189928148': {'Introduction': 15,\n",
       "  'Relation Classification': 3,\n",
       "  'Metric Based Few-Shot Learning': 10,\n",
       "  'Sentence Matching': 10,\n",
       "  'Task Definition': 1,\n",
       "  'Methodology': 0,\n",
       "  'Context Encoder': 4,\n",
       "  'Local Matching and Aggregation': 0,\n",
       "  'Instance Matching and Aggregation': 1,\n",
       "  'Class Matching': 0,\n",
       "  'Joint Training with Inconsistency Measurement': 0,\n",
       "  'Dataset and Evaluation Metrics': 1,\n",
       "  'Training Details and Hyperparameters': 3,\n",
       "  'Comparison with Previous Work': 7,\n",
       "  'Ablation Study': 2,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52842136': {'Introduction': 11,\n",
       "  'Related work': 11,\n",
       "  'Proposed Approach': 6,\n",
       "  'Experimental Setup': 9,\n",
       "  'Results': 6,\n",
       "  'Conclusions': 0},\n",
       " '61274': {'Introduction': 15,\n",
       "  'General Neural Model for Chinese Word Segmentation': 4,\n",
       "  'Embedding layer': 0,\n",
       "  'Feature layers': 2,\n",
       "  'Inference Layer': 1,\n",
       "  'Multi-Criteria Learning for Chinese Word Segmentation': 4,\n",
       "  'Model-I: Parallel Shared-Private Model': 0,\n",
       "  'Model-II: Stacked Shared-Private Model': 0,\n",
       "  'Model-III: Skip-Layer Shared-Private Model': 0,\n",
       "  'Objective function': 0,\n",
       "  'Incorporating Adversarial Training for Shared Layer': 4,\n",
       "  'Adversarial loss function': 1,\n",
       "  'Training': 1,\n",
       "  'Datasets': 2,\n",
       "  'Experimental Configurations': 3,\n",
       "  'Overall Results': 1,\n",
       "  'Speed': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Knowledge Transfer': 0,\n",
       "  'Simplified Chinese to Traditional Chinese': 0,\n",
       "  'Formal Texts to Informal Texts': 1,\n",
       "  'Related Works': 6,\n",
       "  'Conclusions & Future Works': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '174797793': {'Acknowledgements': 0},\n",
       " '119190241': {'Introduction': 8,\n",
       "  'Literary analyses of Invisible Cities': 0,\n",
       "  'A Computational Analysis': 0,\n",
       "  'Embedding city descriptions': 3,\n",
       "  'Clustering city representations': 2,\n",
       "  'Evaluating clustering assignments': 0,\n",
       "  'Quantitative comparison': 0,\n",
       "  'Examining the learned clusters': 0,\n",
       "  'Related work': 20,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '5660940': {'Introduction': 5,\n",
       "  'Model Description': 1,\n",
       "  'Amount of Unlabeled Data': 0,\n",
       "  'Autoencoding of Random Strings': 0,\n",
       "  'Related Work': 18,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '30215041': {'Introduction': 16,\n",
       "  'Proposed Approach': 0,\n",
       "  'Contextual Module': 0,\n",
       "  'Speaker Role Modeling': 2,\n",
       "  'Experiments': 0,\n",
       "  'Setup': 3,\n",
       "  'Results': 1,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '3158692': {'Introduction': 14,\n",
       "  'PPMI-SVD': 9,\n",
       "  'GloVe': 1,\n",
       "  'Skip-gram with Negative Sampling (SGNS)': 2,\n",
       "  'LexVec': 0,\n",
       "  'Materials': 21,\n",
       "  'Results': 3,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '543551': {'Introduction': 15,\n",
       "  'Model': 0,\n",
       "  'Anchored Rules': 2,\n",
       "  'Scoring Anchored Rules': 0,\n",
       "  'Features': 0,\n",
       "  'Grammar Refinements': 3,\n",
       "  'Learning': 3,\n",
       "  'Inference': 1,\n",
       "  'System Ablations': 1,\n",
       "  'Design Choices': 0,\n",
       "  'Test Results': 0,\n",
       "  'Penn Treebank': 3,\n",
       "  'SPMRL': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '168169888': {'Acknowledgments': 0},\n",
       " '13276568': {},\n",
       " '52097135': {'Introduction': 10,\n",
       "  'Related Work': 22,\n",
       "  'Approach': 0,\n",
       "  'Image Encoder': 1,\n",
       "  'Topic Extractor': 4,\n",
       "  'Stepwise Image-Topic Merging Decoder': 4,\n",
       "  'Experiment': 0,\n",
       "  'Datasets and Metrics': 12,\n",
       "  'Settings': 3,\n",
       "  'Results': 4,\n",
       "  'Analysis': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Results on COCO Evaluation Server': 4},\n",
       " '198985900': {'Introduction': 4,\n",
       "  'Background': 4,\n",
       "  'Data': 1,\n",
       "  'Phrase Embeddings': 0,\n",
       "  'Training': 5,\n",
       "  'Output: Unsupervised Phrase Table': 1,\n",
       "  'PBMT Model': 1,\n",
       "  'Output: PBMT Systems (cs→\\\\rightarrow de)': 0,\n",
       "  'Output: Synthetic Corpora': 1,\n",
       "  'Model and Training': 2,\n",
       "  'Post-processing': 0,\n",
       "  'Output: NMT Systems': 1,\n",
       "  'Benchmarks': 4,\n",
       "  'Final Evaluation': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '128344862': {},\n",
       " '174801388': {'Introduction': 20,\n",
       "  'Related Work': 15,\n",
       "  'Background: Encoder-Decoder with Conditional Copy': 5,\n",
       "  'Entity Memory and Hierarchical Attention': 0,\n",
       "  'Entity Memory': 3,\n",
       "  'Hierarchical Attention': 0,\n",
       "  'Training and Inference': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 1},\n",
       " '8225038': {'Introduction': 0,\n",
       "  'Extended typed λ\\\\lambda -calculus': 2,\n",
       "  'λ\\\\lambda -calculus of polymorphic type': 1,\n",
       "  'Extended combinators': 2,\n",
       "  'Cost of unification': 1,\n",
       "  'Chemical Abstract Machine': 1,\n",
       "  'Example: Japanese causative sentence': 1,\n",
       "  'Conclusion': 0},\n",
       " '20159073': {},\n",
       " '3425897': {'Acknowledgments': 0},\n",
       " '19681327': {'Introduction': 5,\n",
       "  'Related Work': 14,\n",
       "  \"Measuring the Veridicality of Users' Predictions\": 1,\n",
       "  'Mechanical Turk Annotation': 1,\n",
       "  'Veridicality Classifier': 1,\n",
       "  'Features': 2,\n",
       "  'Evaluation': 0,\n",
       "  'Performance on held-out event types': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Forecasting Contest Outcomes': 0,\n",
       "  'Prediction': 0,\n",
       "  'Sentiment Baseline': 4,\n",
       "  'Frequency Baseline': 0,\n",
       "  'Results': 0,\n",
       "  'Surprise Outcomes': 0,\n",
       "  'Assessing the Reliability of Accounts': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '173188803': {'Introduction': 11,\n",
       "  'The Goodreads Book Review Dataset': 1,\n",
       "  'The Proposed Approach: SpoilerNet': 2,\n",
       "  'Experiments': 6,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '53080846': {'Introduction': 9,\n",
       "  'Multi-Layer, Multi-Head Attention': 2,\n",
       "  'Integrating with Simple PPDB': 3,\n",
       "  'Impacts of Multi-Layer, Multi-Head Attention Architecture': 1,\n",
       "  'Impacts of Integrating the Simple PPDB': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledge': 0},\n",
       " '6508854': {'Introduction': 20,\n",
       "  'Related Work': 20,\n",
       "  'The Neural Language Generator': 7,\n",
       "  'Training Multi-domain Models': 0,\n",
       "  'Model Fine-Tuning': 0,\n",
       "  'Data Counterfeiting': 0,\n",
       "  'Discriminative Training': 0,\n",
       "  'Datasets': 0,\n",
       "  'Corpus-based Evaluation': 2,\n",
       "  'Experimental Setup': 3,\n",
       "  'Human Evaluation': 1,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6869582': {},\n",
       " '4718230': {},\n",
       " '394987': {'Introduction': 6,\n",
       "  'Related Work': 10,\n",
       "  'Constructive Discussions': 0,\n",
       "  'StreetCrowd': 0,\n",
       "  'Preprocessing': 0,\n",
       "  'Constructiveness in StreetCrowd': 0,\n",
       "  'Conversation analysis': 0,\n",
       "  'Idea flow': 2,\n",
       "  'Interaction dynamics': 4,\n",
       "  'Other linguistic features': 1,\n",
       "  'Experimental setup': 0,\n",
       "  'Discussion of the results (Table )': 0,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '4858508': {'Introduction': 6,\n",
       "  'The Attentional Model of Translation': 4,\n",
       "  '8-bit Translation': 7,\n",
       "  'Measurements': 0,\n",
       "  'Automatic results': 2,\n",
       "  'Human evaluation': 0,\n",
       "  'Discussion': 0,\n",
       "  'Related Work': 3,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '195345652': {'Introduction': 9,\n",
       "  'Data Format': 0,\n",
       "  'Tasks': 1,\n",
       "  'System Description': 0,\n",
       "  'Neural Network Architecture': 10,\n",
       "  'Postprocessing Prediction': 0,\n",
       "  'Data Conversion': 1,\n",
       "  'Training Procedure': 1,\n",
       "  'Evaluation Results': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Related Work': 8,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '46936631': {'Introduction': 6,\n",
       "  'Related Work': 25,\n",
       "  'Our Approach': 1,\n",
       "  'The Basic Framework': 8,\n",
       "  'Structure-Infused Copy Mechanisms': 2,\n",
       "  'Learning Objective and Beam Search': 2,\n",
       "  'Experiments': 0,\n",
       "  'Data Sets': 5,\n",
       "  'Experimental Setup': 1,\n",
       "  'Results': 3,\n",
       "  'Conclusion': 0},\n",
       " '53153643': {'Introduction': 13,\n",
       "  'Generative Explanation Framework': 0,\n",
       "  'Task Definition and Notations': 0,\n",
       "  'Base Text Encoder and Explanation Generator': 2,\n",
       "  'Explanation Factor': 0,\n",
       "  'Minimum Risk Training': 1,\n",
       "  'Example 1: Text Explanations': 2,\n",
       "  'Example 2: Numerical Explanations': 1,\n",
       "  'Dataset': 1,\n",
       "  'PCMag Review Dataset': 0,\n",
       "  'Skytrax User Reviews Dataset': 0,\n",
       "  'Experiment Results and Analyses': 0,\n",
       "  'Experimental Settings': 2,\n",
       "  'Experiment Results': 1,\n",
       "  'Case Study': 0,\n",
       "  'Related Work': 2,\n",
       "  'Conclusion': 0},\n",
       " '49560309': {},\n",
       " '174798343': {'Introduction': 4,\n",
       "  'Introducing the Asymptotic Analysis': 4,\n",
       "  'Recurrent Neural Networks': 2,\n",
       "  'Simple Recurrent Networks': 3,\n",
       "  'Long Short-Term Memory Networks': 6,\n",
       "  'Gated Recurrent Units': 2,\n",
       "  'RNN Complexity Hierarchy': 0,\n",
       "  'Attention': 6,\n",
       "  'Convolutional Networks': 6,\n",
       "  'Empirical Results': 0,\n",
       "  'Counting': 3,\n",
       "  'Counting with Noise': 2,\n",
       "  'Reversing': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Asymptotic Acceptance and State Complexity': 0,\n",
       "  'SRN Lemmas': 0,\n",
       "  'GRU Lemmas': 0,\n",
       "  'Attention Lemmas': 0,\n",
       "  'CNN Lemmas': 0,\n",
       "  'Neural Stack Lemmas': 1},\n",
       " '5062433': {'Conclusion and Future Work': 0, 'Acknowledgments': 0},\n",
       " '3262157': {'Introduction': 0,\n",
       "  'Data and Evaluation': 3,\n",
       "  'Results': 0,\n",
       "  'Concluding Remarks': 1,\n",
       "  'Acknowledgements': 0},\n",
       " '4328410': {'Conclusions and Future Work': 0},\n",
       " '52157566': {'Introduction': 4,\n",
       "  'Annotating Metaphor-Paraphrase Pairs in Contexts': 7,\n",
       "  'Annotation Results': 4,\n",
       "  'Modelling Paraphrase Judgments in Context': 6,\n",
       "  'MPAT Modelling Results': 5,\n",
       "  'Related Cognitive Work on Metaphor Aptness': 10,\n",
       "  'Conclusions and Future Work': 4},\n",
       " '5101528': {},\n",
       " '173990902': {'Introduction': 14,\n",
       "  'Related work': 18,\n",
       "  'Data': 0,\n",
       "  'TL;DRLegal': 0,\n",
       "  'TOS;DR': 0,\n",
       "  'Levels of abstraction and compression': 2,\n",
       "  'Readability': 5,\n",
       "  'Summarization baselines': 2,\n",
       "  'Discussion': 11,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '49687': {'Introduction': 1},\n",
       " '44099358': {'Acknowledgements': 0},\n",
       " '67856607': {'Hierarchy Label Graph Construction': 7,\n",
       "  'Initialization and Implementation Details': 2,\n",
       "  'Case Study': 1,\n",
       "  'Evaluation Results for Different Instances': 2},\n",
       " '174801461': {'Copyright': 0,\n",
       "  'Formatting Requirements in Brief': 0,\n",
       "  'What Files to Submit': 0,\n",
       "  'Using  to Format Your Paper': 0,\n",
       "  'Document Preamble': 0,\n",
       "  'Preparing Your Paper': 0},\n",
       " '52119858': {'Introduction': 14,\n",
       "  'Related Work': 17,\n",
       "  'Background': 0,\n",
       "  'The SPPI Framework': 1,\n",
       "  'Reinforcement Learning': 3,\n",
       "  'Preference Learning': 1,\n",
       "  'APRIL: Decomposing SPPI into Active Preference Learning and RL': 0,\n",
       "  'Simulation Results': 0,\n",
       "  'APL Phase Performance': 6,\n",
       "  'RL Phase Performance': 10,\n",
       "  'Complete Pipeline Performance': 0,\n",
       "  'Human Evaluation': 0,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgementblacks': 0},\n",
       " '6728280': {},\n",
       " '13751762': {'Introduction': 13,\n",
       "  'Related Works': 26,\n",
       "  'Preliminaries': 8,\n",
       "  'Methodology': 0,\n",
       "  'Framework Overview': 5,\n",
       "  'Learning Objective': 0,\n",
       "  'Bi-LSTM Feature Representation Transfer': 3,\n",
       "  'CRF Parameter Transfer': 2,\n",
       "  'Training': 1,\n",
       "  'Experiments': 0,\n",
       "  'Cross-Specialty NER': 11,\n",
       "  'NER Transfer Experiment on Non-medical Corpus': 7,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 9},\n",
       " '195345381': {'Introduction': 14,\n",
       "  'Response Re-ranking Using Event Causality Relations': 2,\n",
       "  'Neural Conversational Model (NCM)': 1,\n",
       "  'Event Causality Pairs': 2,\n",
       "  'Distributed Event Representation Based on Role Factored Tensor Model (RFTM)': 4,\n",
       "  'Event Causality Relation Matching Based on Distributed Event Representation': 0,\n",
       "  'Experiments': 4,\n",
       "  'Model Settings': 11,\n",
       "  'Diversity of Beam Search': 1,\n",
       "  'Comparison in Automatic Metrics': 5,\n",
       "  'Human Evaluation': 2,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '53113716': {'Introduction': 12,\n",
       "  'Joint Embedding': 0,\n",
       "  'Models': 0,\n",
       "  'Translation Tasks': 3,\n",
       "  'Translation Results': 2,\n",
       "  'Related Work': 5,\n",
       "  'Conclusion': 0},\n",
       " '3115914': {'Introduction': 3,\n",
       "  'Learning as Search Optimization': 4,\n",
       "  'Joint EDT Model': 0,\n",
       "  'Search Space': 0,\n",
       "  'An Example': 0,\n",
       "  'Linkage Type': 0,\n",
       "  'Feature Functions': 0,\n",
       "  'Base Features': 8,\n",
       "  'Decision Features': 0,\n",
       "  'Experimental Results': 0,\n",
       "  'Data': 0,\n",
       "  'Evaluation Metrics': 0,\n",
       "  'Joint versus Pipelined': 0,\n",
       "  'Feature Comparison for Coreference': 0,\n",
       "  'Linkage Types': 0,\n",
       "  'Discussion': 0},\n",
       " '5536079': {'IntroductionThe author is currently at Texas Instruments and all inquiries should be addressed to rajeev@csc.ti.com.': 8,\n",
       "  'The Need': 0,\n",
       "  'Evaluation Approach': 2,\n",
       "  'Knowledge Acquisition from Experts': 0,\n",
       "  'Mapping Algorithm': 0,\n",
       "  'Computation of the Overall F-measure': 1,\n",
       "  'Results and Discussion': 1},\n",
       " '52053741': {'Introduction': 19,\n",
       "  'Related Work': 42,\n",
       "  'Limits of the Encoder-Decoder Model': 7,\n",
       "  'Our Method': 4,\n",
       "  'Experimental Setup': 4,\n",
       "  'Results': 2,\n",
       "  'Conclusion': 0},\n",
       " '173188095': {'Introduction': 10,\n",
       "  'Datasets': 1,\n",
       "  'IU X-RAY': 3,\n",
       "  'PEIR GROSS': 0,\n",
       "  'ICLEF-CAPTION': 9,\n",
       "  'Methods': 37,\n",
       "  'Evaluation': 17,\n",
       "  'Frequency Baseline': 0,\n",
       "  'Nearest Neighbor Baseline': 1,\n",
       "  'Experimental Results': 0,\n",
       "  'Related Fields': 21,\n",
       "  'Limitations and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '454696': {'Introduction': 3,\n",
       "  'Composition in distributional models': 5,\n",
       "  'Disambiguation and composition': 1,\n",
       "  'Creating tensors for verbs': 0,\n",
       "  'Experimental setting': 1,\n",
       "  'Supervised disambiguation': 1,\n",
       "  'Unsupervised disambiguation': 2,\n",
       "  'Conclusion and future work': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '85518248': {'Introduction': 20,\n",
       "  'Attention Mechanism': 6,\n",
       "  'Self-Attention Mechanism': 12,\n",
       "  'Proposed Models': 0,\n",
       "  'Tensorized Self-Attention (TSA)': 1,\n",
       "  'Multi-Mask Tensorized Self-Attention (MTSA) Mechanism': 1,\n",
       "  'Computation-Optimized MTSA': 1,\n",
       "  'Experiments': 10,\n",
       "  'Natural Language Inference': 5,\n",
       "  'Semantic Role Labeling': 2,\n",
       "  'Sentence Classifications': 5,\n",
       "  'Machine Translation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Scale functions': 1,\n",
       "  'Equivalence of MTSA and Its Memory-Optimized Computation Scheme': 0,\n",
       "  'More Details about Algorithm 1': 3,\n",
       "  'Training Setups': 12,\n",
       "  'Evaluation on Machine Translation': 0,\n",
       "  'Related Work': 18,\n",
       "  'Discussion': 3,\n",
       "  'Visualization': 0},\n",
       " '2470716': {'Introduction': 2,\n",
       "  'Data and Evaluation': 0,\n",
       "  'Data': 0,\n",
       "  'Data preprocessing': 3,\n",
       "  'Data format': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Participating Systems': 0,\n",
       "  'Learning techniques': 16,\n",
       "  'Features': 3,\n",
       "  'External resources': 6,\n",
       "  'Performances': 12,\n",
       "  'Concluding Remarks': 1,\n",
       "  'Acknowledgements': 0},\n",
       " '49314352': {'Introduction': 2,\n",
       "  'Related work': 9,\n",
       "  'Method': 4,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 5,\n",
       "  'Results': 1,\n",
       "  'Robustness': 0,\n",
       "  'Example': 0,\n",
       "  'Conclusion and future work': 0},\n",
       " '19184155': {'Introduction': 3,\n",
       "  'Related Work': 9,\n",
       "  'Dynamic and Static Topic Model': 0,\n",
       "  'Generative Model': 1,\n",
       "  'Inference and Learning': 4,\n",
       "  'Datasets': 2,\n",
       "  'Evaluation by Perplexity': 2,\n",
       "  'Analysis of Extracted Structure': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Parameter Estimation': 0,\n",
       "  'Data Preprocessing': 0,\n",
       "  'Hyperparameter Setting': 0},\n",
       " '9954976': {'Introduction': 4,\n",
       "  'Related Work': 9,\n",
       "  'Modeling Context and Entities': 2,\n",
       "  'Global Normalization Layer': 0,\n",
       "  'Data and Evaluation Measure': 1,\n",
       "  'Experimental Setups': 2,\n",
       "  'Experimental Results': 5,\n",
       "  'Analysis of Entity Type Aggregation': 0,\n",
       "  'Analysis of CRF Transition Matrix': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Dataset Statistics': 0,\n",
       "  'Hyperparameters': 0},\n",
       " '11805625': {'Introduction': 14,\n",
       "  'Aspects and Opinions Co-Extraction': 14,\n",
       "  'Deep Learning for Sentiment Analysis': 21,\n",
       "  'Problem Statement': 0,\n",
       "  'Recursive Neural CRFs': 0,\n",
       "  'Dependency-Tree RNNs': 0,\n",
       "  'Integration with CRFs': 1,\n",
       "  'Joint Training for RNCRF': 0,\n",
       "  'Discussion': 1,\n",
       "  'Adding Linguistic/Lexicon Features': 0,\n",
       "  'Dataset and Experimental Setup': 8,\n",
       "  'Experimental Results': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '11872486': {'Introduction': 0,\n",
       "  'Related Work': 8,\n",
       "  'Task and Evaluation': 1,\n",
       "  'System Overview': 1,\n",
       "  'Pre-Training the Projection Layer': 2,\n",
       "  'Pre-Training the Recurrent Layer': 1,\n",
       "  'Experiments': 1,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '3725815': {'Introduction': 12,\n",
       "  'Transformer': 2,\n",
       "  'Self-Attention': 0,\n",
       "  'Relation-aware Self-Attention': 0,\n",
       "  'Relative Position Representations': 0,\n",
       "  'Efficient Implementation': 1,\n",
       "  'Experimental Setup': 4,\n",
       "  'Machine Translation': 1,\n",
       "  'Model Variations': 0,\n",
       "  'Conclusions': 0},\n",
       " '155092173': {'Introduction': 19,\n",
       "  'Proposed Framework': 0,\n",
       "  'Dual Supervised Learning': 1,\n",
       "  'Distribution Estimation as Autoregression': 3,\n",
       "  'Experiments': 0,\n",
       "  'Settings': 1,\n",
       "  'Model': 1,\n",
       "  'Results and Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '13266502': {'Introduction': 4,\n",
       "  'Formulation and Standard Softmax': 1,\n",
       "  'Prior Work on Suppressing Complexity of NMT Models': 9,\n",
       "  'Representing Words using Bit Arrays': 0,\n",
       "  'Loss Functions': 0,\n",
       "  'Efficiency of the Binary Code Prediction': 1,\n",
       "  'Hybrid Softmax/Binary Model': 2,\n",
       "  'Applying Error-correcting Codes': 11,\n",
       "  'Experimental Settings': 10,\n",
       "  'Results and Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '661': {'Introduction': 11,\n",
       "  'Terminology': 0,\n",
       "  'Explanation-Based Learning': 4,\n",
       "  'Learning partial-parsers': 0,\n",
       "  'The learning algorithm': 0,\n",
       "  'The parsing algorithm': 2,\n",
       "  'Existing related methods': 6,\n",
       "  'Application to DOP': 14,\n",
       "  'Empirical results': 1,\n",
       "  'OVIS experiments with T-parser': 0,\n",
       "  'OVIS experiments with DOP': 2,\n",
       "  'Conclusions and future work': 1},\n",
       " '44181076': {'Introduction': 5,\n",
       "  'Problem Formulation': 0,\n",
       "  'Baseline: Tag Set Prediction': 1,\n",
       "  'A Relaxation: Tag-wise Prediction': 0,\n",
       "  'Neural Factor Graph Model': 2,\n",
       "  'Neural Factors': 1,\n",
       "  'Pairwise Factors': 0,\n",
       "  'Transition Factors': 1,\n",
       "  'Language-Specific Weights': 0,\n",
       "  'Loopy Belief Propagation': 3,\n",
       "  'Learning and Decoding': 3,\n",
       "  'Dataset': 2,\n",
       "  'Baseline Tagger': 1,\n",
       "  'Training Regimen': 5,\n",
       "  'Main Results': 0,\n",
       "  'What is the Model Learning?': 0,\n",
       "  'Related Work': 22,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '18017180': {'Introduction': 2,\n",
       "  'Related Work': 11,\n",
       "  'Phrase-based and Neural Machine Translation': 6,\n",
       "  'PBMT Pre-translation for NMT (PreMT)': 0,\n",
       "  'Pipeline': 0,\n",
       "  'Mixed Input': 0,\n",
       "  'Training': 1,\n",
       "  'Experiments': 0,\n",
       "  'System description': 7,\n",
       "  'English - German Machine Translation': 0,\n",
       "  'System Comparison': 0,\n",
       "  'Examples': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '189762341': {'Conclusion': 0, 'Acknowledgement': 0},\n",
       " '196621534': {'Introduction': 1,\n",
       "  'Sentence-Level Baselines': 4,\n",
       "  'Model and Training': 5,\n",
       "  'Data-Filtering': 0,\n",
       "  'Noisy Back-Translation': 0,\n",
       "  'Fine-Tuning': 0,\n",
       "  'Deeper Models': 0,\n",
       "  'Ensembling': 0,\n",
       "  'Document-Level Systems': 0,\n",
       "  'Data and Data Preparation': 0,\n",
       "  'Experiments': 0,\n",
       "  'Stacking and Ensembling': 0,\n",
       "  'Submissions': 0},\n",
       " '49311178': {'Motivation': 8,\n",
       "  'Current Practice: Tuning by KK-fold CV': 9,\n",
       "  'Proposed Approach: Tuning By Repeated JJ-KK-fold Cross Validation': 5,\n",
       "  'Case Studies': 0,\n",
       "  'Part-of-speech tagger': 2,\n",
       "  'Document Classification': 5,\n",
       "  'Target-Dependent Sentiment Classification': 2,\n",
       "  'Discussion and Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '3101974': {'Introduction': 18, 'Conclusion': 0},\n",
       " '12051021': {'Acknowledgement': 0},\n",
       " '102353391': {},\n",
       " '85496823': {'AMPERE Dataset': 0, 'Acknowledgements': 0},\n",
       " '21678954': {'Acknowledgments': 0},\n",
       " '47021747': {'Introduction': 10,\n",
       "  'Annotation Framework': 0,\n",
       "  'Elementary Discourse Units': 3,\n",
       "  'Discourse Relations': 5,\n",
       "  'Corpus Construction': 0,\n",
       "  'Corpus Statistics': 1,\n",
       "  'Annotation Consistency': 0,\n",
       "  'Structural Characteristics': 1,\n",
       "  'Benchmark for Discourse Parsers': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '189927878': {'Acknowledgments': 0},\n",
       " '48360418': {'Introduction': 14,\n",
       "  'Background – Poincaré Embeddings': 5,\n",
       "  'Parametric Poincaré Embeddings': 1,\n",
       "  'Non-Parametric Supervised Embeddings': 2,\n",
       "  'Non-Parametric Unsupervised Word Embeddings': 6,\n",
       "  'Parametric Unsupervised Sentence Embeddings': 2,\n",
       "  'WordNet': 2,\n",
       "  'Word Embeddings': 5,\n",
       "  'Sentence Embeddings': 5,\n",
       "  'Discussion': 0,\n",
       "  'Related Work': 13,\n",
       "  'Conclusion': 1,\n",
       "  'WordNet Experiments': 0,\n",
       "  'Word Embedding Experiments': 0,\n",
       "  'Sentence Embedding Experiments': 0},\n",
       " '85542338': {'Introduction': 13, 'Related Work': 17, 'Graph Semantics': 0},\n",
       " '3608203': {'Introduction': 16,\n",
       "  'Proposed Model': 5,\n",
       "  'Experiments': 0,\n",
       "  'Datasets, candidate generation and hyper-parameters': 2,\n",
       "  'Exploration of different design choices': 0,\n",
       "  'Results on ACE 2005': 1,\n",
       "  'Results on Rich ERE 2015': 0,\n",
       "  'Conclusions': 0},\n",
       " '49865798': {'Introduction': 3,\n",
       "  'Methodology': 2,\n",
       "  'Long Short-Term Memory networks': 0,\n",
       "  'Attention': 1,\n",
       "  'Datasets': 0,\n",
       "  'Preprocessing': 2,\n",
       "  'Parameters': 1,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0},\n",
       " '1797936': {'Introduction': 0,\n",
       "  'Related Work': 9,\n",
       "  'Eye-tracking Database for Sarcasm Analysis': 3,\n",
       "  'Document Description': 1,\n",
       "  'Task Description': 2,\n",
       "  'Analysis of Eye-movement Data': 0,\n",
       "  'Variation in the Average Fixation Duration per Word': 3,\n",
       "  'Analysis of Scanpaths': 0,\n",
       "  'Features for Sarcasm Detection': 0,\n",
       "  'Simple Gaze Based Features': 1,\n",
       "  'Complex Gaze Based Features': 0,\n",
       "  'The Sarcasm Classifier': 2,\n",
       "  'Results': 1,\n",
       "  'Considering Reading Time as a Cognitive Feature along with Sarcasm Features': 0,\n",
       "  'How Effective are the Cognitive Features': 0,\n",
       "  'Example Cases': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '20339999': {'Introduction': 14,\n",
       "  'Related Work': 26,\n",
       "  'Data: Parliamentary Question Periods': 0,\n",
       "  'Question Motifs': 2,\n",
       "  'Latent Question Types': 1,\n",
       "  'Validation': 5,\n",
       "  'Career Trajectory Effects': 4,\n",
       "  'Conclusion and Future Work': 6,\n",
       "  'A.1    Further examples of question types': 0,\n",
       "  'A.2    Details about data filtering decisions': 0,\n",
       "  'A.3    Details about the labeled PMQ dataset': 1,\n",
       "  'A.4    Example motif subgraph': 0},\n",
       " '86111': {'Introduction': 3,\n",
       "  'Discriminative Model with Target-Side Context': 0,\n",
       "  'Model Definition': 0,\n",
       "  'Global Model': 0,\n",
       "  'Extraction of Training Examples': 1,\n",
       "  'Training': 0,\n",
       "  'Feature Set': 2,\n",
       "  'Efficient Implementation': 0,\n",
       "  'Feature Extraction': 0,\n",
       "  'Model Training': 0,\n",
       "  'Decoding': 0,\n",
       "  'Experimental Evaluation': 1,\n",
       "  'English-Czech Translation': 6,\n",
       "  'Additional Language Pairs': 6,\n",
       "  'Analysis': 0,\n",
       "  'Related Work': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '4716009': {'Introduction': 11,\n",
       "  'Related Work': 20,\n",
       "  'Model': 0,\n",
       "  'Recurrent Dual Encoder (RDE)': 1,\n",
       "  'Hierarchical Recurrent Dual Encoder (HRDE)': 1,\n",
       "  'Latent Topic Clustering (LTC)': 0,\n",
       "  'Combined Model of (H)RDE and LTC': 0,\n",
       "  'The Ubuntu Dialogue Corpus': 1,\n",
       "  'Consumer Product QA Corpus': 1,\n",
       "  'Implementation Details': 7,\n",
       "  'Evaluation Metrics': 1,\n",
       "  'Performance Evaluation': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52009840': {'Introduction': 32,\n",
       "  'Preliminaries and Framework': 0,\n",
       "  'Candidate Generation': 3,\n",
       "  'Feature Extraction': 1,\n",
       "  'Learning Joint Embeddings of Word and Entity': 2,\n",
       "  'Local Features': 1,\n",
       "  'Global Features': 1,\n",
       "  'Neural Collective Entity Linking': 0,\n",
       "  'Graph Convolutional Network': 1,\n",
       "  'Model Architecture': 0,\n",
       "  'Training': 0,\n",
       "  'Experiments': 0,\n",
       "  'Baselines and Datasets': 21,\n",
       "  'Training Details and Running Time Analysis': 0,\n",
       "  'Results on GERBIL': 1,\n",
       "  'Results on TAC2010 and WW': 1,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6497091': {'Introduction': 14,\n",
       "  'Delimited continuations': 3,\n",
       "  'Polarity sensitivity and evaluation order': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '8845492': {'Introduction': 3,\n",
       "  'Attentional Sequence-to-Sequence model': 3,\n",
       "  'Attentional Tree-to-Sequence model': 2,\n",
       "  'Forest-to-Sequence Attentional Encoder-Decoder Model': 2,\n",
       "  'Datasets': 6,\n",
       "  'Experimental Results': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '6616625': {'Introduction': 21,\n",
       "  'The MoodSwipe Keyboard': 1,\n",
       "  'Use Cases': 1,\n",
       "  'Back-end System, Experiments and Discussions': 1,\n",
       "  'Experimental Materials': 2,\n",
       "  'Emotion Classification': 2,\n",
       "  'Text Suggestion & Results': 3,\n",
       "  'Collecting User-reported Labels': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '189762112': {'Introduction': 14,\n",
       "  'Related Works': 9,\n",
       "  'Overview': 0,\n",
       "  'Type Hierarchy and Knowledge Base': 0,\n",
       "  'Entity Linking': 2,\n",
       "  'Multi-step Typing': 0,\n",
       "  'KCAT': 0,\n",
       "  'Annotator Client': 1,\n",
       "  'Manager Module': 0,\n",
       "  'Experiment': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '639384': {'Introduction': 2,\n",
       "  'Noun Phrase Countability': 0,\n",
       "  'Noun Countability Preferences': 0,\n",
       "  'Determination of NP Referentiality': 0,\n",
       "  'Generic noun phrases': 1,\n",
       "  'Determination of NP Countability and Number': 0,\n",
       "  'Experimental Results': 0,\n",
       "  'Conclusion': 0},\n",
       " '2309801': {'Introduction': 2, 'Grammar extraction algorithm': 15},\n",
       " '36997937': {'Introduction': 39,\n",
       "  'Related Work': 21,\n",
       "  'Model': 2,\n",
       "  'Representation Learning': 8,\n",
       "  'Pretraining': 9,\n",
       "  'Decoding and Training': 6,\n",
       "  'Experimental Settings': 7,\n",
       "  'Development Experiments': 13,\n",
       "  'Final Results': 12,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '4859003': {'Introduction': 3,\n",
       "  'Related Work': 19,\n",
       "  'Creation Process': 0,\n",
       "  'Analysis': 0,\n",
       "  'Models': 2,\n",
       "  'Rule-based Approach': 0,\n",
       "  'Phrase-based Machine Translation': 6,\n",
       "  'Neural Machine Translation': 7,\n",
       "  'Evaluation': 5,\n",
       "  'Human-based Evaluation': 1,\n",
       "  'Automatic Metrics': 4,\n",
       "  'Results': 0,\n",
       "  'Results using Human Judgments': 0,\n",
       "  'Results with Automatic Metrics': 0,\n",
       "  'Metric Correlation': 0,\n",
       "  'Manual Analysis': 0,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Supplementary material': 0,\n",
       "  'Dataset': 0},\n",
       " '10694414': {'Introduction': 0,\n",
       "  'The Distributional Lexicon': 0,\n",
       "  'Typed Co-occurrences': 42},\n",
       " '1381475': {'Introduction': 10,\n",
       "  'Existing Evaluation Approaches': 6,\n",
       "  'Systems': 7,\n",
       "  'Datasets': 1,\n",
       "  'Annotation Process': 3,\n",
       "  'Judgement Data and Inter-Annotator Agreement': 1,\n",
       "  'Experimental Results': 0,\n",
       "  'Testing H1: Comparing the Performance of OIE on Scientific vs. Encyclopedic Text': 0,\n",
       "  'Testing H2: Comparing the Performance of Systems ': 0,\n",
       "  'Other Observations': 0,\n",
       "  'Error Analysis': 1,\n",
       "  'Conclusion': 2},\n",
       " '588859': {'Introduction': 3,\n",
       "  'Background': 4,\n",
       "  'Related Work': 6,\n",
       "  'Models': 0,\n",
       "  'Baseline Convolutional Neural Net': 3,\n",
       "  'Encoder-Decoder': 4,\n",
       "  'Data': 0,\n",
       "  'Training': 2,\n",
       "  'Tuning': 0,\n",
       "  'Results': 0,\n",
       "  'Discussion': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6987624': {'Introduction': 3,\n",
       "  'Related Work': 16,\n",
       "  'Dataset': 1,\n",
       "  'Timex and Event Extraction': 3,\n",
       "  'Tlink Classification': 0,\n",
       "  'Intra-Sentence Model': 1,\n",
       "  'Cross-Sentence Model': 0,\n",
       "  'Relations to DCT': 0,\n",
       "  'Relations between timexes': 0,\n",
       "  'Double-checking': 0,\n",
       "  'Pruning Tlinks': 3,\n",
       "  'Model Settings': 0,\n",
       "  'Experiments': 0,\n",
       "  'Model Selection Experiments': 0,\n",
       "  'QA-TempEval Experiments': 1,\n",
       "  'TimeBank-Dense Experiments': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '155091369': {},\n",
       " '52160745': {'Introduction': 9,\n",
       "  'Related Work': 27,\n",
       "  'Methods': 0,\n",
       "  'Dataset': 0,\n",
       "  'Model': 11,\n",
       "  'Experiment': 6,\n",
       "  'Results': 0,\n",
       "  'Causality Prediction': 3,\n",
       "  'Causal Explanation Identification': 0,\n",
       "  'Architectural Variants': 0,\n",
       "  'Complete Pipeline': 0,\n",
       "  'Exploration': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '83458807': {},\n",
       " '6972699': {'Introduction': 4,\n",
       "  'Noun Co-Occurrence': 2,\n",
       "  'Statistics for selecting and ranking': 0,\n",
       "  'Seed word selection': 0,\n",
       "  'Compound Nouns': 0,\n",
       "  'Outline of the algorithm': 0,\n",
       "  'Empirical Results and Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '15971789': {'Introduction': 6,\n",
       "  'Neural mention ranking': 2,\n",
       "  'Evaluation Metrics': 6,\n",
       "  'From mention ranking to entity centricity': 0,\n",
       "  'Entity-centric heuristic cross entropy loss': 0,\n",
       "  'From non-differentiable metrics to differentiable losses': 1,\n",
       "  'Experiments': 0,\n",
       "  'Setup': 1,\n",
       "  'Resolvers': 1,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Discussion': 0,\n",
       "  'Related work': 14,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '102352497': {'Introduction': 18,\n",
       "  'Compressor': 5,\n",
       "  'Differentiable Word Sampling': 6,\n",
       "  'Reconstructor': 0,\n",
       "  'Decoder Initialization': 1,\n",
       "  'Loss Functions': 4,\n",
       "  'Modeling Details': 4,\n",
       "  'Experiments': 9,\n",
       "  'Limitations and Future Work': 3,\n",
       "  'Acknowledgments': 0,\n",
       "  'Temperature for Gumbel-Softmax': 4,\n",
       "  'Out of Vocabulary (OOV) Words': 2,\n",
       "  'Reconstruction Word Drop': 2,\n",
       "  'Implementation and Hyper-parameters': 7},\n",
       " '13696025': {'Introduction': 12,\n",
       "  'Related Work': 12,\n",
       "  'Article Commenting Dataset': 4,\n",
       "  'Quality Weighted Automatic Metrics': 5,\n",
       "  'Experiments': 0,\n",
       "  'Conclusions and Future Work': 9,\n",
       "  'Human Evaluation Criteria': 2,\n",
       "  'Enhanced Automatic Metrics': 4,\n",
       "  'Weighted BLEU': 2,\n",
       "  'Weighted METEOR': 2,\n",
       "  'Weighted ROUGE': 2,\n",
       "  'Weighted CIDEr': 2,\n",
       "  'Setup': 4,\n",
       "  'Human Correlation of Automatic Metrics': 0,\n",
       "  'Results': 0,\n",
       "  'Example instance of the proposed dataset': 0},\n",
       " '34032948': {'Introduction': 13,\n",
       "  'Related Work': 28,\n",
       "  'Hybrid Neural Inference Models': 0,\n",
       "  'Input Encoding': 6,\n",
       "  'Local Inference Modeling': 7,\n",
       "  'Inference Composition': 1,\n",
       "  'Conclusions and Future Work': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '67856759': {'Introduction': 10,\n",
       "  'Related Work': 17,\n",
       "  'Data': 3,\n",
       "  'Baseline Model': 4,\n",
       "  'Experimental Approaches ': 0,\n",
       "  'Synthetic Noise Induction (SNI)': 1,\n",
       "  'Noise Generation Through Back-Translation ': 2,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '2163392': {'Introduction': 14,\n",
       "  'Methods': 0,\n",
       "  'Natural Language Semantic Parsing': 4,\n",
       "  'Grounding and Learning Instances through Demonstration and Eye tracking (GLIDE)': 3,\n",
       "  'Feature Extraction': 0,\n",
       "  'Symbol Meaning Learning': 0,\n",
       "  'Experiments': 1,\n",
       "  'Dataset': 1,\n",
       "  'Experimental Set up': 0,\n",
       "  'Results': 0,\n",
       "  'Discussion and Future Work': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '158046910': {'Introduction': 4,\n",
       "  'Clinical Notes': 8,\n",
       "  'De-identification': 11,\n",
       "  'Differential Privacy': 4,\n",
       "  'Language Modeling': 8,\n",
       "  'Related Work': 2,\n",
       "  'Conclusions and Future Work': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '2215227': {'Introduction': 4,\n",
       "  'Related work': 0,\n",
       "  'Representations': 0,\n",
       "  'Model': 0,\n",
       "  'Path structure: aligning utterances to actions': 3,\n",
       "  'Action structure: aligning words to percepts': 2,\n",
       "  'Learning and inference': 1,\n",
       "  'Evaluation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5125975': {'Introduction': 27,\n",
       "  'Model': 4,\n",
       "  'Grammaticality Constraints': 11,\n",
       "  'Anaphora Constraints': 6,\n",
       "  'Features': 0,\n",
       "  'Learning': 3,\n",
       "  'Experiments': 4,\n",
       "  'Preprocessing': 2,\n",
       "  'New York Times Corpus': 1,\n",
       "  'New York Times Results': 1,\n",
       "  'RST Treebank': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '10070984': {'Introduction': 2,\n",
       "  'Previous work': 5,\n",
       "  'Machine Learning': 2,\n",
       "  'Normalization of predictions': 0,\n",
       "  'Optimization measure': 0,\n",
       "  'Experiments': 4,\n",
       "  'Best ML method': 0,\n",
       "  'Normalization of predicted post-editing operations': 0,\n",
       "  'ML optimization': 0,\n",
       "  'Submission and post-mortem analysis': 1,\n",
       "  'Conclusion and further work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '21696596': {'Introduction': 18,\n",
       "  'Proposed Approach': 5,\n",
       "  'Auxiliary Ordinal Regression Task': 2,\n",
       "  'Hand-engineered Features': 1,\n",
       "  'Evaluation': 1,\n",
       "  'Classification Performance': 2,\n",
       "  'Latent vs. Hand-engineered Features': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '503475': {},\n",
       " '44243501': {'Introduction': 0,\n",
       "  'Framework Overview': 1,\n",
       "  'Related Work and Contributions': 0,\n",
       "  'Multi-Sentence Compression Graph (MSCG) ': 0,\n",
       "  'More informative MSCG ': 1,\n",
       "  'Graph-based word importance scoring ': 8,\n",
       "  'Fluency-aware, more abstractive MSCG ': 1,\n",
       "  'Submodularity for summarization , ': 1,\n",
       "  'Our Framework': 0,\n",
       "  'Text preprocessing': 0,\n",
       "  'Utterance community detection': 1,\n",
       "  'Multi-Sentence Compression': 0,\n",
       "  'Budgeted submodular maximization': 0,\n",
       "  'Datasets': 3,\n",
       "  'Baselines': 4,\n",
       "  'Parameter tuning': 0,\n",
       "  'Results and Interpretation': 4,\n",
       "  'Conclusion and Next Steps': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Use of WordNet': 0,\n",
       "  'Example Summaries': 0},\n",
       " '195218358': {'Introduction': 10,\n",
       "  'Datasets': 0,\n",
       "  'Methodology': 0,\n",
       "  'Pre-Processing': 4,\n",
       "  'Modeling': 14,\n",
       "  'Training': 4,\n",
       "  'Evaluation': 0,\n",
       "  'Results': 0,\n",
       "  'Discussion': 4,\n",
       "  'Pre-processing': 4,\n",
       "  'Split Bias': 2,\n",
       "  'Hyperparameter Tuning': 2,\n",
       "  'Evaluation Metrics': 0,\n",
       "  'Comparison with SOTA': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Quantitative Literature Review': 53,\n",
       "  'Evaluation Metric Results on Test Data': 0},\n",
       " '19411202': {'Introduction': 5,\n",
       "  'Action Verb Attributes': 0,\n",
       "  'Learning Verb Attributes from Language': 0,\n",
       "  'Learning from Distributed Embeddings': 6,\n",
       "  'Learning from Dictionary Definitions': 3,\n",
       "  'Combining Dictionary and Embedding Representations': 0,\n",
       "  'Verb Attributes as Latent Mappings': 5,\n",
       "  'Verb Embeddings as Latent Mappings': 1,\n",
       "  'Joint Prediction from Attributes and Embeddings': 0,\n",
       "  'Actions and Attributes Dataset': 4,\n",
       "  'Predicting Action Attributes from Text': 0,\n",
       "  'Zero-shot Action Recognition': 3,\n",
       "  'Future Work and Conclusion': 0,\n",
       "  'Implementation details': 2,\n",
       "  'Full list of attributes': 0},\n",
       " '9784700': {},\n",
       " '1175070': {'Experimental Results': 0, 'Acknowledgments': 0},\n",
       " '155099789': {},\n",
       " '8462113': {'Hyperparameter Tuning': 0},\n",
       " '427492': {},\n",
       " '160019345': {'Introduction': 16,\n",
       "  'Related Work': 20,\n",
       "  'Neural Sequence-to-Sequence Learning': 4,\n",
       "  'Fully Supervised Machine Translation': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '29622196': {'Introduction': 0,\n",
       "  'Related Work': 0,\n",
       "  'Current approaches for NLIDB': 0,\n",
       "  'Deep learning solutions for related problems': 0,\n",
       "  'Existing corpora for NLIDB evaluation': 0,\n",
       "  'Dataset construction': 0,\n",
       "  'Data crawling and preprocessing': 0,\n",
       "  'Large dataset for training and validation': 0},\n",
       " '9417783': {'Introduction': 1,\n",
       "  'Related Work': 11,\n",
       "  'Model': 0,\n",
       "  'NER Performance': 0,\n",
       "  'Mention Embeddings': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6278197': {'Introduction': 3,\n",
       "  'Introduction to Dialogue Processing in Verbmobil': 0,\n",
       "  'Maintaining Context': 0,\n",
       "  'Inferences': 0,\n",
       "  'Predictions': 3,\n",
       "  'Robustness': 0,\n",
       "  'Related Work': 4,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '147703948': {'Introduction': 18,\n",
       "  'Baseline': 3,\n",
       "  'Encoder': 1,\n",
       "  'Decoder': 0,\n",
       "  'Our Method': 1,\n",
       "  'Settings': 5,\n",
       "  'Speed Comparison': 0,\n",
       "  'Main Results': 0,\n",
       "  'Analysis': 3,\n",
       "  'Related Work': 24,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '91184122': {'Future Works and Conclusions': 1, 'Acknowledgments': 0},\n",
       " '2162648': {'Acknowledgements': 0},\n",
       " '2347857': {'Introduction': 4,\n",
       "  'Dependency Parsing by Belief Propagation': 0,\n",
       "  'Approximation-aware Learning': 1,\n",
       "  'Inference, Decoding, and Loss as a Feedfoward Circuit': 18},\n",
       " '1520275': {'Introduction': 7,\n",
       "  'Neural Machine Translation': 1,\n",
       "  'Text-based NMT': 3,\n",
       "  'Conditional GRU': 0,\n",
       "  'Multimodal NMT': 1,\n",
       "  'Attention-based Models': 0,\n",
       "  'Soft attention': 4,\n",
       "  'Hard Stochastic attention': 4,\n",
       "  'Local Attention': 2,\n",
       "  'Image attention optimization': 2,\n",
       "  'Experiments': 1,\n",
       "  'Training and model details': 9,\n",
       "  'Quantitative results': 3,\n",
       "  'Qualitative results': 1,\n",
       "  'Conclusion and future work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '10182773': {'INTRODUCTION': 0,\n",
       "  'Generation and Recognition': 1,\n",
       "  'A Tool for Phonologists': 11},\n",
       " '102351583': {'Introduction': 11,\n",
       "  'Degree Adverbs in Linguistics': 10,\n",
       "  'Modifiers in the context of Sentiment and Emotion Analysis': 2,\n",
       "  'Methods': 0,\n",
       "  'Data Collection and Annotation': 7,\n",
       "  'Adaptations of Embeddings': 2,\n",
       "  'Results': 0,\n",
       "  'Corpus Analysis': 2,\n",
       "  'Annotation Analysis': 1,\n",
       "  'Embedding Adaptations': 5,\n",
       "  'Conclusion & Future Work': 4,\n",
       "  'Acknowledgements': 0},\n",
       " '166228669': {'Introduction': 8,\n",
       "  'Related Work': 18,\n",
       "  'Robust Word Recognition': 0,\n",
       "  'ScRNN with Backoff': 4,\n",
       "  'Model Sensitivity': 2,\n",
       "  'Synthesizing Adversarial Attacks': 3,\n",
       "  'Experiments and Results': 0,\n",
       "  'Word Error Correction': 3,\n",
       "  'Robustness to adversarial attacks': 6,\n",
       "  'Understanding Model Sensitivity': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '189898554': {'Introduction': 11,\n",
       "  'Related Work': 35,\n",
       "  'Problem Formulation': 2,\n",
       "  'Sequence-to-Sequence Model': 3,\n",
       "  'Pair-to-Sequence Model': 0,\n",
       "  'Training and Inference': 0,\n",
       "  'Experiments': 1,\n",
       "  'Unanswerable Question Generation': 8,\n",
       "  'Data Augmentation for Machine Reading Comprehension': 5,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52111971': {'Introduction': 17,\n",
       "  'Well-formed Natural Language Question Classifier': 0,\n",
       "  'Dataset Construction': 6,\n",
       "  'Model': 2,\n",
       "  'Experiments': 1,\n",
       "  'Improving Question Generation': 7,\n",
       "  'Related Work': 3,\n",
       "  'Conclusion': 0},\n",
       " '1138221': {'Introduction': 4,\n",
       "  'Memory-Based Language Processing': 1,\n",
       "  'Similarity metrics': 6,\n",
       "  'Smoothing of Estimates': 10,\n",
       "  'A Comparison': 2,\n",
       "  'PP-attachment': 2,\n",
       "  'POS-tagging': 3,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgements': 0},\n",
       " '2367456': {'Introduction': 5,\n",
       "  'Task Definition': 2,\n",
       "  'Approach': 0,\n",
       "  'Learning Within-event Structures': 2,\n",
       "  'Learning Event-Event Relations': 0,\n",
       "  'Entity Extraction': 4,\n",
       "  'Joint Inference': 2,\n",
       "  'Experiments': 0,\n",
       "  'Results': 3,\n",
       "  'Error Analysis': 0,\n",
       "  'Related Work': 14,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '145048418': {},\n",
       " '159041481': {'Introduction': 13,\n",
       "  'Related Work': 16,\n",
       "  'Problem Statement': 5,\n",
       "  'Utterance Segmentation': 0,\n",
       "  'Election Campaign Dataset': 3,\n",
       "  'Proposed Approach': 1},\n",
       " '47012788': {'Introduction': 8,\n",
       "  'Related Work': 13,\n",
       "  'Probabilistic FastText': 0,\n",
       "  'Probabilistic Subword Representation': 3,\n",
       "  'Similarity Measure between Words': 1,\n",
       "  'Loss Function': 1,\n",
       "  'Energy Simplification': 1,\n",
       "  'Word Sampling': 1,\n",
       "  'Experiments': 3,\n",
       "  'Training Details': 2,\n",
       "  'Qualitative Evaluation - Nearest neighbors': 0,\n",
       "  'Word Similarity Evaluation': 14,\n",
       "  'Evaluation on Foreign Language Embeddings': 4,\n",
       "  'Qualitative Evaluation - Subword Decomposition': 1,\n",
       "  'Numbers of Components': 2,\n",
       "  'Conclusion and Future Work': 2},\n",
       " '11866664': {'Introduction': 8, 'Our Contributions': 0},\n",
       " '384520': {'Introduction': 3,\n",
       "  'Related Work': 12,\n",
       "  'Model': 1,\n",
       "  'Vocabulary': 0,\n",
       "  'Independent segments': 1,\n",
       "  'Sequence ordering': 0,\n",
       "  'Model shape': 0,\n",
       "  'Training': 0,\n",
       "  'Dropout and byte-dropout': 2,\n",
       "  'Inference': 0,\n",
       "  'Results': 1,\n",
       "  'Part-of-Speech Tagging': 4,\n",
       "  'Named Entity Recognition': 4,\n",
       "  'Dropout and Stacked LSTMs': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '51993096': {'Introduction': 30,\n",
       "  'Structure and Design': 0,\n",
       "  'The Design of Texar': 10,\n",
       "  'Assemble Arbitrary Model Architectures': 0,\n",
       "  'Plug-in and Swap-out Modules': 0,\n",
       "  'Customize with Extensible Interfaces': 0,\n",
       "  'Experiments': 1,\n",
       "  'One Technique on Many Tasks: Transformer': 9,\n",
       "  'One Task with Many Techniques: Language Modeling': 4,\n",
       "  'Text Style Transfer': 8,\n",
       "  'Related Work': 14,\n",
       "  'Conclusion': 0},\n",
       " '628489': {'Introduction': 0,\n",
       "  'Selection of the closest variant': 5,\n",
       "  'Syntactic patterns': 1,\n",
       "  'Combination': 0,\n",
       "  'Conclusions and future work': 0},\n",
       " '14877585': {'Introduction': 10,\n",
       "  'Proposed Methodology': 0,\n",
       "  'Textual analysis': 0,\n",
       "  'Stylistic features extraction': 0,\n",
       "  'Building classification model': 0,\n",
       "  'Corpus': 1,\n",
       "  'Baseline system (BL)': 2,\n",
       "  'Performances of two different models': 0,\n",
       "  'Comparative analysis': 1,\n",
       "  'Conclusion and Future work': 0},\n",
       " '2476229': {'Introduction': 14,\n",
       "  'Related Work': 18,\n",
       "  'Model': 1,\n",
       "  'Embedding Layer': 0,\n",
       "  'Sequence Layer': 1,\n",
       "  'Entity Detection': 1,\n",
       "  'Dependency Layer': 3,\n",
       "  'Stacking Sequence and Dependency Layers': 0,\n",
       "  'Relation Classification': 0,\n",
       "  'Training': 4,\n",
       "  'Data and Task Settings': 5,\n",
       "  'Experimental Settings': 3,\n",
       "  'End-to-end Relation Extraction Results': 0,\n",
       "  'Relation Classification Analysis Results': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Hyper-parameter Settings': 0},\n",
       " '44091475': {'Introduction': 12,\n",
       "  'The Proposed Approach': 3,\n",
       "  'Hierarchical Decoder': 2,\n",
       "  'Inner- and Inter-Layer Teacher Forcing': 2,\n",
       "  'Repeat-Input Mechanism': 0,\n",
       "  'Curriculum Learning': 1,\n",
       "  'Setup': 1,\n",
       "  'Results and Analysis': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Dataset Detail': 0,\n",
       "  'Parameter Setting': 0},\n",
       " '12262773': {'Introduction': 1},\n",
       " '174797895': {'Introduction': 9,\n",
       "  'Neural Projection Model': 0,\n",
       "  'Vanilla Skip-Gram Model': 1,\n",
       "  'Neural Projection Skip-Gram (NP-SG)': 2,\n",
       "  'Training NP-SG Model': 1,\n",
       "  'Discriminative NP-SG Models': 0,\n",
       "  'Improved NP-SG Training': 1,\n",
       "  'Dataset': 0,\n",
       "  'Implementation Details': 1,\n",
       "  'Experiments': 0,\n",
       "  'Qualitative Evaluation and Results': 0,\n",
       "  'Quantitative Evaluation and Results': 7,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '49742988': {'Introduction': 2,\n",
       "  'Corpus annotation': 2,\n",
       "  'Model description': 0,\n",
       "  'Training procedure': 0,\n",
       "  'Related work': 9,\n",
       "  'Experimental validation': 0,\n",
       "  'Multi-task experiments': 0,\n",
       "  'Transfer between tasks': 0,\n",
       "  'Analysis': 0,\n",
       "  'Dynamics of sentiments and dialog acts': 0,\n",
       "  'Both tasks are sparsely correlated': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '51875609': {'Introduction': 3,\n",
       "  'Proposed Open IE Approach': 1,\n",
       "  'Transformation Stage': 2,\n",
       "  'Relation Extraction': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Experimental Setup': 2,\n",
       "  'Results and Discussion': 0,\n",
       "  'Conclusion': 0},\n",
       " '168170028': {'Introduction': 17,\n",
       "  'Method': 0,\n",
       "  'Stage 1: Template Generation': 4,\n",
       "  'Stage 2: Description Generation': 4,\n",
       "  'Learning': 0,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 1,\n",
       "  'Evaluation Metrics': 4,\n",
       "  'Baselines and Experimental Setup': 6,\n",
       "  'Results and Analysis': 0,\n",
       "  'Related Work': 21,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '5910267': {'Introduction': 0,\n",
       "  'Related Work': 9,\n",
       "  'Task Description': 0,\n",
       "  'Dataset': 0,\n",
       "  'Evaluation': 1,\n",
       "  'Proposed Approach': 0,\n",
       "  'Pre-processing': 2,\n",
       "  'Models': 1,\n",
       "  'Convolution Neural Networks (OhioState-CNN)': 6,\n",
       "  'Bi-Directional LSTM': 3,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0},\n",
       " '14900221': {'Acknowledgment': 0},\n",
       " '13845267': {'Introduction': 6,\n",
       "  'Sentiment Lexicons': 0,\n",
       "  'Existing, Manually Created Lexicons': 4,\n",
       "  'New, Tweet-Specific, Automatically Generated Sentiment Lexicons': 1,\n",
       "  'Task: Automatically Detecting the Sentiment of a Message': 0,\n",
       "  'Classifier and features': 4,\n",
       "  'Experiments': 0,\n",
       "  'Task: Automatically Detecting the Sentiment of a Term in a Message': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '3376033': {'Introduction': 1,\n",
       "  'Related Work': 1,\n",
       "  'Human Evaluation of Machine Translation': 31,\n",
       "  'User Trust in Automation': 5,\n",
       "  'Assessing Trust in MT': 9,\n",
       "  'Survey Methodology': 0,\n",
       "  'Participants': 1,\n",
       "  'Results': 3,\n",
       "  'Limitations and Future Work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '13984360': {'Introduction': 15,\n",
       "  'Wet Lab Protocols': 0,\n",
       "  'Annotation Scheme': 2,\n",
       "  'Annotation Process': 0,\n",
       "  'Inter-Annotator Agreement': 1,\n",
       "  'Methods': 0,\n",
       "  'Maximum Entropy (MaxEnt) Tagger': 4,\n",
       "  'Neural Sequence Tagger': 5,\n",
       "  'Relation Classification': 3,\n",
       "  'Results': 1,\n",
       "  'Entity Identification and Classification': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgement': 0,\n",
       "  'Annotation Guidelines': 0,\n",
       "  'Actions': 0,\n",
       "  'Entities': 0,\n",
       "  'Relations': 0},\n",
       " '160010264': {'Introduction': 17,\n",
       "  'Task Formulation': 0,\n",
       "  'Model Architecture': 1,\n",
       "  'GNN Sub-layer': 2,\n",
       "  'Encoder': 2,\n",
       "  'Decoder': 5,\n",
       "  'Related Work': 31,\n",
       "  'Semantic Parsing Datasets': 4,\n",
       "  'Experimental Setup': 8,\n",
       "  'Results and Analysis': 6,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Entity Candidate Generator Details': 0},\n",
       " '189928114': {'Encoders for Context-Aware Retriever': 0,\n",
       "  'Model Training': 1,\n",
       "  'Semantic Parser': 0,\n",
       "  'Results on CSQA': 0},\n",
       " '31247855': {'Introduction': 3,\n",
       "  'Related Work': 12,\n",
       "  'Approach': 1,\n",
       "  'Skip-thought Model': 0,\n",
       "  'Encoder: GRU': 4,\n",
       "  'Decoder: Conditional GRU': 0,\n",
       "  'Skip-thought Neighbor Model': 0,\n",
       "  'Skip-thought Neighbor with Autoencoder': 1,\n",
       "  'Skip-thought Neighbor with One Target': 0,\n",
       "  'Experiment Settings': 5,\n",
       "  'Quantitative Evaluation': 10,\n",
       "  'Skip-thought Neighbor vs. Skip-thought': 0,\n",
       "  'Skip-thought Neighbor+AE vs. Skip-thought+AE': 0,\n",
       "  'Increasing the Number of Neighbors': 0,\n",
       "  'A Note on Normalizing the Representation': 1,\n",
       "  'Qualitative Investigation': 0,\n",
       "  'Sentence Retrieval': 0,\n",
       "  'Conditional Sentence Generation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '25437880': {'Introduction': 14,\n",
       "  'Data': 0,\n",
       "  'Patterns extraction and analysis': 3,\n",
       "  'Patterns + linear CRF': 1,\n",
       "  'Patterns + LSTM-CRF': 5,\n",
       "  'Discussion of results': 0,\n",
       "  'Semantics of pattern embeddings': 0,\n",
       "  'Syntactic patterns vs bigrams': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '8703520': {'Introduction': 6,\n",
       "  'Corpus Examples Combined with NLP for Language Learning': 6,\n",
       "  'Linguistic Aspects Influencing Context Dependence': 10,\n",
       "  'Datasets': 4,\n",
       "  'Methodology': 2,\n",
       "  'Qualitative Results Based on Thematic Analysis': 6,\n",
       "  'Quantitative Comparison of Positive and Negative Samples': 0,\n",
       "  'An Algorithm for the Assessment of Context Dependence': 8,\n",
       "  'Performance on the Datasets': 0,\n",
       "  'User-based Evaluation Results': 0,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '5666926': {'Introduction': 8,\n",
       "  'Parse Hybridization': 0,\n",
       "  'Parser Switching': 0,\n",
       "  'Experiments': 0,\n",
       "  'Context': 0,\n",
       "  'Performance Testing': 0,\n",
       "  'Robustness Testing': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '174799037': {'Introduction': 6,\n",
       "  'Data': 1,\n",
       "  'Methods': 0,\n",
       "  'Tagger Performance': 0,\n",
       "  'Analyzing Year Embeddings': 0,\n",
       "  'Temporal Prediction': 0,\n",
       "  'Conclusion': 1},\n",
       " '278288': {'Acknowledgments': 0},\n",
       " '855563': {'Introduction': 11,\n",
       "  'RNN Language Model': 2,\n",
       "  'Dynamic Entity Representation': 4,\n",
       "  'Proposed Method: Dynamic Neural Text Modeling': 5,\n",
       "  'Anonymized Language Modeling': 6,\n",
       "  'Setting': 5,\n",
       "  'Results and Analysis': 2,\n",
       "  'Related Work': 48,\n",
       "  'Conclusion': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '109931229': {'Introduction': 15,\n",
       "  'POS Grounding as Decipherment': 1,\n",
       "  'POS Tagger construction': 0,\n",
       "  'Datasets': 4,\n",
       "  'Unsupervised Clustering': 4,\n",
       "  'Grounding via Decipherment': 2,\n",
       "  'Name Tagging': 1,\n",
       "  'Multilingual Dependency Parsing': 7,\n",
       "  'Labeling and Cipher Grounding': 0,\n",
       "  'Extrinsic evaluation': 4,\n",
       "  'Related Work': 24,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '91184134': {'Introduction': 6,\n",
       "  'Implementation': 0,\n",
       "  'Applications': 18,\n",
       "  'Machine translation': 12,\n",
       "  'Language modeling': 8,\n",
       "  'Abstractive document summarization': 7,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '196204488': {'Introduction': 15,\n",
       "  'A Tagging Problem': 0,\n",
       "  'Quantity Tagger': 6,\n",
       "  'Model Variants': 1,\n",
       "  'Experiments': 3,\n",
       "  'Analysis': 10,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '550225': {'Introduction': 11,\n",
       "  'Related Work': 5,\n",
       "  'Training Policies': 1,\n",
       "  'Experiments': 0,\n",
       "  'System Description': 1,\n",
       "  'Corpora': 0,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Conclusions': 3,\n",
       "  'Acknowledgments': 0},\n",
       " '5035109': {'Introduction': 6,\n",
       "  'Previous work': 13,\n",
       "  'Types of cases included': 3,\n",
       "  'Data': 1,\n",
       "  'Feature extraction': 3,\n",
       "  'Predictive ensemble model': 4,\n",
       "  'Analysis of predictors': 2,\n",
       "  'Conclusion': 3},\n",
       " '11503583': {'Introduction': 3,\n",
       "  'Differences in Translations': 1,\n",
       "  'Nominalisations': 0,\n",
       "  'Voice': 0,\n",
       "  'Negation': 0,\n",
       "  'Information Structure': 0,\n",
       "  'Alignment Model': 0,\n",
       "  'Predicates and Arguments': 0,\n",
       "  'Binding Layer': 0,\n",
       "  'Alignment Layer': 0,\n",
       "  'Application and Outlook': 7},\n",
       " '1477127': {'Introduction': 1,\n",
       "  'Lambek calculus': 2,\n",
       "  'Lambek Grammar': 3,\n",
       "  'NP-Completeness of the Parsing Problem': 1,\n",
       "  'Conclusion': 0},\n",
       " '52948134': {'Acknowledgments': 0},\n",
       " '8078153': {'Credits': 0,\n",
       "  'Introduction': 0,\n",
       "  'General Instructions': 0,\n",
       "  'The Ruler': 0,\n",
       "  'Electronically-available resources': 0,\n",
       "  'Format of Electronic Manuscript': 0,\n",
       "  'Layout': 0,\n",
       "  'Fonts': 0,\n",
       "  'The First Page': 0,\n",
       "  'Sections': 11,\n",
       "  'Footnotes': 0,\n",
       "  'Graphics': 0,\n",
       "  'Accessibility': 0,\n",
       "  'XML conversion and supported  packages': 0,\n",
       "  'Translation of non-English Terms': 0,\n",
       "  'Length of Submission': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Supplemental Material': 0,\n",
       "  'Multiple Appendices': 0},\n",
       " '14225710': {'Introduction': 10,\n",
       "  'Overview of Linguistic Typology': 10,\n",
       "  'Multilingual NLP and the Role of Typologies': 10,\n",
       "  'Development and Uses of Typological Information in NLP': 0,\n",
       "  'Development of Typological Information for NLP': 15,\n",
       "  'Uses of Typological Information in NLP': 8,\n",
       "  \"Typological Information and NLP: What's Next?\": 0,\n",
       "  'Commentary; conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '16490197': {'Introduction': 5,\n",
       "  'Related Work': 4,\n",
       "  'Motivation': 1,\n",
       "  'Design Rationale': 1,\n",
       "  'Plate Diagram': 1,\n",
       "  'Experiment Setup': 2,\n",
       "  'Qualitative Evaluation': 0,\n",
       "  'Quantitative Evaluation': 0,\n",
       "  'Conclusion & Future Work': 0},\n",
       " '174798390': {'Introduction': 15,\n",
       "  'Background': 3,\n",
       "  'Our Approach': 0,\n",
       "  'Basic Intuition': 0,\n",
       "  'Details of Our Approach': 0,\n",
       "  'Model Training with Attention Supervision Information': 1,\n",
       "  'Experiments': 10,\n",
       "  'Effects of ϵ α \\\\epsilon _{\\\\alpha }': 0,\n",
       "  'Overall Results': 4,\n",
       "  'Case Study': 0,\n",
       "  'Related Work': 9,\n",
       "  'Conclusion and Future Work': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '196203065': {'Introduction': 11,\n",
       "  'Neural Machine Translation': 3,\n",
       "  'Local Coordinate Coding': 1,\n",
       "  'Reference Network': 0,\n",
       "  'Overview': 0,\n",
       "  'Monolingual Referent Network': 2,\n",
       "  'Bilingual Reference Network': 0,\n",
       "  'Training and Inference': 2,\n",
       "  'Experiments': 0,\n",
       "  'Settings': 5,\n",
       "  'Results on Chinese-English Translation': 5,\n",
       "  'Results on English-German Translation': 5,\n",
       "  'Conclusion and Future Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '4896290': {'Introduction': 5,\n",
       "  'Comparing statistical parsers': 3,\n",
       "  'EC parser': 0,\n",
       "  'BR parser': 0,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 1},\n",
       " '52156390': {'Introduction': 22,\n",
       "  'Related Work': 33,\n",
       "  'Variable-free Semantics': 3,\n",
       "  'Dependency-based Hybrid Trees': 2,\n",
       "  'Dependency Patterns': 1,\n",
       "  'Model': 3,\n",
       "  'Learning and Decoding': 18,\n",
       "  'Features': 3,\n",
       "  'Neural Component': 7,\n",
       "  'Baseline Systems': 8,\n",
       "  'Results and Discussion': 1,\n",
       "  'Conclusions and Future Work': 5,\n",
       "  'Acknowledgments': 0},\n",
       " '52093134': {'Introduction': 5,\n",
       "  'Related Work': 0,\n",
       "  'Our method': 0,\n",
       "  'Importance Score': 3,\n",
       "  'Dropout Probability': 0,\n",
       "  'GI-Dropout Method': 0,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 8,\n",
       "  'CNN Model': 3,\n",
       "  'Self-attentive RNN Model': 2,\n",
       "  'Experiment Settings': 0,\n",
       "  'Effectiveness of GI-Dropout': 1,\n",
       "  'Further Analysis of Our Method': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '13533556': {},\n",
       " '44064978': {'Introduction': 10,\n",
       "  'Related Work': 14,\n",
       "  'Our Approach': 0,\n",
       "  'Question-Focused Reward': 3,\n",
       "  'Reinforcement Learning': 1,\n",
       "  'Experiments': 2,\n",
       "  'Hyperparameters': 3,\n",
       "  'Results': 10,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52119091': {},\n",
       " '1564849': {'Introduction': 8,\n",
       "  'Original Centroid-based Method': 3,\n",
       "  'Modified Summary Selection': 1,\n",
       "  'Preselection of Sentences': 0,\n",
       "  'Datasets': 1,\n",
       "  'Baselines & Evaluation': 3,\n",
       "  'Preprocessing': 0,\n",
       "  'Parameter Tuning': 0,\n",
       "  'Results': 0,\n",
       "  'Example Summaries': 0,\n",
       "  'Related Work': 2,\n",
       "  'Conclusion': 0},\n",
       " '168169995': {'Introduction': 9,\n",
       "  'Related work': 19,\n",
       "  'Model': 3,\n",
       "  'Coherence discriminator: D coherence D_{\\\\text{coherence}}': 25},\n",
       " '52165221': {'Introduction': 20,\n",
       "  'Notation and problem formulation': 0,\n",
       "  'Supervised translation model: from NL to queries': 0,\n",
       "  'Reinforcement learning': 1,\n",
       "  'Model architecture': 1,\n",
       "  'Datasets': 1,\n",
       "  'Metrics and baselines': 1,\n",
       "  'Implementation details': 2,\n",
       "  'Results': 3,\n",
       "  'Conclusion and future work': 1},\n",
       " '174797774': {'Introduction': 19,\n",
       "  'Literature Review': 7,\n",
       "  'HighRES': 3,\n",
       "  'Highlight Annotation': 2,\n",
       "  'Highlight-based Content Evaluation': 1,\n",
       "  'Clarity and Fluency Evaluation': 0,\n",
       "  'Highlight-based ROUGE Evaluation': 1,\n",
       "  'Summarization Dataset and Models': 6,\n",
       "  'Experiments and Results': 0,\n",
       "  'Content Evaluation of Summaries': 3,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1662415': {'Introduction': 11,\n",
       "  'Model Overview': 1,\n",
       "  'An End-to-End Implementation': 0,\n",
       "  'Semantic Matching Functions': 0,\n",
       "  'Decomposition Functions': 0,\n",
       "  'Composition Functions': 0,\n",
       "  'Similarity Assessment Function': 0,\n",
       "  'Training': 2,\n",
       "  'Experimental Setting': 4,\n",
       "  'Model Properties': 0,\n",
       "  'Comparing with State-of-the-art Models': 1,\n",
       "  'Related Work': 2,\n",
       "  'Conclusion': 0},\n",
       " '207468': {'Introduction': 18,\n",
       "  'Long-Short Term Memory (LSTM)': 2,\n",
       "  'Paragraph Autoencoder': 0,\n",
       "  'Notation': 0,\n",
       "  'Model 1: Standard LSTM': 0,\n",
       "  'Model 2: Hierarchical LSTM': 0,\n",
       "  'Model 3: Hierarchical LSTM with Attention': 2,\n",
       "  'Training and Testing': 0,\n",
       "  'Dataset': 0,\n",
       "  'Training Details and Implementation': 2,\n",
       "  'Evaluations': 6,\n",
       "  'Results': 0,\n",
       "  'Discussion and Future Work': 1,\n",
       "  'Acknowledgement': 0},\n",
       " '29154296': {'Introduction': 11,\n",
       "  'Related Work and Motivation': 14,\n",
       "  'Proposed Approach': 3,\n",
       "  'Neighborhood Structure of Mapped Vectors (Experiment 1)': 0,\n",
       "  'Mapping with Untrained Networks (Experiment 2)': 1,\n",
       "  'Experiment 1': 12,\n",
       "  'Experiment 2': 6,\n",
       "  'Results and Discussion': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Hyperparameters and Implementation': 1,\n",
       "  'Textual Feature Extraction': 0},\n",
       " '153311772': {},\n",
       " '4715568': {'Introduction': 15,\n",
       "  'Model': 2,\n",
       "  'Setup': 5,\n",
       "  'Main Results': 1,\n",
       "  'Ablation Study': 0,\n",
       "  'Discussion': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '53083054': {'Conclusion': 0, 'Acknowledgements': 0},\n",
       " '75134948': {},\n",
       " '1553193': {'Introduction': 14,\n",
       "  'Related Work': 27,\n",
       "  'Creating a science exam QA dataset': 0,\n",
       "  'First task: producing in-domain questions': 0,\n",
       "  'Second task: selecting distractors': 3,\n",
       "  'Dataset properties': 0,\n",
       "  'System performance': 5,\n",
       "  'Using SciQ to answer exam questions': 0,\n",
       "  'Conclusion': 0,\n",
       "  'List of Study Books': 0,\n",
       "  'Training and Implementation Details': 2},\n",
       " '102351751': {'Introduction': 6,\n",
       "  'Methodology': 0,\n",
       "  'Node representation': 1,\n",
       "  'Model': 0,\n",
       "  'Baselines': 3,\n",
       "  'Evaluation Metric': 1,\n",
       "  'Intrinsic Evaluation': 1,\n",
       "  'Error Analysis': 1,\n",
       "  'Related Work': 6,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Length of Generated Path': 0},\n",
       " '15140656': {'Introduction': 7,\n",
       "  'Related Work': 15,\n",
       "  'BPE for related languages': 0,\n",
       "  'Motivation': 0,\n",
       "  'Comparison with orthographic syllables': 1,\n",
       "  'The BPE Algorithm': 0,\n",
       "  'Training subword level translation model': 2,\n",
       "  'Experimental Setup': 0,\n",
       "  'Languages and writing systems': 5,\n",
       "  'Datasets': 2,\n",
       "  'System details': 5,\n",
       "  'Evaluation': 3,\n",
       "  'Results and Analysis': 0,\n",
       "  'Comparison of BPE with other units': 1,\n",
       "  'Applicability to different writing systems': 0,\n",
       "  'Choosing number of BPE merges': 0,\n",
       "  'Robustness to Domain Change': 0,\n",
       "  'Effect of training data size': 0,\n",
       "  'Joint bilingual learning of BPE units': 0,\n",
       "  'Why are BPE units better than others?': 0,\n",
       "  'Conclusion & Future Work': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '7883094': {'Introduction': 24,\n",
       "  'Model': 0,\n",
       "  'CNN-based Dialog Utterance Representation': 0,\n",
       "  'Internal Attention Mechanism': 1,\n",
       "  'Neural-based Context Modeling': 0,\n",
       "  'Data': 7,\n",
       "  'Hyperparameters and Training': 4,\n",
       "  'Baseline Models': 1,\n",
       "  'Results': 0,\n",
       "  'Impact of Context Length': 0,\n",
       "  'Comparison with Other Works': 3,\n",
       "  'Conclusions': 0},\n",
       " '8821211': {'Introduction': 4,\n",
       "  'A typology of abusive language': 12,\n",
       "  'Implications for future research': 0,\n",
       "  'Implications for annotation': 20,\n",
       "  'Implications for modeling': 17,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 1},\n",
       " '53046959': {},\n",
       " '14633379': {'Introduction': 1,\n",
       "  'Background': 28,\n",
       "  'Task and Data': 1,\n",
       "  'Task complexity': 1,\n",
       "  'Model': 1,\n",
       "  'Sequence-to-sequence model (s2s)': 1,\n",
       "  's2s with autoencoding (s2s+ae)': 0,\n",
       "  'Experimental methodology': 0,\n",
       "  'Benchmarks': 0,\n",
       "  'Metrics': 0,\n",
       "  'Analysis of content selection': 0,\n",
       "  'Comparison against Wikipedia reference': 0,\n",
       "  'Human preference evaluation': 0,\n",
       "  'Analysis': 0,\n",
       "  'Fact Count': 0,\n",
       "  'Example generated text': 0,\n",
       "  'Content selection and hallucination': 0,\n",
       "  'Discussion and future work': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5076191': {'Introduction': 6,\n",
       "  'Natural Language Adversarial Examples': 14,\n",
       "  'Attack Design': 0,\n",
       "  'Threat model': 3,\n",
       "  'Algorithm': 15},\n",
       " '53047545': {'Introduction': 12,\n",
       "  'The Encoding': 0,\n",
       "  'Theoretical correctness': 0,\n",
       "  'Limitations': 3,\n",
       "  'Sequence Labeling': 1,\n",
       "  'Traditional Sequence Labeling Methods': 4,\n",
       "  'Sequence Labeling Neural Models': 4,\n",
       "  'Experiments': 0,\n",
       "  'Results': 2,\n",
       "  'Discussion': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '21721649': {'Introduction': 15,\n",
       "  'Methodology': 2,\n",
       "  'Adversarial Learning': 0,\n",
       "  'Experiments': 0,\n",
       "  'POS-tagging': 6,\n",
       "  'Sentiment Analysis': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '14352003': {'Introduction': 16,\n",
       "  'Related Work': 18,\n",
       "  'Creating SCL-NMA': 0,\n",
       "  'Term Selection': 2,\n",
       "  'Best–Worst Scaling': 6,\n",
       "  'Annotation process': 0,\n",
       "  'Quality of Annotations': 0,\n",
       "  'Least Perceptible Difference in Sentiment': 1,\n",
       "  'Impact of Negators, Modals, and Degree Adverbs on Sentiment': 0,\n",
       "  'Negation': 0,\n",
       "  'Modals': 0,\n",
       "  'Degree Adverbs': 0,\n",
       "  'Interactive Visualization': 0,\n",
       "  'Conclusions': 0},\n",
       " '49865411': {'Introduction': 8,\n",
       "  'Background': 1,\n",
       "  'Related Work': 1,\n",
       "  'Neural Belief Tracker (NBT)': 1,\n",
       "  'Multi-domain Dialogue State Tracking': 1,\n",
       "  'Method': 3,\n",
       "  'Domain Tracking': 2,\n",
       "  'Candidate Slots and Values Tracking': 0,\n",
       "  'Belief State Update': 3,\n",
       "  'Training Criteria': 0,\n",
       "  'Datasets and Baselines': 2,\n",
       "  'Data Structure': 0,\n",
       "  'Evaluation': 4,\n",
       "  'Results': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52009862': {'Introduction': 7,\n",
       "  'Related work': 15,\n",
       "  'CCMR dataset': 1,\n",
       "  'Observation in Annotation': 0,\n",
       "  'Framework Overview': 0,\n",
       "  'Cross-lingual Cross-platform Features': 2,\n",
       "  'Distance Features': 0,\n",
       "  'Agreement Features': 0,\n",
       "  'Rumor Verification Leveraging Cross-platform Information': 1,\n",
       "  'Baselines': 3,\n",
       "  'Results': 0,\n",
       "  'Analysis': 0,\n",
       "  'Rumor Verification Leveraging Cross-lingual Information': 0,\n",
       "  'Low-resource Rumor Verification via Transfer Learning': 0,\n",
       "  'Conclusion': 0},\n",
       " '491769': {'Introduction': 2,\n",
       "  'Simple Runs': 1,\n",
       "  'Regression Approaches': 0,\n",
       "  'Deep Learning Approaches': 0,\n",
       "  'Submission Results': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '91184245': {'Acknowledgement': 0},\n",
       " '13949438': {'Introduction': 0,\n",
       "  'Related Work': 7,\n",
       "  'Model': 0,\n",
       "  'Training': 0,\n",
       "  'Testing': 0,\n",
       "  'Selecting Regularization Parameters': 2,\n",
       "  'Experiments': 0,\n",
       "  'Simulated Data': 1,\n",
       "  'Contaminated Data': 1,\n",
       "  'Manually Annotated Data': 3,\n",
       "  'Automatically Annotated Data': 2,\n",
       "  'Discussion': 0,\n",
       "  'Comparison to SVMs': 0,\n",
       "  'Future Work': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Appendix A. The Label-Flipping Model': 1,\n",
       "  'EM derivation': 0,\n",
       "  'Connection to Instance-Weighting': 2,\n",
       "  'Appendix B. Comparison with Other Methods': 0,\n",
       "  'Simulation Experiments': 0,\n",
       "  'Natural Language Experiments': 0,\n",
       "  'Appendix C. Implementation Details': 0},\n",
       " '20272964': {},\n",
       " '53037336': {'Introduction': 18,\n",
       "  'Related Work': 11,\n",
       "  'Dataset Creation': 0,\n",
       "  'Corpus Collection': 2,\n",
       "  'Data Partition': 0,\n",
       "  'Coreference Annotation and Refinement': 3,\n",
       "  'Dataset Properties': 0,\n",
       "  'Analysis': 0,\n",
       "  'Baseline Performance': 2,\n",
       "  'Impact of Training-test Overlap': 1,\n",
       "  'Mention Detection': 2,\n",
       "  'Conclusion': 0},\n",
       " '7105713': {'Introduction': 1,\n",
       "  'Previous Work': 17,\n",
       "  'The Movie-Review Domain': 0,\n",
       "  'A Closer Look At the Problem': 0,\n",
       "  'Machine Learning Methods': 0,\n",
       "  'Naive Bayes': 1,\n",
       "  'Maximum Entropy': 1,\n",
       "  'Support Vector Machines': 1,\n",
       "  'Experimental Set-up': 0,\n",
       "  'Results': 6,\n",
       "  'Discussion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '195767175': {'Acknowledgments': 0},\n",
       " '1969092': {'Introduction': 3,\n",
       "  'Our Model': 0,\n",
       "  'TransE and PTransE': 0,\n",
       "  'Relation Path Reliability': 2,\n",
       "  'Relation Path Representation': 3,\n",
       "  'Objective Formalization': 0,\n",
       "  'Optimization and Implementation Details': 0,\n",
       "  'Complexity Analysis': 1,\n",
       "  'Data Sets and Experimental Setting': 1,\n",
       "  'Knowledge Base Completion': 17,\n",
       "  'Relation Extraction from Text': 10,\n",
       "  'Case Study of Relation Inference': 0,\n",
       "  'Related Work': 20,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  ' Acknowledgments': 0},\n",
       " '3036949': {},\n",
       " '106401519': {'Introduction': 13,\n",
       "  'Problem Overview': 0,\n",
       "  'Existing Solution and Background': 4,\n",
       "  'Our Approach': 0,\n",
       "  'Entity Neighborhood Encoder (ENE)': 0,\n",
       "  'Attention Mechanism': 1,\n",
       "  'Joint Model: OpenKI': 0,\n",
       "  'Training Process': 1,\n",
       "  'Explicit Argument Type Constraint': 1,\n",
       "  'Data': 8,\n",
       "  'Verifying Usefulness of Neighborhood Information: Bayesian Methods': 0,\n",
       "  'Baselines and Experimental Setup': 0,\n",
       "  'Results': 0,\n",
       "  'Results on NYT + Freebase Dataset': 9,\n",
       "  'Related Work': 12,\n",
       "  'Conclusion': 0,\n",
       "  'Derivation of Bayesian Inference Baselines': 0},\n",
       " '52154441': {'Introduction': 2,\n",
       "  'The Marian toolkit': 1,\n",
       "  'NMT architectures': 2,\n",
       "  'Deep transition RNN architecture': 2,\n",
       "  'Transformer architectures': 0,\n",
       "  'Training recipe': 2,\n",
       "  'Taking advantage of Paracrawl': 0,\n",
       "  'Dual conditional cross-entropy filtering': 3,\n",
       "  'Cross-entropy difference filtering': 0,\n",
       "  'Data selection': 0,\n",
       "  'Data weighting': 0,\n",
       "  'Final submission': 0,\n",
       "  'Ensembling with a Transformer-style language model': 0,\n",
       "  'Results': 0,\n",
       "  'Conclusions': 0},\n",
       " '52113643': {'Introduction': 11,\n",
       "  'Dependency parsing model': 2,\n",
       "  'Head prediction': 0,\n",
       "  'Label prediction': 2,\n",
       "  'Computing word representations': 13},\n",
       " '9768369': {'Introduction': 12,\n",
       "  'Approach': 0,\n",
       "  'Summarizing Global Context': 0,\n",
       "  'Integrating Global Context into NMT': 2,\n",
       "  'Setup': 4,\n",
       "  'Results': 2,\n",
       "  'Analysis': 0,\n",
       "  'Related Work': 5,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '110317': {'Introduction': 10,\n",
       "  'Baseline System': 12,\n",
       "  'Neural MT Models': 1,\n",
       "  'Experimental Results': 4,\n",
       "  'Analysis': 1,\n",
       "  'Effect of nn-best Size on Reranking': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments:': 0},\n",
       " '195820682': {'Introduction': 7,\n",
       "  'Simplified HPSG on PTB': 2,\n",
       "  'Tree Preprocessing': 2,\n",
       "  'Span Representations of HPSG': 0,\n",
       "  'Overview': 5,\n",
       "  'Token Representation': 1,\n",
       "  'Self-Attention Encoder': 2,\n",
       "  'Decoder for Division Span HPSG': 5,\n",
       "  'Decoder for Joint Span HPSG': 3,\n",
       "  'Experiments': 7,\n",
       "  'Setup': 9,\n",
       "  'Self-attention Layers': 0,\n",
       "  'Moderating constituent and Dependency': 1,\n",
       "  'Joint Span HPSG Parsing': 0,\n",
       "  'Parsing Speed': 0,\n",
       "  'Main Results': 2,\n",
       "  'Related Work': 38,\n",
       "  'Conclusions': 0},\n",
       " '2782776': {'Introduction': 9,\n",
       "  'Problem Definition': 1,\n",
       "  'Simultaneous Translation  with Neural Machine Translation': 0,\n",
       "  'Environment': 1,\n",
       "  'Agent': 1,\n",
       "  'Learning': 0,\n",
       "  'Pre-training': 0,\n",
       "  'Reward Function': 3,\n",
       "  'Reinforcement Learning': 1,\n",
       "  'Simultaneous Beam Search': 1,\n",
       "  'Settings': 4,\n",
       "  'Quantitative Analysis': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Related Work': 5,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1727568': {'Conclusions': 0},\n",
       " '2539078': {'Introduction': 3,\n",
       "  'Related Work': 1,\n",
       "  'Data': 0,\n",
       "  'Methods': 3,\n",
       "  'Results': 0,\n",
       "  'Model capacity': 0,\n",
       "  'Structure of the language space': 0,\n",
       "  'Generating Text': 0,\n",
       "  'Mixing and Interpolating Between Languages': 0,\n",
       "  'Language identification': 0,\n",
       "  'Conclusions': 0},\n",
       " '28217923': {'Introduction': 6,\n",
       "  'Methods and Experiments': 0,\n",
       "  'Data': 0,\n",
       "  'Systems': 0,\n",
       "  'Approaches': 2,\n",
       "  'Plurality Voting': 1,\n",
       "  'Optimal Ensemble and Oracle': 0,\n",
       "  'Lexical Complexity': 1,\n",
       "  'Conclusion and Future Work': 1,\n",
       "  'Acknowledgements': 0},\n",
       " '2612150': {'Introduction': 13,\n",
       "  'Related Work': 2,\n",
       "  'Model Description': 2,\n",
       "  'Datasets': 4,\n",
       "  'Pre-trained Word Embeddings': 4,\n",
       "  'Setup': 1,\n",
       "  'Results and Discussion': 5,\n",
       "  'Conclusions': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '67855706': {'Introduction': 22,\n",
       "  'Semi-parametric NMT': 4,\n",
       "  'Retrieval Approaches': 2,\n",
       "  'NMT with Context Retrieval': 1,\n",
       "  'Data and Evaluation': 2,\n",
       "  'Hyper-parameters and Optimization': 8,\n",
       "  'Results': 0,\n",
       "  'Comparison of retrieval strategies': 0,\n",
       "  'Memory Ablation Experiments': 0,\n",
       "  'Non-Parametric Adaptation': 0,\n",
       "  'Related Work': 20,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '11054023': {'Introduction': 13,\n",
       "  'Task Description and Definition': 0,\n",
       "  'Conditioned Language Model': 3,\n",
       "  'Data-set Collection and Annotation': 0,\n",
       "  'Annotations Based on Meta-data': 0,\n",
       "  'Annotations Derived from Text': 0,\n",
       "  'Dataset Statistics': 0,\n",
       "  'Evaluating Language Model Quality': 0,\n",
       "  'Conditioned vs. Unconditioned': 0,\n",
       "  'Conditioned vs. Dedicated LMs': 0,\n",
       "  'Conditioned vs. Flipped Conditioning': 0,\n",
       "  'Evaluating the Generated Sentences': 0,\n",
       "  'Capturing Individual Properties': 0,\n",
       "  'Examples of Generated Sentences': 0,\n",
       "  'Generalization Ability': 0,\n",
       "  'Related Work': 25,\n",
       "  'Conclusions': 0},\n",
       " '11406047': {'Acknowledgments': 0},\n",
       " '7146903': {'Introduction': 21,\n",
       "  'Related Work': 3,\n",
       "  'Model Description': 8,\n",
       "  'Model Enhancements': 1,\n",
       "  'Experiments': 0,\n",
       "  'Hyperparameters and Training': 2,\n",
       "  'Datasets and Experimental Setup': 7,\n",
       "  'Conclusion': 0,\n",
       "  'Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '7857808': {'WSJ Treebank Inconsistencies': 0},\n",
       " '173991008': {'Acknowledgements': 0},\n",
       " '23339307': {'Introduction': 11,\n",
       "  'Attentional NMT': 2,\n",
       "  'Phrase-based SMT': 2,\n",
       "  'Forced Decoding for NMT': 2,\n",
       "  'Reranking NMT Outputs with Phrase-based Decoding Score': 1,\n",
       "  'Settings': 5,\n",
       "  'Results and Analysis': 2,\n",
       "  'Related Work': 7,\n",
       "  'Conclusion': 0},\n",
       " '1430604': {'Acknowledgments': 0},\n",
       " '52057510': {},\n",
       " '11022639': {'Introduction': 8,\n",
       "  'Task and datasets': 0,\n",
       "  'Formal Task Description': 0,\n",
       "  'Datasets': 4,\n",
       "  'Our Model — Attention Sum Reader': 0,\n",
       "  'Formal Description': 2,\n",
       "  'Model instance details': 2,\n",
       "  'Related Work': 4,\n",
       "  'Attentive and Impatient Readers': 1,\n",
       "  'Chen et al. 2016': 1,\n",
       "  'Memory Networks': 3,\n",
       "  'Dynamic Entity Representation': 1,\n",
       "  'Pointer Networks': 1,\n",
       "  'Summary': 4,\n",
       "  'Evaluation': 0,\n",
       "  'Training Details': 7,\n",
       "  'Evaluation Method': 0,\n",
       "  'Results': 3,\n",
       "  'Analysis': 0,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0,\n",
       "  'Dependence of accuracy on the frequency of the correct answer': 0},\n",
       " '53081097': {'Introduction': 5,\n",
       "  'Background: Multi-Head Attention': 0,\n",
       "  'Approach': 0,\n",
       "  'Framework': 0,\n",
       "  'Disagreement Regularization': 2,\n",
       "  'Related Work': 2,\n",
       "  'Setup': 8,\n",
       "  'Effect of Regularization Terms': 0,\n",
       "  'Effect on Different Attention Networks': 0,\n",
       "  'Main Results': 0,\n",
       "  'Quantitative Analysis of Regularization': 1,\n",
       "  'Conclusion': 4,\n",
       "  'Acknowledgments': 0},\n",
       " '1479529': {},\n",
       " '5083989': {'Acknowledgments': 0},\n",
       " '102352337': {'Introduction': 12,\n",
       "  'Related Work': 16,\n",
       "  'Models': 7,\n",
       "  'Transition Probability Calculation': 0,\n",
       "  'NE-D-VRNN': 0,\n",
       "  'Datasets': 3,\n",
       "  'Experiments': 2,\n",
       "  'Comparison with K-means Clustering': 0,\n",
       "  'Comparison with HMM': 1,\n",
       "  'VRNN Model Variants Comparison': 0,\n",
       "  'Application on RL': 0,\n",
       "  'Reward Function Design': 3,\n",
       "  'Result Analysis': 0,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Reward Functions in RL': 0,\n",
       "  'RL Simulation Setting': 0,\n",
       "  'RL Training Details': 1,\n",
       "  'Dialog Structures by Different Models': 0},\n",
       " '24527658': {'Introduction': 3,\n",
       "  'Model': 3,\n",
       "  'Data sets': 2,\n",
       "  'Baselines': 4,\n",
       "  'Our model': 1,\n",
       "  'Results': 0,\n",
       "  'Related work': 5,\n",
       "  'Discussion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5450801': {'Introduction': 7,\n",
       "  'Related Work': 20,\n",
       "  'Problem Formalization': 0,\n",
       "  'Model Overview': 0,\n",
       "  'Utterance-Response Matching': 0,\n",
       "  'Matching Accumulation': 0,\n",
       "  'Matching Prediction and Learning': 1,\n",
       "  'Response Candidate Retrieval': 0,\n",
       "  'Experiments': 0,\n",
       "  'Ubuntu Corpus': 2,\n",
       "  'Douban Conversation Corpus': 3,\n",
       "  'Baseline': 5,\n",
       "  'Parameter Tuning': 3,\n",
       "  'Evaluation Results': 0,\n",
       "  'Further Analysis': 1,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '43963368': {'Introduction': 0,\n",
       "  'Related Work': 31,\n",
       "  'The L2-Reddit corpus': 0,\n",
       "  'Corpus mining': 11},\n",
       " '52119281': {'Introduction': 5,\n",
       "  'Related Work': 4,\n",
       "  'Baseline Sequence-to-Sequence Model': 1,\n",
       "  'Byte-Pair Encoding': 1,\n",
       "  'Fixed stride Temporal Pooling': 1,\n",
       "  'Learned Temporal Compression': 2,\n",
       "  'Corpora': 0,\n",
       "  'Model sizes, training, and inference': 3,\n",
       "  'Tuning and Evalution': 1,\n",
       "  'Character-level translation': 2,\n",
       "  'Compressing the Source Sequence': 0,\n",
       "  'Conclusion': 0},\n",
       " '196207277': {'Introduction': 3,\n",
       "  'System Description': 2,\n",
       "  'Related Work': 4,\n",
       "  'User Study': 0,\n",
       "  'Feedback Summary': 0,\n",
       "  'Future Development': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '84186122': {'Introduction': 9,\n",
       "  'Related Work': 13,\n",
       "  'Proposed Model': 0,\n",
       "  'Current Utterance Summary': 0,\n",
       "  'Decay-Function-Free Time-Aware Attention': 0,\n",
       "  'Decay-Function-Free Content-and-Time-Aware Attention': 0,\n",
       "  'Speaker Indicator': 0,\n",
       "  'Prediction': 0,\n",
       "  'Experiments': 0,\n",
       "  'Dataset and Settings': 4,\n",
       "  'Results': 0,\n",
       "  'Discussion': 0,\n",
       "  'Comparison with Baseline Models': 0,\n",
       "  'Detailed Analysis on Proposed Methods': 0,\n",
       "  'Effectiveness of Use of Distance in History Representation': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52287106': {'Introduction': 2, 'String Transducer': 0},\n",
       " '2445030': {'Introduction': 22,\n",
       "  'Neural Machine Translation': 8,\n",
       "  'Posterior Regularization': 4,\n",
       "  'Modeling': 8,\n",
       "  'Feature Design': 12,\n",
       "  'Training': 1,\n",
       "  'Search': 4,\n",
       "  'Setup': 7,\n",
       "  'Main Results': 0,\n",
       "  'Effect of Reranking': 0,\n",
       "  'Training Speed': 0,\n",
       "  'Example Translations': 0,\n",
       "  'Related Work': 7,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '3078792': {'Introduction': 8,\n",
       "  'Installation': 0,\n",
       "  'Overview of FrameNet': 2,\n",
       "  'Design Principles': 0,\n",
       "  'Lexicon Access Methods': 0,\n",
       "  'Database Objects': 0,\n",
       "  'Advanced Lexicon Access': 0,\n",
       "  'Corpus Access': 0,\n",
       "  'Limitations and future work': 2,\n",
       "  'Acknowledgments': 1},\n",
       " '44131945': {},\n",
       " '52094979': {'Introduction': 12,\n",
       "  'Overview': 0,\n",
       "  'Keyword Predictor': 3,\n",
       "  'Bidirectional-Asynchronous Decoder': 0,\n",
       "  'Direction Selector': 0,\n",
       "  'Data': 0,\n",
       "  'Metrics': 4,\n",
       "  'Baselines': 3,\n",
       "  'Results and Discussion': 0,\n",
       "  'Case Study and Error Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '67856687': {'Credits': 0,\n",
       "  'Introduction': 0,\n",
       "  'General Instructions': 0,\n",
       "  'The Ruler': 0,\n",
       "  'Electronically-available resources': 0,\n",
       "  'Format of Electronic Manuscript': 0,\n",
       "  'Layout': 0,\n",
       "  'Fonts': 0,\n",
       "  'The First Page': 0,\n",
       "  'Sections': 18,\n",
       "  'Footnotes': 0,\n",
       "  'Graphics': 0,\n",
       "  'Accessibility': 0,\n",
       "  'Translation of non-English Terms': 0,\n",
       "  'Length of Submission': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '44143625': {'Dimensions of CCA and KCCA projections.': 0,\n",
       "  'Kernel parameter estimation.': 0},\n",
       " '1076': {'Overview': 5,\n",
       "  'The Framework for Estimating the Referent': 1,\n",
       "  'Heuristic Rules for Demonstratives': 4,\n",
       "  'Rule in the case when the referent is a noun phrase': 0,\n",
       "  'Rule when the referent is a verb phrase': 0,\n",
       "  'Rule using the feature that demonstrative pronouns usually do not refer to people': 3,\n",
       "  'Rule with feature that “koko” and “soko” often refer to locations': 1,\n",
       "  'Rule when “kokode” or “sokode” is used as a conjunction': 0,\n",
       "  'Rule when an anaphor does not have its antecedent': 0,\n",
       "  'Rule for Demonstrative Adjectives': 1,\n",
       "  'Rules for daikou-reference of so-series demonstrative adjective': 3,\n",
       "  'Rules when non-so-series demonstrative has daikou-reference': 3,\n",
       "  'Rule when a pronoun refers to a verb phrase': 0,\n",
       "  \"Rule for “kon'na noun” (noun like this)\": 0,\n",
       "  'Rule when so-series demonstrative adverb refers to the previous sentences': 0,\n",
       "  'Rule when so-series demonstrative adverb cataphorically Refers to the Verb Phrase in the Same Sentence': 0,\n",
       "  'Heuristic Rule for Personal Pronouns': 0,\n",
       "  'Rule proposing candidate referents of general zero pronoun': 0,\n",
       "  'Rule using semantic relation to verb phrase': 0,\n",
       "  'Rule using the feature that it is difficult for a noun phrase to be filled in multiple case components of the same verb': 0,\n",
       "  'Rule using empathy': 1,\n",
       "  'Experiment': 2,\n",
       "  'Discussion': 0,\n",
       "  'Comparison Experiment': 0,\n",
       "  'Summary': 0},\n",
       " '4932015': {'Introduction': 8,\n",
       "  'Related Work': 13,\n",
       "  'Local Coherence (LC) Model': 3,\n",
       "  'LSTM AES Model': 1,\n",
       "  'Combined Models': 0,\n",
       "  'Data and Evaluation': 2,\n",
       "  'Model Parameters and Baselines': 7,\n",
       "  'Results': 0,\n",
       "  'Further Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '52167992': {'Acknowledgments': 0},\n",
       " '6188348': {'Introduction': 1,\n",
       "  'Related Work': 12,\n",
       "  'Features of Collaborative Negotiation': 8,\n",
       "  'Response Generation in Collaborative Negotiation': 2,\n",
       "  'Evaluating Proposed Beliefs': 8,\n",
       "  'Modifying Unaccepted Proposals': 15,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1461182': {'Introduction': 2,\n",
       "  'Ordinal Common-sense Inference': 3,\n",
       "  'Framework for collecting OCI corpus': 0,\n",
       "  'Generation of Common-sense Inference Candidates': 11,\n",
       "  'Ordinal Label Annotation': 0,\n",
       "  'JOCI Corpus': 2,\n",
       "  'Data sources for Context-Hypothesis Pairs': 1,\n",
       "  'Crowdsourced Ordinal Label Annotation': 0,\n",
       "  'Corpus Characteristics': 0,\n",
       "  'Predicting Ordinal Judgments': 2,\n",
       "  'Analysis': 0,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '6555336': {'Introduction': 12,\n",
       "  'BiLSTM Baseline': 1,\n",
       "  'Character-level Convolutional Neural Network': 3,\n",
       "  'Intra Attention Mechanism': 6,\n",
       "  'Character-level Intra Attention Network': 0,\n",
       "  'Data': 1,\n",
       "  'Hyper Parameters': 2,\n",
       "  'Result': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '11468390': {'Motivation': 0,\n",
       "  'Shortcomings of current summarization models': 17,\n",
       "  'Prior work on alignments': 10,\n",
       "  'Paper structure': 1,\n",
       "  'Human-produced Alignments': 1,\n",
       "  'Annotation guidelines': 1,\n",
       "  'Annotator agreement': 0,\n",
       "  'Results of annotation': 0,\n",
       "  'Statistical Alignment Model': 0,\n",
       "  'Generative story': 0,\n",
       "  'Statistical model': 7,\n",
       "  'Creating the state space': 0,\n",
       "  'Expectation maximization': 4,\n",
       "  'Model inference': 3,\n",
       "  'Model Parameterization': 0,\n",
       "  'Parameterizing the jump model': 4,\n",
       "  'Parameterizing the rewrite model': 0,\n",
       "  'Model priors': 1,\n",
       "  'Parameter initialization': 0,\n",
       "  'Experimental Results': 0,\n",
       "  'Systems compared': 4,\n",
       "  'Evaluation results': 0,\n",
       "  'Error analysis': 0,\n",
       "  'Conclusion and Discussion': 5},\n",
       " '18733074': {'Introduction': 7,\n",
       "  'Related work': 5,\n",
       "  'Data': 2,\n",
       "  'Annotation results': 0,\n",
       "  'Preliminaries': 0,\n",
       "  'Model description': 5,\n",
       "  'Argument structure SVM': 4,\n",
       "  'Argument structure RNN': 1,\n",
       "  'Baseline models': 0,\n",
       "  'Results': 1,\n",
       "  'Discussion and analysis': 0,\n",
       "  'Conclusions and future work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '13848633': {'Introduction': 0,\n",
       "  'Language Modeling': 0,\n",
       "  'The Role of Inflectional Morphology': 2,\n",
       "  'Open-Vocabulary Language Models': 6,\n",
       "  'A Fairer Evaluation: Multi-Text': 0,\n",
       "  'Experiments and Results': 2,\n",
       "  'Discussion and Analysis': 0,\n",
       "  'Related Work': 5,\n",
       "  'Conclusion': 0},\n",
       " '27544235': {'Introduction': 8,\n",
       "  'Proposed Methods': 0,\n",
       "  'Based on word frequency': 0,\n",
       "  'Using sentence level log likelihood': 0,\n",
       "  'Considering left and right word contexts separately': 0,\n",
       "  'Experiments': 0,\n",
       "  'Model architecture': 4,\n",
       "  'Datasets': 6,\n",
       "  'Correlation measures': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Correlation Analysis': 0,\n",
       "  'Analysis of Relevance Scores': 0,\n",
       "  'Positional effects of context words': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Related Work': 12,\n",
       "  'Conclusions and Future Work': 0},\n",
       " '196198761': {'Introduction': 15,\n",
       "  'Motivation': 0,\n",
       "  'Basic Adapter': 0,\n",
       "  'Adversarial Adapter': 7,\n",
       "  'Reconstruction Loss': 0,\n",
       "  'Relation Detection with the Adapter': 1,\n",
       "  'SimpleQuestion-Balance (SQB)': 0,\n",
       "  'Settings': 6,\n",
       "  'Main Results': 1,\n",
       "  'Analysis': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '52070657': {'Introduction': 8,\n",
       "  'Related Work': 27,\n",
       "  'Problem Formalization': 0,\n",
       "  'Seq2Seq Model with Copy Mechanism': 4,\n",
       "  'Model Correlation': 3,\n",
       "  'Implementation Details': 1,\n",
       "  'Datasets': 4,\n",
       "  'Baseline Models': 10,\n",
       "  'Evaluation Metrics': 2,\n",
       "  'Present Phrase Prediction': 0,\n",
       "  'Absent Phrase Prediction': 0,\n",
       "  'Generalization Ability': 7,\n",
       "  'Discussion': 1,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '166228407': {'Introduction': 8,\n",
       "  'Related Works': 1,\n",
       "  'Multimodal Neural Machine Translation': 3,\n",
       "  'Doubly-Attention NMT': 2,\n",
       "  'IMAGINATION': 1,\n",
       "  'Visual Attention Grounding NMT': 1,\n",
       "  'Word Embedding': 8,\n",
       "  'Word Embeddings': 0,\n",
       "  'Dataset': 3,\n",
       "  'Model': 4,\n",
       "  'Results': 0,\n",
       "  'Conclusion': 0},\n",
       " '189898178': {'Introduction': 13,\n",
       "  'Automatic Post-Editing': 5,\n",
       "  'BERT as a Cross-Lingual Encoder': 2,\n",
       "  'BERT as a Decoder': 0,\n",
       "  'Experiments': 2,\n",
       "  'Related Work': 6,\n",
       "  'Conclusion and Future Work': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '480741': {'Introduction': 15,\n",
       "  'Setting': 1,\n",
       "  'Comparing the five approaches': 3,\n",
       "  'About the tuning to new domains': 1,\n",
       "  'About the training data quality': 0,\n",
       "  'Conclusions and Future Work': 1},\n",
       " '49183263': {'Introduction': 13,\n",
       "  'Baselines': 1,\n",
       "  'Word Embeddings and Composition Functions': 6,\n",
       "  'Neural Compositional Models': 6,\n",
       "  'Evaluation Datasets': 0,\n",
       "  'Pairwise Similarity': 3,\n",
       "  'Sentiment Analysis and Text Categorization': 6,\n",
       "  'Training Data': 2,\n",
       "  'Training Settings': 1,\n",
       "  'Pairwise Similarity Evaluation': 2,\n",
       "  'Evaluation on Sentiment Analysis and Categorization': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Discussion and Conclusions': 0},\n",
       " '44087711': {'Introduction': 1,\n",
       "  'Related work': 10,\n",
       "  'Multilingual sentence embeddings': 11,\n",
       "  'Experimental evaluation: BUCC shared task on mining bitexts': 8,\n",
       "  'Baseline NMT systems': 9,\n",
       "  'Filtering Common Crawl': 0,\n",
       "  'Mining Parallel Data in WMT News': 1,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '57759363': {},\n",
       " '195699380': {'Introduction': 2,\n",
       "  'Related Work and Datasets': 18,\n",
       "  'DatasetData is crawled in accordance to the terms and conditions of the website.': 0,\n",
       "  'Task: What makes a debater successful?': 0,\n",
       "  'Data preprocessing': 0,\n",
       "  'Features': 8,\n",
       "  'Results and Analysis': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '162168565': {'Introduction': 6,\n",
       "  'Related Work': 5,\n",
       "  'Tagset': 5,\n",
       "  'Corpora': 2,\n",
       "  'Pretrained word vectors': 1,\n",
       "  'FastText': 0,\n",
       "  'Word2Vec': 1,\n",
       "  'Character level encoder': 1,\n",
       "  'Experiments': 0,\n",
       "  'Methods': 7,\n",
       "  'NCRF++': 2,\n",
       "  'Experimental Results': 2,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '8314118': {'Introduction': 14,\n",
       "  'Our Models': 0,\n",
       "  'Sequence-to-sequence attentional model': 2,\n",
       "  'Pointer-generator network': 1,\n",
       "  'Coverage mechanism': 5,\n",
       "  'Related Work': 32,\n",
       "  'Dataset': 5,\n",
       "  'Experiments': 4,\n",
       "  'Preliminaries': 5,\n",
       "  'Observations': 2,\n",
       "  'Comparison with extractive systems': 1,\n",
       "  'How abstractive is our model?': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0,\n",
       "  'Supplementary Material': 0},\n",
       " '17418530': {'Introduction': 16,\n",
       "  'Related work': 25,\n",
       "  'Dataset': 3,\n",
       "  'Features': 45},\n",
       " '52195444': {'Motivation': 2,\n",
       "  'The task': 0,\n",
       "  'Systems and data': 2,\n",
       "  'Results': 0,\n",
       "  'Looking for zero-shot generalization': 5,\n",
       "  'Conclusion': 15,\n",
       "  'Acknowledgments': 0},\n",
       " '1770102': {'Conclusion': 0},\n",
       " '52237660': {'Introduction': 6,\n",
       "  'Cross-lingual Word Embedding': 4,\n",
       "  'Sentence Translation': 0,\n",
       "  'Context-aware Beam Search': 1,\n",
       "  'Denoising': 3,\n",
       "  'Experiments': 8,\n",
       "  'Ablation Study: Denoising': 0,\n",
       "  'Ablation Study: Vocabulary': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '5068376': {'Introduction': 24,\n",
       "  'Model': 1,\n",
       "  'Self-attention token encoder ': 6,\n",
       "  'Syntactically-informed self-attention ': 3,\n",
       "  'Multi-task learning ': 2,\n",
       "  'Predicting semantic roles ': 0,\n",
       "  'Training ': 5,\n",
       "  'Related work': 41,\n",
       "  'Experimental results': 10,\n",
       "  'Semantic role labeling ': 2,\n",
       "  'Parsing, POS and predicate detection ': 2,\n",
       "  'Analysis ': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Supplemental analysis ': 0,\n",
       "  'Supplemental results': 0,\n",
       "  'Data and pre-processing details': 10,\n",
       "  'Optimization and hyperparameters': 4},\n",
       " '52126537': {'Introduction': 2,\n",
       "  'Related Work': 38,\n",
       "  'Methodology': 0,\n",
       "  'Input saliency computation': 1,\n",
       "  'Input transformation and selection': 1,\n",
       "  'Rule induction': 2,\n",
       "  'Data': 0,\n",
       "  'Model to be explained': 1,\n",
       "  'Metrics': 0,\n",
       "  'Hyperparameter optimization': 0,\n",
       "  'Rules as explanations': 1,\n",
       "  'Consistency of the induced rule-sets': 0,\n",
       "  'Limitations': 0,\n",
       "  'Conclusions and Future work': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Appendix': 0},\n",
       " '174800863': {'Introduction': 15,\n",
       "  'Problem Definition': 1,\n",
       "  'Model': 12,\n",
       "  'Datasets and Settings': 4,\n",
       "  'Results': 4,\n",
       "  'Error Analysis': 0,\n",
       "  'Related Work': 14,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '14036650': {'Introduction': 6,\n",
       "  'Background': 1,\n",
       "  'Symmetry between conjuncts': 3,\n",
       "  'Replaceability': 1,\n",
       "  'Coordination in the PTB': 7,\n",
       "  'Neural Networks and Notation': 2,\n",
       "  'Task Definition and Architecture': 0,\n",
       "  'Candidate Conjunctions Scoring': 0,\n",
       "  'The Symmetry Component': 2,\n",
       "  'The Replacement Component': 0,\n",
       "  'Parser based Features': 0,\n",
       "  'Final Scoring and Training': 0,\n",
       "  'Experiments': 4,\n",
       "  'Evaluation on PTB': 0,\n",
       "  'Evaluation on Genia': 3,\n",
       "  'Technical Details': 0,\n",
       "  'Analysis': 0,\n",
       "  'Related Work': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '7454072': {'Introduction': 17, 'Related Work': 6, 'Our Approach': 0},\n",
       " '3708647': {'Introduction': 3,\n",
       "  'Proposed Model': 0,\n",
       "  'Overview': 0,\n",
       "  'Encoder and Decoder': 1,\n",
       "  'Word Generation by Querying Word Embedding': 1,\n",
       "  'Selection of Candidate Key-value Pairs': 0,\n",
       "  'Training': 0,\n",
       "  'Experiments': 1,\n",
       "  'Text Simplification': 17,\n",
       "  'Large Scale Text Summarization': 13,\n",
       "  'Reducing Parameters': 0,\n",
       "  'Speeding up Convergence': 0,\n",
       "  'Case Study': 0,\n",
       "  'Related Work': 31,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '52307411': {'Introduction': 10,\n",
       "  'Linguistic Categories of SE Types': 6,\n",
       "  'Situation Entity (SE) Type Classification': 8,\n",
       "  'Paragraph-level Sequence Labeling': 12,\n",
       "  'The Hierarchical Recurrent Neural Network for SE Type Classification': 1,\n",
       "  'Fine-tune Situation Entity Predictions with a CRF Layer': 4,\n",
       "  'Parameter Settings and Model Training': 3,\n",
       "  'Dataset and Preprocessing': 6,\n",
       "  'Systems for Comparisons': 3,\n",
       "  'Experimental Results': 2,\n",
       "  '10-Fold Cross-Validation': 1,\n",
       "  'Impact of Genre': 1,\n",
       "  'Impact of Training Data Size': 0,\n",
       "  'Impact of Paragraph Length': 0,\n",
       "  'Impact of Discourse Connective Phrases': 1,\n",
       "  'Confusion Matrix': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '189858502': {'Introduction': 1,\n",
       "  'Comparative Argument Mining': 1,\n",
       "  'Related Work': 12,\n",
       "  'Building a Dataset of Comparative Sentences from a Web-Scale Corpus': 1,\n",
       "  'Supervised Categorization of Comparative Sentences': 1,\n",
       "  'Preprocessing': 0,\n",
       "  'Classification Models': 1,\n",
       "  'Sentence Representations': 9,\n",
       "  'Performance Metrics': 0,\n",
       "  'Trivial Baselines': 0,\n",
       "  'Impact of Classification Models': 1,\n",
       "  'Impact of Feature Representations': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '44147237': {'Introduction': 6,\n",
       "  'Related Work': 22,\n",
       "  'Hubness and authority scores from HITS': 1,\n",
       "  'Vocabulary selection using HITS': 1,\n",
       "  'Word graph construction': 0,\n",
       "  'Experimental setting': 10,\n",
       "  'Results': 1,\n",
       "  'Analysis': 0,\n",
       "  'Result': 0,\n",
       "  'Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '1696837': {'Introduction': 5,\n",
       "  'Bayesian Optimization': 1,\n",
       "  'Prior Model': 1,\n",
       "  'Adding Constraints': 0,\n",
       "  'Acquisition Function': 3,\n",
       "  'Decoupling Constraints': 1,\n",
       "  'Speed Tuning Experiments': 2,\n",
       "  'Decoder Parameters': 0,\n",
       "  'Measuring Decoding Speed': 1,\n",
       "  'BO Details and Baselines': 2,\n",
       "  'Results': 0,\n",
       "  'BO Time Analysis': 1,\n",
       "  'Reoptimizing Feature Weights': 0,\n",
       "  'Related Work': 4,\n",
       "  'Conclusion': 8},\n",
       " '6331': {'Introduction': 0,\n",
       "  'The Standard Definition of Derivation': 0,\n",
       "  'Motivation for an Extended Definition of Derivation': 0,\n",
       "  'Adding Adjoining Constraints': 13},\n",
       " '9884935': {'Introduction': 26,\n",
       "  'Our approach': 0,\n",
       "  'Related work': 29,\n",
       "  'Experiments': 1,\n",
       "  'Task and evaluation protocol': 19,\n",
       "  'Main results': 0,\n",
       "  'Conclusion and future work': 2,\n",
       "  'Acknowledgments': 0},\n",
       " '12009057': {'Introduction': 1,\n",
       "  'The limitations of direct transfer': 0,\n",
       "  'Translating English into Chinese serial verb compounds': 1,\n",
       "  'A multi-domain approach': 5,\n",
       "  'UNICON: An implementation': 2,\n",
       "  'Methodology and experimental results': 0,\n",
       "  'Discussion': 0},\n",
       " '174798001': {'Acknowledgements': 0},\n",
       " '3966892': {'Introduction': 10,\n",
       "  'Chinese Literature Text Corpus': 1,\n",
       "  'Basic BRCNN': 1,\n",
       "  'Structure Regularized BRCNN': 3,\n",
       "  'Various Structure Regularization Methods': 0,\n",
       "  'Experiments': 0,\n",
       "  'Experiment settings': 2,\n",
       "  'Experimental Results': 1,\n",
       "  'Analysis: Effect of SR': 0,\n",
       "  'Analysis: Effect of Different Regularization Methods': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '152282449': {'Supplemental Material': 0,\n",
       "  'Advice Sentence Generation': 0,\n",
       "  'Pre-trained Model Details': 1,\n",
       "  'End-to-end Advice Model Details': 3,\n",
       "  'Model Advice Generation Details': 8},\n",
       " '53298765': {'Introduction': 11,\n",
       "  'Dialogue Consistency and Natural Language Inference': 0,\n",
       "  'Dialogue NLI Dataset': 0,\n",
       "  'Triples Annotation': 0,\n",
       "  'Dataset Properties': 7,\n",
       "  'Consistent Dialogue Agents via Natural Language Inference': 0,\n",
       "  'Experiment 1: NLI': 9,\n",
       "  'Experiment 2: Consistency in Dialogue': 2,\n",
       "  'Experiment 3: Human Evaluation': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Schema': 0,\n",
       "  'Relation Swaps': 0,\n",
       "  'Entity Swaps': 0},\n",
       " '174802767': {'Introduction': 6,\n",
       "  'Related Work': 6,\n",
       "  'The Symptom Extraction Task': 0,\n",
       "  'Corpus Description': 0,\n",
       "  'Evaluation Metrics': 0,\n",
       "  'Models': 2,\n",
       "  'Span-Attribute Tagging (SA-T) Model': 10,\n",
       "  'Sequence-to-sequence (Seq2Seq) Model': 7,\n",
       "  'Encoder Pre-training': 6,\n",
       "  'Empirical Evaluations': 2,\n",
       "  'Hyperparameters': 1,\n",
       "  'Different Encoders and Pre-training': 0,\n",
       "  'Manual Transcript Evaluation': 0,\n",
       "  'ASR vs. Manual Transcript Evaluation': 1,\n",
       "  'Symptom Names vs. Body Systems': 0,\n",
       "  'Analysis': 0,\n",
       "  'Human Performance': 0,\n",
       "  'Attention Weights': 0,\n",
       "  'Error Analysis': 0,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '44095800': {'Introduction': 24,\n",
       "  'Methodology': 11,\n",
       "  'Performance': 0,\n",
       "  'Performance on OOV entities': 0,\n",
       "  'Impact of OOV rate to model performance': 0,\n",
       "  'Conclusion and future work': 0},\n",
       " '13745020': {'Introduction': 17,\n",
       "  'Sequence to Sequence with Attention': 2,\n",
       "  'Encoder': 2,\n",
       "  'Decoder': 1,\n",
       "  'Many Tasks One Sequence to Sequence': 11,\n",
       "  'Scheduled Multi-Task Learning': 12,\n",
       "  'Experimental Setup': 1,\n",
       "  'Data': 5,\n",
       "  'Training Details': 5,\n",
       "  'Results': 0,\n",
       "  'Auxiliary Tasks': 8,\n",
       "  'Translation Task': 1,\n",
       "  'Scheduler Tuning': 1,\n",
       "  'Architecture Comparison': 2,\n",
       "  'Discussion': 6,\n",
       "  'Conclusions and Future Work': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '2324333': {'Introduction': 0,\n",
       "  'Experimental Methodology': 0,\n",
       "  'System Descriptions': 3,\n",
       "  'Discussion': 0,\n",
       "  'Features Matter Most': 0,\n",
       "  '50/25/25 Rule': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '2140766': {'Acknowledgments': 0},\n",
       " '1137329': {'Conclusion': 1, 'Acknowledgments': 0},\n",
       " '67855269': {},\n",
       " '17866076': {'Introduction': 24,\n",
       "  'A Corpus of Everyday Events': 5,\n",
       "  'Learning Contingency Relation between Narrative Events': 0,\n",
       "  'Event Representation': 2,\n",
       "  'Causal Potential Method': 2,\n",
       "  'Baseline Methods': 4,\n",
       "  'Evaluation Experiments': 1,\n",
       "  'Comparison to Rel-gram Tuple Collections': 1,\n",
       "  'Automatic Two-Choice Test': 1,\n",
       "  'Topic-Indicative Contingent Event Pairs': 0,\n",
       "  'Discussion and Conclusions': 0},\n",
       " '4894843': {'Introduction': 17,\n",
       "  'Related Work': 0,\n",
       "  'Criteria': 1,\n",
       "  'Resource for Concept Choice:       Vietnamese Computational Lexicon': 1,\n",
       "  'Choice of Concepts': 0,\n",
       "  'Annotation of ViSim-400': 0,\n",
       "  'Agreement in ViSim-400': 5,\n",
       "  'Verification of Datasets': 0,\n",
       "  'Verification of ViSim-400': 5,\n",
       "  'Verification of ViCon': 1,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '7178598': {},\n",
       " '67788603': {'Introduction': 9,\n",
       "  'Overview of (sci)spaCy': 0,\n",
       "  'POS Tagging and Dependency Parsing': 3,\n",
       "  'Datasets': 11,\n",
       "  'Experiments': 7,\n",
       "  'Named Entity Recognition': 1},\n",
       " '8932111': {'Introduction': 10,\n",
       "  'Related Work': 21,\n",
       "  'Learning to Detect Context Dependent Messages': 0,\n",
       "  'Learning Weak Supervision Using Responses': 0,\n",
       "  'Model Learning': 3,\n",
       "  'Experiment Setup': 0,\n",
       "  'Parameter Tuning': 0,\n",
       "  'Quantitative Evaluation': 0,\n",
       "  'Qualitative Evaluation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0},\n",
       " '1931472': {'Introduction': 1,\n",
       "  'Singular Value Decomposition': 4,\n",
       "  'Approach': 1,\n",
       "  'Term Weighting': 0,\n",
       "  'Scoring functions': 1,\n",
       "  'Recall on training and test data': 0,\n",
       "  'Error analysis': 0,\n",
       "  'Related work': 3,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '3501731': {'Introduction': 18,\n",
       "  'Log-linear models': 0,\n",
       "  'SMT Features': 2,\n",
       "  'Tuning via Pairwise Ranking': 6,\n",
       "  'Listwise Learning Framework': 3,\n",
       "  'Training Objectives': 1,\n",
       "  'Training with Instance Aggregating': 2,\n",
       "  'Top-Rank Enhanced Losses': 0,\n",
       "  'Data and Preparation': 4,\n",
       "  'Tuning Settings': 5,\n",
       "  'Experiments of Listwise Learning Framework': 0,\n",
       "  'Effect of Top-rank Enhanced Losses': 0,\n",
       "  'Impact of the Size of Candidate Lists': 0,\n",
       "  'Performance on Basic Feature Set': 0,\n",
       "  'Related Work': 14,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '3626819': {'Introduction': 5,\n",
       "  'Related work': 20,\n",
       "  'ELMo: Embeddings from Language Models': 1,\n",
       "  'Bidirectional language models': 4,\n",
       "  'ELMo': 3,\n",
       "  'Using biLMs for supervised NLP tasks': 1,\n",
       "  'Pre-trained bidirectional language model architecture': 6,\n",
       "  'Evaluation': 23,\n",
       "  'Analysis': 0,\n",
       "  'Alternate layer weighting schemes': 2,\n",
       "  'Where to include ELMo?': 0,\n",
       "  \"What information is captured by the biLM's representations?\": 13,\n",
       "  'Sample efficiency': 0,\n",
       "  'Visualization of learned weights': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Supplemental Material to accompany Deep contextualized word representations': 0,\n",
       "  'Fine tuning biLM': 0,\n",
       "  'Importance of γ\\\\gamma  in Eqn. (1)': 0,\n",
       "  'Textual Entailment': 4,\n",
       "  'Question Answering': 3,\n",
       "  'Semantic Role Labeling': 8,\n",
       "  'Coreference resolution': 1,\n",
       "  'Named Entity Recognition': 2,\n",
       "  'Sentiment classification': 3},\n",
       " '49211529': {'Introduction': 13,\n",
       "  'Related Work': 24,\n",
       "  'Data': 0,\n",
       "  'Models': 5,\n",
       "  'Features': 4,\n",
       "  'Results': 1,\n",
       "  'Analysis': 3,\n",
       "  'Conclusion': 0},\n",
       " '6981674': {'Introduction': 20,\n",
       "  'Related Work': 2,\n",
       "  'Data': 3,\n",
       "  'Methods': 0,\n",
       "  'Evaluation': 0,\n",
       "  'Results': 12,\n",
       "  'Conclusions and Future Work': 1,\n",
       "  'Acknowledgements': 0},\n",
       " '174798391': {'Introduction': 30,\n",
       "  'Step 1: Response Selection Pretraining': 10,\n",
       "  'Step 2: Target Domain Fine-Tuning': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Reddit Pretraining': 8,\n",
       "  'Target-Domain Fine-Tuning': 0,\n",
       "  'Related Work': 31,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '11899969': {'Introduction': 7,\n",
       "  'Background': 0,\n",
       "  'Canonical Correlation Analysis (CCA)': 1,\n",
       "  'CRFs for Named Entity Recognition': 3,\n",
       "  'Method': 0,\n",
       "  'Obtaining Candidate Phrases': 0,\n",
       "  'Classification of Candidate Phrases': 3,\n",
       "  'Related Work': 4,\n",
       "  'Experiments': 0,\n",
       "  'Data': 2,\n",
       "  ' Results using a dictionary-based tagger': 0,\n",
       "  'Results using a CRF tagger': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '19227045': {'Introduction': 13,\n",
       "  'Related Work': 31,\n",
       "  'Model': 4,\n",
       "  'Character-Based Model': 5,\n",
       "  'Word-Based Model': 3,\n",
       "  'Lattice Model': 0,\n",
       "  'Decoding and Training': 0,\n",
       "  'Experiments': 0,\n",
       "  'Experimental Settings': 11,\n",
       "  'Development Experiments': 0,\n",
       "  'Final Results': 9,\n",
       "  'Discussion': 2,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '67877148': {'Introduction': 21,\n",
       "  'Related Work': 0,\n",
       "  'Word-Based CWS': 3,\n",
       "  'Cross-Domain CWS': 4,\n",
       "  'CWS Using Embeddings': 3,\n",
       "  'Word-Embedding-Based CWS': 0,\n",
       "  'CWS-Oriented Word Embedding Model': 9,\n",
       "  'Word-Embedding-Based Segmentater': 1,\n",
       "  'Experiments': 0,\n",
       "  'Experimental Setup': 9,\n",
       "  'Baseline Segmenter': 8,\n",
       "  'Results': 0,\n",
       "  'Comparison with State-of-the-Art Models': 6,\n",
       "  'Run Time for Decoding': 1,\n",
       "  'Analysis and Discussion': 0,\n",
       "  'Ablation Experiments': 0,\n",
       "  'Improvements in Noun Entity Segmentation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '12700615': {'Introduction': 8,\n",
       "  'Attention Mechanism in NMT': 4,\n",
       "  'Flexible Attention': 1,\n",
       "  'Reducing Window Size of Vision Span': 0,\n",
       "  'Fine-tuning for Better Performance': 0,\n",
       "  'Related Work': 2,\n",
       "  'Experiments': 2,\n",
       "  'Experimental Settings': 7,\n",
       "  'Evaluations of Flexible Attention': 1,\n",
       "  'Trade-off between Window Size and Accuracy': 0,\n",
       "  'Effects on Character-level Attention': 0,\n",
       "  'Impact on Real Decoding Speed': 3,\n",
       "  'Qualitative Analysis of Flexible Attention': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgement': 0,\n",
       "  'Supplemental Material': 0},\n",
       " '8024533': {'Introduction': 3,\n",
       "  'Related work': 2,\n",
       "  'Model': 2,\n",
       "  'Preprocessing': 0,\n",
       "  'Hyperparameters': 2,\n",
       "  'Aspect Category Detection': 2,\n",
       "  'Sentiment Polarity': 2,\n",
       "  'Evaluation': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '122829': {'Introduction': 26,\n",
       "  'Motivation': 0,\n",
       "  'Model Definition': 0,\n",
       "  'Network Architecture': 1,\n",
       "  'Training the Model': 2,\n",
       "  'Experiments': 0,\n",
       "  'Results': 0,\n",
       "  'Related Work': 15,\n",
       "  'Conclusion': 0},\n",
       " '9787203': {'Introduction': 9,\n",
       "  'Description of the problem': 5,\n",
       "  'The semantic analysis method': 0,\n",
       "  'Complexity of a recursive algorithm': 0,\n",
       "  'The chart algorithm': 0,\n",
       "  'Complexity of the chart algorithm': 0,\n",
       "  'Discussion': 0},\n",
       " '13751352': {'Introduction': 12,\n",
       "  'Problem Statement': 0,\n",
       "  'Model': 1,\n",
       "  'Learning': 1,\n",
       "  'Experimental Setup': 6,\n",
       "  'Automatic Evaluation': 4,\n",
       "  'Human Evaluation': 0,\n",
       "  'Qualitative Analysis': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0,\n",
       "  'Training Details': 0,\n",
       "  'Amazon Mechanical Turk': 0},\n",
       " '65081': {'Acknowledgments': 0},\n",
       " '90868': {'Conclusion': 0, 'Resources': 0, 'Acknowledgments': 0},\n",
       " '8031974': {'Introduction': 15,\n",
       "  'Hierarchical Embeddings': 0,\n",
       "  'Learning Hierarchical Embeddings': 2,\n",
       "  'Unsupervised Hypernymy Measure': 0,\n",
       "  'Experiments': 0,\n",
       "  'Experimental Settings': 2,\n",
       "  'Unsupervised Hypernymy Detection and Directionality': 10,\n",
       "  'Supervised Hypernymy Detection': 1,\n",
       "  'Graded Lexical Entailment': 16,\n",
       "  'Generalizing Hypernymy': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '44130548': {'Introduction': 21,\n",
       "  'System Overview': 0,\n",
       "  'Embedding': 9,\n",
       "  'Hypernym Learning': 4,\n",
       "  'Experiment': 0,\n",
       "  'Setting': 1,\n",
       "  'Result and analysis': 0,\n",
       "  'Conclusion': 0},\n",
       " '13699041': {'Introduction': 4,\n",
       "  'Existing Datasets': 10,\n",
       "  'ParallelQA': 2,\n",
       "  'Proof of Concept': 1,\n",
       "  'Analysis of Existing Models': 3,\n",
       "  'Discussion & Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '52100282': {'Introduction': 16,\n",
       "  'Related work': 0,\n",
       "  'NMT Architectures': 2,\n",
       "  'Contrastive Evaluation of Machine Translation': 0,\n",
       "  'Subject-verb Agreement': 4,\n",
       "  'Experimental Settings': 8,\n",
       "  'Overall Results': 1,\n",
       "  'WSD': 0,\n",
       "  'Experimental settings': 3,\n",
       "  'Hybrid Encoder-Decoder Model': 0,\n",
       "  'Post-publication Experiments': 0,\n",
       "  'Pre-trained Fairseq CNN Model': 1,\n",
       "  'Reducing Model Differences': 0,\n",
       "  'Conclusion': 1,\n",
       "  'Acknowledgments': 0},\n",
       " '5610313': {'Introduction': 5,\n",
       "  'Evaluating By Spelling Variants': 1,\n",
       "  'Gathering Spelling Variants': 2,\n",
       "  'UrbanDictionary': 0,\n",
       "  'Experiments': 0,\n",
       "  'Filtering by a Formal Vocabulary List': 0,\n",
       "  'Results on GloVe': 1,\n",
       "  'Biases and Drawbacks': 1,\n",
       "  'Conclusions': 2},\n",
       " '10159938': {'Introduction': 2,\n",
       "  'Proposed Model': 2,\n",
       "  'Experiments': 2,\n",
       "  'Results': 1,\n",
       "  'Conclusions and Future work': 1,\n",
       "  'Resources': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '10494183': {'Introduction': 0,\n",
       "  'Linguistic Steganography': 0,\n",
       "  'Related Work': 9,\n",
       "  'Our Proposal: Steganographic LSTM': 4,\n",
       "  'Steganographic LSTM Model': 0,\n",
       "  'LSTM Modification': 1,\n",
       "  'Evaluation Metrics': 3,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 3,\n",
       "  'Implementation Details': 4,\n",
       "  'Results and Discussion': 0,\n",
       "  'Comparison with Other Stegosystems': 5,\n",
       "  'Conclusion and Future Work': 4},\n",
       " '16386838': {'Introduction': 8,\n",
       "  'Neural Sequence Labeling': 3,\n",
       "  'Language Modeling Objective': 3,\n",
       "  'Evaluation Setup': 4,\n",
       "  'Error Detection': 1,\n",
       "  'NER and Chunking': 3,\n",
       "  'POS tagging': 2,\n",
       "  'Related Work': 0,\n",
       "  'Conclusion': 0},\n",
       " '3536005': {'Introduction': 7,\n",
       "  'Overview': 0,\n",
       "  'Phase 1: Candidate Selection (NNSelect)': 0,\n",
       "  'Phase 2: Reranking Candidates (NNRank)': 0,\n",
       "  'Experiments': 0,\n",
       "  'Related Work': 28,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '19809681': {'Introduction': 13,\n",
       "  'Bootstrapping a First-Person Sentiment Corpus': 9,\n",
       "  'Experimental Setup': 4,\n",
       "  'Results and Analysis': 7,\n",
       "  'Related Work': 6,\n",
       "  'Conclusion': 1},\n",
       " '1618800': {'Acknowledgments': 0},\n",
       " '44108645': {'Motivation': 4,\n",
       "  'Related Work': 10,\n",
       "  'Neural Machine Translation': 3,\n",
       "  'Syntactic Attention Model': 0,\n",
       "  'Head Word Selection': 8,\n",
       "  'Incorporating Syntactic Context': 0,\n",
       "  'Hard Attention over Tree Structures': 4,\n",
       "  'Experiments': 0,\n",
       "  'Data': 3,\n",
       "  'Baselines': 4,\n",
       "  'Hyper-parameters and Training': 2,\n",
       "  'Translation Results': 1,\n",
       "  'Gate Activation Visualization': 0,\n",
       "  'Grammar Induction': 4,\n",
       "  'Extracting a Tree': 2,\n",
       "  'Grammatical Analysis': 7,\n",
       "  'Dependency Accuracies & Discrepancies': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0,\n",
       "  'Attention Visualization': 1},\n",
       " '4829361': {'Acknowledgments': 0},\n",
       " '52194540': {'Introduction': 1},\n",
       " '1572802': {},\n",
       " '19718228': {'Introduction': 45,\n",
       "  'Vector Space Specialisation': 9,\n",
       "  'Clustering Algorithm': 9,\n",
       "  'Results and Discussion': 0,\n",
       "  'Further Discussion and Future Work': 8,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '7250091': {'Introduction': 5,\n",
       "  'Sentence Modelling with LSTM': 4,\n",
       "  'Coupled-LSTMs for Strong Sentence Interaction': 6,\n",
       "  'Loosely Coupled-LSTMs (LC-LSTMs)': 2,\n",
       "  'Tightly Coupled-LSTMs (TC-LSTMs)': 1,\n",
       "  'Analysis of Two Proposed Models': 0,\n",
       "  'End-to-End Architecture for Sentence Matching': 0,\n",
       "  'Embedding Layer': 0,\n",
       "  'Stacked Coupled-LSTMs Layers': 0,\n",
       "  'Pooling Layer': 1,\n",
       "  'Fully-Connected Layer': 0,\n",
       "  'Output Layer': 0,\n",
       "  'Training': 0,\n",
       "  'Experiment': 0,\n",
       "  'Hyperparameters and Training': 1,\n",
       "  'Competitor Methods': 3,\n",
       "  'Experiment-I: Recognizing Textual Entailment': 1,\n",
       "  'Experiment-II: Matching Question and Answer': 0,\n",
       "  'Related Work': 6,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '90241831': {'Introduction': 12,\n",
       "  'Embeddings for semantics and emotion': 3,\n",
       "  'Hierarchical RNN for context': 2,\n",
       "  'BERT': 1,\n",
       "  'Importance Weighting': 1,\n",
       "  'Models': 0,\n",
       "  'SA-LSTM (SL)': 2,\n",
       "  'SA-LSTM-DeepMoji (SLD)': 0,\n",
       "  'HRLCE': 0,\n",
       "  'Data preprocessing': 1,\n",
       "  'Environment and hyper-parameters': 1,\n",
       "  'Results and analysis': 0,\n",
       "  'Conclusions': 0},\n",
       " '102351414': {'Introduction': 1,\n",
       "  'Related work': 4,\n",
       "  'Dataset': 1,\n",
       "  'Representing online communities': 0,\n",
       "  'Generating author profiles': 4,\n",
       "  'Classification methods': 1,\n",
       "  'Experimental setup': 4,\n",
       "  'Results and analysis': 1,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '184487983': {'Introduction': 20,\n",
       "  'The Summarization Task': 4,\n",
       "  'Generation with Content Guidance': 4,\n",
       "  'Document-level Decoder': 1,\n",
       "  'Sentence-level Decoder': 2,\n",
       "  'Hierarchical Convolutional Decoder': 1,\n",
       "  'Topic Guidance': 1,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 12},\n",
       " '13744121': {'Acknowledgments': 0},\n",
       " '2783746': {'Acknowledgments': 0},\n",
       " '195847865': {'Introduction': 7,\n",
       "  'Related Work': 18,\n",
       "  'Dataset Construction': 9,\n",
       "  'Model': 0,\n",
       "  'Feature-based Classifier': 9,\n",
       "  'Deep Learning Model with Dialogue Features': 2,\n",
       "  'Evaluation and Analysis': 0,\n",
       "  'Feature-based Classifier and Dialogue Feature Selection': 2,\n",
       "  'Deep Learning Models': 0,\n",
       "  'Conclusion and Future Work': 0},\n",
       " '1065088': {'Introduction': 5,\n",
       "  'Related Work': 7,\n",
       "  'Tag induction': 1,\n",
       "  'Induction based on word type only': 3,\n",
       "  'Induction based on word type and context': 0,\n",
       "  'Generalized context vectors': 2,\n",
       "  'Results': 1,\n",
       "  'Future Work': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '146121245': {},\n",
       " '2937095': {'Introduction': 11,\n",
       "  'Distributed Sentence Representations': 0,\n",
       "  'Existing Models Trained on Text': 4,\n",
       "  'Models Trained on Structured Resources': 4,\n",
       "  'Novel Text-Based Models': 6,\n",
       "  'Training and Model Selection': 0,\n",
       "  'Evaluating Sentence Representations': 6,\n",
       "  'Supervised Evaluations': 6,\n",
       "  'Unsupervised Evaluations': 2,\n",
       "  'Results': 3,\n",
       "  'Discussion': 4,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '102351034': {'Acknowledgments': 0},\n",
       " '14878319': {'Introduction': 8},\n",
       " '182953152': {'Introduction': 9,\n",
       "  'Annotated Enron Subject Line Corpus': 2,\n",
       "  'Data Preprocessing': 0,\n",
       "  'Subject Annotation': 0,\n",
       "  'Our Model': 5,\n",
       "  'Multi-sentence Extractor': 3,\n",
       "  'Multi-sentence Abstractor': 2,\n",
       "  'Email Subject Quality Estimator': 0,\n",
       "  'Multi-Stage Training': 1,\n",
       "  'Evaluation': 2,\n",
       "  'Baselines': 5,\n",
       "  'Implementation Details': 3,\n",
       "  'Automatic Metric Evaluation': 0,\n",
       "  'Human Evaluation': 0,\n",
       "  'Metric Correlation Analysis': 0,\n",
       "  'Case Study': 0,\n",
       "  'Related Work': 28,\n",
       "  'Conclusions and Future Work': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Amazon Mechanical Turk': 0},\n",
       " '59553499': {'Acknowledgments': 0},\n",
       " '12938495': {'Introduction': 15,\n",
       "  'Task Definition': 2,\n",
       "  'Embedding Questions and Answers': 1,\n",
       "  'Representing Candidate Answers': 0,\n",
       "  'Training and Loss Function': 2,\n",
       "  'Multitask Training of Embeddings': 0,\n",
       "  'Inference': 0,\n",
       "  'Experiments': 8,\n",
       "  'Conclusion': 0},\n",
       " '195767519': {'Acknowledgments': 0, 'Examples': 1},\n",
       " '102352626': {'Introduction': 6,\n",
       "  'The BoSsNet Architecture': 0,\n",
       "  'Bag-of-Sequences Memory': 0,\n",
       "  'The BoSsNet Encoder': 4,\n",
       "  'The BoSsNet Decoder': 4,\n",
       "  'Loss': 0,\n",
       "  'Experimental Setup': 3,\n",
       "  'Knowledge Adaptability (KA) Test Sets': 0,\n",
       "  'Baselines': 6,\n",
       "  'Evaluation Metrics': 3,\n",
       "  'Human Evaluation': 0,\n",
       "  'Training': 1,\n",
       "  'Experimental Results': 0},\n",
       " '23678406': {'Introduction': 9,\n",
       "  'Background': 6,\n",
       "  'Impact of Randomness in the Evaluation of Neural Networks': 2,\n",
       "  'Experimental Setup': 1,\n",
       "  'Model': 3,\n",
       "  'Evaluated Parameters': 9,\n",
       "  'Robust Model Evaluation': 1,\n",
       "  'Results': 1,\n",
       "  'Classifier': 0,\n",
       "  'Optimizer': 0,\n",
       "  'Word Embeddings': 0,\n",
       "  'Character Representation': 0,\n",
       "  'Gradient Clipping and Normalization': 2,\n",
       "  'Dropout': 2,\n",
       "  'Further Evaluated Parameters': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0},\n",
       " '91183895': {'Introduction': 3,\n",
       "  'Task and Architecture': 0,\n",
       "  'Single- and Multi-Sense Word Embeddings': 1,\n",
       "  'Reverse Dictionaries': 4,\n",
       "  'Target Vectors': 0,\n",
       "  'Input Vectors': 0,\n",
       "  'Multi-Sense Vector Selection': 0,\n",
       "  'Experimental Evaluation': 0,\n",
       "  'Data': 1,\n",
       "  'Embeddings': 1,\n",
       "  'Baselines': 7,\n",
       "  'Hyperparameters': 4,\n",
       "  'Metrics': 1,\n",
       "  'Results': 2,\n",
       "  'Study of Senses and Attention': 0,\n",
       "  'Related Work': 15,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgment': 0},\n",
       " '167217453': {'Introduction': 8,\n",
       "  'Topic Entity Graph': 0,\n",
       "  'Graph Matching Model': 0,\n",
       "  'Experiments': 3,\n",
       "  'Conclusions': 1,\n",
       "  'Matching Function f m f_m': 0},\n",
       " '159041465': {'Acknowledgments': 0},\n",
       " '102353837': {'Introduction': 15,\n",
       "  'Related Works': 48,\n",
       "  'BERT and Review-based Tasks': 0,\n",
       "  'BERT': 8,\n",
       "  'Review Reading Comprehension (RRC)': 1,\n",
       "  'Aspect Extraction': 2,\n",
       "  'Aspect Sentiment Classification': 0,\n",
       "  'Post-training': 0,\n",
       "  'Experiments': 0,\n",
       "  'End Task Datasets': 3,\n",
       "  'Post-training datasets': 3,\n",
       "  'Hyper-parameters': 0,\n",
       "  'Compared Methods': 3,\n",
       "  'Evaluation Metrics and Model Selection': 2,\n",
       "  'Result Analysis': 1,\n",
       "  'Conclusions': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '51979567': {'Introduction': 4,\n",
       "  'Task and Model': 10,\n",
       "  'Visualization of Attention and Gating': 0,\n",
       "  'Attention': 7,\n",
       "  'LSTM Gating Signals': 0,\n",
       "  'Conclusion': 0},\n",
       " '184488333': {'Acknowledgements': 0},\n",
       " '52156878': {'Introduction': 14,\n",
       "  'Language models': 4,\n",
       "  'Dependent variable: Surprisal': 3,\n",
       "  'Experimental design': 4,\n",
       "  'Representation of filler–gap dependencies': 0,\n",
       "  'Flexibility of Wh-Licensing': 1,\n",
       "  'Robustness of Wh-Licensing to Intervening Material': 2,\n",
       "  'Multiple Gaps': 0,\n",
       "  'Syntactic islands': 8,\n",
       "  'Wh-Island Constraint': 0,\n",
       "  'Adjunct Island Constraint': 0,\n",
       "  'Complex NP and Subject Islands': 1,\n",
       "  'Conclusion': 2,\n",
       "  'Acknowledgements': 0},\n",
       " '4382470': {},\n",
       " '102354684': {},\n",
       " '3264519': {'Introduction': 0,\n",
       "  'The Basic Idea and Terminology': 0,\n",
       "  'Probabilistic Model': 0,\n",
       "  'The first model': 1,\n",
       "  'The second model': 2,\n",
       "  'Preliminary Experiments': 2,\n",
       "  'Acknowledgements': 1},\n",
       " '56657817': {'Introduction': 8,\n",
       "  'What linguistic information is captured in neural networks': 0,\n",
       "  'Methods': 8,\n",
       "  'Linguistic phenomena': 11,\n",
       "  'Neural network components': 1,\n",
       "  'Limitations': 4,\n",
       "  'Visualization': 36,\n",
       "  'Challenge sets': 7,\n",
       "  'Task': 12,\n",
       "  'Languages': 0,\n",
       "  'Scale': 3,\n",
       "  'Construction method': 10,\n",
       "  'Evaluation': 5,\n",
       "  'Adversarial examples': 2,\n",
       "  \"Adversary's knowledge\": 17,\n",
       "  'Attack specificity': 8,\n",
       "  'Linguistic unit': 3,\n",
       "  'Coherence & perturbation measurement': 9,\n",
       "  'Explaining predictions': 4,\n",
       "  'Other methods': 19,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgments': 0},\n",
       " '584': {'Introduction': 17,\n",
       "  'Preliminary Considerations': 7,\n",
       "  'Implementation': 0},\n",
       " '15412473': {'Introduction': 9,\n",
       "  'Related Work': 34,\n",
       "  'Problem Formulation': 0,\n",
       "  'Sequence-to-Sequence Model': 0,\n",
       "  'Sequence-to-Tree Model': 0,\n",
       "  'Attention Mechanism': 3,\n",
       "  'Model Training': 2,\n",
       "  'Inference': 0,\n",
       "  'Argument Identification': 2,\n",
       "  'Experiments': 0,\n",
       "  'Datasets': 1,\n",
       "  'Settings': 3,\n",
       "  'Results': 2,\n",
       "  'Error Analysis': 2,\n",
       "  'Conclusions': 0},\n",
       " '1356419': {'Motivation': 0,\n",
       "  'Focus on the Noun': 0,\n",
       "  'Focus on the Verb': 0,\n",
       "  'On Genre Detection': 0,\n",
       "  'Initial Observations': 0,\n",
       "  'Event Profile: WordNet and EVCA': 0},\n",
       " '53023292': {'Introduction': 14,\n",
       "  'Related work': 19,\n",
       "  'Notations': 0,\n",
       "  'Sparse variational dropout for RNNs': 11,\n",
       "  'Multiplicative weights for vocabulary sparsification': 1,\n",
       "  'Experiments': 1,\n",
       "  'Text Classification': 4,\n",
       "  'Language Modeling': 2,\n",
       "  'Acknowledgments': 0,\n",
       "  'Experimental setup': 1,\n",
       "  'A list of remaining words on IMDB': 0},\n",
       " '5649853': {'Introduction': 17,\n",
       "  'Related Work': 6,\n",
       "  'The SimVerb-3500 Data Set': 0,\n",
       "  'Design Motivation': 1,\n",
       "  'Choice of Verb Pairs and Coverage': 8,\n",
       "  'Word Pair Scoring': 0,\n",
       "  'Survey Structure': 0,\n",
       "  'Post-Processing': 1,\n",
       "  'Evaluating Subsets': 0,\n",
       "  'Conclusions': 3,\n",
       "  'Acknowledgments': 0,\n",
       "  'Unsupervised Text-Based Models': 19,\n",
       "  'Models Relying on External Resources': 4},\n",
       " '52333947': {'Introduction': 7,\n",
       "  'Related Work': 31,\n",
       "  'Dataset': 0,\n",
       "  'Curating a list of popular movies': 0,\n",
       "  'Collecting background knowledge': 0,\n",
       "  'Collecting conversation starters': 0,\n",
       "  'Collecting background knowledge aware conversations via crowdsourcing': 1,\n",
       "  'Verification of the collected chats': 2,\n",
       "  'Statistics': 0,\n",
       "  'Models': 0,\n",
       "  'Generation based models': 2,\n",
       "  'Generate-or-Copy models': 1,\n",
       "  'Span prediction models': 4,\n",
       "  'Experimental Setup': 0,\n",
       "  'Creating train/valid/test splits': 0,\n",
       "  'Creating training instances': 0,\n",
       "  'Merging resources into a single document': 1,\n",
       "  'Evaluation metrics': 1,\n",
       "  'Collecting multiple reference responses': 0,\n",
       "  'Results and Discussion': 0,\n",
       "  'Conclusion': 0,\n",
       "  'Acknowledgements': 0,\n",
       "  'Model details - GTTP': 1,\n",
       "  'Example from the multiple reference test set': 0,\n",
       "  'Hyper-parameters': 6,\n",
       "  'Sample responses produced by the models': 0,\n",
       "  'Data Collection Interfaces used on Amazon Mechanical Turk': 0},\n",
       " ...}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_with_sect_latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Пример без названий секции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'abstract': [],\n",
       "  'body_text': [{'text': 'Bang Liu INLINEFORM0 , Ting Zhang INLINEFORM1 , Di Niu INLINEFORM2 , Jinghong Lin INLINEFORM3 , Kunfeng Lai INLINEFORM4 , Yu Xu INLINEFORM5 INLINEFORM6 University of Alberta, Edmonton, AB, Canada INLINEFORM7 Mobile Internet Group, Tencent, Shenzhen, China',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [{'start': 9,\n",
       "      'end': 20,\n",
       "      'text': ' 1 ',\n",
       "      'latex': '^1',\n",
       "      'ref_id': None},\n",
       "     {'start': 34, 'end': 45, 'text': ' 1 ', 'latex': '^1', 'ref_id': None},\n",
       "     {'start': 55, 'end': 66, 'text': ' 1 ', 'latex': '^1', 'ref_id': None},\n",
       "     {'start': 82, 'end': 93, 'text': ' 2 ', 'latex': '^2', 'ref_id': None},\n",
       "     {'start': 108, 'end': 119, 'text': ' 2 ', 'latex': '^2', 'ref_id': None},\n",
       "     {'start': 128, 'end': 139, 'text': ' 2 ', 'latex': '^2', 'ref_id': None},\n",
       "     {'start': 140, 'end': 151, 'text': ' 1 ', 'latex': '^1', 'ref_id': None},\n",
       "     {'start': 196, 'end': 207, 'text': ' 2 ', 'latex': '^2', 'ref_id': None}],\n",
       "    'section': None},\n",
       "   {'text': 'Identifying the relationship between two text objects is a core research problem underlying many natural language processing tasks. A wide range of deep learning schemes have been proposed for text matching, mainly focusing on sentence matching, question answering or query document matching. We point out that existing approaches do not perform well at matching long documents, which is critical, for example, to AI-based news article understanding and event or story formation. The reason is that these methods either omit or fail to fully utilize complicated semantic structures in long documents. In this paper, we propose a graph approach to text matching, especially targeting long document matching, such as identifying whether two news articles report the same event in the real world, possibly with different narratives. We propose the Concept Interaction Graph to yield a graph representation for a document, with vertices representing different concepts, each being one or a group of coherent keywords in the document, and with edges representing the interactions between different concepts, connected by sentences in the document. Based on the graph representation of document pairs, we further propose a Siamese Encoded Graph Convolutional Network that learns vertex representations through a Siamese neural network and aggregates the vertex features though Graph Convolutional Networks to generate the matching result. Extensive evaluation of the proposed approach based on two labeled news article datasets created at Tencent for its intelligent news products show that the proposed graph approach to long document matching significantly outperforms a wide range of state-of-the-art methods.',\n",
       "    'cite_spans': [],\n",
       "    'ref_spans': [],\n",
       "    'eq_spans': [],\n",
       "    'section': None}],\n",
       "  'ref_entries': {},\n",
       "  'bib_entries': {'BIBREF3': {'ref_id': 'BIBREF3',\n",
       "    'title': 'The mathematics of statistical machine translation: Parameter estimation',\n",
       "    'authors': [{'first': 'F', 'middle': [], 'last': 'Peter', 'suffix': ''},\n",
       "     {'first': 'Vincent J Della', 'middle': [], 'last': 'Brown', 'suffix': ''},\n",
       "     {'first': 'Stephen A Della',\n",
       "      'middle': [],\n",
       "      'last': 'Pietra',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Robert L', 'middle': [], 'last': 'Pietra', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Mercer', 'suffix': ''}],\n",
       "    'year': 1993,\n",
       "    'venue': 'Computational linguistics',\n",
       "    'volume': '19',\n",
       "    'issn': '',\n",
       "    'pages': '263--311',\n",
       "    'other_ids': {},\n",
       "    'links': '13259913'},\n",
       "   'BIBREF4': {'ref_id': 'BIBREF4',\n",
       "    'title': 'Convolutional neural networks on graphs with fast localized spectral filtering',\n",
       "    'authors': [{'first': 'Michaël',\n",
       "      'middle': [],\n",
       "      'last': 'Defferrard',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Xavier', 'middle': [], 'last': 'Bresson', 'suffix': ''},\n",
       "     {'first': 'Pierre', 'middle': [], 'last': 'Vandergheynst', 'suffix': ''}],\n",
       "    'year': 2016,\n",
       "    'venue': 'Advances in Neural Information Processing Systems',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '3844--3852',\n",
       "    'other_ids': {},\n",
       "    'links': '3016223'},\n",
       "   'BIBREF5': {'ref_id': 'BIBREF5',\n",
       "    'title': '',\n",
       "    'authors': [{'first': 'Yixing', 'middle': [], 'last': 'Fan', 'suffix': ''},\n",
       "     {'first': 'Liang', 'middle': [], 'last': 'Pang', 'suffix': ''},\n",
       "     {'first': 'Jianpeng', 'middle': [], 'last': 'Hou', 'suffix': ''},\n",
       "     {'first': 'Jiafeng', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Yanyan', 'middle': [], 'last': 'Lan', 'suffix': ''},\n",
       "     {'first': 'Xueqi', 'middle': [], 'last': 'Cheng', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {'arXiv': ['arXiv:1707.07270']},\n",
       "    'links': None},\n",
       "   'BIBREF16': {'ref_id': 'BIBREF16',\n",
       "    'title': 'Learning to match using local and distributed representations of text for web search',\n",
       "    'authors': [{'first': 'Bhaskar',\n",
       "      'middle': [],\n",
       "      'last': 'Mitra',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Fernando', 'middle': [], 'last': 'Diaz', 'suffix': ''},\n",
       "     {'first': 'Nick', 'middle': [], 'last': 'Craswell', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1291--1299',\n",
       "    'other_ids': {},\n",
       "    'links': '8497672'},\n",
       "   'BIBREF18': {'ref_id': 'BIBREF18',\n",
       "    'title': 'The PageRank citation ranking: Bringing order to the web',\n",
       "    'authors': [{'first': 'Lawrence',\n",
       "      'middle': [],\n",
       "      'last': 'Page',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Sergey', 'middle': [], 'last': 'Brin', 'suffix': ''},\n",
       "     {'first': 'Rajeev', 'middle': [], 'last': 'Motwani', 'suffix': ''},\n",
       "     {'first': 'Terry', 'middle': [], 'last': 'Winograd', 'suffix': ''}],\n",
       "    'year': 1999,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '1508503'},\n",
       "   'BIBREF19': {'ref_id': 'BIBREF19',\n",
       "    'title': 'Text Matching as Image Recognition',\n",
       "    'authors': [{'first': 'Liang', 'middle': [], 'last': 'Pang', 'suffix': ''},\n",
       "     {'first': 'Yanyan', 'middle': [], 'last': 'Lan', 'suffix': ''},\n",
       "     {'first': 'Jiafeng', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Jun', 'middle': [], 'last': 'Xu', 'suffix': ''},\n",
       "     {'first': 'Shengxian', 'middle': [], 'last': 'Wan', 'suffix': ''},\n",
       "     {'first': 'Xueqi', 'middle': [], 'last': 'Cheng', 'suffix': ''}],\n",
       "    'year': 2016,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '2793--2799',\n",
       "    'other_ids': {},\n",
       "    'links': '3993933'},\n",
       "   'BIBREF20': {'ref_id': 'BIBREF20',\n",
       "    'title': 'Summarizing complex development artifacts by mining heterogeneous data',\n",
       "    'authors': [{'first': 'Luca',\n",
       "      'middle': [],\n",
       "      'last': 'Ponzanelli',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Andrea', 'middle': [], 'last': 'Mocci', 'suffix': ''},\n",
       "     {'first': 'Michele', 'middle': [], 'last': 'Lanza', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'Proceedings of the 12th Working Conference on Mining Software Repositories',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '401--405',\n",
       "    'other_ids': {},\n",
       "    'links': '14260167'},\n",
       "   'BIBREF22': {'ref_id': 'BIBREF22',\n",
       "    'title': 'Convolutional Neural Tensor Network Architecture for Community-Based Question Answering',\n",
       "    'authors': [{'first': 'Xipeng', 'middle': [], 'last': 'Qiu', 'suffix': ''},\n",
       "     {'first': 'Xuanjing', 'middle': [], 'last': 'Huang', 'suffix': ''}],\n",
       "    'year': 2015,\n",
       "    'venue': 'IJCAI',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '1305--1311',\n",
       "    'other_ids': {},\n",
       "    'links': '17562582'},\n",
       "   'BIBREF25': {'ref_id': 'BIBREF25',\n",
       "    'title': 'A graph analytical approach for topic detection',\n",
       "    'authors': [{'first': 'Hassan',\n",
       "      'middle': [],\n",
       "      'last': 'Sayyadi',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Louiqa', 'middle': [], 'last': 'Raschid', 'suffix': ''}],\n",
       "    'year': 2013,\n",
       "    'venue': 'ACM Transactions on Internet Technology (TOIT)',\n",
       "    'volume': '13',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {},\n",
       "    'links': '11186546'},\n",
       "   'BIBREF26': {'ref_id': 'BIBREF26',\n",
       "    'title': 'Clustering of web documents using a graph model',\n",
       "    'authors': [{'first': 'Adam',\n",
       "      'middle': [],\n",
       "      'last': 'Schenker',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Mark', 'middle': [], 'last': 'Last', 'suffix': ''},\n",
       "     {'first': 'Horst', 'middle': [], 'last': 'Bunke', 'suffix': ''},\n",
       "     {'first': 'Abraham', 'middle': [], 'last': 'Kandel', 'suffix': ''}],\n",
       "    'year': 2003,\n",
       "    'venue': 'SERIES IN MACHINE PERCEPTION AND ARTIFICIAL INTELLIGENCE',\n",
       "    'volume': '55',\n",
       "    'issn': '',\n",
       "    'pages': '3--18',\n",
       "    'other_ids': {},\n",
       "    'links': '15125219'},\n",
       "   'BIBREF29': {'ref_id': 'BIBREF29',\n",
       "    'title': 'A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations',\n",
       "    'authors': [{'first': 'Yanyan',\n",
       "      'middle': [],\n",
       "      'last': 'Shengxian Wan',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Jiafeng', 'middle': [], 'last': 'Lan', 'suffix': ''},\n",
       "     {'first': 'Jun', 'middle': [], 'last': 'Guo', 'suffix': ''},\n",
       "     {'first': 'Liang', 'middle': [], 'last': 'Xu', 'suffix': ''},\n",
       "     {'first': 'Xueqi', 'middle': [], 'last': 'Pang', 'suffix': ''},\n",
       "     {'first': '', 'middle': [], 'last': 'Cheng', 'suffix': ''}],\n",
       "    'year': 2016,\n",
       "    'venue': 'AAAI',\n",
       "    'volume': '16',\n",
       "    'issn': '',\n",
       "    'pages': '2835--2841',\n",
       "    'other_ids': {},\n",
       "    'links': '3259607'},\n",
       "   'BIBREF30': {'ref_id': 'BIBREF30',\n",
       "    'title': 'End-to-end neural ad-hoc ranking with kernel pooling',\n",
       "    'authors': [{'first': 'Chenyan',\n",
       "      'middle': [],\n",
       "      'last': 'Xiong',\n",
       "      'suffix': ''},\n",
       "     {'first': 'Zhuyun', 'middle': [], 'last': 'Dai', 'suffix': ''},\n",
       "     {'first': 'Jamie', 'middle': [], 'last': 'Callan', 'suffix': ''},\n",
       "     {'first': 'Zhiyuan', 'middle': [], 'last': 'Liu', 'suffix': ''},\n",
       "     {'first': 'Russell', 'middle': [], 'last': 'Power', 'suffix': ''}],\n",
       "    'year': 2017,\n",
       "    'venue': 'Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '55--64',\n",
       "    'other_ids': {},\n",
       "    'links': '5878197'},\n",
       "   'BIBREF31': {'ref_id': 'BIBREF31',\n",
       "    'title': 'Deep learning for answer sentence selection',\n",
       "    'authors': [{'first': 'Lei', 'middle': [], 'last': 'Yu', 'suffix': ''},\n",
       "     {'first': 'Karl', 'middle': ['Moritz'], 'last': 'Hermann', 'suffix': ''},\n",
       "     {'first': 'Phil', 'middle': [], 'last': 'Blunsom', 'suffix': ''},\n",
       "     {'first': 'Stephen', 'middle': [], 'last': 'Pulman', 'suffix': ''}],\n",
       "    'year': 2014,\n",
       "    'venue': '',\n",
       "    'volume': '',\n",
       "    'issn': '',\n",
       "    'pages': '',\n",
       "    'other_ids': {'arXiv': ['arXiv:1412.1632']},\n",
       "    'links': '12211448'}}}]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[article['latex_parse'] for article in all_articles if article['paper_id'] == '198922003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "RW_names = [\n",
    "    'related work','background','previous w'\n",
    "]\n",
    "rw_list = []\n",
    "for k,v in article_with_sect_latex.items():\n",
    "    if len(v)>0:\n",
    "        title.append(max(article_with_sect_latex[k],key=article_with_sect_latex[k].get).lower())\n",
    "    else\n",
    "    line = ' '.join(article_with_sect_latex[k].keys()).lower()\n",
    "    if mult_in(RW_names,line):\n",
    "        rw_list.append(mult_in(RW_names,line))\n",
    "    else:\n",
    "        rw_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "introduction                                       1503\n",
       "related work                                        803\n",
       "acknowledgments                                     181\n",
       "acknowledgements                                     83\n",
       "conclusion                                           64\n",
       "background                                           35\n",
       "related works                                        23\n",
       "conclusions                                          23\n",
       "sections                                             18\n",
       "previous work                                        18\n",
       "motivation                                           13\n",
       "datasets                                             13\n",
       "experiments                                          11\n",
       "experimental setup                                   11\n",
       "conclusion and future work                            9\n",
       "discussion                                            9\n",
       "acknowledgement                                       9\n",
       "methods                                               7\n",
       "model                                                 6\n",
       "related work and discussion                           6\n",
       "models                                                5\n",
       "data                                                  5\n",
       "acknowledgment                                        5\n",
       "features                                              4\n",
       "introduction and related work                         4\n",
       "overview                                              4\n",
       "setup                                                 3\n",
       "discussion and conclusions                            3\n",
       "introduction and background                           3\n",
       "natural language inference                            3\n",
       "                                                   ... \n",
       "learning what to share                                1\n",
       "paragraph-level sequence labeling                     1\n",
       "non-dl learning algorithms                            1\n",
       "paraphrase generation                                 1\n",
       "tokenization-based approaches                         1\n",
       "word-sense induction                                  1\n",
       "performance guarantees for independence systems       1\n",
       "document-level sentiment classification               1\n",
       "neural models for nested ner                          1\n",
       "consistent positive-unlabeled learning                1\n",
       "related tasks                                         1\n",
       "further analysis                                      1\n",
       "tagging                                               1\n",
       "fine-grained entity typing                            1\n",
       "machine learning algorithms                           1\n",
       "combinatory category grammar supertagging             1\n",
       "finite-state calculus                                 1\n",
       "standard setup                                        1\n",
       "unimorph                                              1\n",
       "generation of common-sense inference candidates       1\n",
       "distributional semantics                              1\n",
       "experimental setup and results                        1\n",
       "previous work in composition models                   1\n",
       "bridging nlp models and neurolinguistics              1\n",
       "the source data                                       1\n",
       "non-autoregressive sequence modeling                  1\n",
       "detailed description of reasonet++                    1\n",
       "curriculum learning for nmt                           1\n",
       "response selection task                               1\n",
       "applications and evaluation                           1\n",
       "Length: 742, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(title).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1784, 4039)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 for rw in rw_list if rw!=0] ),len(rw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(article_with_sect_latex['10164018'],key=article_with_sect_latex['10164018'].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction overview reader-aware salience estimation summary construction data description background data collection data properties dataset and metrics comparative methods experimental settings results on our dataset further investigation of our framework  case study conclusions\n"
     ]
    }
   ],
   "source": [
    "line = ' '.join(article_with_sect_latex['10164018'].keys()).lower()\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'background'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mult_in(RW_names,line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
